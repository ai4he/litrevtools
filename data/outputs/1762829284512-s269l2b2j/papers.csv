ID,Title,Authors,Year,Venue,Citations,URL,PDF URL,DOI,Abstract,Keywords,Included,Exclusion Reason,Extracted At
chainofthoughtprompt-2022,Chain of Thought Prompting Elicits Reasoning in Large Language Models,Jason Wei; Xuezhi Wang; Dale Schuurmans; Maarten Bosma; Ed H. Chi; F. Xia; Quoc Le; Denny Zhou,2022,Neural Information Processing Systems,13165,https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5,,,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",arxiv:2201.11903,Yes,,2025-11-11T02:48:05.506Z
codeaspolicieslangua-2022,Code as Policies: Language Model Programs for Embodied Control,Jacky Liang; Wenlong Huang; F. Xia; Peng Xu; Karol Hausman; Brian Ichter; Peter R. Florence; Andy Zeng,2022,IEEE International Conference on Robotics and Automation,1177,https://www.semanticscholar.org/paper/91deaf9d324c8feafc189da0da03e60a60287bca,https://arxiv.org/pdf/2209.07753,10.1109/ICRA48891.2023.10160591,"Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",arxiv:2209.07753,Yes,,2025-11-11T02:48:05.506Z
distillingmultistepr-2022,Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions,Kumar Shridhar; Alessandro Stolfo; Mrinmaya Sachan,2022,arXiv.org,47,https://www.semanticscholar.org/paper/72123a86eae2cb5c4eae8650f43524039d48875d,http://arxiv.org/pdf/2212.00193,10.48550/arXiv.2212.00193,,,Yes,,2025-11-11T02:48:05.506Z
draftsketchandproveg-2022,"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",Albert Qiaochu Jiang; S. Welleck; J. Zhou; Wenda Li; Jiacheng Liu; M. Jamnik; Timothée Lacroix; Yuhuai Wu; Guillaume Lample,2022,International Conference on Learning Representations,226,https://www.semanticscholar.org/paper/7de36d6b14aadc8cdb6ad1340b9ca64b15375bca,http://arxiv.org/pdf/2210.12283,10.48550/arXiv.2210.12283,"The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.",arxiv:2210.12283,Yes,,2025-11-11T02:48:05.506Z
dynamicpromptlearnin-2022,Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning,Pan Lu; Liang Qiu; Kai-Wei Chang; Y. Wu; Song-Chun Zhu; Tanmay Rajpurohit; Peter Clark; A. Kalyan,2022,International Conference on Learning Representations,365,https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4,http://arxiv.org/pdf/2209.14610,10.48550/arXiv.2209.14610,"Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.",arxiv:2209.14610,Yes,,2025-11-11T02:48:05.506Z
emergentanalogicalre-2022,Emergent analogical reasoning in large language models,Taylor W. Webb; K. Holyoak; Hongjing Lu,2022,Nature Human Behaviour,395,https://www.semanticscholar.org/paper/3cbffab9d7981da6662d474aaa056dcbd3c1701e,,10.1038/s41562-023-01659-w,"The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of Generative Pre-trained Transformer (GPT)-3) on a range of analogical tasks, including a non-visual matrix reasoning task based on the rule structure of Raven’s Standard Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT-4 indicated even better performance. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems. Webb et al. show that new artificial intelligence language models, such as Generative Pre-trained Transformer 3, are able to solve analogical reasoning problems at a human-like level of performance.",arxiv:2212.09196,Yes,,2025-11-11T02:48:05.506Z
faithfulreasoningusi-2022,Faithful Reasoning Using Large Language Models,Antonia Creswell; M. Shanahan,2022,arXiv.org,133,https://www.semanticscholar.org/paper/f0a0e8b6e84207f50db4d24cc4016e40601214ef,,,"Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.",arxiv:2208.14271,Yes,,2025-11-11T02:48:05.506Z
galacticaalargelangu-2022,Galactica: A Large Language Model for Science,Ross Taylor; Marcin Kardas; Guillem Cucurull; Thomas Scialom; A. Hartshorn; Elvis Saravia; Andrew Poulton; Viktor Kerkez; Robert Stojnic,2022,arXiv.org,892,https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1,,,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",arxiv:2211.09085,Yes,,2025-11-11T02:48:05.506Z
humanlikeintuitivebe-2022,Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT,Thilo Hagendorff; Sarah Fabi; Michal Kosinski,2022,Nature Computational Science,202,https://www.semanticscholar.org/paper/0bfc05adcddd4fe5d1335d96cc313c41526d4558,https://www.nature.com/articles/s43588-023-00527-x.pdf,10.1038/s43588-023-00527-x,"We design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics. The reasoning capabilities of OpenAI’s generative pre-trained transformer family were tested using semantic illusions and cognitive reflection tests that are typically used in human studies. While early models were prone to human-like cognitive errors, ChatGPT decisively outperformed humans, avoiding the cognitive traps embedded in the tasks.",arxiv:2306.07622,Yes,,2025-11-11T02:48:05.506Z
languagemodelsaregre-2022,Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,Abulhair Saparov; He He,2022,International Conference on Learning Representations,384,https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a,http://arxiv.org/pdf/2210.01240,10.48550/arXiv.2210.01240,"Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",arxiv:2210.01240,Yes,,2025-11-11T02:48:05.506Z
largelanguagemodelsa-2022,Large Language Models Are Reasoning Teachers,Namgyu Ho; Laura Schmid; Se-Young Yun,2022,Annual Meeting of the Association for Computational Linguistics,406,https://www.semanticscholar.org/paper/a9e3e5dd7b30890553b7ae1c41f932e99192bb44,http://arxiv.org/pdf/2212.10071,10.48550/arXiv.2212.10071,"Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model’s ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.",arxiv:2212.10071,Yes,,2025-11-11T02:48:05.506Z
largelanguagemodelse-2022,Large language models encode clinical knowledge,K. Singhal; Shekoofeh Azizi; T. Tu; S. Mahdavi; Jason Wei; Hyung Won Chung; Nathan Scales; A. Tanwani; H. Cole-Lewis; S. Pfohl; P. Payne; Martin G. Seneviratne; P. Gamble; C. Kelly; Nathaneal Scharli; A. Chowdhery; P. A. Mansfield; B. A. Y. Arcas; D. Webster; Greg S. Corrado; Yossi Matias; K. Chou; Juraj Gottweis; Nenad Tomašev; Yun Liu; A. Rajkomar; J. Barral; Christopher Semturs; A. Karthikesalingam; Vivek Natarajan,2022,Nature,3053,https://www.semanticscholar.org/paper/6052486bc9144dc1730c12bf35323af3792a1fd0,https://www.nature.com/articles/s41586-023-06291-2.pdf,10.1038/s41586-023-06291-2,"Med-PaLM, a state-of-the-art large language model for medicine, is introduced and evaluated across several medical question answering tasks, demonstrating the promise of these models in this domain. Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model^ 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM^ 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA^ 3 , MedMCQA^ 4 , PubMedQA^ 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics^ 6 ), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.",arxiv:2212.13138,Yes,,2025-11-11T02:48:05.506Z
learningtoreasonwith-2022,Learning to Reason With Relational Abstractions,A. Nam; Mengye Ren; Chelsea Finn; James L. McClelland,2022,arXiv.org,5,https://www.semanticscholar.org/paper/6d5555348f453bac901c5b57e8a4eeb3074b4071,https://arxiv.org/pdf/2210.02615,10.48550/arXiv.2210.02615,"Large language models have recently shown promising progress in mathematical reasoning when fine-tuned with human-generated sequences walking through a sequence of solution steps. However, the solution sequences are not formally structured and the resulting model-generated sequences may not reflect the kind of systematic reasoning we might expect an expert human to produce. In this paper, we study how to build stronger reasoning capability in language models using the idea of relational abstractions. We introduce new types of sequences that more explicitly provide an abstract characterization of the transitions through intermediate solution steps to the goal state. We find that models that are supplied with such sequences as prompts can solve tasks with a significantly higher accuracy, and models that are trained to produce such sequences solve problems better than those that are trained with previously used human-generated sequences and other baselines. Our work thus takes several steps toward elucidating and improving how language models perform on tasks requiring multi-step mathematical reasoning.",arxiv:2210.02615,Yes,,2025-11-11T02:48:05.506Z
leasttomostprompting-2022,Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,Denny Zhou; Nathanael Scharli; Le Hou; Jason Wei; Nathan Scales; Xuezhi Wang; D. Schuurmans; O. Bousquet; Quoc Le; Ed H. Chi,2022,International Conference on Learning Representations,1366,https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321,http://arxiv.org/pdf/2205.10625,10.48550/arXiv.2205.10625,"Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",arxiv:2205.10625,Yes,,2025-11-11T02:48:05.506Z
mindseyegroundedlang-2022,Mind's Eye: Grounded Language Model Reasoning through Simulation,Ruibo Liu; Jason Wei; S. Gu; Te-Yen Wu; Soroush Vosoughi; Claire Cui; Denny Zhou; Andrew M. Dai,2022,International Conference on Learning Representations,89,https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8,http://arxiv.org/pdf/2210.05359,10.48550/arXiv.2210.05359,"Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.",arxiv:2210.05359,Yes,,2025-11-11T02:48:05.506Z
naturalprovergrounde-2022,NaturalProver: Grounded Mathematical Proof Generation with Language Models,S. Welleck; Jiacheng Liu; Ximing Lu; Hannaneh Hajishirzi; Yejin Choi,2022,Neural Information Processing Systems,85,https://www.semanticscholar.org/paper/d593b9b8d63426f0d6a795dd7f2294619bc03610,https://arxiv.org/pdf/2205.12910,10.48550/arXiv.2205.12910,"Theorem proving in natural mathematical language - the mixture of symbolic and natural language used by humans - plays a central role in mathematical advances and education, and tests aspects of reasoning that are core to intelligence. Yet it has remained underexplored with modern generative models. We study large-scale language models on two new generation tasks: suggesting the next step in a mathematical proof, and full proof generation. We develop NaturalProver, a language model that generates proofs by conditioning on background references (e.g. theorems and definitions that are either retrieved or human-provided), and optionally enforces their presence with constrained decoding. On theorems from the NaturalProofs benchmark, NaturalProver improves the quality of next-step suggestions and generated proofs over fine-tuned GPT-3, according to human evaluations from university-level mathematics students. NaturalProver is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40% of the time, which is to our knowledge the first demonstration of these capabilities using neural language models.",arxiv:2205.12910,Yes,,2025-11-11T02:48:05.506Z
numglueasuiteoffunda-2022,NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,Swaroop Mishra; Arindam Mitra; Neeraj Varshney; Bhavdeep Singh Sachdeva; Peter Clark; Chitta Baral; A. Kalyan,2022,Annual Meeting of the Association for Computational Linguistics,118,https://www.semanticscholar.org/paper/39238a92de090c104936a4f78375b95600e42ce5,http://arxiv.org/pdf/2204.05660,10.48550/arXiv.2204.05660,"Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.",arxiv:2204.05660,Yes,,2025-11-11T02:48:05.506Z
overcomingbarriersto-2022,Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic,Mandar Sharma; N. Muralidhar; Naren Ramakrishnan,2022,arXiv.org,6,https://www.semanticscholar.org/paper/1ec4ae64ded99846bcd8a1a3b16fe2875f55c7c1,http://arxiv.org/pdf/2211.02098,10.48550/arXiv.2211.02098,"Through their transfer learning abilities, highly-parameterized large pre-trained language models have dominated the NLP landscape for a multitude of downstream language tasks. Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning. However, as we illustrate in this paper, building a general purpose language model that also happens to be proficient in mathematical reasoning is not as straight-forward as training it on a numeric dataset. In this work, we develop a novel framework that enables language models to be mathematically proficient while retaining their linguistic prowess. Specifically, we offer information-theoretic interventions to overcome the catastrophic forgetting of linguistic skills that occurs while injecting non-linguistic skills into language models.",arxiv:2211.02098,Yes,,2025-11-11T02:48:05.506Z
planbenchanextensibl-2022,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,Karthik Valmeekam; Alberto Olmo; S. Sreedharan; Subbarao Kambhampati,2022,Neural Information Processing Systems,306,https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc,,,"Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.",arxiv:2206.10498,Yes,,2025-11-11T02:48:05.507Z
reactsynergizingreas-2022,ReAct: Synergizing Reasoning and Acting in Language Models,Shunyu Yao; Jeffrey Zhao; Dian Yu; Nan Du; Izhak Shafran; Karthik Narasimhan; Yuan Cao,2022,International Conference on Learning Representations,4452,https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d,,,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",arxiv:2210.03629,Yes,,2025-11-11T02:48:05.506Z
responsiblereasoning-2022,Responsible Reasoning with Large Language Models and the Impact of Proper Nouns,Sumit Kumar Jha,2022,,6,https://www.semanticscholar.org/paper/a70fcd82d8d26bf4c8b0bdde88e96b6ce2c80ecf,,,,,Yes,,2025-11-11T02:48:05.506Z
rethinkingwithretrie-2022,Rethinking with Retrieval: Faithful Large Language Model Inference,Hangfeng He; Hongming Zhang; D. Roth,2022,arXiv.org,191,https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1,http://arxiv.org/pdf/2301.00303,10.48550/arXiv.2301.00303,"Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.",arxiv:2301.00303,Yes,,2025-11-11T02:48:05.506Z
selectioninferenceex-2022,Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning,Antonia Creswell; M. Shanahan; I. Higgins,2022,International Conference on Learning Representations,408,https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd,,,"Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",arxiv:2205.09712,Yes,,2025-11-11T02:48:05.506Z
selfconsistencyimpro-2022,Self-Consistency Improves Chain of Thought Reasoning in Language Models,Xuezhi Wang; Jason Wei; D. Schuurmans; Quoc Le; Ed H. Chi; Denny Zhou,2022,International Conference on Learning Representations,4975,https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2,,,"Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",arxiv:2203.11171,Yes,,2025-11-11T02:48:05.506Z
structuredflexiblean-2022,"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",K. M. Collins; Catherine Wong; Jiahai Feng; Megan Wei; J. Tenenbaum,2022,Annual Meeting of the Cognitive Science Society,66,https://www.semanticscholar.org/paper/7ef9aafc68511afab5b287e62b754576ea37b4ce,http://arxiv.org/pdf/2205.05718,10.48550/arXiv.2205.05718,"Human language offers a powerful window into our thoughts -- we tell stories, give explanations, and express our beliefs and goals through words. Abundant evidence also suggests that language plays a developmental role in structuring our learning. Here, we ask: how much of human-like thinking can be captured by learning statistical patterns in language alone? We first contribute a new challenge benchmark for comparing humans and distributional large language models (LLMs). Our benchmark contains two problem-solving domains (planning and explanation generation) and is designed to require generalization to new, out-of-distribution problems expressed in language. We find that humans are far more robust than LLMs on this benchmark. Next, we propose a hybrid Parse-and-Solve model, which augments distributional LLMs with a structured symbolic reasoning module. We find that this model shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.",arxiv:2205.05718,Yes,,2025-11-11T02:48:05.506Z
textgraphs2022shared-2022,TextGraphs 2022 Shared Task on Natural Language Premise Selection,Marco Valentino; Deborah Ferreira; Mokanarangan Thayaparan; André Freitas; Dmitry Ustalov,2022,Workshop on Graph-based Methods for Natural Language Processing,12,https://www.semanticscholar.org/paper/43845c54af0246baab17d572953e671487e1dcd1,,,,,Yes,,2025-11-11T02:48:05.506Z
thinksumprobabilisti-2022,ThinkSum: Probabilistic reasoning over sets using large language models,Batu Mehmet Ozturkler; Nikolay Malkin; Zhen Wang; N. Jojic,2022,Annual Meeting of the Association for Computational Linguistics,23,https://www.semanticscholar.org/paper/370cea8b4220917f45a69358c0303df71f5063c7,http://arxiv.org/pdf/2210.01293,10.48550/arXiv.2210.01293,"Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think – retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum – probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.",arxiv:2210.01293,Yes,,2025-11-11T02:48:05.506Z
towardsreasoninginla-2022,Towards Reasoning in Large Language Models: A Survey,Jie Huang; K. Chang,2022,Annual Meeting of the Association for Computational Linguistics,758,https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,http://arxiv.org/pdf/2212.10403,10.48550/arXiv.2212.10403,"Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.",arxiv:2212.10403,Yes,,2025-11-11T02:48:05.506Z
unigeounifyinggeomet-2022,UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression,Jiaqi Chen; Tong Li; Jinghui Qin; Pan Lu; Liang Lin; Chongyu Chen; Xiaodan Liang,2022,Conference on Empirical Methods in Natural Language Processing,133,https://www.semanticscholar.org/paper/72fce949725b20428e5f56247fef5c6bd1ce6154,https://arxiv.org/pdf/2212.02746,10.48550/arXiv.2212.02746,"Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning capability on multiple math tasks. However, in essence, these two tasks have similar problem representations and overlapped math knowledge which can improve the understanding and reasoning ability of a deep model on both two tasks. Therefore, we construct a large-scale Unified Geometry problem benchmark, UniGeo, which contains 4,998 calculation problems and 9,543 proving problems. Each proving problem is annotated with a multi-step proof with reasons and mathematical expressions. The proof can be easily reformulated as a proving sequence that shares the same formats with the annotated program sequence for calculation problems. Naturally, we also present a unified multi-task Geometric Transformer framework, Geoformer, to tackle calculation and proving problems simultaneously in the form of sequence generation, which finally shows the reasoning ability can be improved on both two tasks by unifying formulation. Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that aims to predict the mathematical expressions in the problem solution, thus improving the Geoformer model. Experiments on the UniGeo demonstrate that our proposed Geoformer obtains state-of-the-art performance by outperforming task-specific model NGS with over 5.6% and 3.2% accuracies on calculation and proving problems, respectively.",arxiv:2212.02746,Yes,,2025-11-11T02:48:05.506Z
