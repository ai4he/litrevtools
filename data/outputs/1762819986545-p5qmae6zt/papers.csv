ID,Title,Authors,Year,Venue,Citations,URL,PDF URL,DOI,Abstract,Keywords,Included,Exclusion Reason,Extracted At
thisismyunicornfluff-2022,"""This is my unicorn, Fluffy"": Personalizing frozen vision-language representations",Niv Cohen; Rinon Gal; E. Meirom; Gal Chechik; Y. Atzmon,2022,European Conference on Computer Vision,101,https://www.semanticscholar.org/paper/0791a0441e1f672c43aecb2d6708fbc8725c8cad,http://arxiv.org/pdf/2204.01694,10.48550/arXiv.2204.01694,"Large Vision&Language models pretrained on web-scale data provide representations that are invaluable for numerous V&L problems. However, it is unclear how they can be used for reasoning about user-specific visual concepts in unstructured language. This problem arises in multiple domains, from personalized image retrieval to personalized interaction with smart devices. We introduce a new learning setup called Personalized Vision&Language (PerVL) with two new benchmark datasets for retrieving and segmenting user-specific""personalized""concepts""in the wild"". In PerVL, one should learn personalized concepts (1) independently of the downstream task (2) allowing a pretrained model to reason about them with free language, and (3) does not require personalized negative examples. We propose an architecture for solving PerVL that operates by extending the input vocabulary of a pretrained model with new word embeddings for the new personalized concepts. The model can then reason about them by simply using them in a sentence. We demonstrate that our approach learns personalized visual concepts from a few examples and can effectively apply them in image retrieval and semantic segmentation using rich textual queries.",arxiv:2204.01694,Yes,,2025-11-11T00:14:11.169Z
acausalframeworktoqu-2022,A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,Alessandro Stolfo; Zhijing Jin; Kumar Shridhar; B. Scholkopf; Mrinmaya Sachan,2022,Annual Meeting of the Association for Computational Linguistics,76,https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe,http://arxiv.org/pdf/2210.12023,10.48550/arXiv.2210.12023,"We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution.By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems.Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.",arxiv:2210.12023,Yes,,2025-11-11T00:13:07.427Z
aclusteringapproachf-2022,A Clustering Approach for the Optimal Siting of Recharging Stations in the Electric Vehicle Routing Problem with Time Windows,Danny García Sánchez; Alejandra Tabares; L. Faria; Juan Carlos Rivera; J. Franco,2022,Energies,25,https://www.semanticscholar.org/paper/f1164514c7180331c3b059c19eab5169c9c921a7,https://www.mdpi.com/1996-1073/15/7/2372/pdf?version=1648115999,10.3390/en15072372,"Transportation has been incorporating electric vehicles (EVs) progressively. EVs do not produce air or noise pollution, and they have high energy efficiency and low maintenance costs. In this context, the development of efficient techniques to overcome the vehicle routing problem becomes crucial with the proliferation of EVs. The vehicle routing problem concerns the freight capacity and battery autonomy limitations in different delivery-service scenarios, and the challenge of best locating recharging stations. This work proposes a mixed-integer linear programming model to solve the electric location routing problem with time windows (E-LRPTW) considering the state of charge, freight and battery capacities, and customer time windows in the decision model. A clustering strategy based on the k-means algorithm is proposed to divide the set of vertices (EVs) into small areas and define potential sites for recharging stations, while reducing the number of binary variables. The proposed model for E-LRPTW was implemented in Python and solved using mathematical modeling language AMPL together with CPLEX. Performed tests on instances with 5 and 10 clients showed a large reduction in the time required to find the solution (by about 60 times in one instance). It is concluded that the strategy of dividing customers by sectors has the potential to be applied and generate solutions for larger geographical areas and numbers of recharging stations, and determine recharging station locations as part of planning decisions in more realistic scenarios.",,Yes,,2025-11-11T00:15:19.325Z
acontributiononrelat-2022,"A Contribution on Relationship Banking. Economic, Anthropological and Mathematical Reasoning, Empirical Evidence from Italy",Marco Desogus; Elisa Casu,2022,,6,https://www.semanticscholar.org/paper/dd88cc9b1f6d71ef82631b4e1c98c077ccdf291a,,,,,Yes,,2025-11-11T00:13:07.427Z
ahybridgeneticalgori-2022,A Hybrid Genetic Algorithm for Flexible Job Shop Scheduling Problem,Xianglong Wang; Changyi Liu,2022,2022 5th World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM),0,https://www.semanticscholar.org/paper/3cff420b5a41a06291b68f5b6600935c090f8ad8,,10.1109/WCMEIM56910.2022.10021523,"Partially flexible job shop scheduling problem (P-FJSP) is a NP Hard problem more complex than fully flexi-ble job shop scheduling problem (T -FJSP). In this paper, the mathematical model of flexible job shop scheduling is established with the goal of minimizing the maximum completion time (makespan). It combines the local search ability of simu-lated annealing algorithm and the global search ability of ge-netic algorithm. In the process of chromosome decoding, greedy decoding method is used to get a better scheduling solution as far as possible. The hybrid scheduling algorithm is implemented based on Visual Studio and C # language. Finally, 8×8 classic scheduling instance are used for simulation scheduling experiments to verify that the hybrid genetic algorithm proposed in this paper is effective in solving large-scale FJSP.",,Yes,,2025-11-11T00:15:16.543Z
amultilayerattention-2022,A Multi-Layer Attention Network for Visual Commonsense Reasoning,Wenqi Zhang; Yongchao Gao; Heng Qian; Hongli Lyu,2022,International Conference on Data Science and Information Technology,1,https://www.semanticscholar.org/paper/0e0f20f3af3650b5a97b0ec3f046ba8160b45279,,10.1109/DSIT55514.2022.9943834,"Visual Commonsense Reasoning (VCR) is a challenging multimodal task involving several research fields such as vision, cognition, and reasoning, which combines images and natural language for reasoning. Existing VCR methods focus on global attention or use pre-training models, but these methods lack attention to local features of visual and language. In this paper, a multi-layer attention network is proposed for the VCR task, including an intra-modal attention module and an inter-modal attention module. The intra-modal attention module complements important features of visual and language modalities with fine-grained visual attention to improve the relevance of visual and language. The inter-modal attention module captures the internal dependencies between visual and language. Finally, the two modules are integrated into an end-to-end reasoning framework. Experiments on the VCR large-scale dataset show that the proposed method exhibits a decent improvement in the VCR task and illustrates the effectiveness of the method on three subtasks.",,Yes,,2025-11-11T00:14:11.169Z
apetrinetbasedapproa-2022,A Petri-Net-Based Approach for Enhancing Clinical Reasoning in Medical Education,F. Ricci; F. Consorti; F. Pecoraro; D. Luzi; Oscar Tamburis,2022,IEEE Transactions on Learning Technologies,10,https://www.semanticscholar.org/paper/98b0fb67a6fb222998e3449621f3f5eecaed758e,,10.1109/tlt.2022.3157391,"Medical students are called to acquire competence to manage disease in its dynamic evolution over time, learning to analyze how clinical conditions evolve in a patient's history and how each condition interferes with the evolution of the other coexisting conditions. In this article, the health issue network (HIN) approach is introduced as a formal language based on Petri nets (PNs) to model properties that are particularly apposite for the graphical representation of HIN evolutionary paths. Moreover, the PNs’ underlying mathematical model allows users to draw coherent and well-formed graphs representing rather complex clinical cases. Finally, HIN can be easily integrated into a simulation environment to support case-based learning activities and assessment. The examples of the exercises provided in this article show, on the one hand, the ways the introduced methodology is figured out and implemented; on the other hand, they outline the variety of learning questions that users may deal with when deploying the HIN approach.",,Yes,,2025-11-11T00:14:11.169Z
aratiocinativestudya-2022,A Ratiocinative Study and Assessment of W. V. O. Quine’s “Criterion of Ontological Commitment”,Joseph T. Ekong,2022,International Journal of Philosophy,0,https://www.semanticscholar.org/paper/7b6955111d3bd91b13e7a9c7fbdfd75d43825c36,https://www.carijournals.org/journals/index.php/IJP/article/download/1052/1269,10.47941/ijp.1052,"Purpose: This work has three main objectives: Firstly, it offers an elucidation of the notion of ontological commitment. Secondly, it assesses the adequacy of the criterion of ontological commitment for different languages. Thirdly, it offers some speculative and evaluative remarks regarding the significance of Quine’s criterion of ontological commitment. Many ontologists, within the analytic tradition, often appeal to Quine's criterion of ontological commitment, when debating whether an assertion or theory implies the existence of a certain entity. Regarding his goal in formulating this criterion, he says that the criterion does not aim to help us discover what it is that there is, but only what a theory says there is: “I look to variables and quantification for evidence as to what a theory says that there is, not for evidence as to what there is” (Quine, 1960: 225). Its most popular formulation, using textual evidence from Quine's oeuvre, is: “To be is to be the value of a bound variable,” (Quine, 1961: 15). However, this formulation is susceptible to gross misunderstanding, especially if one is influenced by the formalities and technical maneuvers of model theory. In mathematical logic, model theory is the study of the relationship between formal theories (a collection of sentences in a formal language expressing statements about a mathematical structure), and their models (those structures in which the statements of the theory hold). Model theory is a branch of mathematical logic where we study mathematical structures by considering the first-order sentences true in those structures and the sets definable by first-order formulas. Model theory studies the relations between sentences of a formal language and the interpretations (or ‘structures’) which make these sentences true or false. It offers precise definitions of truth, logical truth and consequence, meanings and modalities. 
Methodology: This work is expository, analytic, critical and evaluative in its methodology. Of course, there are familiar philosophical problems which are within the discursive framework of ‘ontology,’ often phrased by asking if something or some category of things are “real,” or whether “they exist,” concretely. An outstanding example is provided by the traditional problem of universals, which issues in the nominalist-realist controversy, as to the real existence of universals, or of abstract entities such as classes (in the mathematical sense) or propositions (in the abstract sense, referring to the content of an assertion in abstraction from the particular words used to convey it). 
Results: In as much as one might agree with Quine’s Criterion of Ontological Commitment, one might also opine that it is nonetheless a feature of first-order language (i.e. the language embodied in first-order logic; a symbolized reasoning process comprising relations, functions and constants, in which each sentence or statement is broken down into a subject and a predicate. In this regard, the predicate modifies or defines the properties of the subject) that there should be an exact correspondence between the ontological commitments carried by a sentence and the objects that must be counted among the values of the variables in order for the sentence to be true. However, this in itself is not a reason for thinking that such a feature will generalize beyond first-order languages. It is possible for Quine’s Criterion to degenerate, when the language contains atomic predicates expressing extrinsic properties. 
Unique Contribution to theory, practice and policy: Based on Quine’s analysis, a theory is committed to those and only those entities that in the last analysis serve as the values of its bound variables. Thus, ordinary first-order theory commits one to an ontology only of individuals (particulars), whereas higher order logic commits one to the existence of sets, i.e. of collections of definite and distinct entities (or, alternatively, of properties and relations). Likewise, if bound first-order variables are assumed to range over sets (as they do in set theory), a commitment to the existence of these sets is incurred. Admittedly, the precise import of Quine’s criterion of ontological commitment, however, is not completely clear, nor is it clear in what other sense one is perhaps committed by a theory to those entities that are named or otherwise referred to in it, but not quantified over in it. However, it despite its limitations, it has made is possible for one to measure the ontological cost of theories, an important component in deciding which theories to accept, thus offering a partial foundation for theory choice.",,Yes,,2025-11-11T00:15:19.325Z
ascenariobasedexplor-2022,"A Scenario-based Exploration of Expected Usefulness, Privacy Concerns, and Adoption Likelihood of Learning Analytics",X. Li; M. Rosson; Jenay Robert,2022,ACM Conference on Learning @ Scale,5,https://www.semanticscholar.org/paper/067b8489b028d931b751cb9413225b761e51dcf3,,10.1145/3491140.3528271,"Learning analytics has become a robust research area in the last decade, as innovative analytic models of learning data have been created with the goal of enhancing teaching and learning. However, barriers to large scale adoption of such technologies in higher education still exist. In recent years, a strand of research has begun to investigate stakeholders' expectations of learning analytics, hoping to find ways to integrate the innovations into everyday teaching practices. For instance, studies have investigated instructors' ideas about how learning analytics might be helpful, as well as concerns about student data privacy. However, most studies have taken a general approach rather than considering instructors' day-to-day experiences. Using survey methods, we presented instructors with hypothetical scenarios of learning analytics in use across disciplines, class sizes, teaching activities, and types of student data. We asked for ratings of both usefulness and privacy concerns for each proposed teaching situation. Our respondents considered scenarios involving learning outcomes-related data (e.g. grades) to be more useful than those that involve student interactions (e.g. language, social activity). In contrast, privacy concerns were lower for outcomes-oriented scenarios than interactions-focused scenarios. An interesting new finding was a negative correlation of usefulness and privacy; we discuss this in the context of instructors' possible cost-benefit reasoning. We reflect on our findings with respect to future efforts in developing and fielding learning analytics tools.",,Yes,,2025-11-11T00:15:19.325Z
asurveyofdeeplearnin-2022,A Survey of Deep Learning for Mathematical Reasoning,Pan Lu; Liang Qiu; Wenhao Yu; S. Welleck; Kai-Wei Chang,2022,Annual Meeting of the Association for Computational Linguistics,166,https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,http://arxiv.org/pdf/2212.10535,10.48550/arXiv.2212.10535,"Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.",arxiv:2212.10535,Yes,,2025-11-11T00:13:07.427Z
asurveyofknowledgeen-2022,A Survey of Knowledge Enhanced Pre-Trained Language Models,Linmei Hu; Zeyi Liu; Ziwang Zhao; Lei Hou; Liqiang Nie; Juanzi Li,2022,IEEE Transactions on Knowledge and Data Engineering,181,https://www.semanticscholar.org/paper/a26623d52d24e03044a158cddad931ec5ab7304c,https://arxiv.org/pdf/2211.05994,10.1109/TKDE.2023.3310002,"Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are categorized into KG-based and retrieval-based methods. Finally, we point out some promising future directions of KE-PLMs.",arxiv:2211.05994,Yes,,2025-11-11T00:14:11.169Z
asurveyonneuralopeni-2022,A Survey on Neural Open Information Extraction: Current Status and Future Directions,Shaowen Zhou; Yu Bowen; Aixin Sun; Cheng Long; Jingyang Li; Haiyang Yu; Jianguo Sun,2022,International Joint Conference on Artificial Intelligence,39,https://www.semanticscholar.org/paper/5de6ecf62f14c9263882f9f30d6448df9efd34e0,https://arxiv.org/pdf/2205.11725,10.48550/arXiv.2205.11725,"Open Information Extraction (OpenIE) facilitates domain-independent discovery of relational facts from large corpora. The technique well suits many open-world natural language understanding scenarios, such as automatic knowledge base construction, open-domain question answering, and explicit reasoning. Thanks to the rapid development in deep learning technologies, numerous neural OpenIE architectures have been proposed and achieve considerable performance improvement. In this survey, we provide an extensive overview of the state-of-the-art neural OpenIE models, their key design decisions, strengths and weakness. Then, we discuss limitations of current solutions and the open issues in OpenIE problem itself. Finally we list recent trends that could help expand its scope and applicability, setting up promising directions for future research in OpenIE. To our best knowledge, this paper is the first review on neural OpenIE.",arxiv:2205.11725,Yes,,2025-11-11T00:15:14.026Z
acomparisonofapproac-2022,A comparison of approaches for imbalanced classification problems in the context of retrieving relevant documents for an analysis,Sandra Wankmüller,2022,Journal of Computational Social Science,3,https://www.semanticscholar.org/paper/a12e9a6863c8453787575172599389d2ddcd9f62,https://link.springer.com/content/pdf/10.1007/s42001-022-00191-7.pdf,10.1007/s42001-022-00191-7,"One of the first steps in many text-based social science studies is to retrieve documents that are relevant for an analysis from large corpora of otherwise irrelevant documents. The conventional approach in social science to address this retrieval task is to apply a set of keywords and to consider those documents to be relevant that contain at least one of the keywords. But the application of incomplete keyword lists has a high risk of drawing biased inferences. More complex and costly methods such as query expansion techniques, topic model-based classification rules, and active as well as passive supervised learning could have the potential to more accurately separate relevant from irrelevant documents and thereby reduce the potential size of bias. Yet, whether applying these more expensive approaches increases retrieval performance compared to keyword lists at all, and if so, by how much, is unclear as a comparison of these approaches is lacking. This study closes this gap by comparing these methods across three retrieval tasks associated with a data set of German tweets (Linder in SSRN, 2017. https://doi.org/10.2139/ssrn.3026393 ), the Social Bias Inference Corpus (SBIC) (Sap et al. in Social bias frames: reasoning about social and power implications of language. In: Jurafsky et al. (eds) Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, p 5477–5490, 2020. https://doi.org/10.18653/v1/2020.aclmain.486 ), and the Reuters-21578 corpus (Lewis in Reuters-21578 (Distribution 1.0). [Data set], 1997. http://www.daviddlewis.com/resources/testcollections/reuters21578/ ). Results show that query expansion techniques and topic model-based classification rules in most studied settings tend to decrease rather than increase retrieval performance. Active supervised learning, however, if applied on a not too small set of labeled training instances (e.g. 1000 documents), reaches a substantially higher retrieval performance than keyword lists.",arxiv:2205.01600,Yes,,2025-11-11T00:15:19.325Z
acomparisonofthreeap-2022,A comparison of three approaches to covariate effects on latent factors,Ze Wang,2022,Large-scale Assessments in Education,2,https://www.semanticscholar.org/paper/c40412109167ae57baab6505edf9b628efca6d3a,https://largescaleassessmentsineducation.springeropen.com/counter/pdf/10.1186/s40536-022-00148-2,10.1186/s40536-022-00148-2,"In educational and psychological research, it is common to use latent factors to represent constructs and then to examine covariate effects on these latent factors. Using empirical data, this study applied three approaches to covariate effects on latent factors: the multiple-indicator multiple-cause (MIMIC) approach, multiple group confirmatory factor analysis (MG-CFA) approach, and the structural equation model trees (SEM Trees) approach. The MIMIC approach directly models covariate effects on latent factors. The MG-CFA approach allows testing of measurement invariance before latent factor means could be compared. The more recently developed SEM Trees approach partitions the sample into homogenous subsets based on the covariate space; model parameters are estimated separately for each subgroup. We applied the three approaches using an empirical dataset extracted from the eighth-grade U.S. data from the Trends in International Mathematics and Science Study 2019 database. All approaches suggested differences among mathematics achievement categories for the latent factor of mathematics self-concept. In addition, language spoken at home did not seem to affect students’ mathematics self-concept. Despite these general findings, the three approaches provided different pieces of information regarding covariate effects. For all models, we appropriately considered the complex data structure and sampling weights following recent recommendations for analyzing large-scale assessment data.",,Yes,,2025-11-11T00:15:19.325Z
amethodologytocharac-2022,A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America,L. A. Alemany; Luciana Benotti; Hernán Maina; Luc'ia M. Gonz'alez; Mariela Rajngewerc; Lautaro Mart'inez; Jos'e L. S'anchez; M. Schilman; Guido Ivetta; Alexia Halvorsen; Amanda Rojo; M. Bordone; Beatriz Busaniche,2022,,3,https://www.semanticscholar.org/paper/a82a08b5e6a11f4d6fdff95dd30177957ed7855e,,,"Automated decision-making systems, especially those based on natural language processing, are pervasive in our lives. They are not only behind the internet search engines we use daily, but also take more critical roles: selecting candidates for a job, determining suspects of a crime, diagnosing autism and more. Such automated systems make errors, which may be harmful in many ways, be it because of the severity of the consequences (as in health issues) or because of the sheer number of people they affect. When errors made by an automated system affect a population more than others, we call the system \textit{biased}. Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to audit them. In this paper, we present a methodology that spells out how social scientists, domain experts, and machine learning experts can collaboratively explore biases and harmful stereotypes in word embeddings and large language models. Our methodology is based on the following principles: * focus on the linguistic manifestations of discrimination on word embeddings and language models, not on the mathematical properties of the models * reduce the technical barrier for discrimination experts%, be it social scientists, domain experts or other * characterize through a qualitative exploratory process in addition to a metric-based approach * address mitigation as part of the training process, not as an afterthought",arxiv:2207.06591,Yes,,2025-11-11T00:15:14.026Z
anovelmodularmodelin-2022,A novel modular modeling approach for understanding different electromechanics between left and right heart in rat,Nari Kim; Julius D. Pronto; D. Nickerson; A. Taberner; Peter J. Hunter,2022,Frontiers in Physiology,1,https://www.semanticscholar.org/paper/2fba0d7b1293e4b13180fb3bccf86ed52ddcaf70,https://www.frontiersin.org/articles/10.3389/fphys.2022.965054/pdf,10.3389/fphys.2022.965054,"While ion channels and transporters involved in excitation-contraction coupling have been linked and constructed as comprehensive computational models, validation of whether each individual component of a model can be reused has not been previously attempted. Here we address this issue while using a novel modular modeling approach to investigate the underlying mechanism for the differences between left ventricle (LV) and right ventricle (RV). Our model was developed from modules constructed using the module assembly principles of the CellML model markup language. The components of three existing separate models of cardiac function were disassembled as to create smaller modules, validated individually, and then the component parts were combined into a new integrative model of a rat ventricular myocyte. The model was implemented in OpenCOR using the CellML standard in order to ensure reproducibility. Simulated action potential (AP), Ca2+ transient, and tension were in close agreement with our experimental measurements: LV AP showed a prolonged duration and a more prominent plateau compared with RV AP; Ca2+ transient showed prolonged duration and slow decay in LV compared to RV; the peak value and relaxation of tension were larger and slower, respectively, in LV compared to RV. Our novel approach of module-based mathematical modeling has established that the ionic mechanisms underlying the APs and Ca2+ handling play a role in the variation in force production between ventricles. This simulation process also provides a useful way to reuse and elaborate upon existing models in order to develop a new model.",,Yes,,2025-11-11T00:15:19.325Z
areviewdevelopmentof-2022,A review: development of named entity recognition (NER) technology for aeronautical information intelligence,Baigang Mi; Fan Yi,2022,Artificial Intelligence Review,34,https://www.semanticscholar.org/paper/ca2da2420fd25c8633641542730d3f0867c50f60,,10.1007/s10462-022-10197-2,,,Yes,,2025-11-11T00:15:14.026Z
asemioticanalysisofm-2022,"A semiotic analysis of multiple systems of logic: using tagmemic theory to assess the usefulness and limitations of formal logics, and to produce a mathematical lattice model including multiple systems of logic",V. Poythress,2022,Semiotica: Journal of the International Association for Semiotic Studies,0,https://www.semanticscholar.org/paper/606db29a9d5cad5cd06b8eeb1f8beee390c87ca4,,10.1515/sem-2020-0051,"Abstract Tagmemic theory as a semiotic theory can be used to analyze multiple systems of logic and to assess their strengths and weaknesses. This analysis constitutes an application of semiotics and also a contribution to understanding of the nature of logic within the context of human meaning. Each system of logic is best adapted to represent one portion of human rationality. Acknowledging this correlation between systems and their targets helps explain the usefulness of more than one system. Among these systems, the two-valued system of classical logic takes its place. All the systems of logic can be incorporated into a complex mathematical model that has a place for each system and that represents a larger whole in human reasoning. The model can represent why tight formal systems of logic can be applied in some contexts with great success, but in other contexts are not directly applicable. The result suggests that human reasoning is innately richer than any one formal system of logic.",,Yes,,2025-11-11T00:14:11.169Z
athesissubmittedtoth-2022,A thesis submitted to the Faculty of Graduate and Postdoctoral Affairs in partial fulfillment of the requirements for the degree of Master of Arts,Charlene Song,2022,,0,https://www.semanticscholar.org/paper/25be22274b72f1337e977d94d0c94026d13a67d0,,,,,Yes,,2025-11-11T00:15:16.543Z
accuracyofpupilsself-2022,ACCURACY OF PUPILS´ SELF-ASSESSMENT,Švamberk Šauerová Markéta; Smetáčková Irena,2022,EduPort,0,https://www.semanticscholar.org/paper/fcfabc1d551304cde28a4f0658ed20e36559e05f,http://eduport.pf.ujep.cz/doi/10.21062/edp.2022.009.pdf,10.21062/edp.2022.009,"In this study, we investigated the accuracy of pupils´ self-assessment in two main school domains – mathematics and Czech language. The analysis explores whether pupils are able to evaluate adequately their own results in the didactic tests and then use some individual parameters to explain the level of self-assessment. The aim of the study was to analyze whether groups of pupils with different self-assessments of school tasks in the Czech language and mathematics (significant underestimation, adequate self-assessment, significant overestimation) differ in some of the cognitive skills studied. Our study questions were as follows: (1) Do pupils assess their achievements in particular school tasks accurately, or inaccurately? (2) Do pupils´ self-assessments differ in mathematics and language? (3) Do the pupil´s self-assessment correlate with individual parameters? The main tool used in the study was a didactic test on mathematics and a didactic test on the Czech language based on the Czech National Curricula Document and created by an expert team. In addition, Raven's Color Progressive Matrices (CPM), Similarities from the Wechsler Intelligence (WISC-SIM), and the Rey-Osterrieth Complex Figure (ROCF) were used. Considering the nature of the data, the non-parametric Kruskal-Wallis ANOVA was used. The present study is a part of the larger research project, involving 29 primary school classes, 657 pupils in total. Based on the data obtained, it can be concluded that the accuracy of pupils' self-assessments is low, while the accuracy of pupils' self-assessments in mathematics and Czech language differs (in mathematics there are more children with more accurate estimates and more pupils who underestimate themselves, in Czech language there are more pupils who overestimate their performance. Statistically significant differences were observed in the domains of Raven's Color Progressive Matrices and Rey-Osterrieth Figure, and in terms of the focus of each test, it could be concluded that there are significant differences between the groups in the domain of non-verbal reasoning skills and in the domain of analytical and organizational perceptual activity and memory. In the area of verbal intellectual abilities, there were no significant differences between the groups.",,Yes,,2025-11-11T00:15:19.325Z
afrbertattentionbase-2022,AFR-BERT: Attention-based mechanism feature relevance fusion multimodal sentiment analysis model,Mingyu Ji; Jiawei Zhou; Wei Ning,2022,PLoS ONE,11,https://www.semanticscholar.org/paper/918f34bd4274316d684dd6c267b13fe010a74a6e,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0273936&type=printable,10.1371/journal.pone.0273936,"Multimodal sentiment analysis is an essential task in natural language processing which refers to the fact that machines can analyze and recognize emotions through logical reasoning and mathematical operations after learning multimodal emotional features. For the problem of how to consider the effective fusion of multimodal data and the relevance of multimodal data in multimodal sentiment analysis, we propose an attention-based mechanism feature relevance fusion multimodal sentiment analysis model (AFR-BERT). In the data pre-processing stage, text features are extracted using the pre-trained language model BERT (Bi-directional Encoder Representation from Transformers), and the BiLSTM (Bi-directional Long Short-Term Memory) is used to obtain the internal information of the audio. In the data fusion phase, the multimodal data fusion network effectively fuses multimodal features through the interaction of text and audio information. During the data analysis phase, the multimodal data association network analyzes the data by exploring the correlation of fused information between text and audio. In the data output phase, the model outputs the results of multimodal sentiment analysis. We conducted extensive comparative experiments on the publicly available sentiment analysis datasets CMU-MOSI and CMU-MOSEI. The experimental results show that AFR-BERT improves on the classical multimodal sentiment analysis model in terms of relevant performance metrics. In addition, ablation experiments and example analysis show that the multimodal data analysis network in AFR-BERT can effectively capture and analyze the sentiment features in text and audio.",,Yes,,2025-11-11T00:15:14.026Z
aiassistedprogrammin-2022,"AI-assisted programming: applications, user experiences, and neuro-symbolic techniques (keynote)",Sumit Gulwani,2022,ESEC/SIGSOFT FSE,5,https://www.semanticscholar.org/paper/11230f03465d8ab073815397717d8afa3f3dae1c,,10.1145/3540250.3569444,,,Yes,,2025-11-11T00:15:19.325Z
alertadaptlanguagemo-2022,ALERT: Adapt Language Models to Reasoning Tasks,Ping Yu; Tianlu Wang; O. Yu. Golovneva; Badr AlKhamissi; Gargi Ghosh; Mona T. Diab; Asli Celikyilmaz,2022,Annual Meeting of the Association for Computational Linguistics,20,https://www.semanticscholar.org/paper/95c11cc5820ba32c60d5f2671f6567b9914a4978,https://arxiv.org/pdf/2212.08286,10.48550/arXiv.2212.08286,"Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.To address this question, we introduce {pasted macro ‘OUR’}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro ‘OUR’}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro ‘OUR’}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.",arxiv:2212.08286,Yes,,2025-11-11T00:13:07.427Z
algorithmmethodintea-2022,ALGORITHM METHOD IN TEACHING RUSSIAN AT SECONDARY SCHOOL,Юлия Владимировна Подкина,2022,Tomsk state pedagogical university bulletin,0,https://www.semanticscholar.org/paper/ded393f5b3432f3d0b9258fff2b9db33b204bf84,https://vestnik.tspu.edu.ru/files/vestnik/PDF/articles/podkina_y._v._80_87_6_224_2022.pdf,10.23951/1609-624x-2022-6-80-87,"Введение. Обучение русскому языку в средней школе, развитие речи и формирование орфографических и пунктуационных навыков – важная задача, которая сопряжена с рядом трудностей. Эффективному изучению русского языка в общеобразовательной школе зачастую препятствуют такие факторы, как плохая усидчивость, отсутствие интереса к предмету, билингвизм и другое. Метод алгоритмизированного представления правил русской орфографии и пунктуации способствует наилучшему усвоению учебного материала и позволяет повысить качество обучения русскому языку школьников среднего и старшего звена. Цель − обоснование эффективности метода алгоритма в обучении русскому языку детей общеобразовательных средних школ, рассмотрение примерных моделей обучающих алгоритмов. Материал и методы. В работе применялись теоретические методы (моделирование, анализ, синтез); эмпирические методы (наблюдение, сравнение, эксперимент). Результаты и обсуждение. Простое заучивание правил не всегда приводит к повышению грамотности учащихся. Метод алгоритма предусматривает совместное с учениками составление алгоритмизированных схем различных видов, которые иллюстрируют изучаемое правило, позволяют пошагово отработать механизм рассуждения при выполнении орфографических и пунктуационных заданий. Такой подход способствует достижению высокого качества знаний путем систематической отработки практических навыков с помощью схем, адаптируемых под потребности каждого ребенка. Обучающий алгоритм может иметь разные виды: от четко сформулированной схемы (похожей на математический пример) до красочной иллюстрации, которая будет понятна детям с творческими способностями. Заключение. Метод алгоритма применяют для изучения практически любого правила русской орфографии и пунктуации. В созданной совместно с учащимися схеме должно быть отведено место для исключений и для примеров, которые ребенок впишет самостоятельно. При создании обучающей схемы школьник является активным соавтором. Схема никогда не является замкнутой системой. Она дорабатывается и совершенствуется в процессе практической деятельности учащихся. У детей из одного класса схемы могут быть совершенно различны, так как усовершенствованы и доработаны самостоятельно под руководством учителя.
 Introduction. Teaching Russian in secondary school, speech development and the formation of spelling and punctuation skills is an important task that involves a number of difficulties. Effective study of the Russian language in a secondary school is often hindered by factors such as poor perseverance, lack of interest in the subject, bilingualism, and more. Russian Russian spelling rules algorithmized representation method is considered in this paper, which allows to improve the quality of teaching Russian to middle and senior school students. The purpose is to substantiate the effectiveness of the algorithm method in teaching the Russian language to children of secondary schools, to consider approximate models of training algorithms. Material and methods. Theoretical methods (modeling, analysis, synthesis) were used in the work; empirical methods (observation, comparison, experiment). Results and discussion. Simple memorizing of the rules does not always lead to increased literacy of students. The algorithm method provides for the joint compilation of algorithmic schemes of various types with students, which illustrate the rule being studied, allow you to work out the mechanism of reasoning step by step when performing spelling and punctuation tasks. This approach contributes to the achievement of a high quality of knowledge through the systematic development of practical skills with the help of schemes adapted to the needs of each child. The training algorithm can have different types: from a clearly formulated scheme (similar to a mathematical example) up to a colorful illustration that will be understandable to children with creative abilities. Conclusion. The algorithm method can be applied to study almost any rule of Russian spelling and punctuation. In the scheme created jointly with the students, there should be a place for exceptions and for examples that the child will enter independently. When creating a training scheme, the student is an active co-author. A circuit is never a closed system. It is being refined and improved in the process of practical activity of students. For children from the same class, the schemes can be completely different, as they have been improved and finalized independently.",,Yes,,2025-11-11T00:15:19.325Z
anupdateofthermalerr-2022,AN UPDATE OF THERMAL ERROR COMPENSATION MODEL VIA ON-MACHINE MEASUREMENT,M. Mareš; O. Horejš; Michal Straka; J. Švéda; Tomáš Kozlok,2022,MM Science Journal,6,https://www.semanticscholar.org/paper/796f47a4059604f27ad57c3760cc7ebea9f6a020,https://www.mmscience.eu/journal/issues/december-2022/articles/calibration-of-the-robotic-arm-with-corrections-using-local-linear-neuro-fuzzy-models/download,10.17973/mmsj.2022_12_2022150,"Software compensation is state-of-the-art technology used to reduce CNC machine tool thermal errors, and it belongs to a key intelligent functions of modern machine tools. However, a pretrained and nonadaptive model may not be accurate and robust enough for long-term application. This research presents a transfer function based thermal error compensation model updated via on-machine measurement. A mathematical model is implemented into the machine management software of a large horizontal machining centre to compensate for thermal errors in real time using C#/C++ programming language. The results show that after the thermal error compensation model is updated via on-machine measurement, the prediction accuracy, measured as peak-to-peak values, and the normalized root mean squared error are significantly improved. The prediction accuracy of the compensation model updated via on-machine measurement strongly depends on the sampling interval of the on machine measurements.",,Yes,,2025-11-11T00:15:14.026Z
achievingandundersta-2022,Achieving and Understanding Out-of-Distribution Generalization in Systematic Reasoning in Small-Scale Transformers,A. Nam; Mustafa Abdool; Trevor Maxfield; James L. McClelland,2022,,3,https://www.semanticscholar.org/paper/8283064365ae7594d891e8b7daf36fd37ca809b0,,,"Out-of-distribution generalization (OODG) is a longstanding challenge for neural networks. This challenge is quite apparent in tasks with well-defined variables and rules, where explicit use of the rules could solve problems independently of the particular values of the variables, but networks tend to be tied to the range of values sampled in their training data. Large transformer-based language models have pushed the boundaries on how well neural networks can solve previously unseen problems, but their complexity and lack of clarity about the relevant content in their training data obfuscates how they achieve such robustness. As a step toward understanding how transformer-based systems generalize, we explore the question of OODG in small scale transformers trained with examples from a known distribution. Using a reasoning task based on the puzzle Sudoku, we show that OODG can occur on a complex problem if the training set includes examples sampled from the whole distribution of simpler component tasks. Successful generalization depends on carefully managing positional alignment when absolute position encoding is used, but we find that suppressing sensitivity to absolute positions overcomes this limitation. Taken together our results represent a small step toward understanding and promoting systematic generalization in transformers.",arxiv:2210.03275,Yes,,2025-11-11T00:14:11.169Z
advantagesanddisadva-2022,Advantages and disadvantages of (dedicated) model transformation languages,S. Höppner; Yves Haas; Matthias Tichy; Katharina Juhnke,2022,Empirical Software Engineering,120,https://www.semanticscholar.org/paper/d96fa397010fa107aadcedbff577feead334e3be,https://link.springer.com/content/pdf/10.1007/s10664-022-10194-7.pdf,10.1007/s10664-022-10194-7,"Model driven development envisages the use of model transformations to evolve models. Model transformation languages, developed for this task, are touted with many benefits over general purpose programming languages. However, a large number of these claims have not yet been substantiated. They are also made without the context necessary to be able to critically assess their merit or built meaningful empirical studies around them. The objective of our work is to elicit the reasoning, influences and background knowledge that lead people to assume benefits or drawbacks of model transformation languages. We conducted a large-scale interview study involving 56 participants from research and industry. Interviewees were presented with claims about model transformation languages and were asked to provide reasons for their assessment thereof. We qualitatively analysed the responses to find factors that influence the properties of model transformation languages as well as explanations as to how exactly they do so. Our interviews show, that general purpose expressiveness of GPLs, domain specific capabilities of MTLs as well as tooling all have strong influences on how people view properties of model transformation languages. Moreover, the Choice of MTL, the Use Case for which a transformation should be developed as well as the Skill s of involved stakeholders have a moderating effect on the influences, by changing the context to consider. There is a broad body of experience, that suggests positive and negative influences for properties of MTLs. Our data suggests, that much needs to be done in order to convey the viability of model transformation languages. Efforts to provide more empirical substance need to be undergone and lacklustre language capabilities and tooling need to be improved upon. We suggest several approaches for this that can be based on the results of the presented study.",arxiv:2201.13348,Yes,,2025-11-11T00:15:14.026Z
anapplicationofpseud-2022,An Application of Pseudo-Log-Likelihoods to Natural Language Scoring,Darren Abramson; Ali Emami,2022,arXiv.org,3,https://www.semanticscholar.org/paper/16bf88a6d172699cb9a26a6936efb4941e3f3c13,,,"Language models built using semi-supervised machine learning on large corpora of natural language have very quickly enveloped the fields of natural language generation and understanding. In this paper we apply a zero-shot approach independently developed by a number of researchers now gaining recognition as a significant alternative to fine-tuning for evaluation on common sense tasks. A language model with relatively few parameters and training steps compared to a more recent language model (T5) can outperform it on a recent large data set (TimeDial), while displaying robustness in its performance across a similar class of language tasks. Surprisingly, this result is achieved by using a hyperparameter-free zero-shot method with the smaller model, compared to fine-tuning to the larger model. We argue that robustness of the smaller model ought to be understood in terms of compositionality, in a sense that we draw from recent literature on a class of similar models. We identify a practical cost for our method and model: high GPU-time for natural language evaluation. The zero-shot measurement technique that produces remarkable stability, both for ALBERT and other BERT variants, is an application of pseudo-log-likelihoods to masked language models for the relative measurement of probability for substitution alternatives in forced choice language tasks such as the Winograd Schema Challenge, Winogrande, and others. One contribution of this paper is to bring together a number of similar, but independent strands of research. We produce some absolute state-of-the-art results for common sense reasoning in binary choice tasks, performing better than any published result in the literature, including fine-tuned efforts. We show a remarkable consistency of the model's performance under adversarial settings, which we argue is best explained by the model's compositionality of representations.",arxiv:2201.09377,Yes,,2025-11-11T00:15:14.026Z
anempiricalinvestiga-2022,An Empirical Investigation of Commonsense Self-Supervision with Knowledge Graphs,Jiarui Zhang; Filip Ilievski; Kaixin Ma; Jonathan M Francis; A. Oltramari,2022,arXiv.org,5,https://www.semanticscholar.org/paper/651ae53112e73b02440773727b68cedbf8322705,https://arxiv.org/pdf/2205.10661,10.48550/arXiv.2205.10661,"Self-supervision based on the information extracted from large knowledge graphs has been shown to improve the generalization of language models, in zero-shot evaluation on various downstream language reasoning tasks. Since these improvements are reported in aggregate, however, little is known about (i) how to select the appropriate knowledge for solid performance across tasks, (ii) how to combine this knowledge with neural language models, and (iii) how these pairings affect granular task performance. In this paper, we study the effect of knowledge sampling strategies and sizes that can be used to generate synthetic data for adapting language models. We study the effect of different synthetic datasets on language models with various architectures and sizes. The resulting models are evaluated against four task properties: domain overlap, answer similarity, vocabulary overlap, and answer length. Our experiments show that encoder-decoder models benefit from more data to learn from, whereas sampling strategies that balance across different aspects yield best performance. Most of the improvement occurs on questions with short answers and dissimilar answer candidates, which corresponds to the characteristics of the data used for pre-training.",arxiv:2205.10661,Yes,,2025-11-11T00:15:16.543Z
anexecutableformalmo-2022,An Executable Formal Model of the VHDL in Isabelle/HOL,Wilayat Khan; Zhé Hóu; David Sanán; J. Nebhen; Yang Liu; Alwen Tiu,2022,arXiv.org,4,https://www.semanticscholar.org/paper/37b0b6db785f8c37460e2bb80da138c1443af5b4,,,"In the hardware design process, hardware components are usually described in a hardware description language. Most of the hardware description languages, such as Verilog and VHDL, do not have mathematical foundation and hence are not fit for formal reasoning about the design. To enable formal reasoning in one of the most commonly used description language VHDL, we define a formal model of the VHDL language in Isabelle/HOL. Our model targets the functional part of VHDL designs used in industry, specifically the design of the LEON3 processor's integer unit. We cover a wide range of features in the VHDL language that are usually not modelled in the literature and define a novel operational semantics for it. Furthermore, our model can be exported to OCaml code for execution, turning the formal model into a VHDL simulator. We have tested our simulator against simple designs used in the literature, as well as the div32 module in the LEON3 design. The Isabelle/HOL code is publicly available: https://zhehou.github.io/apps/VHDLModel.zip",arxiv:2202.04192,Yes,,2025-11-11T00:15:14.026Z
anexperimentationfra-2022,An Experimentation Framework for Specification and Verification of Web Services,Szymon Katra; Wiktor B. Daszczuk; Danny Czejdo,2022,Conference on Computer Science and Information Systems,0,https://www.semanticscholar.org/paper/9fbe3dc7a2229a5435fc7ace6978550af5ac3268,https://annals-csis.org/proceedings/2022/drp/pdf/188.pdf,10.15439/2022F188,"Designing and implementing Web Services constitutes a large and constantly growing part of the information technology market. Web Services have specific scenarios in which distributed processes and network resources are used. This aspect of services requires integration with the model checkers. This article presents the experimentation framework in which services can be specified and then formally analyzed for deadlock-freedom, achievement of process goals, and similar features. Rybu4WS language enriches the basic Rybu language with the ability to use variables in processes, service calls between servers, new structural instructions, and other constructions known to programmers while remaining in line with declarative, mathematical IMDS formalism. Additionally, the development environment allows simulation of a counterexample or a witness - obtained as a result of the model checking - in a similar way to traditional debuggers.",,Yes,,2025-11-11T00:15:16.543Z
aninformationtheoret-2022,An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws,Hong Jun Jeon; Benjamin Van Roy,2022,arXiv.org,0,https://www.semanticscholar.org/paper/dab053b7713b77ab09f50b90b3176607912e913a,https://arxiv.org/pdf/2212.01365,10.48550/arXiv.2212.01365,"We study the compute-optimal trade-off between model and training data set sizes for large neural networks. Our result suggests a linear relation similar to that supported by the empirical analysis of chinchilla. While that work studies transformer-based large language models trained on the MassiveText corpus gopher, as a starting point for development of a mathematical theory, we focus on a simpler learning model and data generating process, each based on a neural network with a sigmoidal output unit and single hidden layer of ReLU activation units. We introduce general error upper bounds for a class of algorithms which incrementally update a statistic (for example gradient descent). For a particular learning model inspired by barron 1993, we establish an upper bound on the minimal information-theoretically achievable expected error as a function of model and data set sizes. We then derive allocations of computation that minimize this bound. We present empirical results which suggest that this approximation correctly identifies an asymptotic linear compute-optimal scaling. This approximation also generates new insights. Among other things, it suggests that, as the input dimension or latent space complexity grows, as might be the case for example if a longer history of tokens is taken as input to a language model, a larger fraction of the compute budget should be allocated to growing the learning model rather than training data.",arxiv:2212.01365,Yes,,2025-11-11T00:15:16.541Z
anoverviewofvadaloga-2022,An Overview of Vadalog: a System for Reasoning over Large Knowledge Graphs,Luigi Bellomarini; Davide Benedetto; Emanuel Sallinger,2022,Sistemi Evoluti per Basi di Dati,1,https://www.semanticscholar.org/paper/83dc0eca1a453e2970d32923bb48bb84976bd968,,,,,Yes,,2025-11-11T00:13:07.427Z
analisiskesulitanbel-2022,Analisis Kesulitan Belajar Matematika dalam Menyelesaikan Soal Cerita di Kelas IV Sekolah Dasar Negeri Pakujaya 02,Aam Amaliyah; Luthfia Nur Maulida; N. Safitri; Ratri Hersita Dewi; Sabgi Wulan Septiara,2022,ALSYS,0,https://www.semanticscholar.org/paper/5023bebd78bb5f55a0d706f94b27f718b9c83cfc,https://ejournal.yasin-alsys.org/index.php/alsys/article/download/386/301,10.58578/alsys.v2i3.386,"Students with learning difficulties in mathematics often make mistakes in solving story problems on fractional material. This research uses descriptive qualitative. The purpose of this study was to determine the types of learning difficulties in mathematics experienced by students, the factors that influence learning difficulties, and to reveal the efforts that can be made to overcome the difficulties in learning mathematics in grade IV Pakujaya 02 State Elementary School. Data collection techniques were observation and interviews. . Based on data analysis and discussion, students experienced errors, namely: 1. Understanding the problem, namely errors in interpreting language and making mathematical models. The reason is incomplete/wrong reasoning and low student ability. 2. Planning for problem solving is an error in connecting one concept with another concept. The cause of this error is the humanistic thinking of students. 3. Implement problem solving planning, namely errors in implementing incorrect formulas. Errors in this aspect are caused by incomplete or incorrect reasoning and students' humanistic thinking.",,Yes,,2025-11-11T00:15:16.543Z
analysisofstudentsma-2022,Analysis of Students’ Mathematical Thinking Ability in Terms of Self Efficacy,Adiba Idlal Shidqiya; Sukestiyarno Sukestiyarno,2022,Unnes Journal of Mathematics Education,1,https://www.semanticscholar.org/paper/fa5b5d97f15b5244e34a49e44317a1822b3e0daa,https://doi.org/10.15294/ujme.v11i3.58772,10.15294/ujme.v11i3.58772,"Mathematical thinking ability must be owned by students to solve various problems. Students are considered capable of fulfilling the indicators of mathematical thinking ability properly if they are balanced with good self-efficacy abilities. This research method is qualitative which aims to find new indicators and describe mathematical thinking ability in terms of self-efficacy and provide recommendations for teachers. The research subjects were six students from the first year of senior high school using purposive sampling. Indicators of mathematical thinking ability, include 1) Reasoning: identifying concepts and problems; 2) Generalizing: demonstrating mathematical ideas in writing and using mathematical language to express ideas correctly; 3) Critical Thinking: using representations to create mathematical models; 4) Problem Solving: planning problem solving strategies, implementing and checking results. 5) Communicating: revealing the results of problem solving. The results: 1) low self-efficacy’s students were only able to master reasoning; 2) moderate self-efficacy’s students are able to master reasoning, generalizing, and critical thinking; 3) high self-efficacy’s students are able to master all indicators. Recommendations for teachers are by giving opportunity to low self-efficacy’s students to speak in public, give appreciation for their efforts and reprimand if it doesn’t lower their confidence when they make mistakes.",,Yes,,2025-11-11T00:14:11.169Z
analysisofthecorrela-2022,Analysis of the Correlation between Academic Performance and Learning Motivation in English Course under a Corpus-Data-Driven Blended Teaching Model,Lan Yu; Jun Shen,2022,Scientific Programming,19,https://www.semanticscholar.org/paper/6f554d023d8e403e5ee70268e55f5b2fe1be574e,https://doi.org/10.1155/2022/3407270,10.1155/2022/3407270,"To explore the correlation between academic performance and learning motivation in English course under a corpus-data-driven blended teaching model, this study set research objects as 62 year-2020-enrolled undergraduate students majoring in English from a university in Jinan City, Shandong Province, eastern China. According to their previous frequencies of using information technology to learn English, these 62 students were divided into two groups: practice group with high frequency and control group with low frequency, with 31 students in each group. The two groups of students were taught 3 English lessons per week for a total of 15 weeks by the exact same teachers using a corpus-data-driven blended teaching model. The students’ English academic performances were assessed by well-organized final tests, and their English learning motivations were measured by a motivation scale and questionnaires. The results show that the correlation coefficients between the average score of motivation questionnaires, intrinsic motivation factors, extrinsic motivation factors, and the average score of academic performances in practice group were 0.894, 0.682, and 0.724, respectively, while those in control group were 0.749, 0.836, and 0.904. In all the above correlation analyses, the significance level is 0.01, and all coefficient values are higher than critical value. Hence, there is a positive correlation between learning motivation and academic performance of the two groups of subjects. It is found that the corpus-data-driven blended teaching model has a significant impact on college students’ English academic performance and learning motivation, and it has a positive effect on the improvement of their English academic performance and the cultivation of learning motivation. In general, the key to this teaching model lies in reasoning and acquisition by analyzing the language provided by the corpus, and the whole process of data-driven learning is student-centered. Students are exposed to a large number of authentic language knowledge and cultural information, which promotes the sensitivity to relevant points. The results of this paper provide a reference for further research on the analysis of the correlation between academic performance and learning motivation in English course under the corpus-data-driven blended teaching model.",,Yes,,2025-11-11T00:15:16.543Z
answerlevelcalibrati-2022,Answer-level Calibration for Free-form Multiple Choice Question Answering,Sawan Kumar,2022,Annual Meeting of the Association for Computational Linguistics,23,https://www.semanticscholar.org/paper/a5584d2d9b0de9e1692241d46d0c70942919cd60,https://aclanthology.org/2022.acl-long.49.pdf,10.18653/v1/2022.acl-long.49,"Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks. This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem. While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration. In this work, we consider the question answering format, where we need to choose from a set of (free-form) textual choices of unspecified lengths given a context. We present ALC (Answer-Level Calibration), where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context. We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks. Further, we show that popular datasets potentially favor models biased towards easy cues which are available independent of the context. We analyze such biases using an associated F1-score. Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability.",,Yes,,2025-11-11T00:15:14.026Z
applicationofthreefl-2022,Application of Three-Flow Fusion Technology Based on Modelica in Thermal Power Digital Twin,Dongyan Zhou; Haidong Gao; Wenyu Wang; Jun Cao; Wenfei Yang; Ruirui Zeng; Yuan He,2022,IEEE Journal of Radio Frequency Identification,5,https://www.semanticscholar.org/paper/5391cf3bf8f2cc858ee1a532be7d9e2e6b6f6983,,10.1109/JRFID.2022.3205855,"Thermal power plants gather large energy infrastructure; therefore, massive historical and real-time data of equipment operation will be generated in daily operation. Digital industrialization puts forward higher requirements for the use of big data than simple tasks, such as generating reports. MWorks is a multidomain unified modeling and simulation platform based on the Modelica language. In this study, MWorks is used to realize the modeling of multidomain systems, including electrical, thermal, mechanical, fluid, and heat transfer, in power plants. The test, calibration, verification, parameter optimization, and fault diagnosis of the thermal power plant mathematical models, which are historical and real-time data-driven, are discussed. The technology of three-flow fusion, including material flow, energy flow, and information flow, and its application in thermal power digital twin are explored.",,Yes,,2025-11-11T00:15:16.543Z
armathadatasetforsol-2022,ArMATH: a Dataset for Solving Arabic Math Word Problems,Reem Alghamdi; Zhenwen Liang; Xiangliang Zhang,2022,International Conference on Language Resources and Evaluation,13,https://www.semanticscholar.org/paper/4aca69be58a271b1be45ec7ebb3586569cec50b0,,,,,Yes,,2025-11-11T00:15:14.026Z
arggenpromptingtextg-2022,ArgGen: Prompting Text Generation Models for Document-Level Event-Argument Aggregation,Debanjana Kar; S. Sarkar; Pawan Goyal,2022,AACL/IJCNLP,2,https://www.semanticscholar.org/paper/61f49465c0d53663ad5264c8f683c6724d31eef1,,10.18653/v1/2022.findings-aacl.37,"Most of the existing discourse-level Information Extraction tasks have been modeled to be extractive in nature. However, we argue that extracting information from larger bodies of discourse-like documents requires more natural language understanding and reasoning capabilities. In our work, we propose the novel task of document-level event argument aggregation which generates consolidated event-arguments at a document-level with minimal loss of information. More specifically, we focus on generating precise document-level information frames in a multilingual setting using prompt-based methods. In this paper, we show the effectiveness of prompt-based text generation approach to generate document-level argument spans in a low-resource and zero-shot setting. We also release the first of its kind multilingual event argument aggregation dataset that can be lever-aged in other related multilingual text generation tasks as well: https://github.com/",,Yes,,2025-11-11T00:15:19.325Z
artificialintelligen-2022,Artificial Intelligence in the American Healthcare Industry: Looking Forward to 2030,F. Tewes,2022,Journal of Medical Research and Surgery,0,https://www.semanticscholar.org/paper/6baa97e2ca007eb2eeb51490f604d2bfd767fa0c,https://respubjournals.com/medical-research-surgery/pdf/Artificial-Intelligence-in-the-American-Healthcare-Industry-Looking-Forward-to-2030.pdf,10.52916/jmrs224089,"Artificial intelligence (AI) has the potential to speed up the exponential growth of cutting-edge technology, much way the Internet did. Due to intense competition from the private sector, governments, and businesspeople around the world, the Internet has already reached its peak as an exponential technology. In contrast, artificial intelligence is still in its infancy, and people all over the world are unsure of how it will impact their lives in the future. Artificial intelligence, is a field of technology that enables robots and computer programmes to mimic human intellect by teaching a predetermined set of software rules to learn by repetitive learning from experience and slowly moving toward maximum performance. Although this intelligence is still developing, it has already demonstrated five different levels of independence. Utilized initially to resolve issues. Next, think about solutions. Third, respond to inquiries. Fourth, use data analytics to generate forecasts. Fifth, make tactical recommendations. Massive data sets and ""iterative algorithms,"" which use lookup tables and other data structures like stacks and queues to solve issues, make all of this possible. Iteration is a strategy where software rules are regularly adjusted to patterns in the data for a certain number of iterations. The artificial intelligence continuously makes small, incremental improvements that result in exponential growth, which enables the computer to become incredibly proficient at whatever it is trained to do. For each round of data processing, the artificial intelligence tests and measures its performance to develop new expertise. In order to address complicated problems, artificial intelligence aims to create computer systems that can mimic human behavior and exhibit human-like thought processes [1]. Artificial intelligence technology is being developed to give individualized medication in the field of healthcare. By 2030, six different artificial intelligence sectors will have considerably improved healthcare delivery through the utilization of larger, more accessible data sets. The first is machine learning. This area of artificial intelligence learns automatically and produces improved results based on identifying patterns in the data, gaining new insights, and enhancing the outcomes of whatever activity the system is intended to accomplish. It does this without being trained to learn a particular topic. Here are several instances of machine learning in the healthcare industry. The first is the IBM Watson Genomics, which aids in rapid disease diagnosis and identification by fusing cognitive computing with genome-based tumour sequencing. Second, a project called Nave Bayes allows for the prediction of diabetes years before an official diagnosis, before it results in harm to the kidneys, the heart, and the nerves. Third, employing two machine learning approaches termed classification and clustering to analyse the Indian Liver Patient Data (ILPD) set in order to predict liver illness before this organ that regulates metabolism becomes susceptible to chronic hepatitis, liver cancer, and cirrhosis [2]. Second, deep learning. Deep learning employs artificial intelligence to learn from data processing, much like machine learning does. Deep learning, on the other hand, makes use of synthetic neural networks that mimic human brain function to analyse data, identify relationships between the data, and provide outputs based on positive and negative reinforcement. For instance, in the fields of Magnetic Resonance Imaging (MRI) and Computed Tomography (CT), deep learning aids in the processes of picture recognition and object detection. Deep learning algorithms for the early identification of Alzheimer's, diabetic retinopathy, and breast nodule ultrasound detection are three applications of this cutting-edge technology in the real world. Future developments in deep learning will make considerable improvements in pathology and radiology pictures [3]. Third, neural networks. The artificial intelligence system can now accept massive data sets, find patterns within the data, and respond to queries regarding the information processed because the computer learning process resembles a network of neurons in the human brain. Let's examine a few application examples that are now applicable to the healthcare sector. According to studies from John Hopkins University, surgical errors are a major contributor to medical malpractice claims since they happen more than 4,000 times a year in just the United States due to the human error of surgeons. Neural networks can be used in robot-assisted surgery to model and plan procedures, evaluate the abilities of the surgeon, and streamline surgical activities. In one study of 379 orthopaedic patients, it was discovered that robotic surgery using neural networks results in five times fewer complications than surgery performed by a single surgeon. Another application of neural networks is in visualising diagnostics, which was proven to physicians by Harvard University researchers who inserted an image of a gorilla to x-rays. Of the radiologists who saw the images, 83% did not recognise the gorilla. The Houston Medical Research Institute has created a breast cancer early detection programme that can analyse mammograms with 99 percent accuracy and offer diagnostic information 30 times faster than a human [4]. Cognitive computing is the fourth. Aims to replicate the way people and machines interact, showing how a computer may operate like the human brain when handling challenging tasks like text, speech, or image analysis. Large volumes of patient data have been analysed, with the majority of the research to date focusing on cancer, diabetes, and cardiovascular disease. Companies like Google, IBM, Facebook, and Apple have shown interest in this work. Cognitive computing made up the greatest component of the artificial market in 2020, with 39% of the total [5]. Hospitals made up 42% of the market for cognitive computing end users because of the rising demand for individualised medical data. IBM invested more than $1 billion on the development of the WATSON analytics platform ecosystem and collaboration with startups committed to creating various cloud and application-based systems for the healthcare business in 2014 because it predicted the demand for cognitive computing in this sector. Natural Language Processing (NLP) is the fifth. This area of artificial intelligence enables computers to comprehend and analyse spoken language. The initial phase of this pre-processing is to divide the data up into more manageable semantic units, which merely makes the information simpler for the NLP system to understand. Clinical trial development is experiencing exponential expansion in the healthcare sector thanks to NLP. First, the NLP uses speech-to-text dictation and structured data entry to extract clinical data at the point of care, reducing the need for manual assessment of complex clinical paperwork. Second, using NLP technology, healthcare professionals can automatically examine enormous amounts of unstructured clinical and patient data to select the most suitable patients for clinical trials, perhaps leading to an improvement in the patients' health [6]. Computer vision comes in sixth. Computer vision, an essential part of artificial intelligence, uses visual data as input to process photos and videos continuously in order to get better results faster and with higher quality than would be possible if the same job were done manually. Simply put, doctors can now diagnose their patients with diseases like cancer, diabetes, and cardiovascular disorders more quickly and at an earlier stage. Here are a few examples of real-world applications where computer vision technology is making notable strides. Mammogram images are analysed by visual systems that are intended to spot breast cancer at an early stage. Automated cell counting is another example from the real world that dramatically decreases human error and raises concerns about the accuracy of the results because they might differ greatly depending on the examiner's experience and degree of focus. A third application of computer vision in the real world is the quick and painless early-stage tumour detection enabled by artificial intelligence. Without a doubt, computer vision has the unfathomable potential to significantly enhance how healthcare is delivered. Other than for visual data analysis, clinicians can use this technology to enhance their training and skill development. Currently, Gramener is the top company offering medical facilities and research organisations computer vision solutions [7]. The usage of imperative rather than functional programming languages is one of the key difficulties in creating artificial intelligence software. As artificial intelligence starts to increase exponentially, developers employing imperative programming languages must assume that the machine is stupid and supply detailed instructions that are subject to a high level of maintenance and human error. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures of research and development. As a result, software developers have contributed to the unreasonably high cost of medical care. Functional programming languages, on the other hand, demand that the developer use their problem-solving abilities as though the computer were a mathematician. As a result, compared to the number of lines of code needed by the programme to perform the same operation, mathematical functions are orders of magnitude shorter. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures o",,Yes,,2025-11-11T00:15:19.325Z
assessingphysicsquan-2022,Assessing physics quantitative literacy in algebra-based physics: lessons learned,Charlotte Zimmerman; Andrew McCarty; Suzanne White Brahmia; Alexis Olsho; Mieke De Cock; A. Boudreaux; Trevor I. Smith; Philip Eaton,2022,Physics Education Research Conference Proceedings,0,https://www.semanticscholar.org/paper/8f16d42771af2c1227c7a4cf6ad219e54351c9f7,https://www.per-central.org/items/perc/5655.pdf,10.1119/perc.2022.pr.zimmerman,"Physics quantitative literacy (PQL)—applying familiar mathematics in novel ways in the context of physics— is ubiquitous across physics classrooms. The Physics Inventory for Quantitative Literacy, or PIQL, is a recently published reasoning inventory that can be used to assess PQL from calculus-based introductory physics through upper division courses (White Brahmia et al. 2021). There remains a need, however, for assessment of quantitative reasoning at the algebra-based level which includes not only algebra-based college courses but also pre-college physics courses. We present recent work adapting the PIQL to an algebra-based context towards developing the GERQN—the Generalized Equation-based Reasoning inventory for Quantities and Negativity. We report lessons learned from our efforts to adapt items from the calculus-based PIQL to the algebra-based GERQN, and provide examples of how items were revised to be within students proximal zone. We also report on our experience translating the GERQN into Flemish as part of a larger, on-going research project, and what we learned about language accessibility for native and non-native English speakers alike for developing assessment items, curricular materials, and when speaking with students.",,Yes,,2025-11-11T00:15:19.325Z
assessingtheacademic-2022,Assessing the Academic Recovery of Ohio Students: An Analysis of Spring 2022 Ohio State Tests,Vladimir Kogan,2022,,0,https://www.semanticscholar.org/paper/76ac1af061d5d2ccf19d748ca8b744a9461260f3,,,,,Yes,,2025-11-11T00:15:19.325Z
attributedtextgenera-2022,Attributed Text Generation via Post-hoc Research and Revision,Luyu Gao; Zhuyun Dai; Panupong Pasupat; Anthony Chen; Arun Tejasvi Chaganty; Yicheng Fan; Vincent Zhao; N. Lao; Hongrae Lee; Da-Cheng Juan; Kelvin Guu,2022,arXiv.org,26,https://www.semanticscholar.org/paper/4ef5410ec4b546eda642fe786cc1bdbb5a7251e1,http://arxiv.org/pdf/2210.08726,10.48550/arXiv.2210.08726,,,Yes,,2025-11-11T00:15:14.026Z
autoformalizationfor-2022,Autoformalization for Neural Theorem Proving,Yuhuai Wu; Albert Qiaochu Jiang; Wenda Li; M. Rabe; Charles Staats; M. Jamnik; Christian Szegedy,2022,,1,https://www.semanticscholar.org/paper/15e767aa26a14455da95a2b2f11e3d59f2c250f6,,,,,Yes,,2025-11-11T00:15:16.543Z
autoformalizationwit-2022,Autoformalization with Large Language Models,Yuhuai Wu; Albert Qiaochu Jiang; Wenda Li; M. Rabe; Charles Staats; M. Jamnik; Christian Szegedy,2022,Neural Information Processing Systems,220,https://www.semanticscholar.org/paper/c28e95a06dfcf13fc65a1cac83722f53e34f12a5,https://arxiv.org/pdf/2205.12615,10.48550/arXiv.2205.12615,"Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from $29.6\%$ to $35.2\%$.",arxiv:2205.12615,Yes,,2025-11-11T00:13:07.427Z
automaticchainofthou-2022,Automatic Chain of Thought Prompting in Large Language Models,Zhuosheng Zhang; Aston Zhang; Mu Li; Alexander J. Smola,2022,International Conference on Learning Representations,779,https://www.semanticscholar.org/paper/90350aa626bed47b02d0c162462e5b0ca82be6b2,,,"Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like""Let's think step by step""to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the""Let's think step by step""prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot",arxiv:2210.03493,Yes,,2025-11-11T00:13:07.427Z
automaticgenerationo-2022,Automatic Generation of Socratic Subquestions for Teaching Math Word Problems,Kumar Shridhar; Jakub Macina; Mennatallah El-Assady; Tanmay Sinha; Manu Kapur; Mrinmaya Sachan,2022,Conference on Empirical Methods in Natural Language Processing,53,https://www.semanticscholar.org/paper/e6745fb621481ccb0ed53c267a37292e499c1b42,https://arxiv.org/pdf/2211.12835,10.48550/arXiv.2211.12835,"Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers.In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning.On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.",arxiv:2211.12835,Yes,,2025-11-11T00:15:14.026Z
auxiliaryteachingsys-2022,Auxiliary Teaching System of Higher Mathematics Based on Random Matrix Model,Yabin Xiao; Bing Zhou; Dan-ni He; Jingzhong Liu,2022,Mathematical Problems in Engineering,2,https://www.semanticscholar.org/paper/869011d58c272450dc8cf95bd4f81601e17b8511,https://downloads.hindawi.com/journals/mpe/2022/7983989.pdf,10.1155/2022/7983989,"With the development of computer technology, computers have become a part of people’s lives and the Internet has connected the world’s networks as a whole. Computer technology is changing people’s study, life, and work. People’s traditional education mode, thinking, content, method, and talent training program have a significant impact. The development from traditional to computer technology-based teaching methods has brought new developments and leaps in educational technology. This paper analyzes the research background, significance, and research status of the advanced mathematics auxiliary teaching system, introduces the related technologies and development modes used in the development of the system, and especially discusses the access database technology by ADO and the mathematical expression based on MathML language. Secondly, starting from the actual teaching, we analyze the functional requirements and performance requirements of the system in detail and make detailed planning and design for the system architecture, database selection, functional modules, etc. The design and implementation process of this teaching system are summarized. The teaching strategy inference engine is the key to the personalization and intelligence of the ICAI system. According to the learning models provided by different students, the system designs a corresponding teaching sequence for the learners by controlling the meta-knowledge of the domain knowledge base. The teaching strategy inference engine cuts the domain knowledge tree, selects the knowledge points suitable for the student, and sorts the selected knowledge points reasonably to generate an optimal teaching sequence. According to the students’ learning situation, combined with the teaching rules in the teaching rule library, the students’ grades are dynamically adjusted, so as to select new learning content for students and provide teaching suggestions in time. The student model is the premise of the ICAI system to achieve individualization and intelligence. The system makes a comprehensive evaluation and diagnosis of students through fuzzy comprehensive evaluation and fuzzy reasoning. On this basis, a cognitive student model is established, which is the teaching strategy that provided the basis for the formulation.",,Yes,,2025-11-11T00:15:14.026Z
bevbertmultimodalmap-2022,BEVBert: Multimodal Map Pre-training for Language-guided Navigation,Dongyan An; Yuankai Qi; Yangguang Li; Yan Huang; Liangsheng Wang; T. Tan; Jing Shao,2022,,94,https://www.semanticscholar.org/paper/d7abc3bcf368c7c0e3487da7cecae1ac209a7284,,,"Large-scale pre-training has shown promising results on the vision-and-language navigation (VLN) task. However, most existing pre-training methods employ discrete panoramas to learn visual-textual associations. This requires the model to implicitly correlate incomplete, duplicate observations within the panoramas, which may impair an agent's spatial understanding. Thus, we propose a new map-based pre-training paradigm that is spatial-aware for use in VLN. Concretely, we build a local metric map to explicitly aggregate incomplete observations and remove duplicates, while modeling navigation dependency in a global topological map. This hybrid design can balance the demand of VLN for both short-term reasoning and long-term planning. Then, based on the hybrid map, we devise a pre-training framework to learn a multimodal map representation, which enhances spatial-aware cross-modal reasoning thereby facilitating the language-guided navigation goal. Extensive experiments demonstrate the effectiveness of the map-based pre-training route for VLN, and the proposed method achieves state-of-the-art on four VLN benchmarks.",arxiv:2212.04385,Yes,,2025-11-11T00:14:11.169Z
btpkbasedlearningani-2022,BTPK-based learning: An Interpretable Method for Named Entity Recognition,Yulin Chen; Zelai Yao; Haixiao Chi; D. Gabbay; Bo Yuan; Bruno Bentzen; Beishui Liao,2022,arXiv.org,2,https://www.semanticscholar.org/paper/811151315ac5fefb1629a4d02c0274370db468a7,,,,,Yes,,2025-11-11T00:15:16.543Z
bayesvarbrulaunified-2022,BayesVarbrul: a unified multidimensional analysis of language change in a speaker community,Xia Hua,2022,Journal of Language Evolution,4,https://www.semanticscholar.org/paper/c294f2479c50d70b8e6b32b630e197aea1d9309d,,10.1093/jole/lzac004,"
 Exchange in ideas between language evolution and biological evolution has a long history, due to a shared theoretical foundation between language and biology as two evolving systems. Both systems evolve in terms of the frequency of a variant in a population for each of a large number of variables, that is how often a particular variant of a language variable is used in a speaker community and how many individuals in a biological population carry a particular variant of a gene. The way these frequencies change has been modelled under a similar mathematical framework. Here, I show how we can use concepts from genome wide association studies that identify the source of natural selection and the genes under selection in a biological population to study how social factors affect the usage of language variables in a speaker community or how some social groups use some language variables differently from other groups. Using the Gurindji Kriol language as a case study, I show how this approach unifies existing mathematical and statistical tools in studying language evolution over a large number of speakers and a large number of language variables, which provides a promising link between micro- and macro-evolution in language. The approach is named BayesVarbrul and is ready to apply to datasets other than the Gurindji Kriol dataset, including existing corpus data. The code and the instructions are available at https://github.com/huaxia1985/BayesVarbrul.",,Yes,,2025-11-11T00:15:14.026Z
benchmarkinggpt3forc-2022,Benchmarking GPT-3 For Closed-Book QA: Strengths and Weaknesses,Chenglei Si; Naman Molri; Gurmehar Cheema; Elliot Huang; Arjun Akkiraju,2022,,1,https://www.semanticscholar.org/paper/b9a0bc80aa136027327697fe40189792a32c8b0c,,,,,Yes,,2025-11-11T00:15:16.543Z
benchmarkinglargesca-2022,Benchmarking Large-Scale ACOPF Solutions and Optimality Bounds,S. Gopinath; H. Hijazi,2022,IEEE Power & Energy Society General Meeting,12,https://www.semanticscholar.org/paper/dea559bde46a1b9efc64c4418eebb3e9f3b775b7,https://arxiv.org/pdf/2203.11328,10.1109/PESGM48719.2022.9916662,"We present the results of a comprehensive bench-marking effort aimed at evaluating and comparing state-of-the-art open-source tools for solving the Alternating-Current Optimal Power Flow (ACOPF) problem. Our numerical experiments include all instances found in the public library PGLIB with network sizes up to 30,000 nodes. The benchmarked tools span a number of programming languages (Python, Julia, Matlab/Octave, and C++), nonlinear optimization solvers (Ipopt, MIPS, and INLP) as well as different mathematical modeling tools (JuMP and Gravity). We also present state-of-the-art optimality bounds obtained using sparsity-exploiting semidefinite programming approaches and corresponding computational times.",arxiv:2203.11328,Yes,,2025-11-11T00:15:14.026Z
benchmarkingspatialr-2022,Benchmarking Spatial Relationships in Text-to-Image Generation,Tejas Gokhale; Hamid Palangi; Besmira Nushi; Vibhav Vineet; E. Horvitz; Ece Kamar; Chitta Baral; Yezhou Yang,2022,arXiv.org,82,https://www.semanticscholar.org/paper/4bf77d64b860ed0cd84a63aecd92a3cb295b88ee,http://arxiv.org/pdf/2212.10015,10.48550/arXiv.2212.10015,"Spatial understanding is a fundamental aspect of computer vision and integral for human-level reasoning about images, making it an important component for grounded language understanding. While recent text-to-image synthesis (T2I) models have shown unprecedented improvements in photorealism, it is unclear whether they have reliable spatial understanding capabilities. We investigate the ability of T2I models to generate correct spatial relationships among objects and present VISOR, an evaluation metric that captures how accurately the spatial relationship described in text is generated in the image. To benchmark existing models, we introduce a dataset, $\mathrm{SR}_{2D}$, that contains sentences describing two or more objects and the spatial relationships between them. We construct an automated evaluation pipeline to recognize objects and their spatial relationships, and employ it in a large-scale evaluation of T2I models. Our experiments reveal a surprising finding that, although state-of-the-art T2I models exhibit high image quality, they are severely limited in their ability to generate multiple objects or the specified spatial relations between them. Our analyses demonstrate several biases and artifacts of T2I models such as the difficulty with generating multiple objects, a bias towards generating the first object mentioned, spatially inconsistent outputs for equivalent relationships, and a correlation between object co-occurrence and spatial understanding capabilities. We conduct a human study that shows the alignment between VISOR and human judgement about spatial understanding. We offer the $\mathrm{SR}_{2D}$ dataset and the VISOR metric to the community in support of T2I reasoning research.",arxiv:2212.10015,Yes,,2025-11-11T00:15:14.026Z
beyondtheimitationga-2022,Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models,Aarohi Srivastava; Abhinav Rastogi; Abhishek Rao; Abu Awal Md Shoeb; Abubakar Abid; Adam Fisch; Adam R. Brown; Adam Santoro; Aditya Gupta; Adrià Garriga-Alonso; Agnieszka Kluska; Aitor Lewkowycz; Akshat Agarwal; Alethea Power; Alex Ray; Alex Warstadt; Alexander W. Kocurek; Ali Safaya; Ali Tazarv; Alice Xiang; Alicia Parrish; Allen Nie; Aman Hussain; Amanda Askell; A. Dsouza; Ambrose Slone; Ameet Rahane; Anantharaman S. Iyer; Anders Andreassen; Andrea Madotto; Andrea Santilli; Andreas Stuhlmuller; Andrew M. Dai; A. La; Andrew Kyle Lampinen; Andy Zou; Angela Jiang; Angelica Chen; Anh Vuong; Animesh Gupta; Anna Gottardi; Antonio Norelli; Anu Venkatesh; Arash Gholamidavoodi; A. Tabassum; Arul Menezes; Arun Kirubarajan; A. Mullokandov; Ashish Sabharwal; Austin Herrick; Avia Efrat; Aykut Erdem; Ayla Karakacs; B. R. Roberts; B. S. Loe; Barret Zoph; Bartlomiej Bojanowski; Batuhan Ozyurt; Behnam Hedayatnia; Behnam Neyshabur; Benjamin Inden; Benno Stein; Berk Ekmekci; Bill Yuchen Lin; B. Howald; Bryan Orinion; Cameron Diao; Cameron Dour; Catherine Stinson; Cedrick Argueta; C'esar Ferri Ram'irez; Chandan Singh; Charles Rathkopf; Chenlin Meng; Chitta Baral; Chiyu Wu; Chris Callison-Burch; Chris Waites; Christian Voigt; Christopher D. Manning; Christopher Potts; Cindy Ramirez; Clara E. Rivera; Clemencia Siro; Colin Raffel; Courtney Ashcraft; Cristina Garbacea; Damien Sileo; Dan Garrette; Dan Hendrycks; D. Kilman; Dan Roth; Daniel Freeman; Daniel Khashabi; Daniel Levy; D. Gonz'alez; Danielle R. Perszyk; Danny Hernandez; Danqi Chen; Daphne Ippolito; Dar Gilboa; David Dohan; D. Drakard; David Jurgens; Debajyoti Datta; Deep Ganguli; Denis Emelin; Denis Kleyko; Deniz Yuret; Derek Chen; Derek Tam; Dieuwke Hupkes; Diganta Misra; Dilyar Buzan; Dimitri Coelho Mollo; Diyi Yang; Dong-Ho Lee; Dylan Schrader; Ekaterina Shutova; E. D. Cubuk; Elad Segal; Eleanor Hagerman; Elizabeth Barnes; Elizabeth Donoway; Ellie Pavlick; Emanuele Rodolà; Emma Lam; Eric Chu; Eric Tang; Erkut Erdem; Ernie Chang; Ethan A. Chi; Ethan Dyer; E. Jerzak; Ethan Kim; Eunice Engefu Manyasi; Evgenii Zheltonozhskii; Fanyue Xia; Fatemeh Siar; Fernando Mart'inez-Plumed; Francesca Happ'e; François Chollet; Frieda Rong; Gaurav Mishra; Genta Indra Winata; Gerard de Melo; Germán Kruszewski; Giambattista Parascandolo; Giorgio Mariani; Gloria Xinyue Wang; Gonzalo Jaimovitch-L'opez; Gregor Betz; Guy Gur-Ari; Hana Galijasevic; Hannah Kim; Hannah Rashkin; Hannaneh Hajishirzi; Harsh Mehta; H. Bogar; Henry Shevlin; Hinrich Schutze; H. Yakura; Hongming Zhang; Hugh Mee Wong; Ian Ng; Isaac Noble; Jaap Jumelet; Jack Geissinger; John Kernion; Jacob Hilton; Jaehoon Lee; J. Fisac; James B. Simon; James Koppel; James Zheng; James Zou; Jan Koco'n; Jana Thompson; Janelle Wingfield; Jared Kaplan; Jarema Radom; Jascha Narain Sohl-Dickstein; Jason Phang; Jason Wei; J. Yosinski; Jekaterina Novikova; Jelle Bosscher; Jennifer Marsh; Jeremy Kim; Jeroen Taal; Jesse Engel; Jesujoba Oluwadara Alabi; Jiacheng Xu; Jiaming Song; Jillian Tang; Jane W Waweru; John Burden; John Miller; John U. Balis; Jonathan Batchelder; Jonathan Berant; Jorg Frohberg; Jos Rozen; J. Hernández-Orallo; Joseph Boudeman; J. Guerr; Joseph Jones; Joshua B. Tenenbaum; Joshua S. Rule; Joyce Chua; Kamil Kanclerz; Karen Livescu; K. Krauth; Karthik Gopalakrishnan; Katerina Ignatyeva; K. Markert; Kaustubh D. Dhole; Kevin Gimpel; Kevin Omondi; K. Mathewson; Kristen Chiafullo; Ksenia Shkaruta; Kumar Shridhar; Kyle McDonell; Kyle Richardson; Laria Reynolds; Leo Gao; Li Zhang; Liam Dugan; Lianhui Qin; Lidia Contreras-Ochando; Louis-philippe Morency; Luca Moschella; Luca Lam; Lucy Noble; Ludwig Schmidt; Luheng He; Luis Oliveros Col'on; Luke Metz; Lutfi Kerem cSenel; Maarten Bosma; Maarten Sap; Maartje ter Hoeve; Maheen Farooqi; Manaal Faruqui; Mantas Mazeika; Marco Baturan; Marco Marelli; Marco Maru; Maria Jose Ram’irez Quintana; M. Tolkiehn; Mario Giulianelli; Martha Lewis; Martin Potthast; Matthew L. Leavitt; Matthias Hagen; M. Schubert; Medina Baitemirova; Melody Arnaud; M. McElrath; Michael A. Yee; Michael Cohen; Michael Gu; Michael Ivanitskiy; Michael Starritt; M. Strube; Michal Swkedrowski; Michele Bevilacqua; Michihiro Yasunaga; Mihir Kale; Mike Cain; Mimee Xu; Mirac Suzgun; Mitch Walker; Monica Tiwari; Mohit Bansal; Moin Aminnaseri; Mor Geva; Mozhdeh Gheini; T. MukundVarma; Nanyun Peng; Nathan A. Chi; Nayeon Lee; Neta Gur-Ari Krakover; Nicholas Cameron; Nicholas Roberts; Nick Doiron; Nicole Martinez; Nikita Nangia; Niklas Deckers; Niklas Muennighoff; N. Keskar; Niveditha Iyer; Noah Constant; Noah Fiedel; Nuan Wen; Oliver Zhang; Omar Agha; Omar Elbaghdadi; Omer Levy; Owain Evans; Pablo Antonio Moreno Casares; P. Doshi; Pascale Fung; Paul Pu Liang; Paul Vicol; Pegah Alipoormolabashi; Peiyuan Liao; Percy Liang; Peter Chang; P. Eckersley; Phu Mon Htut; P. Hwang; P. Milkowski; P. Patil; Pouya Pezeshkpour; Priti Oli; Qiaozhu Mei; Qing Lyu; Qinlang Chen; Rabin Banjade; Rachel Etta Rudolph; Raefer Gabriel; Rahel Habacker; Ramon Risco; Raphael Milliere; Rhythm Garg; Richard Barnes; R. Saurous; Riku Arakawa; Robbe Raymaekers; Robert Frank; Rohan Sikand; Roman Novak; Roman Sitelew; Ronan Le Bras; Rosanne Liu; Rowan Jacobs; Rui Zhang; R. Salakhutdinov; Ryan Chi; Ryan Lee; Ryan Stovall; R. Teehan; Rylan Yang; Sahib Singh; Saif Mohammad; Sajant Anand; Sam Dillavou; Sam Shleifer; Sam Wiseman; Samuel Gruetter; Samuel R. Bowman; S. Schoenholz; Sanghyun Han; Sanjeev Kwatra; Sarah A. Rous; Sarik Ghazarian; Sayan Ghosh; Sean Casey; Sebastian Bischoff; Sebastian Gehrmann; Sebastian Schuster; Sepideh Sadeghi; Shadi S. Hamdan; Sharon Zhou; Shashank Srivastava; Sherry Shi; Shikhar Singh; Shima Asaadi; S. Gu; Shubh Pachchigar; Shubham Toshniwal; Shyam Upadhyay; Shyamolima Debnath; Siamak Shakeri; Simon Thormeyer; S. Melzi; Siva Reddy; S. Makini; Soo-Hwan Lee; Spencer Bradley Torene; Sriharsha Hatwar; S. Dehaene; Stefan Divic; Stefano Ermon; Stella Biderman; Stephanie Lin; Stephen Prasad; Steven T Piantadosi; Stuart M. Shieber; Summer Misherghi; S. Kiritchenko; Swaroop Mishra; Tal Linzen; Tal Schuster; Tao Li; Tao Yu; Tariq Ali; Tatsunori Hashimoto; Te-Lin Wu; T. Desbordes; Theodore Rothschild; Thomas Phan; Tianle Wang; Tiberius Nkinyili; Timo Schick; T. Kornev; T. Tunduny; Tobias Gerstenberg; T. Chang; Trishala Neeraj; Tushar Khot; Tyler Shultz; Uri Shaham; Vedant Misra; Vera Demberg; Victoria Nyamai; Vikas Raunak; V. Ramasesh; Vinay Uday Prabhu; Vishakh Padmakumar; Vivek Srikumar; W. Fedus; W. Saunders; William Zhang; Wout Vossen; Xiang Ren; Xiaoyu Tong; Xinran Zhao; Xinyi Wu; Xudong Shen; Yadollah Yaghoobzadeh; Yair Lakretz; Yangqiu Song; Yasaman Bahri; Yejin Choi; Yichi Yang; Yiding Hao; Yifu Chen; Yonatan Belinkov; Yu Hou; Yu Hou; Yuntao Bai; Zachary Seid; Zhuoye Zhao; Zijian Wang; Zijie J. Wang; Zirui Wang; Ziyi Wu,2022,arXiv.org,2020,https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881,,,"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit""breakthrough""behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.",arxiv:2206.04615,Yes,,2025-11-11T00:14:11.169Z
blankcollapsecompres-2022,Blank Collapse: Compressing CTC emission for the faster decoding,Minkyu Jung; Ohhyeok Kwon; S. Seo; Soonshin Seo,2022,Interspeech,3,https://www.semanticscholar.org/paper/6498d95d3f988e684bc6a70004decbefec655222,http://arxiv.org/pdf/2210.17017,10.48550/arXiv.2210.17017,"Connectionist Temporal Classification (CTC) model is a very efficient method for modeling sequences, especially for speech data. In order to use CTC model as an Automatic Speech Recognition (ASR) task, the beam search decoding with an external language model like n-gram LM is necessary to obtain reasonable results. In this paper we analyze the blank label in CTC beam search deeply and propose a very simple method to reduce the amount of calculation resulting in faster beam search decoding speed. With this method, we can get up to 78% faster decoding speed than ordinary beam search decoding with a very small loss of accuracy in LibriSpeech datasets. We prove this method is effective not only practically by experiments but also theoretically by mathematical reasoning. We also observe that this reduction is more obvious if the accuracy of the model is higher.",arxiv:2210.17017,Yes,,2025-11-11T00:15:14.026Z
bridgingthegapbetwee-2022,Bridging the Gap between Recognition-level Pre-training and Commonsensical Vision-language Tasks,Yue Wan; Yueen Ma; Haoxuan You; Zhecan Wang; Shih-Fu Chang,2022,CSRR,1,https://www.semanticscholar.org/paper/ac13afe6c5f456a1f250e03e3258f5cfecc99373,https://aclanthology.org/2022.csrr-1.4.pdf,10.18653/v1/2022.csrr-1.4,"Large-scale visual-linguistic pre-training aims to capture the generic representations from multimodal features, which are essential for downstream vision-language tasks. Existing methods mostly focus on learning the semantic connections between visual objects and linguistic content, which tend to be recognitionlevel information and may not be sufficient for commonsensical reasoning tasks like VCR. In this paper, we propose a novel commonsensical vision-language pre-training framework to bridge the gap. We first augment the conventional image-caption pre-training datasets with commonsense inferences from a visuallinguistic GPT-2. To pre-train models on image, caption and commonsense inferences together, we propose two new tasks: masked commonsense modeling (MCM) and commonsense type prediction (CTP). To reduce the shortcut effect between captions and commonsense inferences, we further introduce the domain-wise adaptive masking that dynamically adjusts the masking ratio. Experimental results on downstream tasks, VCR and VQA, show the improvement of our pre-training strategy over previous methods. Human evaluation also validates the relevance, informativeness, and diversity of the generated commonsense inferences. Overall, we demonstrate the potential of incorporating commonsense knowledge into the conventional recognition-level visual-linguistic pre-training.",,Yes,,2025-11-11T00:15:14.026Z
coherenceevaluationo-2022,C OHERENCE E VALUATION OF V ISUAL C ONCEPTS WITH O BJECTS AND L ANGUAGE,Tobias Leemann; Yao Rong; Stefan Kraft; Enkelejda Kasneci; Gjergji Kasneci,2022,,0,https://www.semanticscholar.org/paper/75e3f61b69dc6bc8bfb7fd28aa1001edbbc8eab4,,,,,Yes,,2025-11-11T00:15:16.543Z
capecorrectiveaction-2022,CAPE: Corrective Actions from Precondition Errors using Large Language Models,S. S. Raman; Vanya Cohen; Eric Rosen; Ifrah Idrees; D. Paulius; Stefanie Tellex,2022,IEEE International Conference on Robotics and Automation,47,https://www.semanticscholar.org/paper/c82d8d80ea68400adb7faebb2f1cff38dd83093a,,10.1109/ICRA57147.2024.10611376,"Extracting knowledge and reasoning from large language models (LLMs) offers a path to designing intelligent robots. Common approaches that leverage LLMs for planning are unable to recover when actions fail and resort to retrying failed actions without resolving the underlying cause. We propose a novel approach (CAPE) that generates corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans through few-shot reasoning on action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while maintaining semantic correctness and minimizing re-prompting. In VirtualHome, CAPE improves a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan, whilst achieving competitive executability. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves correctness by 76.49% with higher executability compared to SayCan. Our approach enables embodied agents to follow natural language commands and robustly recover from failures.",arxiv:2211.09935,Yes,,2025-11-11T00:13:07.427Z
clevrmathadatasetfor-2022,"CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning",Adam Dahlgren Lindström; Savitha Sam Abraham,2022,International Workshop on Neural-Symbolic Learning and Reasoning,80,https://www.semanticscholar.org/paper/74cc3d340039c67bdabaef090d1386fe2c5376ca,http://arxiv.org/pdf/2208.05358,10.48550/arXiv.2208.05358,"We introduce CLEVR-Math, a multi-modal math word problems dataset consisting of simple math word problems involving addition/subtraction, represented partly by a textual description and partly by an image illustrating the scenario. The text describes actions performed on the scene that is depicted in the image. Since the question posed may not be about the scene in the image, but about the state of the scene before or after the actions are applied, the solver envision or imagine the state changes due to these actions. Solving these word problems requires a combination of language, visual and mathematical reasoning. We apply state-of-the-art neural and neuro-symbolic models for visual question answering on CLEVR-Math and empirically evaluate their performances. Our results show how neither method generalise to chains of operations. We discuss the limitations of the two in addressing the task of multi-modal word problem solving.",arxiv:2208.05358,Yes,,2025-11-11T00:13:07.427Z
corrpusdetectingstor-2022,CORRPUS: Detecting Story Inconsistencies via Codex-Bootstrapped Neurosymbolic Reasoning,Yi Dong; Lara J. Martin; Chris Callison-Burch,2022,arXiv.org,2,https://www.semanticscholar.org/paper/4bea09d4c897fb201c032b9eb605a943b1e70435,,10.48550/arXiv.2212.10754,,,Yes,,2025-11-11T00:14:11.169Z
crosscontaminationac-2022,CROSS-CONTAMINATION: ACCELERATING LARGE LANGUAGE MODELS WITHOUT IMPACTING PERFORMANCE,M. M. Krell; Matej Kosec,2022,,76,https://www.semanticscholar.org/paper/85cac89ba01a07f3dbf6dbb1e0c56067a3105714,,,,,Yes,,2025-11-11T00:13:07.427Z
cupcurriculumlearnin-2022,CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction,Jiaju Lin; Qin Chen; Jie Zhou; Jian Jin; Liangye He,2022,International Joint Conference on Artificial Intelligence,25,https://www.semanticscholar.org/paper/65d88194a902332b78dd5a7b919fa577bfa7ee9f,https://arxiv.org/pdf/2205.00498,10.48550/arXiv.2205.00498,"Implicit event argument extraction (EAE) aims to identify arguments that could scatter over the document. Most previous work focuses on learning the direct relations between arguments and the given trigger, while the implicit relations with long-range dependency are not well studied. Moreover, recent neural network based approaches rely on a large amount of labeled data for training, which is unavailable due to the high labelling cost. In this paper, we propose a Curriculum learning based Prompt tuning (CUP) approach, which resolves implicit EAE by four learning stages. The stages are defined according to the relations with the trigger node in a semantic graph, which well captures the long-range dependency between arguments and the trigger. In addition, we integrate a prompt-based encoder-decoder model to elicit related knowledge from pre-trained language models (PLMs) in each stage, where the prompt templates are adapted with the learning progress to enhance the reasoning for arguments. Experimental results on two well-known benchmark datasets show the great advantages of our proposed approach. In particular, we outperform the state-of-the-art models in both fully-supervised and low-data scenarios.",arxiv:2205.00498,Yes,,2025-11-11T00:15:14.026Z
canfoundationmodelst-2022,Can Foundation Models Talk Causality?,Moritz Willig; M. Zecevic; D. Dhami; K. Kersting,2022,arXiv.org,31,https://www.semanticscholar.org/paper/6745381bfa99a3b979766cca05e91559f1b770e3,http://arxiv.org/pdf/2206.10591,10.48550/arXiv.2206.10591,"Foundation models are subject to an ongoing heated debate, leaving open the question of progress towards AGI and dividing the community into two camps: the ones who see the arguably impressive results as evidence to the scaling hypothesis, and the others who are worried about the lack of interpretability and reasoning capabilities. By investigating to which extent causal representations might be captured by these large scale language models, we make a humble efforts towards resolving the ongoing philosophical conflicts.",arxiv:2206.10591,Yes,,2025-11-11T00:15:14.026Z
canincontextlearners-2022,Can In-context Learners Learn a Reasoning Concept from Demonstrations?,Michal Tefnik; Marek Kadlcík,2022,NLRSE,7,https://www.semanticscholar.org/paper/e7cfc3362dd85b17c747e9f9636749696f87a88b,https://aclanthology.org/2023.nlrse-1.8.pdf,10.18653/v1/2023.nlrse-1.8,"Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations.However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input.However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models’ ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.To disentangle models’ in-context learning ability independent of models’ memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in few-shot demonstrations.We find that smaller models are more sensitive to the presented concepts. While some of the models are able to benefit from concept-presenting demonstrations for each assessed concept, we find that none of the assessed in-context learners can benefit from all presented reasoning concepts consistently, leaving the in-context concept learning an open challenge.",arxiv:2212.01692,Yes,,2025-11-11T00:14:11.169Z
canretrieveraugmente-2022,Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model,Parishad BehnamGhader; Santiago Miret; Siva Reddy,2022,Conference on Empirical Methods in Natural Language Processing,42,https://www.semanticscholar.org/paper/e4758d05c3d4231dd30c656330e156ccc9dbb07b,http://arxiv.org/pdf/2212.09146,10.48550/arXiv.2212.09146,"Augmenting pretrained language models with retrievers to select the supporting documents has shown promise in effectively solving common NLP problems, including language modeling and question answering, in an interpretable way. In this paper, we first study the strengths and weaknesses of different retriever-augmented language models (REALM, $k$NN-LM, FiD coupled with DPR, and ATLAS and Flan-T5 coupled with Contriever) in reasoning over the retrieved statements in different tasks. We show how the retrieve-then-read models' limitations in reasoning are rooted both in the retriever module as well as the language model. Our experimental results demonstrate that the similarity metric used by the retrievers is generally insufficient for reasoning tasks. Additionally, we show that the language models in retriever-augmented models do not take the complicated relations between the statements into account, which leads to poor reasoning performance even when using the larger models. Moreover, we analyze the reasoning performance of large language models using multihop retrieval but we only observe minor improvements. Overall, this shows great room for further research in this area.",arxiv:2212.09146,Yes,,2025-11-11T00:13:07.427Z
centralsubmonadsandn-2022,Central Submonads and Notions of Computation,T. Carette; Louis Lemonnier; V. Zamdzhiev,2022,arXiv.org,0,https://www.semanticscholar.org/paper/af36ab7fe4e10aad2e01be5dcde0784241742832,http://arxiv.org/pdf/2207.09190,10.48550/arXiv.2207.09190,,,Yes,,2025-11-11T00:15:19.325Z
chainofthoughtprompt-2022,Chain of Thought Prompting Elicits Reasoning in Large Language Models,Jason Wei; Xuezhi Wang; Dale Schuurmans; Maarten Bosma; Ed H. Chi; F. Xia; Quoc Le; Denny Zhou,2022,Neural Information Processing Systems,13165,https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5,,,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",arxiv:2201.11903,Yes,,2025-11-11T00:13:07.427Z
chartingthespaceofqu-2022,Charting the Space of Quantum Field Theories,,2022,,0,https://www.semanticscholar.org/paper/cb39717895b9fa4cd2cc59748d59e20ce5eb4521,,,,,Yes,,2025-11-11T00:15:19.325Z
chiqaalargescaleimag-2022,ChiQA: A Large Scale Image-based Real-World Question Answering Dataset for Multi-Modal Understanding,Bingning Wang; Feiya Lv; Ting Yao; Yiming Yuan; Jin Ma; Yu Luo; Haijin Liang,2022,International Conference on Information and Knowledge Management,3,https://www.semanticscholar.org/paper/730efc9d93a2b34f02a98aa46d9357f05111fd99,,10.1145/3511808.3557258,"Visual question answering is an important task in both natural language and vision understanding. However, in most of the public visual question answering datasets such as VQA, CLEVR, the questions are human generated that specific to the given image, such as 'What color are her eyes?'. The human generated crowdsourcing questions are relatively simple and sometimes have the bias toward certain entities or attributes [1, 55]. In this paper, we introduce a new question answering dataset based on image-ChiQA. It contains the real-world queries issued by internet users, combined with several related open-domain images. The system should determine whether the image could answer the question or not. Different from previous VQA datasets, the questions are real-world image-independent queries that are more various and unbiased. Compared with previous image-retrieval or image-caption datasets, the ChiQA not only measures the relatedness but also measures the answerability, which demands more fine-grained vision and language reasoning. ChiQA contains more than 40K questions and more than 200K question-images pairs. A three-level 2/1/0 label is assigned to each pair indicating perfect answer, partially answer and irrelevant. Data analysis shows ChiQA requires a deep understanding of both language and vision, including grounding, comparisons, and reading. We evaluate several state-of-the-art visual-language models such as ALBEF, demonstrating that there is still a large room for improvements on ChiQA.",arxiv:2208.03030,Yes,,2025-11-11T00:14:11.169Z
classificationofopen-2022,Classification of open-ended responses to a research-based assessment using natural language processing,Joseph Wilson; Benjamin Pollard; J. M. Aiken; Marcos D. Caballero; H. Lewandowski,2022,Physical Review Physics Education Research,24,https://www.semanticscholar.org/paper/932cb50f541c3141fabe156ecf3bbafb0aa61c29,http://link.aps.org/pdf/10.1103/PhysRevPhysEducRes.18.010141,10.1103/physrevphyseducres.18.010141,"Surveys have long been used in physics education research to understand student reasoning and inform course improvements. However, to make analysis of large sets of responses practical, most surveys use a closed-response format with a small set of potential responses. Open-ended formats, such as written free response, can provide deeper insights into student thinking, but take much longer to analyze, especially with a large number of responses. Here, we explore natural language processing as a computational solution to this problem. We create a machine learning model that can take student responses from the Physics Measurement Questionnaire as input, and output a categorization of student reasoning based on different reasoning paradigms. Our model yields classifications with the same level of agreement as that between two humans categorizing the data, but can be done by a computer, and thus can be scaled for large datasets. In this work, we describe the algorithms and methodologies used to create, train, and test our natural language processing system. We also present the results of the analysis and discuss the utility of these approaches for analyzing open-response data in education research. DOI: 10.1103/PhysRevPhysEducRes.18.010141",,Yes,,2025-11-11T00:14:11.169Z
corrpuscodebasedstru-2022,CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding,Yi Dong; Lara J. Martin; Chris Callison-Burch,2022,Annual Meeting of the Association for Computational Linguistics,9,https://www.semanticscholar.org/paper/76f54657eb0893a0b203da57dcf0b4fffeebfc2c,https://aclanthology.org/2023.findings-acl.832.pdf,10.18653/v1/2023.findings-acl.832,"Story generation and understanding -- as with all NLG/NLU tasks -- has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI Task 2 and Re^3) with minimal hand engineering. We hope that this work can help highlight the importance of symbolic representations and specialized prompting for LLMs as these models require some guidance for performing reasoning tasks properly.",arxiv:2212.10754,Yes,,2025-11-11T00:15:14.026Z
cosimcommonsensereas-2022,CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination,Hyounghun Kim; Abhaysinh Zala; Mohit Bansal,2022,North American Chapter of the Association for Computational Linguistics,6,https://www.semanticscholar.org/paper/153b51c7871f82c8966a8d744d3630ef791f00f4,http://arxiv.org/pdf/2207.03961,10.48550/arXiv.2207.03961,"As humans, we can modify our assumptions about a scene by imagining alternative objects or concepts in our minds. For example, we can easily anticipate the implications of the sun being overcast by rain clouds (e.g., the street will get wet) and accordingly prepare for that. In this paper, we introduce a new dataset called Commonsense Reasoning for Counterfactual Scene Imagination (CoSIm) which is designed to evaluate the ability of AI systems to reason about scene change imagination. To be specific, in this multimodal task/dataset, models are given an image and an initial question-response pair about the image. Next, a counterfactual imagined scene change (in textual form) is applied, and the model has to predict the new response to the initial question based on this scene change. We collect 3.5K high-quality and challenging data instances, with each instance consisting of an image, a commonsense question with a response, a description of a counterfactual change, a new response to the question, and three distractor responses. Our dataset contains various complex scene change types (such as object addition/removal/state change, event description, environment change, etc.) that require models to imagine many different scenarios and reason about the changed scenes. We present a baseline model based on a vision-language Transformer (i.e., LXMERT) and ablation studies. Through human evaluation, we demonstrate a large human-model performance gap, suggesting room for promising future work on this challenging, counterfactual multimodal task.",arxiv:2207.03961,Yes,,2025-11-11T00:15:14.026Z
codeaspolicieslangua-2022,Code as Policies: Language Model Programs for Embodied Control,Jacky Liang; Wenlong Huang; F. Xia; Peng Xu; Karol Hausman; Brian Ichter; Peter R. Florence; Andy Zeng,2022,IEEE International Conference on Robotics and Automation,1177,https://www.semanticscholar.org/paper/91deaf9d324c8feafc189da0da03e60a60287bca,https://arxiv.org/pdf/2209.07753,10.1109/ICRA48891.2023.10160591,"Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",arxiv:2209.07753,Yes,,2025-11-11T00:13:07.427Z
codequeriesadataseto-2022,CodeQueries: A Dataset of Semantic Queries over Code,Surya Prakash Sahu; Madhurima Mandal; Shikhar Bharadwaj; Aditya Kanade; Petros Maniatis; S. Shevade,2022,International Symposium on Electronic Commerce,8,https://www.semanticscholar.org/paper/cd937849a314b3e5eb4862a3b55aa823811a5996,https://dl.acm.org/doi/pdf/10.1145/3641399.3641408,10.1145/3641399.3641408,"Developers often have questions about semantic aspects of code they are working on, e.g., “Is there a class whose parent classes declare a conflicting attribute?”. Answering them requires understanding code semantics such as attributes and inheritance relation of classes. An answer to such a question should identify code spans constituting the answer (e.g., the declaration of the subclass) as well as supporting facts (e.g., the definitions of the conflicting attributes). The existing work on question-answering over code has considered yes/no questions or method-level context. We contribute a labeled dataset, called CodeQueries, of semantic queries over Python code. Compared to the existing datasets, in CodeQueries, the queries are about code semantics, the context is file level and the answers are code spans. We curate the dataset based on queries supported by a widely-used static analysis tool, CodeQL, and include both positive and negative examples, and queries requiring single-hop and multi-hop reasoning. To assess the value of our dataset, we evaluate baseline neural approaches. We study a large language model (GPT3.5-Turbo) in zero-shot and few-shot settings on a subset of CodeQueries. We also evaluate a BERT style model (CuBERT) with fine-tuning. We find that these models achieve limited success on CodeQueries. CodeQueries is thus a challenging dataset to test the ability of neural models, to understand code semantics, in the extractive question-answering setting.",arxiv:2209.08372,Yes,,2025-11-11T00:15:19.325Z
collaborativereasoni-2022,Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation,Xueliang Zhao; Yuxuan Wang; Chongyang Tao; Chenshuo Wang; Dongyan Zhao,2022,Conference on Empirical Methods in Natural Language Processing,8,https://www.semanticscholar.org/paper/256fd60c692ebe12fe2bbf65d46722f511aa3117,http://arxiv.org/pdf/2210.12460,10.48550/arXiv.2210.12460,"We study video-grounded dialogue generation, where a response is generated based on the dialogue context and the associated video. The primary challenges of this task lie in (1) the difficulty of integrating video data into pre-trained language models (PLMs) which presents obstacles to exploiting the power of large-scale pre-training; and (2) the necessity of taking into account the complementarity of various modalities throughout the reasoning process. Although having made remarkable progress in video-grounded dialogue generation, existing methods still fall short when it comes to integrating with PLMs in a way that allows information from different modalities to complement each other. To alleviate these issues, we first propose extracting pertinent information from videos and turning it into reasoning paths that are acceptable to PLMs. Additionally, we propose a multi-agent reinforcement learning method to collaboratively perform reasoning on different modalities (i.e., video and dialogue context). Empirical experiment results on two public datasets indicate that the proposed model can significantly outperform state-of-the-art models by large margins on both automatic and human evaluations.",arxiv:2210.12460,Yes,,2025-11-11T00:14:11.169Z
combininglocalandglo-2022,Combining local and global approaches to ascertain semantic similarity,Shahrukh Gouhar; Anupam Misra; Radha Rathore; Mansoor Ali Shaik; Dr. Subhasis Dasgupta,2022,2022 IEEE India Council International Subsections Conference (INDISCON),0,https://www.semanticscholar.org/paper/1f9ae8a3f6be60d10e9d1d3eeecc0a8fda0404b2,,10.1109/INDISCON54605.2022.9862898,"Interviewing potential candidates is both time consuming and resource intensive. This is particularly prominent in organizations which go for large scale recruitment processes. In the current study, a client based application has been proposed for interviewing candidates for data science profiles where interviewee’s answers are scored using machine learning. Different approaches were tried with pretrained models but as the application was very much domain specific, those models did not provide good results. Hence a custom embedding layer was built on open source data science textbooks. These embeddings were used with Gated Recurrent Units (GRU) to capture a local approach (subject specific) in the interview answers. However, this neglected the nuances of the English language involved in critical reasoning. Hence Bi-directional Encoder Representation from Transformers (BERT) was employed to capture the global approach (interaction between words in the English language) in the interview answers. The similarity scores from these two approaches were ensembled into a machine learning model which allotted the final score to the interviewee’s answer. The proposed method outperformed the pretrained models with significant margin when tested with the validation data.",,Yes,,2025-11-11T00:15:16.543Z
commonsensereasoning-2022,Commonsense Reasoning for Conversational AI: A Survey of Recent Datasets and Benchmarks,Alon Albalak; Varun R. Embar; Yi-Lin Tuan; L. Getoor; Ankur Bapna; Gokhan Tur; Dilek Hakkani-Tur; Larry Heck. 2017; Lisa Bauer; Yicheng Wang; Mohit Bansal; Antoine Bosselut; Hannah Rashkin; Maarten Sap; Chaitanya Malaviya; Asli Celikyilmaz; Yejin Choi; Yulong Chen; Y. Liu; Liang Chen; Leyang Cui; Yu Wu; Shujie Liu; Yue-Feng Zhang; J. Devlin; Ming-Wei Chang; Kenton Lee; Leilei Gan; Yating Zhang; Kun Kuang; Shuo Lin Yuan; Changlong Li; Xiaozhong Sun; Liu Fei; Wu; R. Speer; Joshua Chin; Catherine Havasi; Kai Sun; Dian Yu; Jianshu Chen; Dong Yu; Jai Desai; Aaron Wade; Haoran Li; Asli Celikyil-879 maz; Yashar Mehdad; Dragomir R. Radev; Geng Tu; Ji-Rong Wen; Cheng Liu; Dazhi Jiang; A. Stolcke; Lynn Voss; Dilek Peters; John Hakkani-Tur; Benoit Dowding; Raquel Favre; Matthew Fernández; Mike Frampton; Ashish Vaswani; Noam M. Shazeer; Niki Parmar; Llion Uszkoreit; Aidan N Jones; Łukasz Gomez; Kaiser Illia; Polosukhin. 2017; Attention; S. Welleck; Jason Weston; Arthur Szlam,2022,,2,https://www.semanticscholar.org/paper/4e37589fb896d1578ba4282f40c20708079ae8e5,,,,,Yes,,2025-11-11T00:14:11.169Z
complementaryexplana-2022,Complementary Explanations for Effective In-Context Learning,Xi Ye; Srini Iyer; Asli Celikyilmaz; Ves Stoyanov; Greg Durrett; Ramakanth Pasunuru,2022,Annual Meeting of the Association for Computational Linguistics,108,https://www.semanticscholar.org/paper/097dc73d5d422b3c09286e72d16b2561ae5fb395,http://arxiv.org/pdf/2211.13892,10.48550/arXiv.2211.13892,"Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.",arxiv:2211.13892,Yes,,2025-11-11T00:15:14.026Z
complexitybasedpromp-2022,Complexity-Based Prompting for Multi-Step Reasoning,Yao Fu; Hao-Chun Peng; Ashish Sabharwal; Peter Clark; Tushar Khot,2022,International Conference on Learning Representations,517,https://www.semanticscholar.org/paper/c88cafa3e980765a64febe369ceb7c2aa7261d2a,http://arxiv.org/pdf/2210.00720,10.48550/arXiv.2210.00720,"We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",arxiv:2210.00720,Yes,,2025-11-11T00:13:07.427Z
composingensemblesof-2022,Composing Ensembles of Pre-trained Models via Iterative Consensus,Shuang Li; Yilun Du; J. Tenenbaum; A. Torralba; Igor Mordatch,2022,International Conference on Learning Representations,27,https://www.semanticscholar.org/paper/f3a13abf23afecf534c955954d70c3b0fc41d334,http://arxiv.org/pdf/2210.11522,10.48550/arXiv.2210.11522,"Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information, while vision models such as DALL-E can generate photorealistic photos but fail to understand complex language descriptions. In this work, we propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as""generators""or""scorers""and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5%, without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation. Project page: https://energy-based-model.github.io/composing-pretrained-models.",arxiv:2210.11522,Yes,,2025-11-11T00:15:14.026Z
comprehensiveeventre-2022,Comprehensive Event Representations using Event Knowledge Graphs and Natural Language Processing,Tin Kuculo,2022,The Web Conference,2,https://www.semanticscholar.org/paper/7fcb917cd4b6d0c77c41b4a1b1dc0c4d1965ba7b,https://dl.acm.org/doi/pdf/10.1145/3487553.3524199,10.1145/3487553.3524199,"Recent work has utilised knowledge-aware approaches to natural language understanding, question answering, recommendation systems, and other tasks. These approaches rely on well-constructed and large-scale knowledge graphs that can be useful for many downstream applications and empower knowledge-aware models with commonsense reasoning. Such knowledge graphs are constructed through knowledge acquisition tasks such as relation extraction and knowledge graph completion. This work seeks to utilise and build on the growing body of work that uses findings from the field of natural language processing (NLP) to extract knowledge from text and build knowledge graphs. The focus of this research project is on how we can use transformer-based approaches to extract and contextualise event information, matching it to existing ontologies, to build comprehensive knowledge graph-based event representations. Specifically, sub-event extraction is used as a way of creating sub-event-aware event representations. These event representations are then further enriched through fine-grained location extraction and contextualised through the alignment of historically relevant quotes.",arxiv:2303.04794,Yes,,2025-11-11T00:15:14.026Z
computationalexperim-2022,"Computational experiment – nondimensionalization of equations, computational stability and program testing",M. G. Evtikhov; V. G. Evtikhov,2022,Radioelectronics Nanosystems Information Technologies,0,https://www.semanticscholar.org/paper/094d38867c42533f3d61f76b92c0c3b82f54e3fb,https://doi.org/10.17725/rensit.2022.14.331,10.17725/rensit.2022.14.331,"From the stages of the computational experiment, the stage of non-dimensionalization of the initial equation (system of equations) of the problem is considered - the replacement of its variables by the product of the corresponding dimensionless quantities by their units of measurement with subsequent transformations. Such a transition from a physical model to a mathematical (dimensionless) one makes it possible to obtain software implementations for research. A critical evaluation of its complexity is carried out and possible errors in the results are evaluated. At the same time, new versions of software are formed. Object-oriented programming tools and version control systems (for example, git) allow you to create versions of software tools adapted to different conditions of their use and for different types of users. Parallelization of work on versions is carried out. At the same time, for further software implementation, the set-theoretic language of formulas with partially recursive functions is effective. To implement versions with large amounts of calculations and data, high-performance computing systems based on software and hardware acceleration, parallel information processing and cloud architectures are used. As a rule, a difference model of the problem and iterative methods for solving it are constructed for a program version. Computational stability conditions are usually stipulated in modern instructions for standard program libraries. For new algorithms, it is necessary to analyze the stability of difference schemes based on the refinement of their spectral properties and the use of functional analysis methods. For storage and subsequent application of the results of computational experiments, it is advisable to use modern databases. As a kind of computational experiment, testing of alpha and beta versions of programs and their releases is also considered.",,Yes,,2025-11-11T00:15:16.543Z
computersimulationof-2022,Computer Simulation of Intelligent Control Systems for High-Precision Cruise Missiles,Moldamurat Khuralay; Akhmetov Kayrat Telektesovich; Otegen Alikhan Serikovich; Brimzhanova Saule Serikovna; Otyzbayeva Karlygash Zhalenovna; Zhiyenbek Arailym Oteulievna,2022,2022 International Conference on Smart Information Systems and Technologies (SIST),2,https://www.semanticscholar.org/paper/84b3bd3fbcf52e2aa7c2595a6880586f12ce8f59,,10.1109/SIST54437.2022.9945703,,,Yes,,2025-11-11T00:15:16.543Z
computerverifiedfoun-2022,Computer-Verified Foundations of Metaphysics and an Ontology of Natural Numbers in Isabelle/HOL,,2022,,3,https://www.semanticscholar.org/paper/02ad77812348e299794d5bc83a999090a7ee2139,,,,,Yes,,2025-11-11T00:15:19.325Z
constructvldatafreec-2022,ConStruct-VL: Data-Free Continual Structured VL Concepts Learning*,James Smith; Paola Cascante-Bonilla; Assaf Arbelle; Donghyun Kim; Rameswar Panda; David D. Cox; Diyi Yang; Z. Kira; R. Feris; Leonid Karlinsky,2022,Computer Vision and Pattern Recognition,24,https://www.semanticscholar.org/paper/6b3e939d93c82c269f552e7e2050524c3ad9b73b,http://arxiv.org/pdf/2211.09790,10.1109/CVPR52729.2023.01440,"Recently, large-scale pre-trained Vision-and-Language (VL) foundation models have demonstrated remarkable capabilities in many zero-shot downstream tasks, achieving competitive results for recognizing objects defined by as little as short text prompts. However, it has also been shown that VL models are still brittle in Structured VL Concept (SVLC) reasoning, such as the ability to recognize object attributes, states, and inter-object relations. This leads to reasoning mistakes, which need to be corrected as they occur by teaching VL models the missing SVLC skills; often this must be done using private data where the issue was found, which naturally leads to a data-free continual (no task-id) VL learning setting. In this work, we introduce the first Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark11Our code is publicly available at https://github.com/jamessealesmith/ConStruct-VL and show it is challenging for many existing data-free CL strategies. We, therefore, propose a data-free method comprised of a new approach of Adversarial Pseudo-Replay (APR) which generates adversarial reminders of past tasks from past task models. To use this method efficiently, we also propose a continual parameter-efficient Layered-LoRA (LaLo) neural architecture allowing no-memory-cost access to all past models at train time. We show this approach outperforms all data-free methods by as much as ~ 7% while even matching some levels of experience-replay (prohibitive for applications where data-privacy must be preserved).",arxiv:2211.09790,Yes,,2025-11-11T00:15:14.026Z
concurrentnetkatmode-2022,"Concurrent NetKAT: Modeling and analyzing stateful, concurrent networks",J. Wagemaker; Nate Foster; Tobias Kapp'e; D. Kozen; J. Rot; Alexandra Silva,2022,European Symposium on Programming,7,https://www.semanticscholar.org/paper/eb5a72315f84c7234ca6697d327de571f96ffb28,https://link.springer.com/content/pdf/10.1007/978-3-030-99336-8_21.pdf,10.1007/978-3-030-99336-8_21,"We introduce Concurrent NetKAT (CNetKAT), an extension of NetKAT with operators for specifying and reasoning about concurrency in scenarios where multiple packets interact through state. We provide a model of the language based on partially-ordered multisets (pomsets), which are a well-established mathematical structure for defining the denotational semantics of concurrent languages. We provide a sound and complete axiomatization of this model, and we illustrate the use of CNetKAT through examples. More generally, CNetKAT can be understood as an algebraic framework for reasoning about programs with both local state (in packets) and global state (in a global store).",arxiv:2201.10485,Yes,,2025-11-11T00:15:16.541Z
connectingsymbolicfr-2022,Connecting symbolic fractions to their underlying proportions using iterative partitioning.,M. Hurst; Jacob R Butts; S. Levine,2022,Developmental Psychology,3,https://www.semanticscholar.org/paper/60e87d430f117b616c456cf8f1955926036f128a,,10.1037/dev0001384,"Fractions are a challenging mathematics topic for many elementary and middle school students, and even for adults. However, a growing body of developmental research suggests that young children can reason about visually presented proportions, well before fraction instruction, providing insight into how fractions might be introduced to improve learning. We designed a card game to teach first and second grade children (N = 195, including a racially and economically diverse sample from the United States) about fractions in one of three ways. In the Actively Divided condition we iteratively divided an area model into equal-sized units, in the Predivided condition we used an area model with the end-state of the Actively Divided condition, and in the Nondivided condition we used a continuous representation of the fraction magnitude that was not divided into unit-sized parts. Children in the actively divided condition demonstrated larger improvements matching symbolic fractions and visual fractions (i.e., pie charts) than children in the other two conditions. Posthoc analyses of children's gameplay revealed that the actively divided condition may have provided a more optimal level of difficulty for young children than the predivided condition, which was particularly difficult, and the nondivided condition, which was trivially easy. These differences in gameplay performance provide insights into possible mechanisms for our results. We discuss open research questions highlighted by this work and implications of these findings for both the development of proportional reasoning and fraction learning. (PsycInfo Database Record (c) 2022 APA, all rights reserved).",,Yes,,2025-11-11T00:15:19.325Z
constructingarabicre-2022,Constructing Arabic Reading Comprehension Datasets: Arabic WikiReading and KaifLematha,Eman Albilali; Nora Al-Twairesh; M. Hosny,2022,Language Resources and Evaluation,8,https://www.semanticscholar.org/paper/169f4557b4b9909c82eb1fb5e621c68763ddec2d,,10.1007/s10579-022-09577-5,,,Yes,,2025-11-11T00:15:16.543Z
constructionofgoodne-2022,Construction of goodness-of-fit criteria for the type of impulse response function,I. Rozora; A. Melnyk,2022,Science Technology and Innovation,1,https://www.semanticscholar.org/paper/d01bb60413ff14940ec6af9fdf0a1868d1d2acef,,10.35668/2520-6524-2022-2-07,"The article is devoted to the study of the impulse response function, its estimation and properties, square-Gaussian random variables and processes, the rate of convergence of the unknown impulse response function, testing the hypothesis about the type of impulse response function, building a simulation model. The study showed that the pulse response function is the output signal of the system during signal processing, when the input signal is a short pulse. In a more general form, the impulse response function describes the response or output of the system as a function of time. Also, the impulse response function is considered a property of linear displacement systems. During the study of the estimation of the impulse response function on orthonormal and trigonometric bases, two conditions A, B and remarks to them were formed, which are used in the future to find different coefficients. The study of square-Gaussian random variables and processes has shown the benefits of using them in relation to the impulse response function. A theorem was also presented, which estimated the probability of a large deviation of the square-Gaussian process in the norm of a continuous function. To study the rate of convergence of the unknown impulse response function in the space of continuous functions and in the space L2, a lemma was formed, as well as a theorem that directly showed the rate of convergence of the impulse response function in the space of continuous functions. Zero and alternative hypotheses were formed. The null hypothesis claimed that the impulse response function existed, and the alternative hypothesis suggested the opposite. To test the hypothesis about the form of the impulse response function, a theorem was used by which a criterion was formed. Visual Studio Community 2022 integrated development environment (C ++ programming language) and Wolfram Mathematica computer algebra system for analytical transformations and numerical calculations were used to build the simulation model, which allowed to make mathematical calculations quite accurately.",,Yes,,2025-11-11T00:15:19.325Z
continuallearningon3-2022,Continual learning on 3D point clouds with random compressed rehearsal,M. Zamorski; Michal Stypulkowski; Konrad Karanowski; Tomasz Trzciński; Maciej Ziȩba,2022,Computer Vision and Image Understanding,13,https://www.semanticscholar.org/paper/0fbb0c96dd9bf65e1af877a377bb4c63767906b8,https://arxiv.org/pdf/2205.08013,10.48550/arXiv.2205.08013,"Contemporary deep neural networks offer state-of-the-art results when applied to visual reasoning, e.g., in the context of 3D point cloud data. Point clouds are important datatype for precise modeling of three-dimensional environments, but effective processing of this type of data proves to be challenging. In the world of large, heavily-parameterized network architectures and continuously-streamed data, there is an increasing need for machine learning models that can be trained on additional data. Unfortunately, currently available models cannot fully leverage training on additional data without losing their past knowledge. Combating this phenomenon, called catastrophic forgetting, is one of the main objectives of continual learning. Continual learning for deep neural networks has been an active field of research, primarily in 2D computer vision, natural language processing, reinforcement learning, and robotics. However, in 3D computer vision, there are hardly any continual learning solutions specifically designed to take advantage of point cloud structure. This work proposes a novel neural network architecture capable of continual learning on 3D point cloud data. We utilize point cloud structure properties for preserving a heavily compressed set of past data. By using rehearsal and reconstruction as regularization methods of the learning process, our approach achieves a significant decrease of catastrophic forgetting compared to the existing solutions on several most popular point cloud datasets considering two continual learning settings: when a task is known beforehand, and in the challenging scenario of when task information is unknown to the model.",arxiv:2205.08013,Yes,,2025-11-11T00:15:16.541Z
contrastivelanguagei-2022,Contrastive Language-Image Pre-Training with Knowledge Graphs,Xuran Pan; Tianzhu Ye; Dongchen Han; S. Song; Gao Huang,2022,Neural Information Processing Systems,60,https://www.semanticscholar.org/paper/b3d8233b1d1368ccfe691f3a0cc80d5874439198,http://arxiv.org/pdf/2210.08901,10.48550/arXiv.2210.08901,"Recent years have witnessed the fast development of large-scale pre-training frameworks that can extract multi-modal representations in a unified form and achieve promising performances when transferred to downstream tasks. Nevertheless, existing approaches mainly focus on pre-training with simple image-text pairs, while neglecting the semantic connections between concepts from different modalities. In this paper, we propose a knowledge-based pre-training framework, dubbed Knowledge-CLIP, which injects semantic information into the widely used CLIP model. Through introducing knowledge-based objectives in the pre-training process and utilizing different types of knowledge graphs as training data, our model can semantically align the representations in vision and language with higher quality, and enhance the reasoning ability across scenarios and modalities. Extensive experiments on various vision-language downstream tasks demonstrate the effectiveness of Knowledge-CLIP compared with the original CLIP and competitive baselines.",arxiv:2210.08901,Yes,,2025-11-11T00:14:11.169Z
convfinqaexploringth-2022,ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering,Zhiyu Chen; SHIYANG LI; Charese Smiley; Zhiqiang Ma; Sameena Shah; William Yang Wang,2022,Conference on Empirical Methods in Natural Language Processing,161,https://www.semanticscholar.org/paper/d96997265f8146e93b4c9350f19d55e46d1317f0,http://arxiv.org/pdf/2210.03849,10.48550/arXiv.2210.03849,"With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/ConvFinQA.",arxiv:2210.03849,Yes,,2025-11-11T00:13:07.427Z
correctiontothelangu-2022,Correction to: “The language of Dirac’s theory of radiation”: the inception and initial reception of a tool for the quantum field theorist,Markus Ehberger,2022,Archive for History of Exact Sciences,3,https://www.semanticscholar.org/paper/5bf8381ea5755c176bdd7e3d7cf576554ed7d697,https://link.springer.com/content/pdf/10.1007/s00407-022-00293-8.pdf,10.1007/s00407-022-00293-8,"In 1927, Paul Dirac first explicitly introduced the idea that electrodynamical processes can be evaluated by decomposing them into virtual (modern terminology), energy non-conserving subprocesses. This mode of reasoning structured a lot of the perturbative evaluations of quantum electrodynamics during the 1930s. Although the physical picture connected to Feynman diagrams is no longer based on energy non-conserving transitions but on off-shell particles, emission and absorption subprocesses still remain their fundamental constituents. This article will access the introduction and the initial reception of this picture of subsequent transitions (PST) by conceiving of concepts, models, and their representations as tools for the practitioners. I will argue for a multi-factorial explanation of Dirac’s initial, verbally explicit introduction: the mathematical representation he had developed was highly suggestive and already partly conceptualized; Dirac was philosophical flexible enough to talk about transitions when no actual transitions, according to the general interpretation of quantum mechanics of the time, occurred; and, importantly, Dirac eventually used the verbal exposition in the same paper in which he introduced it. The direct impact of PST on the conception of quantum electrodynamical processes will be exemplified by its reflection in diagrammatical representations. The study of the diverging ontological commitments towards PST immediately after its introduction opens up the prehistory of a philosophical debate that stretches out into the present: the dispute about the representational and ontological status of the physical picture connected to the evaluation of the perturbative series of QED and QFT.",,Yes,,2025-11-11T00:14:11.169Z
counterfactualdecodi-2022,Counterfactual Decoding for Anti-Hallucination Knowledge-grounded Dialogue Generation,Anthony Chen; Chris DuBois; Sameer Singh,2022,,0,https://www.semanticscholar.org/paper/708a9cfc47136377f192f996329d8ff3289280e4,,,,,Yes,,2025-11-11T00:15:16.543Z
counterfactualreason-2022,Counterfactual reasoning: Do language models need world knowledge for causal understanding?,Jiaxuan Li; Lang-Chi Yu; Allyson Ettinger,2022,arXiv.org,2,https://www.semanticscholar.org/paper/91a82593721c03ecffdef1c72ea55c6d87c42473,https://arxiv.org/pdf/2212.03278,10.48550/arXiv.2212.03278,"Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",arxiv:2212.03278,Yes,,2025-11-11T00:14:11.169Z
courseguides270180dc-2022,Course guides 270180 - DCS - Curve and Surface Design,Rodrigo Ignacio; Silveira Isoba; 10 SILVEIRAISOBA-,2022,,0,https://www.semanticscholar.org/paper/ee2f4d01ddee42df906209ec07ae060971148ac0,,,,,Yes,,2025-11-11T00:15:16.543Z
creativemathematical-2022,Creative Mathematical Reasoning: Does Need for Cognition Matter?,B. Jonsson; Julia Mossegård; Johan Lithner; Linnea Karlsson Wirebring,2022,Frontiers in Psychology,19,https://www.semanticscholar.org/paper/9f6f01cba1158e6bcb17aaa43070ef3b64c59550,https://www.frontiersin.org/articles/10.3389/fpsyg.2021.797807/pdf,10.3389/fpsyg.2021.797807,"A large portion of mathematics education centers heavily around imitative reasoning and rote learning, raising concerns about students’ lack of deeper and conceptual understanding of mathematics. To address these concerns, there has been a growing focus on students learning and teachers teaching methods that aim to enhance conceptual understanding and problem-solving skills. One suggestion is allowing students to construct their own solution methods using creative mathematical reasoning (CMR), a method that in previous studies has been contrasted against algorithmic reasoning (AR) with positive effects on test tasks. Although previous studies have evaluated the effects of CMR, they have ignored if and to what extent intrinsic cognitive motivation play a role. This study investigated the effects of intrinsic cognitive motivation to engage in cognitive strenuous mathematical tasks, operationalized through Need for Cognition (NFC), and working memory capacity (WMC). Two independent groups, consisting of upper secondary students (N = 137, mean age 17.13, SD = 0.62, 63 boys and 74 girls), practiced non-routine mathematical problem solving with CMR and AR tasks and were tested 1 week later. An initial t-test confirmed that the CMR group outperformed the AR group. Structural equation modeling revealed that NFC was a significant predictor of math performance for the CMR group but not for the AR group. The results also showed that WMC was a strong predictor of math performance independent of group. These results are discussed in terms of allowing for time and opportunities for struggle with constructing own solution methods using CMR, thereby enhancing students conceptual understanding.",,Yes,,2025-11-11T00:13:07.427Z
criticalanalysisofbi-2022,Critical Analysis of Big Data Applications using Functional Linguistics and Diversified Integration,D. Kumar; Srinivasa Rao; R. Vijaya; Kumar Reddy; D. Kumar; D. Sai; Assoc.Professor,2022,2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC),0,https://www.semanticscholar.org/paper/d0d2a1b809cf3a78f3439b89b39ce5f11e571664,,10.1109/ICAAIC53929.2022.9792589,"In recent times, the top-down use of big information data analysis and the mechanical manipulation of man-made intellectual abilities has provided the center with specialized means to advance the practical coordination of semantic structure. Potentially related to the great information research of artificial reasoning (AI), the various embodiments of utilitarian phonetics have routinely experienced problems in applications. As a result, it created critical difficulties for the far-reaching improvement of useful etymology. In this unique situation, ideas related to the improvement of useful phonetics and artificial reasoning are explained. Starting from this premise, the useful etymologies of innovative union and internal incorporation are analyzed with respect to the scenario of large-scale information AI research. In addition, in order to stimulate the useful advancement of phonetics inside and outside, this study makes some proposals, among which are the rationalization of a dispersed group climate, the promotion of a practical phase of information about the language, the assumption of models, modalities of equality of information and the transmission of an equitable preparation of the brain network. These ideas constitute an initial phase of hypothetical etymological analyzes and capacity for improvement. In addition, some ideas were made to advance the internal and external improvement of the utilitarian etymology, such as structuring a useful linguistic information phase, updating an adequate group climate, preparing the circulating brain network, assuming equal information patterns and modalities to provide a reference to skills development and hypothetical phonetic exploration.",,Yes,,2025-11-11T00:15:19.325Z
crosslingualspeakeri-2022,Cross-Lingual Speaker Identification from Weak Local Evidence,Thomas Wolf; Lysandre Debut; Julien Victor Sanh; Clement Chaumond; Anthony Delangue; Pier-339 Moi; Clara ric Cistac; Yacine Ma; Julien Jernite; Plu; Teven Xu; Sylvain Le Scao; Gugger; Mariama; Quentin Drame; M. LhoestAlexander; Rush; Michael Miller Yoder; Sopan Khosla; Qinlan Shen; Ben Zhou; Qiang Ning; Daniel Khashabi; Kyle Richardson; Tushar Khot,2022,,0,https://www.semanticscholar.org/paper/385b71fb56b54b019b855ff0265bbdbb01ad01ea,,,,,Yes,,2025-11-11T00:15:16.541Z
crunchqaasyntheticda-2022,CrunchQA: A Synthetic Dataset for Question Answering over Crunchbase Knowledge Graph,Lifan Yu; Nadya Abdel Madjid; D. Difallah,2022,2022 IEEE International Conference on Big Data (Big Data),0,https://www.semanticscholar.org/paper/dcb17d546ce8aec11174bafad2fb3d913e6b2e98,,10.1109/BigData55660.2022.10021012,"The digital transformation in the finance and enterprise sector has been driven by the advances made in big data and artificial intelligence technologies. For instance, data integration enables businesses to make better decisions by consolidating and mining heterogeneous data repositories. In particular, knowledge graphs (KGs) are used to facilitate the integration of disparate data sources and can be utilized to answer complex queries. This work proposes a new dataset for question-answering on knowledge graphs (KGQA) to reflect the challenges we identified in real-world applications which are not covered by existing benchmarks, namely, multi-hop constraints, numeric and literal embeddings, ranking, reification, and hyper-relations. To build the dataset, we create a new Knowledge Graph from the Crunchbase database using a lightweight schema to support high-quality entity embeddings in large graphs. Next, we create a Question Answering dataset based on natural language question generation using predefined multiple-hop templates and paraphrasing. Finally, we conduct extensive experiments with state-of-the-art KGQA models and compare their performance on CrunchQA. The results show that the existing models do not perform well, for example, on multi-hop constrained queries. Hence, CrunchQA can be used as a challenging benchmark dataset for future KGQA reasoning models. The dataset and scripts are available on the project repository. 1",,Yes,,2025-11-11T00:15:19.325Z
curriculumabroadcove-2022,Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding,Zeming Chen; Qiyue Gao,2022,North American Chapter of the Association for Computational Linguistics,4,https://www.semanticscholar.org/paper/32c6607346e0bbe21844275f55fb368bbffd4699,http://arxiv.org/pdf/2204.06283,10.48550/arXiv.2204.06283,"In the age of large transformer language models, linguistic evaluation play an important role in diagnosing models’ abilities and limitations on natural language understanding. However, current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets. In this paper, we introduce Curriculum as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. Curriculum contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. We show that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality. In addition, our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models that may encourage future research on re-designing datasets, model architectures, and learning objectives.",arxiv:2204.06283,Yes,,2025-11-11T00:14:11.169Z
dallevalprobingthere-2022,DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers,Jaemin Cho; Abhaysinh Zala; Mohit Bansal,2022,arXiv.org,130,https://www.semanticscholar.org/paper/804b27dc02becf7bbbd89ba949e1e07e8677c459,,,,,Yes,,2025-11-11T00:13:07.427Z
desainbahanajarberba-2022,DESAIN BAHAN AJAR BERBASIS AKTIVITAS PENALARAN MATEMATIS MENGGUNAKAN MODEL MISSOURI MATHEMATIC PROJECT MATA KULIAH ANALISIS KOMPLEKS,Nuraina Nuraina; Muliana - Muliana; M. Mursalin; Mila Kartika Sari Bangun; U. Rahayu,2022,Numeracy,0,https://www.semanticscholar.org/paper/24f768f302b2a18082d523c11bc1c60a89fa76db,https://ejournal.bbg.ac.id/numeracy/article/download/1891/1417,10.46244/numeracy.v9i2.1891,"The failure of students in studying complex analysis courses is based on student learning habits that only focus on memorizing concepts from the material studied without understanding it properly, and lack of motivation to repeat the material that has been studied. The teaching materials used in this course are textbooks. However, the textbook used still does not contain activities for students' mathematical reasoning abilities, students cannot learn independently from the book, because the material presented is difficult to understand, even lecturers who teach this complex analysis course have to redesign the material, writing in written form. hand is then given to the student. One effort that could be to improve students' mathematical reasoning abilities is to facilitate learning resources with supporting teaching materials. In this study, teaching materials will be designed based on mathematical reasoning activities using the Missauri Mathematical Project (MMP) learning model. The research method used in this research is the Analysis, Design, Development, Implementation, and Evaluation (ADDIE) development model. The development of complex analysis textbooks based on reasoning activities using the Missauri Mathematical Project learning model is feasible to be developed with the percentage of assessment by media expert validators obtained an average score of 91.79% in the ""very valid"" categories, and media expert validators obtained the average score. a score of 88.62% with the ""very valid"" categories. The results of the small group validation obtained a score of 83.5% with the ""very valid"" criteria. The results of the large group trial obtained a score of 85.5% with the ""very practical"" criteria. 
Abstrak 
Salah satu kendala dalam mata kuliah analisis kompleks adalah kegagalan mahasiswa dalam mempelajari materi yang disebabkan oleh kebiasaan belajar mahasiswa yang hanya terfokus untuk menghafal konsep dari materi yang diajarkan tanpa memahaminya secara mendalam, serta kurangnya motivasi untuk mengkaji ulang materi yang telah dipelajari. Sumber utama yang digunakan sebagai bahan ajar pada mata kuliah ini yaitu buku paket. Namun, buku paket yang digunakan masih belum memuat  aktivitas kemampuan penalaran matematis mahasiswa, mahasiswa tidak bisa belajar secara mandiri dari buku tersebut, karena materi yang disajikan sulit untuk dipahami, bahkan dosen  yang  mengajarkan  mata  kuliah  analisis  kompleks  ini  harus  mendesain  ulang materinya,  menulis  dengan  tulisan  tangan  kemudian  diberikan  kepada  mahasiswa. Salah satu usaha yang bisa dilakukan untuk meningkatkan kemampuan penalaran matematis mahasiswa adalah dengan memfasilitasi sumber belajar dengan bahan ajar yang mendukung. Dalam penelitian pengembangan ini, bahan ajar akan disusun dengan berbasis aktivitas penalaran matematis menggunakan model pembelajaran missauri mathematic project. Metode penelitian yang dipakai dalam penelitian ini yaitu model pengembangan  analysis, design, development, implementation, and evaluation. Pengembangan buku ajar analisis kompleks berbasis aktivitas penlaran dengan menggunakan model pembelajaran missauri mathematic project layak dikembangkan dengan persentase penilaian oleh validator ahli media diperoleh skor rata-rata sebesar 91,79% dengan kriteria “sangat valid”, dan validator ahli media didapat hasil skor rata-rata sebesar 88,62% dengan kriteria “sangat valid”. Hasil validasi kelompok kecil didapat nilai sebesar 83,5% dengan kriteria “sangat valid”. Hasil uji coba kelompok besar didapat nilai sebesar 85,5%  dengan kriteria “sangat praktis”.",,Yes,,2025-11-11T00:15:14.026Z
deplotoneshotvisuall-2022,DePlot: One-shot visual language reasoning by plot-to-table translation,Fangyu Liu; Julian Martin Eisenschlos; Francesco Piccinno; Syrine Krichene; Chenxi Pang; Kenton Lee; Mandar Joshi; Wenhu Chen; Nigel Collier; Y. Altun,2022,Annual Meeting of the Association for Computational Linguistics,133,https://www.semanticscholar.org/paper/4d3a49d1439a0b8fbb0e9f588970ad0f1d70dec8,http://arxiv.org/pdf/2212.10505,10.48550/arXiv.2212.10505,"Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than>28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.",arxiv:2212.10505,Yes,,2025-11-11T00:13:07.427Z
debiasingnlumodelsvi-2022,Debiasing NLU Models via Causal Intervention and Counterfactual Reasoning,Bing Tian; Yixin Cao; Yong Zhang; Chunxiao Xing,2022,AAAI Conference on Artificial Intelligence,39,https://www.semanticscholar.org/paper/d1eb051c6b13eba8a9b333d5ee0a55250717195d,https://ojs.aaai.org/index.php/AAAI/article/download/21389/21138,10.1609/aaai.v36i10.21389,"Recent studies have shown that strong Natural Language Understanding (NLU) models are prone to relying on annotation biases of the datasets as a shortcut, which goes against the underlying mechanisms of the task of interest. To reduce such biases, several recent works introduce debiasing methods to regularize the training process of targeted NLU models. In this paper, we provide a new perspective with causal inference to find out the bias. On one hand, we show that there is an unobserved confounder for the natural language utterances and their respective classes, leading to spurious correlations from training data. To remove such confounder, the backdoor adjustment with causal intervention is utilized to find the true causal effect, which makes the training process fundamentally different from the traditional likelihood estimation. On the other hand, in inference process, we formulate the bias as the direct causal effect and remove it by pursuing the indirect causal effect with counterfactual reasoning. We conduct experiments on large-scale natural language inference and fact verification benchmarks, evaluating on bias sensitive datasets that are specifically designed to assess the robustness of models against known biases in the training data. Experimental results show that our proposed debiasing framework outperforms previous state-of-the-art debiasing methods while maintaining the original in-distribution performance.",,Yes,,2025-11-11T00:14:11.169Z
decomposedpromptinga-2022,Decomposed Prompting: A Modular Approach for Solving Complex Tasks,Tushar Khot; H. Trivedi; Matthew Finlayson; Yao Fu; Kyle Richardson; Peter Clark; Ashish Sabharwal,2022,International Conference on Learning Representations,556,https://www.semanticscholar.org/paper/07955e96cbd778d0ae2a68f09d073b866dd84c2a,http://arxiv.org/pdf/2210.02406,10.48550/arXiv.2210.02406,"Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.",arxiv:2210.02406,Yes,,2025-11-11T00:15:14.026Z
deepstructuralcausal-2022,Deep Structural Causal Shape Models,Rajat Rasal; Daniel Coelho de Castro; Nick Pawlowski; Ben Glocker,2022,ECCV Workshops,14,https://www.semanticscholar.org/paper/8975f550eed321c203d3990692f82e3f7b112b8f,http://arxiv.org/pdf/2208.10950,10.48550/arXiv.2208.10950,"Causal reasoning provides a language to ask important interventional and counterfactual questions beyond purely statistical association. In medical imaging, for example, we may want to study the causal effect of genetic, environmental, or lifestyle factors on the normal and pathological variation of anatomical phenotypes. However, while anatomical shape models of 3D surface meshes, extracted from automated image segmentation, can be reliably constructed, there is a lack of computational tooling to enable causal reasoning about morphological variations. To tackle this problem, we propose deep structural causal shape models (CSMs), which utilise high-quality mesh generation techniques, from geometric deep learning, within the expressive framework of deep structural causal models. CSMs enable subject-specific prognoses through counterfactual mesh generation (""How would this patient's brain structure change if they were ten years older?""), which is in contrast to most current works on purely population-level statistical shape modelling. We demonstrate the capabilities of CSMs at all levels of Pearl's causal hierarchy through a number of qualitative and quantitative experiments leveraging a large dataset of 3D brain structures.",arxiv:2208.10950,Yes,,2025-11-11T00:15:16.541Z
designandplanningofa-2022,"Design and Planning of a Transdisciplinary Investigation into Farmland Pollinators: Rationale, Co-Design, and Lessons Learned",S. Hodge; O. Schweiger; A. Klein; S. Potts; Cecilia Costa; M. Albrecht; J. D. de Miranda; M. Mand; P. De la Rúa; M. Rundlöf; Eleanor Attridge; R. Dean; P. Bulet; D. Michez; R. Paxton; A. Babin; N. Cougoule; M. Laurent; Anne-Claire Martel; Laurianne Paris; M. Rivière; E. Dubois; M. Chauzat; K. Arafah; Dalel Askri; S. Voisin; T. Kiljanek; Irene Bottero; Christophe Dominik; Giovanni Tamburini; M. Pereira-Peixoto; Dimitry Wintermantel; T. Breeze; E. Cini; D. Senapathi; G. Di Prisco; P. Mędrzycki; S. Hagenbucher; A. Knauer; Janine M. Schwarz; Risto Raimets; Vicente Martínez-López; K. Ivarsson; C. Hartfield; P. Hunter; Mark Brown; J C Stout,2022,Sustainability,14,https://www.semanticscholar.org/paper/400a8763ed493d109f63a7ca7529811630839d8d,https://www.mdpi.com/2071-1050/14/17/10549/pdf?version=1661430375,10.3390/su141710549,"To provide a complete portrayal of the multiple factors negatively impacting insects in agricultural landscapes it is necessary to assess the concurrent incidence, magnitude, and interactions among multiple stressors over substantial biogeographical scales. Trans-national ecological field investigations with wide-ranging stakeholders typically encounter numerous challenges during the design planning stages, not least that the scientific soundness of a spatially replicated study design must account for the substantial geographic and climatic variation among distant sites. ‘PoshBee’ (Pan-European assessment, monitoring, and mitigation of Stressors on the Health of Bees) is a multi-partner transdisciplinary agroecological project established to investigate the suite of stressors typically encountered by pollinating insects in European agricultural landscapes. To do this, PoshBee established a network of 128 study sites across eight European countries and collected over 50 measurements and samples relating to the nutritional, toxicological, pathogenic, and landscape components of the bees’ environment. This paper describes the development process, rationale, and end-result of each aspect of the of the PoshBee field investigation. We describe the main issues and challenges encountered during the design stages and highlight a number of actions or processes that may benefit other multi-partner research consortia planning similar large-scale studies. It was soon identified that in a multi-component study design process, the development of interaction and communication networks involving all collaborators and stakeholders requires considerable time and resources. It was also necessary at each planning stage to be mindful of the needs and objectives of all stakeholders and partners, and further challenges inevitably arose when practical limitations, such as time restrictions and labour constraints, were superimposed upon prototype study designs. To promote clarity for all stakeholders, for each sub-component of the study, there should be a clear record of the rationale and reasoning that outlines how the final design transpired, what compromises were made, and how the requirements of different stakeholders were accomplished. Ultimately, multi-national agroecological field studies such as PoshBee benefit greatly from the involvement of diverse stakeholders and partners, ranging from field ecologists, project managers, policy legislators, mathematical modelers, and farmer organisations. While the execution of the study highlighted the advantages and benefits of large-scale transdisciplinary projects, the long planning period emphasized the need to formally describe a design framework that could facilitate the design process of future multi-partner collaborations.",,Yes,,2025-11-11T00:15:19.325Z
designandsimulationa-2022,Design and Simulation Application of Fuzzy Controller Based on Granular Computing,Huiyue Li; Jianhua Yang; Wei Lu,2022,International Conference on Computer and Information Application,1,https://www.semanticscholar.org/paper/c54f2d59dca66c50ddc4e8f745f76ac659a1ac6f,,10.1109/iccia55271.2022.9828430,"Fuzzy control theory generates fuzzy rules based on expert experience and experimental data, so fuzzy control method can control complex and large-scale systems without precise mathematical models. But fuzzy control method still has problems: complex structure and rule explosion problem. Aiming at the above problems, this paper proposes a fuzzy control method based on granular computing. Firstly, design the fuzzy controller, the fuzzy rules are generated by the fuzzy space division method. Then, using the information granulation method, each fuzzy rule is granulated into an information granule. Taking points in the information granules to fit the realization function of the granular function, using that function control the object instead of the fuzzy controller. The fuzzy reasoning process is omitted, and the number of rules has nothing to do with the accuracy, it is only related to the number of granules. Therefore, the structure of the control system can be simplified under the condition of ensuring the accuracy of the system, and the problem of rule explosion can be avoided at the same time. The simulation experiment using the second-order inverted pendulum as the control object proves the feasibility and effectiveness of the fuzzy control method based on granular computing.",,Yes,,2025-11-11T00:15:16.543Z
designofcircularairi-2022,Design of circular air intakes for subsonic turbofans,Ruslan Tsukanov,2022,Aerospace technic and technology,1,https://www.semanticscholar.org/paper/e3b0d513d05ced7c6323462e74e858db318a2247,http://nti.khai.edu/ojs/index.php/aktt/article/download/aktt.2022.4.01/1843,10.32620/aktt.2022.4.01,"The subject matter of this article is the process of subsonic air intake shaping for high-bypass ratio turbofan at the airplane preliminarily designing stage. The goal was to improve a mathematical model of V. I. Polikovskii method of subsonic air intake shaping for high-bypass ratio turbofan. The tasks are to consider the presence of cant of inlet cross-section, required to perform effective operation at airplane cruising angle-of-attacks; to increase the radius of curvature of the air intake lip to provide air flow near it without flow separation, which was definitely determined and could not be increased in the existing method; to improve constant length velocity gradient law (used in this method) so that too large duct expansion angles near the air intake outlet cross-section can be avoided; to consider the engine inlet spinner presence. The methods used are analytical and digital mathematical methods, implemented in MathCAD and Microsoft Visual Studio systems. The following results were obtained: based on the proposed method, new calculation module for the Power Unit software version 11.8 has been developed (С-language Win32 UNICODE application) with a friendly user interface. Conclusions. The scientific novelty of the results obtained is as follows: 1) mathematical model (algorithm and its program implementation) for circular turbofan air intake shaping has been improved considering cant of the inlet cross-section, air intake lip rounding with two radiuses, presence of engine inlet spinner, and zero expansion angles in the diffuser outlet cross-section; 2) adequacy of calculation results using the improved mathematical model is shown using comparison with shapes of circular turbofan air intakes, developed by the leading aviation companies.",,Yes,,2025-11-11T00:15:16.543Z
designofsupplementar-2022,Design of supplementary mathematics module for preparation of minimum competency assessment for fifth grade elementary school students,Heru Heru; R. E. Yuliani; Izah Zulpah,2022,Jurnal Math Educator Nusantara Wahana Publikasi Karya Tulis Ilmiah di Bidang Pendidikan Matematika,2,https://www.semanticscholar.org/paper/11fa440d51f0257ebeb2b6e7e08d75b6d6c83530,https://ojs.unpkediri.ac.id/index.php/matematika/article/download/17682/2712,10.29407/jmen.v8i1.17682,"Students' skills in solving numeracy questions in the AKM are still varied, especially for SD Negeri 3 Mendo Barat students. The implementation of AKM, especially numeracy skills, impacts the learning given to students. Students are not only required to understand mathematical concepts but must demonstrate other mathematical abilities such as reasoning and problem-solving abilities. Therefore, teaching materials are needed that can support the AKM-based learning process. At SD Negeri 3 Mendo Barat, there are still very few teaching materials owned by students, especially those based on AKM. This study aims to develop a companion mathematics module for preparing AKM for fifth-grade elementary school students that is valid, practical, and potentially affects learning outcomes. Development research using a 4-D model is used as a research technique. The research subjects were 22 students of class V SD Negeri 3 Mendo Barat, as many as 22 people. The results showed that the mathematics module was valid and practical—valid seen from the experts' assessment of the module. Experts consist of material, media, and language experts. Practicality is obtained from the results of student questionnaire analysis in a limited field test which shows a practicality percentage of 91.32 per cent. Based on the operational field test results, the average final score of students is 87.05, which indicates that student learning outcomes are in the very good category. The module can potentially affect student learning outcomes in the good category.",,Yes,,2025-11-11T00:15:16.543Z
designingeffectivesp-2022,Designing Effective Sparse Expert Models,Barret Zoph; Irwan Bello; Sameer Kumar; Nan Du; Yanping Huang; J. Dean; Noam M. Shazeer; W. Fedus,2022,"IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum",100,https://www.semanticscholar.org/paper/e47da75675b9a3fe02ef1efadca39bc8cdfcdc17,,10.1109/IPDPSW55747.2022.00171,"Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).",,Yes,,2025-11-11T00:15:14.026Z
despitesuperhumanper-2022,"Despite ""super-human"" performance, current LLMs are unsuited for decisions about ethics and safety",Joshua Albrecht; Ellie Kitanidis; Abraham J. Fetterman,2022,arXiv.org,23,https://www.semanticscholar.org/paper/7d5175db1b99552491063d2d9581b0b51e1d2932,http://arxiv.org/pdf/2212.06295,10.48550/arXiv.2212.06295,"Large language models (LLMs) have exploded in popularity in the past few years and have achieved undeniably impressive results on benchmarks as varied as question answering and text summarization. We provide a simple new prompting strategy that leads to yet another supposedly""super-human""result, this time outperforming humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset). Unfortunately, we find that relying on average performance to judge capabilities can be highly misleading. LLM errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label. We also observe signs of inverse scaling with model size on some examples, and show that prompting models to""explain their reasoning""often leads to alarming justifications of unethical actions. Our results highlight how human-like performance does not necessarily imply human-like understanding or reasoning.",arxiv:2212.06295,Yes,,2025-11-11T00:15:14.026Z
developmentofanalgor-2022,Development of an Algorithm to Analyze Vacancies in the Labor Market Based on Open-Source Data,O. Khokhlova; A. N. Khokhlova; A. Choyzhalsanova,2022,Voprosy statistiki,2,https://www.semanticscholar.org/paper/e58a983ea50412432b36dfe9c4c8b935e1d57c3d,,10.34023/2313-6383-2022-29-4-33-41,"In the introductory part of the article, the authors substantiate the relevance of developing methodological tools for analyzing job vacancies in the labor market in the context of the modern technological revolution, which significantly increases requirements for professional knowledge and experience of working personnel and changes the ratio between traditional and new professions.To assess the current situation on the labor market and the demand for currently existing professions, the main section of the published results of the study presents the algorithm for analyzing vacancies using large data arrays from open sources using mathematical and statistical tools and machine learning methods using the Python programming language and the IBM SPSS modeler analytical platform. The algorithm includes: parsing data on vacancies, analyzing vacancies by the main criteria, clustering vacancies by salary level and building a neural network model – a multilayer perceptron of the dependence of salary on a number of predictors. It should be noted that the developed algorithm is universal, because it can be used to analyze big data from any open source at a certain point in time.The results of the analysis will allow researchers and specialists of management structures to more realistically assess the current situation on the labor market, educational institutions will be able to adjust training programs in accordance with the modern requirements of employers, employers will make decisions on the development of competencies in their field of activity and conduct a comparative analysis of demanded vacancies in terms of quantitative and qualitative characteristics, and for the applicant it will be easier to see the demand for vacancies in the labor market and develop new skills.",,Yes,,2025-11-11T00:15:16.543Z
developmentofanattit-2022,Development of an Attitude Scale for Moral Literacy Skills: Validity and Reliability Study,İshak Tekin,2022,Marife Dini Araştırmalar Dergisi,3,https://www.semanticscholar.org/paper/eef5d97b3b2d750009d76c0b814a7bbb4cfe6964,https://dergipark.org.tr/tr/download/article-file/2377253,10.33420/marife.1104203,"In recent years, some changes have emerged in the general aims of education programs, and in this context, educational authorities have adopted new approaches aiming at nurturing skills. Especially in moral education, which is an indispensable part of citizenship education, the concept of moral literacy is one of the important isssue of educational discussions. In the literature, there are many studies asserting that schools should educate children for moral literacy as well as primitive language literacy, science and mathematics literacy, intercultural literacy. On the other hand, it should be noted that there are not enough studies dealing with the application of the subject in the field. In addition, it is seen that countries have not yet taken sufficient steps for a change in their educational systems and have not demonstrated a strong will. 
Moral literacy refers to a skill that includes thinking about one's own moral values, determining the possible consequences of various alternatives and their effects, making logical decisions about which option is compatible with one's values, acting in line with one's values, and taking responsibility for one's own actions. Moral literacy, conceptualized by Nancy Tuana, consists of three basic elements, namely moral moral sensitivity, moral reasoning skills, and moral imagination, and three sub-skills under them. These skills generally include the intellectual skills that lead the moral decision-making process. 
This study aimed to develop a valid and reliable attitude scale based on Tuana's theory. Within the scope of the research designed in the general survey model, the relevant literature was scanned and the studies on moral literacy were examined. With the clarification of the theoretical structure, the scale development process was started. The following stages were followed in the development of the moral literacy scale: i. Creating the item pool, ii. Content validaty, iii. Reviewing and finalizing the draft form, iv. Application of the scale, v. Item analysis, analysis of construct validity and reliability analysis, vi. Examining the correlation between subscales and the total score, vii. Putting into the final form of the scale and reporting. 
In the first stage, the draft form consisting of 27 items was sent to five field experts and a language expert, 1 item was removed from the scale as they were not compatible with the scope in line with the suggestions of the field experts, and 1 more item was added. The draft form, which was finalized with changes, was applied to 653 university students studying at Eskişehir Osmangazi University and Anadolu University in the fall semester of the 2018-2019 academic year. 88 data, which were not marked carefully and were not found reliable during the data entry to excel, were excluded from the analysis. In the item analysis of the scale, 3 items were excluded from the draft scale and the construct validity was passed. 
In the exploratory factor analysis, 7 more items with values less than .40 and overlapping were excluded from the analysis. As a result of the factor analysis performed after this process, a structure consisting of 5 factors and 20 items was obtained. As a result of the reliability analysis, it was seen that the whole scale had high reliability, while the sub-dimensions had medium and low reliability levels. It was also determined that the structure obtained by exploratory factor analysis was confirmed as a result of testing with confirmatory factor analysis. According to the path analysis, it can be said that the existing structure fits well. When the relationship between the sub-dimensions of the scale is examined, it can be stated that there is a significant positive relationship between the sub-dimensions and a total score can be obtained for the attitudes towards the moral literacy skill of the scale. As a result, it can be said that the scale obtained as a result of these processes can measure the attitudes of university students towards moral literacy skills in a reliable and valid way.",,Yes,,2025-11-11T00:15:19.325Z
developmentofselflea-2022,Development of self-learning intelligent decision support system to control of steel production technological processes,I. Ziborov; T. Zheldak,2022,Systems and Technologies,0,https://www.semanticscholar.org/paper/4c4ca4d83f47ba86df0706bd0271f82a1a879316,https://doi.org/10.34185/1562-9945-3-140-2022-04,10.34185/1562-9945-3-140-2022-04,"Taking to the consideration the current state of converter production and measuring equipment at Ukrainian enterprises, it follows that the smelting process is based on a complex dynamic non-deterministic system. The process is complicated by the large number of param-eters, the inability to accurately identify the state of the system at any time, as well as the dif-ficulty of forecasting system requirements. Preliminary analysis has shown that in the conditions of this production converter manufacturing efficiency increase can be reached at the expense of: - reducing the cost of raw materials, such as iron-containing additives, deoxidizers, non-metallic elements in steel; - reduction of melting time, especially blowing time; - reducing defects and improving product quality. It is proposed the architecture of integrated control DSS in converter steel production based on the principle of minimal interference in the production process. The primary aim of such a system is to predict the behavior of the production process, providing the recommen-dations for its impact in order to optimize the external criterion of efficiency. The source and amount of data required for the database formation and DSS knowledge base are substantiated. The mechanism of self-learning in the course of technological tasks is described. The structural scheme of self-learning DSS, self-learning algorithm, which is mainly featured with modularity, is offered in the paper. The approach allows testing of any number of existing algorithms for learning, forecasting and optimization in order to further select the most effective ones, modifies the system in the future and allows the parallel use of a number of com-peting algorithms. The operator has the opportunity to choose as a control solution one of the proposed systems, or the formation of its own, better by a certain external criterion of result quality. Based on the suggested software structure, a number of tasks are formulated that need to be performed to build a decision support system. It is also considered to apply the mathematical apparatus of fuzzy sets to describe certain pa-rameters of the technological process and quality criteria, fuzzy neural network for modeling reasoning processes and the choice of algorithm for its training.",,Yes,,2025-11-11T00:15:19.325Z
differentiablereason-2022,Differentiable Reasoning over Long Stories - Assessing Systematic Generalisation in Neural Models,Wanshui Li; Pasquale Minervini,2022,arXiv.org,1,https://www.semanticscholar.org/paper/59494dcb572cebb577a1bcb2d6f87dfca93d6591,http://arxiv.org/pdf/2203.10620,10.48550/arXiv.2203.10620,"Contemporary neural networks have achieved a series of developments and successes in many aspects; however, when exposed to data outside the training distribution, they may fail to predict correct answers. In this work, we were concerned about this generalisation issue and thus analysed a broad set of models systematically and robustly over long stories. Related experiments were conducted based on the CLUTRR, which is a diagnostic benchmark suite that can analyse generalisation of natural language understanding (NLU) systems by training over small story graphs and testing on larger ones. In order to handle the multi-relational story graph, we consider two classes of neural models:""E-GNN"", the graph-based models that can process graph-structured data and consider the edge attributes simultaneously; and""L-Graph"", the sequence-based models which can process linearized version of the graphs. We performed an extensive empirical evaluation, and we found that the modified recurrent neural network yield surprisingly accurate results across every systematic generalisation tasks which outperform the modified graph neural network, while the latter produced more robust models.",arxiv:2203.10620,Yes,,2025-11-11T00:15:14.026Z
discursivesocraticqu-2022,Discursive Socratic Questioning: (Unsupervised) Interpreting Neural Language Models for Discourse Understanding,Rahul Aralikatte; Matthew Lamm; Daniel Hardt; Regina Barzilay; Mirella Lapata. 2008; Modeling; Yonatan Belinkov; Sebastian Gehrmann; Ayal Klein; Eran Hirsch; Ron Eliav; Valentina Pyatkin; J. Mamou; Wei-Jen Ko; Cutter Dalton; Mark Simmons; Greg Fisher; Durrett Junyi; Jessy Li; Dis-737; Yating Wu; Dananjay Srini-740; Meichun Webber; Tat-Seng Liu; F. ChuaNancy; 746; Wenqiang Lei; Yuanxin Xiang; Qian Yuwei Wang; Meichun Zhong; Liu Min-Yen; Kan; Lin-753; Belinda Z. Li; Maxwell I. Nye; Hwee Ziheng Lin; Tou Ng; Min-Yen Kan; Au-766,2022,,0,https://www.semanticscholar.org/paper/76e5069425547d4f53b5aa843a765a305b7fa470,,,,,Yes,,2025-11-11T00:15:14.026Z
distillingmultistepr-2022,Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions,Kumar Shridhar; Alessandro Stolfo; Mrinmaya Sachan,2022,arXiv.org,47,https://www.semanticscholar.org/paper/72123a86eae2cb5c4eae8650f43524039d48875d,http://arxiv.org/pdf/2212.00193,10.48550/arXiv.2212.00193,,,Yes,,2025-11-11T00:13:07.427Z
distillingreasoningc-2022,Distilling Reasoning Capabilities into Smaller Language Models,Kumar Shridhar; Alessandro Stolfo; Mrinmaya Sachan,2022,Annual Meeting of the Association for Computational Linguistics,202,https://www.semanticscholar.org/paper/8fd462f6248d5e3f1b6602697c09489086b5655f,https://aclanthology.org/2023.findings-acl.441.pdf,10.18653/v1/2023.findings-acl.441,"Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM",arxiv:2212.00193,Yes,,2025-11-11T00:13:07.427Z
divingintothedeepend-2022,Diving into the Deep End: Machine Learning for the Chemist,S. Imberti,2022,ACS Omega,1,https://www.semanticscholar.org/paper/7c838fcffb45a6c191130627911ef50c5795e9d3,https://doi.org/10.1021/acsomega.2c04373,10.1021/acsomega.2c04373,"I the past year, ACS Omega has seen a dramatic increase in the number of articles published with an Artificial Intelligence (AI) or Machine Learning, Deep Learning, Neural Networks theme. In 2021, 105 articles were published vs 45 articles published in the previous year, an increase of 133%. This exceptional growth for a fully open access broad scope journal pairs with the growth seen at many other journals in the ACS portfolio. Interestingly, the growth registered in the past 2−3 years is not confined to the journals that specialize in chemical informatics: the Journal of Chemical Information and Modeling, the Journal of Chemical Theory and Computation, and, to some extent, the Journal of Physical Chemistry C. It also encompasses journals in the materials sciences, the physical sciences, measurement science, chemical engineering, and environmental science in the broader ACS portfolio (Figure 1). Looking at the wider publication landscape, the Directory of Open Access Journal lists 65 journal entries for scientific publications that pertain to the topic of AI. Eleven of these were added in the last year alone, and this includes only those journals queried in the computational science category. In addition to these, numerous other open access, broader scope journals also publish work without any perceived evaluation of immediate impact and where existing AI tools have been successfully applied to a variety of chemistry questions. Over the past 10 to 15 years, AI, especially deep learning, has effected dramatic technological progress and proven success in areas such as computer vision, speech recognition, natural language processing, common sense knowledge, strategic reasoning, and robotics. Exceptional results have also been reported in the medical sciences; for example, deep neural networks facilitated accurate diagnosis of skin cancer, and deep learning enabled extraction of new knowledge from old data, enabling accurate prediction of age, gender, smoking status, blood pressure, and heart attack propensity of individuals just by analyzing previously acquired retinal images. But, what about the status of AI and its perception in chemistry? In the ensuing text, as an entreé to the associated Virtual Issue, I will present a brief overview of the perceived usefulness of AI at this time in some fields of the chemical sciences and related areas, based on recently published reviews and perspectives by experts in the area, as well as other resources. A review by Baum et al. charted the growth and distribution of AI-related chemistry publications in the last two decades using the CAS Content Collection, which includes patents as well as research articles. In their paper, they refer to the “Hype Cycle of Emerging Technologies”, and from the data gathered, they determine that AI adoption in life sciences and analytical chemistry has navigated the so-called “peak of inflated expectations” and “trough of disillusionment” and successfully progressed to the “plateau of productivity”. It is common to overestimate the effect of a technology in the short run and underestimate it in the long run (anecdotally known as Amara’s law). For example, despite commercial interests and consequent investments being enormous, to this day, no new drug has yet been synthesized using AI. As another (counter) example, Peiretti and Brunel, in their Perspective published in 2018, ponder whether organic chemistry and in particular retrosynthesis might be the ideal next application of AI techniques. After all, it “f its perfectly” the definition of AI as a problem with “complex input−output relationships [that] are dif f icult or impractical to model procedurally”. However, Baum et al. firmly assess that “there are still areas of Chemistry like organic synthetic chemistry where AI is yet to make an impact”. Still, work is in progress, as standardized formats for reporting a chemical synthesis procedure are being developed and even classified (an essential step for scientific fields to exist) in the new taxonomy of digital chemistry or chemputation. At this stage in the discussion, it may be interesting to explore what the drivers are to greater or lesser success in applying AI methods to scientific problems. This point is examined in the Editorial by Jones et al., which accompanies an excellent JACS Au special collection on “Emerging Chemistry & Machine Learning”. In their overview, three main reasons are indicated. Two of them are quite intrinsic to the method (algorithm development and theoretical derivation of descriptors), while one of them is, notably, not specific but resides in the availability of a collection of standardized, highquality data. A case in point, and unanimously defined as the most spectacular recent success of AI, is the recent prediction of a protein’s three-dimensional structure from its amino acid sequence via AlphaFold. The tool was trained on large publicly available databases, such as the RSCB Protein Data Bank, as well as protein sequences of unknown structure. In return, it is somewhat expected, although not guaranteed, that the new information generated is also made available to the public. The AlphaFold code is now publicly available. It can",,Yes,,2025-11-11T00:15:19.325Z
docinferdocumentleve-2022,DocInfer: Document-level Natural Language Inference using Optimal Evidence Selection,Puneet Mathur; Gautam Kunapuli; Riyaz Ahmad Bhat; Manish Shrivastava; Dinesh Manocha; M. Singh,2022,Conference on Empirical Methods in Natural Language Processing,11,https://www.semanticscholar.org/paper/4430cb7ddb3c4a9860ddabf4f92568a8a03c2b18,https://aclanthology.org/2022.emnlp-main.51.pdf,10.18653/v1/2022.emnlp-main.51,"We present DocInfer - a novel, end-to-end Document-level Natural Language Inference model that builds a hierarchical document graph enriched through inter-sentence relations (topical, entity-based, concept-based), performs paragraph pruning using the novel SubGraph Pooling layer, followed by optimal evidence selection based on REINFORCE algorithm to identify the most important context sentences for a given hypothesis. Our evidence selection mechanism allows it to transcend the input length limitation of modern BERT-like Transformer models while presenting the entire evidence together for inferential reasoning. We show this is an important property needed to reason on large documents where the evidence may be fragmented and located arbitrarily far from each other. Extensive experiments on popular corpora - DocNLI, ContractNLI, and ConTRoL datasets, and our new proposed dataset called CaseHoldNLI on the task of legal judicial reasoning, demonstrate significant performance gains of 8-12% over SOTA methods. Our ablation studies validate the impact of our model. Performance improvement of 3-6% on annotation-scarce downstream tasks of fact verification, multiple-choice QA, and contract clause retrieval demonstrates the usefulness of DocInfer beyond primary NLI tasks.",,Yes,,2025-11-11T00:14:11.169Z
doesclipbindconcepts-2022,Does CLIP Bind Concepts? Probing Compositionality in Large Image Models,Martha Lewis; Nihal V. Nayak; Qinan Yu; Jack Merullo; Ellie Pavlick,2022,Findings,85,https://www.semanticscholar.org/paper/2de7790ed868510c8001a90c11737fe4e8a01930,http://arxiv.org/pdf/2212.10537,10.48550/arXiv.2212.10537,"Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ‘red cube’ by reasoning over the constituents ‘red’ and ‘cube’. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ‘cube behind sphere’ from ‘sphere behind cube’). To inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We benchmark them on three synthetic datasets – single-object, two-object, and relational – designed to test concept binding. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance drops dramatically. At the same time, CDSMs also perform poorly, with best performance at chance level.",arxiv:2212.10537,Yes,,2025-11-11T00:14:11.169Z
downloadfreefilmtheo-2022,Download Free Film Theory An Introduction Through The Senses Thomas Elsaesser Pdf File Free,T. Elsaesser,2022,,0,https://www.semanticscholar.org/paper/213b3e97d6cbf31ba582b4787c612507a2d545cf,,,,,Yes,,2025-11-11T00:15:19.325Z
draftsketchandproveg-2022,"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",Albert Qiaochu Jiang; S. Welleck; J. Zhou; Wenda Li; Jiacheng Liu; M. Jamnik; Timothée Lacroix; Yuhuai Wu; Guillaume Lample,2022,International Conference on Learning Representations,226,https://www.semanticscholar.org/paper/7de36d6b14aadc8cdb6ad1340b9ca64b15375bca,http://arxiv.org/pdf/2210.12283,10.48550/arXiv.2210.12283,"The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.",arxiv:2210.12283,Yes,,2025-11-11T00:13:07.427Z
dynamicpromptlearnin-2022,Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning,Pan Lu; Liang Qiu; Kai-Wei Chang; Y. Wu; Song-Chun Zhu; Tanmay Rajpurohit; Peter Clark; A. Kalyan,2022,International Conference on Learning Representations,365,https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4,http://arxiv.org/pdf/2209.14610,10.48550/arXiv.2209.14610,"Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.",arxiv:2209.14610,Yes,,2025-11-11T00:13:07.427Z
economicandmathemati-2022,ECONOMIC AND MATHEMATICAL TOOLS FOR PREDICTING THE CURRENCY EXCHANGE RATE,Ostap Melnyk; Oleksandr Novoseletskyy,2022,Scientific opinion: Economics and Management,0,https://www.semanticscholar.org/paper/46d121662cd6b020e9945b20c399559bd4c276ab,https://doi.org/10.32836/2521-666x/2022-78-24,10.32836/2521-666x/2022-78-24,"The article deals with the analysis of existing approaches to exchange rate forecasting. It also includes the review of Ukrainian and foreign scientists on this topic. The authors of this article have considered the main disadvantages and benefits of existing forecasting dimensions, as well as individual methods and models. They indicated ways to facilitate the implementation of currency exchange rate forecasting using neural networks with software libraries for various programming languages and individual software applications, as well. As a result, the authors have systematized knowledge about existing approaches used in the process of currency exchange rate forecasting. There are two dimensions of currency exchange rate forecasting, in particular, intuitive and formalized ones. The intuitive dimension is peculiar to short-term forecasting and is often used in trading. Its main advantages include the ability to consider structural changes in the economy that can significantly affect the exchange rate formation itself and the speed of forecasting. However, the disadvantage of intuitive methods is the inability to prove formally the quality of the obtained forecasts. The advantages of the formalized dimension of forecasting include the ability to prove the quality. Businesses and government agencies use it the most often. Extrapolation methods and machine learning methods are mainly used to predict the exchange rate using formalized methods. Moreover, the reviewed studies indicate that among the well-known extrapolation methods for predicting the exchange rate, autoregressive models (VAR, AR, ARMA, ARIMA, SARIMA, ARCH, GARCH, ARDL) and smoothing methods (floating averages, adaptive methods and models) are used the most frequently. Machine learning methods include neural networks. Trend models have proved to be ineffective for currency exchange rate forecasting. The reason for this appeared to be using large amounts of data for currency exchange rate forecasting, and each fluctuation there directly affects the whole phenomenon.",,Yes,,2025-11-11T00:15:14.026Z
ertestevaluatingexpl-2022,ER-TEST Evaluating Explanation Regularization Methods for NLP Models,Brihi Joshi; Aaron Chan; Ziyi Liu; Shaoliang Nie; Maziar Sanjabi; Hamed Firooz; Xiang Ren,2022,TRUSTNLP,4,https://www.semanticscholar.org/paper/506f4614f2be3b02984a1b293553ce07d18b38bb,http://arxiv.org/pdf/2205.12542,10.48550/arXiv.2205.12542,,,Yes,,2025-11-11T00:15:16.543Z
erecenhancedlanguage-2022,EREC: Enhanced Language Representations with Event Chains,Huajie Wang; Yinglin Wang,2022,Inf.,1,https://www.semanticscholar.org/paper/a41c89afad3e5cfbcaa6dcd4acb02d0cd53a15b1,https://www.mdpi.com/2078-2489/13/12/582/pdf?version=1671094903,10.3390/info13120582,"The natural language model BERT uses a large-scale unsupervised corpus to accumulate rich linguistic knowledge during its pretraining stage, and then, the information is fine-tuned for specific downstream tasks, which greatly improves the understanding capability of various natural language tasks. For some specific tasks, the capability of the model can be enhanced by introducing external knowledge. In fact, these methods, such as ERNIE, have been proposed for integrating knowledge graphs into BERT models, which significantly enhanced its capabilities in related tasks such as entity recognition. However, for two types of tasks, commonsense causal reasoning and predicting the ending of stories, few previous studies have combined model modification and process optimization to integrate external knowledge. Therefore, referring to ERNIE, in this paper, we propose enhanced language representation with event chains (EREC), which focuses on keywords in the text corpus and their implied relations. Event chains are integrated into EREC as external knowledge. Furthermore, various graph networks are used to generate embeddings and to associate keywords in the corpus. Finally, via multi-task training, external knowledge is integrated into the model generated in the pretraining stage so as to enhance the effect of the model in downstream tasks. The experimental process of the EREC model is carried out with a three-stage design, and the experimental results show that EREC has a deeper understanding of the causal relationship and event relationship contained in the text by integrating the event chains, and it achieved significant improvements on two specific tasks.",,Yes,,2025-11-11T00:14:11.169Z
editorial-2022,Editorial,B. Fredericks; M. Nakata; Katelyn Barney,2022,The Australian Journal of Indigenous Education,0,https://www.semanticscholar.org/paper/c5186d69aaa32a3f34d8b77217995114ded864db,https://ajie.atsis.uq.edu.au/ajie/article/download/624/569,10.55146/ajie.v51i2.624,"Welcome to Volume 51.2 of The Australian Journal of Indigenous Education. This is our second volume since our shift to being an open access journal. We are very pleased that AJIE has recently been accepted into the Directory of Open Access Journals and was awarded the DOAJ Seal for best practice in open access. DOAJ is an extensive index of diverse open access journals internationally and their aim is to increase the visibility, accessibility, reputation, usage and impact of quality, peer-reviewed, open access scholarly research journals globally. We are also excited that since the journal became open access in August 2022 there has been over 20,000 views of whole articles and over 24,000 views of abstracts on our new open access website. 
This is a larger volume of AJIE than usual, and we thank the authors and reviewers for their contributions. You play a vital role in ensuring the quality of the journal. We would also like to thank Michelle James for her detailed and astute copyediting for the journal. Special thanks to Senior Publications Officer Sonia Nitchell for her continuing work on importing the large AJIE archive onto the new platform. 
The first suite of articles in this volume focuses on the early childhood context with articles by Locke and Webb providing us with insights into the inclusion of Indigenous knowledges and perspectives in early education and care settings in the first paper and how Aboriginal educators integrated their cultural knowledge and experiences to develop Aboriginal children’s skills in the second. In a South Saami context, Kroik explores preschool teachers’ identity as linguistic role models by means of analysing their own descriptions of language learning. In Canada, Schroeder et al. demonstrate the importance of making curricula relevant to Indigenous children by including content that is culturally relevant and developmentally appropriate. The interrelationships between language, identity and culture from Māori kaumātua (elders both male and female) and whānau (parents and extended family members) from Aotearoa (New Zealand) is explored by Berryman et al. 
The second suite of papers take us into the context of schools. Johnson and Flückiger explore the important role for Aboriginal Education Workers in remote Australian communities, while Goodall et al. draw on student and teacher memories of the early days of Indigenous-controlled adult education provider Tranby Aboriginal Co-operative Ltd. The paper by Guenther et al. analyses My School data for Very Remote Aboriginal schools, showing how the Remote School Attendance Strategy school attendance results compare with similar non-Remote School Attendance Strategy schools. Their findings raise ethical and accountability concerns about the Remote School Attendance Strategy, which they argue lacks evidence of attendance improvement, and which potentially causes harm. Whitau et al. also examine school attendance but in relation to Western Australian Aboriginal young women and the links between racism, teacher–student relationships, and peer connectedness, and how these were related to participant attendance and engagement at school. Moore et al. discuss the Whole of Community Engagement (WCE) initiative, which sought to identify barriers and enablers in Aboriginal students’ pathways to post-compulsory education in six remote communities in Arnhem Land and central Australia. They describe the features that led them to characterise the initiative and the remote community and school context as intercultural and complex. Also in relation to the Whole of Community Engagement initiative, Moore et al. propose an intercultural perspective as a refinement to the both-ways approach to remote education. Osborne et al. focus on aspirations of students, their families and communities at Nyangatjatjara College an independent Aboriginal school distributed across three campuses in the southern region of the Northern Territory. Macdonald and Gringart present a new measurement instrument, the Multi-Dimensional Student Perceptions of School Questionnaire (MSPSQ), validated with a moderate-sized sample of Indigenous and non-Indigenous secondary students in Western Australia. 
The next suite of papers has an international focus with papers from Canada, Aotearoa (New Zealand), Brazil, and Tonga. Stavrou and Murphy explore tensions surrounding Indigenising school mathematics in a Western Canadian prairie province conducted with three Cree elementary school teachers while Denston et al. examine teachers’ perceptions and experiences of a collaborative case study to adapt a literacy approach originally designed for an Aotearoa (New Zealand) English-medium context. Ioris et al. explore the main trends and pending gaps related to indigenous education in Brazil while Fonua et al. shares the stories of 26 successful Tongan science learners who participated in talanoa (open discussion without an agenda) about their engagement, enjoyment, and success in secondary and university science education in Aotearoa (New Zealand). 
The final papers in this volume shift to the university context with Hogarth exploring a small pilot study conducted at a Queensland university examining how academics perceive the inclusion of Indigenous Knowledges within institutional and professional contexts and initial teacher education programs. Forsyth et al. speak to the importance of employing Indigenous methodologies when conducting Indigenous research to improve dental and medical health outcomes for Indigenous peoples. Hook and Jessen reflect on the contentious nature of non-Indigenous academics teaching Indigenous Studies and draw on student survey data to illustrate the conflict between their pedagogic practices, student expectations and the structural impediments to their teaching aims. Smith et al. also provide a personal reflection on the higher education context by discussing the need to have institutional conversations about coloniality, institutional racism and white fragility within tertiary institutions. The final paper in this volume by Gibbs et al. explores the relationships between racism, cultural resilience, and educational engagement and academic outcomes for Aboriginal tertiary students. They highlight that cultural resilience and support is critical to Aboriginal student success within universities. Racism continues to be particularly important to address because, as the 2022 Australian Reconciliation Barometer recently highlighted, experiences of racial prejudice have increased for Aboriginal and Torres Strait Islander people over the last two years and certainly there is much work needed to improve relationships between Indigenous and non-Indigenous people. 
We hope you enjoy reading the articles in this volume and hope the articles lead to further dialogue and discussion about Indigenous educational success both in Australia and internationally. 
Bronwyn Fredericks, Martin Nakata, and Katelyn Barney",,Yes,,2025-11-11T00:15:19.325Z
editorialforinvestig-2022,Editorial for “Investigation of the Inter‐ and Intra‐Scanner Reproducibility and Repeatability of Radiomics Features in Magnetic Resonance Imaging”,J. Brabec; F. Lennartsson,2022,Journal of Magnetic Resonance Imaging,2,https://www.semanticscholar.org/paper/26f5fc56ff98df31c9e54b1e760e976eb46028a2,,10.1002/jmri.28190,"Radiomics refers to a multistep process of feature extraction from medical images and their analysis by computer algorithms. The key idea is that the information contained in an image may not reveal itself by a naked eye but may become apparent through algorithms. This could lead to new imaging contrasts, perhaps because of the fundamentally different nature of how we, man and computer, arrive at image characterization—in the case of radiologists, the analysis relies on the wiring of a brain, whereas radiomics relies on simple but incremental mathematical operations. Furthermore, radiomics as a process can be divided into consecutive steps: acquisition of the medical images, identification of the volume of interest that relates to the pathology and its segmentation, extraction of relevant features describing the pathology, creating or sharing databases, and, finally, proposition with validation of predictive models. This is the main reason why radiomics holds the promise of discovering hidden correlations and bringing transparency into radiologists’ decisionmaking, respectively. One could summarize that radiomics is rather an approach than a specific method and that images are regarded as data rather than pictures. Radiomics is used to solve tasks by utilizing several families of features: these may be based on morphologic characteristics such as lesion size or volume, image intensity-based histograms or descriptors of the relationships between image voxels, such as gray-level co-occurrence, run-length, size-zone, or distance zone matrices. Features can also be extracted from images that are preprocessed by filters (eg, wavelet or Laplacian filters), as well as may stem from image fractal features. They can also be, however, based on machine learning techniques. One of such important tasks is a texture analysis—which is defined as a spatial distribution of an image that contains information on the local aspects of the tissue. To describe texture, we can use words in natural language such as coarseness, smoothness, or perhaps because it is difficult to grasp this concept in natural language a description by quantifiable features could be orthogonal and thus useful. MRI-based radiomics has already been applied in research, for example, to predict treatment response or outcome based on intensity histogram-based radiomic features. Radiomics could also bridge the gap between radiology and biology by correlating the features with underlying tumor genetics. Other applications include identification of malignant tissue, tumor classification, image guided radiotherapy, or distinguishing true progression from pseudo-progression. The results can be complementary to information obtained from histology, genotyping, laboratory results, clinical reports, or standard radiological reports or can be even corroborated in a decision support system. Although the possibilities of applications seem to be vast, and the theory is appealing, radiomics has not yet lived up to this promise and has not yet transitioned into the clinics because the key limitation of radiomics is its reproducibility. It is an issue even in the case when we use the same computational method and scan the participants on the same MR scanners! This is the question that is addressed in current issue of Journal of Magnetic Resonance in Imaging by the work titled “Investigation of the interand intra-scanner reproducibility and repeatability of radiomics features in magnetic resonance imaging” where the authors studied interand intra-scanner variability of radiomics variables in MRI trough multicenter validation. The authors found that, albeit few do, most of the radiomics features do not pass this minimal test! A way forward is to find a consensus on features that are reproducible, which is where this study contributes. However, one cannot be successful without a deeper understanding of the dependencies: the feature values depend both on parameter choices during calculations, but also on the parameters of the MR pulse-sequence. Based on this understanding, we can define features that are robust with respect to the dependencies. To further facilitate the data-driven approach, although challenging, it is crucial to create large and curated databases that also include a large span of the underlying dependencies. Because, if these are not at hand,",,Yes,,2025-11-11T00:15:19.325Z
effectsofselfregulat-2022,"Effects of self-regulated learning and procrastination on academic stress, subjective well-being, and academic achievement in secondary education",R. García-Ros; F. Pérez-Gónzalez; J. Tomás; P. Sancho,2022,Current Psychology,41,https://www.semanticscholar.org/paper/0e9087ca2feb8400039833999290ec0ef09d7bc0,https://link.springer.com/content/pdf/10.1007/s12144-022-03759-8.pdf,10.1007/s12144-022-03759-8,"The main objective of this study was to test a structural theoretical model of the effects of self-regulated learning on academic stress, subjective well-being, and academic achievement in Secondary Education, considering academic procrastination as a mediator. An additional aim was to explore whether these relationships were moderated by gender and educational level. Participants were 728 students in compulsory and post-compulsory secondary education in a large city in Eastern Spain. Path analysis results indicated that the proposed model showed satisfactory fit, with the three dimensions of self-regulated learning significantly predicting the educational outcomes considered, and that procrastination mediated these relationships. Overall, the model is able to predict 9.8% of the variance of academic stress, 23.1% of students wellbeing, and 14% of academic achievement. Moreover, the multi-group routine revealed no moderation effects due to gender, but educational level moderated two relationships, between self-efficacy and academic achievement and between metacognitive strategies and procrastination. Additionally, supplementary models were tested for three specific subjects (Spanish Language, Foreign Language and Mathematics), which showed an improvement in explained variance, being respectively: 29%, 28% and 27%. Results are discussed in light of previous research and in terms of their impact on educational practice.",,Yes,,2025-11-11T00:15:16.543Z
efficientvisionlangu-2022,Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment,Mustafa Shukor; Guillaume Couairon; M. Cord,2022,British Machine Vision Conference,27,https://www.semanticscholar.org/paper/966ccb741d4e8d92db931852f2d9480fb5c497a0,http://arxiv.org/pdf/2208.13628,10.48550/arXiv.2208.13628,"Vision and Language Pretraining has become the prevalent approach for tackling multimodal downstream tasks. The current trend is to move towards ever larger models and pretraining datasets. This computational headlong rush does not seem reasonable in the long term to move toward sustainable solutions, and de facto excludes academic laboratories with limited resources. In this work, we propose a new framework, dubbed ViCHA, that efficiently exploits the input data to boost the learning by: (a) a new hierarchical cross-modal alignment loss, (b) new self-supervised scheme based on masked image modeling, (c) leveraging image-level annotations, called Visual Concepts, obtained with existing foundation models such as CLIP to boost the performance of the image encoder. Although pretrained on four times less data, our ViCHA strategy outperforms other approaches on several downstream tasks such as Image-Text Retrieval, VQA, Visual Reasoning, Visual Entailment and Visual Grounding. The code will be made publicly available here: https://github.com/mshukor/ViCHA",arxiv:2208.13628,Yes,,2025-11-11T00:15:14.026Z
eliteplmanempiricals-2022,ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models,Junyi Li; Tianyi Tang; Zheng Gong; Lixin Yang; Zhuohao Yu; Z. Chen; Jingyuan Wang; Wayne Xin Zhao; Ji-rong Wen,2022,North American Chapter of the Association for Computational Linguistics,8,https://www.semanticscholar.org/paper/7f84d56fb8feb4e50cd6c3da3e3fd4ff6c4772cf,http://arxiv.org/pdf/2205.01523,10.48550/arXiv.2205.01523,"Nowadays, pretrained language models (PLMs) have dominated the majority of NLP tasks. While, little research has been conducted on systematically evaluating the language abilities of PLMs. In this paper, we present a large-scale empirical study on general language ability evaluation of PLMs (ElitePLM). In our study, we design four evaluation dimensions, memory, comprehension, reasoning, and composition, to measure ten widely-used PLMs within five categories. Our empirical results demonstrate that: (1) PLMs with varying training objectives and strategies are good at different ability tests; (2) fine-tuning PLMs in downstream tasks is usually sensitive to the data size and distribution; (3) PLMs have excellent transferability between similar tasks. Moreover, the prediction results of PLMs in our experiments are released as an open resource for more deep and detailed analysis on the language abilities of PLMs. This paper can guide the future work to select, apply, and design PLMs for specific tasks. We have made all the details of experiments publicly available at https://github.com/RUCAIBox/ElitePLM.",arxiv:2205.01523,Yes,,2025-11-11T00:15:14.026Z
emergentbilingualmid-2022,Emergent Bilingual Middle Schoolers’ Syncretic Reasoning in Statistical Modeling,Sarah C. Radke; Sara Vogel; Jasmine Y. Ma; C. Hoadley; Laura Ascenzi‐Moreno,2022,Teachers College Record,12,https://www.semanticscholar.org/paper/f51e959cc8a504de2549ef038da597f624b25ee8,,10.1177/01614681221104141,"Background/Context: Bi/multilingual students’ STEM learning is better supported when educators leverage their language and cultural practices as resources, but STEM subject divisions have been historically constructed based on oppressive, dominant values and exclude the ways of knowing of nondominant groups. Truly promoting equity requires expanding and transforming STEM disciplines. Purpose/Objective/Research Question/Focus of Study: This article contributes to efforts to illuminate emergent bi/multilingual students’ ways of knowing, languaging, and doing in STEM. We follow the development of syncretic literacies in relation to translanguaging practices, asking, How do knowledges and practices from different communities get combined and reorganized by students and teachers in service of new modeling practices? Setting and Participants: We focus on a seventh-grade science classroom, deliberately designed to support syncretic literacies and translanguaging practices, where computer science concepts were infused into the curriculum through modeling activities. The majority of the students in the bilingual program had arrived in the United States at most three years before enrolling, from the Caribbean and Central and South America. Research Design: We analyze one lesson that was part of a larger research–practice partnership focused on teaching computer science through leveraging translanguaging practices and syncretic literacies. The lesson was a modeling and computing activity codesigned by the teacher and two researchers about post–Hurricane María outmigration from Puerto Rico. Analysis used microethnographic methods to trace how students assembled translanguaging, social, and schooled practices to make sense of and construct models. Findings/Results: Findings show how students assembled representational forms from a variety of practices as part of accomplishing and negotiating both designed and emergent goals. These included sensemaking, constructing, explaining, justifying, and interpreting both the physical and computational models of migration. Conclusions/Recommendations: Implications support the development of theory and pedagogy that intentionally make space for students to engage in meaning-making through translanguaging and syncretic practices in order to provide new possibilities for lifting up STEM learning that may include, but is not constrained by, disciplinary learning. Additional implications for teacher education and student assessment practices call for reconceptualizing schooling beyond day-to-day curriculum as part of making an ontological shift away from prioritizing math, science, and CS disciplinary and language objectives as defined by and for schooling, and toward celebrating, supporting, and centering students’ diverse, syncretic knowledges and knowledge use.",,Yes,,2025-11-11T00:15:14.026Z
emergentanalogicalre-2022,Emergent analogical reasoning in large language models,Taylor W. Webb; K. Holyoak; Hongjing Lu,2022,Nature Human Behaviour,395,https://www.semanticscholar.org/paper/3cbffab9d7981da6662d474aaa056dcbd3c1701e,,10.1038/s41562-023-01659-w,"The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of Generative Pre-trained Transformer (GPT)-3) on a range of analogical tasks, including a non-visual matrix reasoning task based on the rule structure of Raven’s Standard Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT-4 indicated even better performance. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems. Webb et al. show that new artificial intelligence language models, such as Generative Pre-trained Transformer 3, are able to solve analogical reasoning problems at a human-like level of performance.",arxiv:2212.09196,Yes,,2025-11-11T00:13:07.427Z
energyefficientbnnac-2022,Energy Efficient BNN Accelerator using CiM and a Time-Interleaved Hadamard Digital GRNG in 22nm CMOS,R. Dorrance; D. Dasalukunte; Hechen Wang; Renzhi Liu; B. Carlton,2022,International Symposium on Security in Computing and Communications,6,https://www.semanticscholar.org/paper/49b7eeb8357c28527a05831c081785e1ddff9b5e,,10.1109/A-SSCC56115.2022.9980539,"In recent years, Neural Networks (NNs) have achieved tremendous success in a variety of fields, such as computer vision, natural language processing, speech recognition, autonomous driving, and healthcare [1] –[4]. However, conventional NNs rely heavily on large, labeled training datasets, which can lead to overfitting and overconfident decision making (especially when faced with unfamiliar, out-of-distribution inputs) [1] –[4]. Unlike conventional NNs, the weights of Bayesian Neural Network (BNNs) are represented by probability distributions, providing a mathematical framework to quantify the uncertainties in a model’s final prediction [3]. These uncertainty estimates allow BNNs to mitigate overfitting issues, enable training with smaller datasets, and help increase overall model accuracy through the implicit use of stochastic rounding [5]. Figure 1 shows an example with a BNN version of LeNet-5 [6], where weights of the network are represented by Gaussian distributions. The ambiguous digit is classified as a ‘5’ and ‘3’ with probabilities of 80% and 20%, respectively. However, these uncertainty estimates come at great computational cost, as multiple forward inference passes are required to generate the necessary posterior distributions. As such, BNNs require not only efficient, high-performance multiply-accumulation (MAC), but an efficient Gaussian Random Number Generator (GRNG) with high-quality statistics as well.",,Yes,,2025-11-11T00:15:16.541Z
englishproficiencyge-2022,"English Proficiency, Gender and the Occupations of Childhood Immigrants in the US",A. Adserà; Aditi Bhowmick,2022,Journal of Labor Research,2,https://www.semanticscholar.org/paper/aecf7b957e043ccdff84eeff7e520fbe94dc6c51,,10.1007/s12122-022-09339-w,,,Yes,,2025-11-11T00:15:16.543Z
enhancingcommunicati-2022,Enhancing Communication Reliability from the Semantic Level under Low SNR,Yueling Liu; Yichi Zhang; Peng Luo; Shengteng Jiang; Kuo Cao; Haitao Zhao; Jibo Wei,2022,Electronics,4,https://www.semanticscholar.org/paper/d5b032831439dc252fee9fd6075ed7efd2d1e473,https://www.mdpi.com/2079-9292/11/9/1358/pdf?version=1650857126,10.3390/electronics11091358,"In the low signal-to-noise ratio region, a large number of bit errors occur, and it may exceed the channel error correction capability of the receiver. Traditional communication system may use the technology of automatic repeat-request to deal with this problem, which is time consuming and a waste of resources. To enhance the reliability of the communication system, we investigate reasoning and decoding at the semantic level instead of the grammar level. In particular, we propose a semantic communication model for text transmission, assisting the communication system to be more robust in terrible channel environments. Based on the traditional communication system, the language model BERT, part of speech tagging, and prior information concerning bit-flipping are introduced to enhance the semantic reasoning ability of the transceiver. Furthermore, this paper analyzes the effects of the sub-strategies on the performances of the improved communication model, such as the existence of a candidate set and language model. The numerical results show the effectiveness of our model in terms of improving the semantic accuracy measured by BLUE, the METEOR score, and the similarity score based on BERT between transmitted messages and recovered messages.",,Yes,,2025-11-11T00:15:14.026Z
enhancingfinancialta-2022,Enhancing Financial Table and Text Question Answering with Tabular Graph and Numerical Reasoning,Rungsiman Nararatwong; Natthawut Kertkeidkachorn; R. Ichise,2022,AACL,10,https://www.semanticscholar.org/paper/e470b2cac11c9cbef93d395b7d778481b39a8735,,10.18653/v1/2022.aacl-main.72,"Typical financial documents consist of tables, texts, and numbers. Given sufficient training data, large language models (LM) can learn the tabular structures and perform numerical reasoning well in question answering (QA). However, their performances fall significantly when data and computational resources are limited. This study improves this performance drop by infusing explicit tabular structures through a graph neural network (GNN). We proposed a model developed from the baseline of a financial QA dataset named TAT-QA. The baseline model, TagOp, consists of answer span (evidence) extraction and numerical reasoning modules. As our main contributions, we introduced two components to the model: a GNN-based evidence extraction module for tables and an improved numerical reasoning module. The latter provides a solution to TagOp’s arithmetic calculation problem specific to operations requiring number ordering, such as subtraction and division, which account for a large portion of numerical reasoning. Our evaluation shows that the graph module has the advantage in low-resource settings, while the improved numerical reasoning significantly outperforms the baseline model.",,Yes,,2025-11-11T00:14:11.169Z
enhancinguppersecond-2022,Enhancing Upper Secondary Learners’ Problem-solving Abilities using Problem-based Learning in Mathematics,Aline Dorimana; Alphonse Uworwabayeho; Gabriel Nizeyimana,2022,"International Journal of Learning, Teaching and Educational Research",19,https://www.semanticscholar.org/paper/12721d9e9e11b75e2e279d0d0e13995b89772004,https://ijlter.org/index.php/ijlter/article/download/5389/pdf,10.26803/ijlter.21.8.14,"Developing problem-solving abilities is a major objective of learning mathematics at school. However, learners’ problem-solving abilities are still critical. The main purpose of this study was to investigate how the problem-based learning model could enhance learners’ problem-solving abilities in mathematics. The study used quasi-experimental research with one group pre-test-post-test design. The population in this study consisted of fifty-four grade eleven learners (aged between 16 to 19 years old) from one school in Kayonza District in Rwanda. Data were collected using mathematical problem-solving tests and interviews and were analysed using paired t-tests for dependent sample means and descriptive analysis. The study results indicate that problem-based learning potentially impacts learners’ problem-solving abilities. It is shown from learners’ work in problem-solving that all indicators of problem-solving abilities, namely understanding the problem, planning ways to approach the problem, monitoring the progress while tackling the problem and reviewing the solution process, emerged as being fairly well improved. In addition, based on the interview results from some learners and their teachers, they like the PBL model because embedded tasks helped them to apply the knowledge that can improve their reasoning, creativity and thinking capability. The study recommends that schools encourage teachers to adopt PBL for enhancing learners’ problem-solving abilities. Additionally, researchers are urged to use findings from this study as a reference for further research. Furthermore, researchers could conduct similar research on a large scale using different methodologies.",,Yes,,2025-11-11T00:15:19.325Z
equityandparityinpri-2022,Equity and Parity in Primary Education: A Study on Performance in Language and Mathematics Using Hierarchical Linear Models,Inés Lucas-Oliva; Jesús García-Jiménez; J. Torres-Gordillo; Javier Rodríguez-Santero,2022,Sustainability,3,https://www.semanticscholar.org/paper/9267cdd12a9f958c7ce94e9ea04b656926055d0b,https://www.mdpi.com/2071-1050/14/19/12404/pdf?version=1664524882,10.3390/su141912404,"Education plays a crucial role in the development and consolidation of equality in society, which is reflected in the SDGs of the UN 2030 Agenda. Knowing the educational performance of schools is necessary to diagnose needs, evaluate proposals and undertake improvements in education policies. This study pursued a twofold objective: (1) to assess the equity and parity of Andalusian schools in relation to the competencies of mathematical reasoning and linguistic communication and (2) to study the relationship among educational performance, equity and parity in these competences. Hierarchical linear model research was designed and implemented in a population of 79,806 schoolchildren and 2092 schools. The results confirmed differences in equity and parity among schools. A relation was found between higher effectiveness and higher parity. Nonpublic schools are not more efficient than public schools; rather, it is the average economic and sociocultural status of schools that controls for their effectiveness. In conclusion, the educational system does not guarantee the same opportunities for all children; thus, the equity and parity of educational systems should be key criteria for their evaluation, ensuring that quality education reaches everyone equally. Further implications are also discussed.",,Yes,,2025-11-11T00:14:11.169Z
eruptingarcvolcanovi-2022,"Erupting Arc Volcano ( Villarrica , Chile ) from 2 Unsupervised Machine Learning Analysis of Mineral 3 Compositions",Felix O. Boschetty; D. Ferguson; J. Cortés; Eduardo; Morgado; S. Ebmeier; D. Morgan; J. E. Romero; C. S. Parejas,2022,,0,https://www.semanticscholar.org/paper/e37d4137eba5cecac985e104aeed94209972d16a,,,,,Yes,,2025-11-11T00:15:19.325Z
eventsrealmeventreas-2022,EvEntS ReaLM: Event Reasoning of Entity States via Language Models,Evangelia Spiliopoulou; Artidoro Pagnoni; Yonatan Bisk; E. Hovy,2022,Conference on Empirical Methods in Natural Language Processing,11,https://www.semanticscholar.org/paper/748a2700ec11f51560a69ec05c67ca9f97014be7,https://arxiv.org/pdf/2211.05392,10.48550/arXiv.2211.05392,"This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.",arxiv:2211.05392,Yes,,2025-11-11T00:13:07.427Z
evaluateconfidencein-2022,Evaluate Confidence Instead of Perplexity for Zero-shot Commonsense Reasoning,Letian Peng; Z. Li; Hai Zhao,2022,arXiv.org,1,https://www.semanticscholar.org/paper/8e73435fba7d3e02fe5599524bfa62a12f9f63a8,http://arxiv.org/pdf/2208.11007,10.48550/arXiv.2208.11007,"Commonsense reasoning is an appealing topic in natural language processing (NLP) as it plays a fundamental role in supporting the human-like actions of NLP systems. With large-scale language models as the backbone, unsupervised pre-training on numerous corpora shows the potential to capture commonsense knowledge. Current pre-trained language model (PLM)-based reasoning follows the traditional practice using perplexity metric. However, commonsense reasoning is more than existing probability evaluation, which is biased by word frequency. This paper reconsiders the nature of commonsense reasoning and proposes a novel commonsense reasoning metric, Non-Replacement Confidence (NRC). In detail, it works on PLMs according to the Replaced Token Detection (RTD) pre-training objective in ELECTRA, in which the corruption detection objective reflects the confidence on contextual integrity that is more relevant to commonsense reasoning than existing probability. Our proposed novel method boosts zero-shot performance on two commonsense reasoning benchmark datasets and further seven commonsense question-answering datasets. Our analysis shows that pre-endowed commonsense knowledge, especially for RTD-based PLMs, is essential in downstream reasoning.",arxiv:2208.11007,Yes,,2025-11-11T00:14:11.169Z
evaluatingbertonclou-2022,Evaluating BERT on cloud-edge time series forecasting and sentiment analysis via prompt learning,Qizhi Li; Xianyong Li; Yujia Song; Maolin Zhang; Longqi Chen; Gang Wang; Yajun Du,2022,"2022 IEEE 24th Int Conf on High Performance Computing & Communications; 8th Int Conf on Data Science & Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)",5,https://www.semanticscholar.org/paper/534909858fa7d4799d59bdcaf813f60036589f2c,,10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00051,"Existing pre-trained language models (PTLMs), like BERT, have shown their powerful ca-pabilities in many natural language processing tasks. In sequence analysis, such as time series forecasting, anomaly detection, and sentiment analysis, PTLMs have also achieved new state-of-the-art results. However, does this mean that PTLMs know sequence analysis? This paper explores whether BERT pre-trained on a large amount of data contains knowledge of sequence analysis. Specifically, we adopt prompt learning to see whether BERT will achieve good results on cloud-edge time series forecasting and sentiment analysis tasks. For the cloud-edge time series forecasting task, we give BERT some regular cloud-edge data and let it predict the features of the next time step; For the sentiment analysis task, we give BERT some sentence with sentiment and ask it what sentiment these sen-tences carry. Our experimental results reveal that: (1) BERT performs not well on the cloud-edge time series forecasting task, which means the logical reasoning of BERT is not good; (2) for sentiment analysis task, BERT with the prompt template performs poorly on both English and Chinese datasets; and (3) for sentiment analysis task, BERT appears to be more likely to perceive the text as carrying positive sentiment.",,Yes,,2025-11-11T00:15:16.543Z
evaluationofjapanese-2022,Evaluation of Japanese Teaching Quality Based on Deep Neural Network,Hailing Liu,2022,Security and Communication Networks,5,https://www.semanticscholar.org/paper/02433ab5edb91298be86431ea55e1087db8ce4c7,https://downloads.hindawi.com/journals/scn/2022/3466632.pdf,10.1155/2022/3466632,"The 21st century is an era of rapid development of information and frequent international exchanges, and Japanese language teaching has received increasing attention. Because of this, colleges and universities are now focused on improving the quality of Japanese education, both now and in the future. We need to boost the whole management of teaching quality, notably the assessment of instructors’ teaching quality, in order to improve teaching quality. However, because a number of factors influence the quality of instruction, and each factor’s weight varies, the evaluation results are difficult to express in a mathematical analytical formula, resulting in a complex nonlinear classification problem that traditional classification methods cannot solve well. As a new technology, as a result of the artificial neural networks (ANNs) fundamental qualities, it has been extensively applied in different evaluation issues for pattern recognition, nonlinear classification, and other research. This subject introduces the optimized deep neural network theory into Japanese teaching quality evaluation and completes the following work: (1) the algorithm of discrete Hopfield neural network is introduced in detail, and the neural network theory is introduced into teaching evaluation. (2) Then, based on the evaluation data of teachers’ teaching quality in a school, a large number of simulation experiments and training were carried out, and a neural network model for evaluation of teachers’ teaching effect was constructed and designed. Experiments reveal that the neural network model proposed in this paper is a nonlinear mapping method, which increases the evaluation’s dependability and makes the outcomes more effective and objective.",,Yes,,2025-11-11T00:15:19.325Z
evolutionoftechnolog-2022,Evolution of Technology in Artificial Intelligence (AI),Mr. Nagesh U B; Vaishnavi PS; Varshith; Vshker Mayengbam,2022,"International Journal of Advanced Research in Science, Communication and Technology",1,https://www.semanticscholar.org/paper/b0106f636afa8f8e6f42d13ddc0200365f5425bb,https://doi.org/10.48175/ijarsct-5830,10.48175/ijarsct-5830,"Artificial Intelligence (A.I.) is a multidisciplinary field whose objective is to mechanize exercises that by and by require human knowledge. Late accomplishments in A.I. incorporate mechanized clinical diagnosticians and frameworks that naturally redo equipment to specific client prerequisites. The serious pain points tended to in A.I. can be summed up as Perception, Manipulation, Reasoning, Communication, and Learning. Discernment is worried about building models of the actual world from tactile information (visual, sound, and so on) Control is worried about articulating extremities (e.g., mechanical arms, velocity gadgets) to affect an optimal state in the real world. Thinking is worried about more significant level mental capacities like preparation, reaching inferential determinations from a world model, diagnosing, planning, and so on Correspondence treats the issue comprehension and passing on data using language. At long last, Learning treats the issue of consequently further developing framework execution after some time in view of the framework's insight. Various huge particular thoughts have risen up out of A.I. that bind together these different trouble spots and that structure the underpinning of the logical discipline. By and large, A.I. frameworks work in view of a Knowledge Base of realities and decides that portray the framework's space of capability. The components of a Knowledge Base comprise of autonomously legitimate (or if nothing else conceivable) lumps of data. The framework should naturally sort out and use this data to tackle the particular issues that it experiences. This association cycle can be for the most part described as a Search coordinated toward explicit objectives. The pursuit is made complex in light of the need to decide the significance of data and in view of the incessant event of unsure and uncertain information. Heuristics give the A.I. framework with a component for centering its consideration and controlling its looking through processes. The fundamentally versatile association of A.I. frameworks yields the necessity for A.I. computational Architectures. All data utilized by the system ought to be tended to inside such a plan. The obtaining and encoding of true information into A.I. design contains the subfield of Knowledge Engineering",,Yes,,2025-11-11T00:15:16.543Z
evolutionarylossofco-2022,Evolutionary loss of complexity in human vocal anatomy as an adaptation for speech,Takeshi Nishimura; Isao T. Tokuda; S. Miyachi; Jacob C. Dunn; C. Herbst; Kazuyoshi Ishimura; Akihisa Kaneko; Yuki Kinoshita; H. Koda; J. Saers; H. Imai; Tetsuya Matsuda; O. Larsen; U. Jürgens; Hideki Hirabayashi; S. Kojima; W. Fitch,2022,Science,49,https://www.semanticscholar.org/paper/1065c2a6c6c71d9d60bdef7b775395bfab2f1636,https://figshare.com/articles/journal_contribution/Evolutionary_loss_of_complexity_in_human_vocal_anatomy_as_an_adaptation_for_speech/23769099/1/files/42210294.pdf,10.1126/science.abm1574,"Human speech production obeys the same acoustic principles as vocal production in other animals but has distinctive features: A stable vocal source is filtered by rapidly changing formant frequencies. To understand speech evolution, we examined a wide range of primates, combining observations of phonation with mathematical modeling. We found that source stability relies upon simplifications in laryngeal anatomy, specifically the loss of air sacs and vocal membranes. We conclude that the evolutionary loss of vocal membranes allows human speech to mostly avoid the spontaneous nonlinear phenomena and acoustic chaos common in other primate vocalizations. This loss allows our larynx to produce stable, harmonic-rich phonation, ideally highlighting formant changes that convey most phonetic information. Paradoxically, the increased complexity of human spoken language thus followed simplification of our laryngeal anatomy. Description Complexity from simplification Human speech and language are highly complex, consisting of a large number of sounds. The human phonal apparatus, the larynx, has acquired the capability to create a wider array of sounds, even though previous work has revealed many similarities between our larynx and those in other primates. Looking across a large number of primates, Nishimura et al. used a combination of anatomical, phonal, and modeling approaches to characterize sound production in the larynx (see the Perspective by Gouzoules). They found that instead of the human larynx having increased complexity, it has actually simplified relative to other primates, allowing for clearer sound production with less aural chaos. —SNV The human larynx has undergone evolutionary simplification, facilitating the increased acoustic complexity of the spoken language.",,Yes,,2025-11-11T00:15:14.026Z
examininghomophilyla-2022,"Examining Homophily, Language Coordination, and Analytical Thinking in Web-Based Conversations About Vaccines on Reddit: Study Using Deep Neural Network Language Models and Computer-Assisted Conversational Analyses",Yue Li; W. Gee; Kun Jin; Robert M. Bond,2022,Journal of Medical Internet Research,2,https://www.semanticscholar.org/paper/e18680527f12dc9f1cbdcf241e0e6b01af4498c1,https://www.jmir.org/2023/1/e41882/PDF,10.2196/41882,"Background Vaccine hesitancy has been deemed one of the top 10 threats to global health. Antivaccine information on social media is a major barrier to addressing vaccine hesitancy. Understanding how vaccine proponents and opponents interact with each other on social media may help address vaccine hesitancy. Objective We aimed to examine conversations between vaccine proponents and opponents on Reddit to understand whether homophily in web-based conversations impedes opinion exchange, whether people are able to accommodate their languages to each other in web-based conversations, and whether engaging with opposing viewpoints stimulates higher levels of analytical thinking. Methods We analyzed large-scale conversational text data about human vaccines on Reddit from 2016 to 2018. Using deep neural network language models and computer-assisted conversational analyses, we obtained each Redditor’s stance on vaccines, each post’s stance on vaccines, each Redditor’s language coordination score, and each post or comment’s analytical thinking score. We then performed chi-square tests, 2-tailed t tests, and multilevel modeling to test 3 questions of interest. Results The results show that both provaccine and antivaccine Redditors are more likely to selectively respond to Redditors who indicate similar views on vaccines (P<.001). When Redditors interact with others who hold opposing views on vaccines, both provaccine and antivaccine Redditors accommodate their language to out-group members (provaccine Redditors: P=.044; antivaccine Redditors: P=.047) and show no difference in analytical thinking compared with interacting with congruent views (P=.63), suggesting that Redditors do not engage in motivated reasoning. Antivaccine Redditors, on average, showed higher analytical thinking in their posts and comments than provaccine Redditors (P<.001). Conclusions This study shows that although vaccine proponents and opponents selectively communicate with their in-group members on Reddit, they accommodate their language and do not engage in motivated reasoning when communicating with out-group members. These findings may have implications for the design of provaccine campaigns on social media.",,Yes,,2025-11-11T00:15:14.026Z
examiningcomputation-2022,Examining computational thinking processes in modeling unstructured data,Shiyan Jiang; Yingxiao Qian; Hengtao Tang; Rabia Yalcinkaya; C. Rosé; J. Chao; W. Finzer,2022,Education and Information Technologies : Official Journal of the IFIP technical committee on Education,15,https://www.semanticscholar.org/paper/06f065d0938522794fba6ba07e89652a8f4817af,,10.1007/s10639-022-11355-3,,,Yes,,2025-11-11T00:15:16.543Z
explainingpatternsin-2022,Explaining Patterns in Data with Language Models via Interpretable Autoprompting,Chandan Singh; John X. Morris; J. Aneja; Alexander M. Rush; Jianfeng Gao,2022,,0,https://www.semanticscholar.org/paper/224b8cd8c31cfa86c2a84bec3a65d9ba44f38280,,,"Large language models (LLMs) have displayed an impressive ability to harness natural language to perform complex tasks. In this work, we explore whether we can leverage this learned ability to find and explain patterns in data. Specifically, given a pre-trained LLM and data examples, we introduce interpretable autoprompting (iPrompt), an algorithm that generates a natural-language string explaining the data. iPrompt iteratively alternates between generating explanations with an LLM and reranking them based on their performance when used as a prompt. Experiments on a wide range of datasets, from synthetic mathematics to natural-language understanding, show that iPrompt can yield meaningful insights by accurately finding groundtruth dataset descriptions. Moreover, the prompts produced by iPrompt are simultaneously human-interpretable and highly effective for generalization: on real-world sentiment classification datasets, iPrompt produces prompts that match or even improve upon human-written prompts for GPT-3. Finally, experiments with an fMRI dataset show the potential for iPrompt to aid in scientific discovery. All code for using the methods and data here is made available on Github.",arxiv:2210.01848,Yes,,2025-11-11T00:15:14.026Z
explanationsfromlarg-2022,Explanations from Large Language Models Make Small Reasoners Better,SHIYANG LI; Jianshu Chen; Yelong Shen; Zhiyu Chen; Xinlu Zhang; Zekun Li; Hong Wang; Jingu Qian; Baolin Peng; Yi Mao; Wenhu Chen; Xifeng Yan,2022,arXiv.org,150,https://www.semanticscholar.org/paper/7d29a84a589aa5655e5d3fed8d725ea472816599,http://arxiv.org/pdf/2210.06726,10.48550/arXiv.2210.06726,"Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.",arxiv:2210.06726,Yes,,2025-11-11T00:13:07.427Z
explicitobjectrelati-2022,Explicit Object Relation Alignment for Vision and Language Navigation,Yue Zhang; Parisa Kordjamshidi,2022,Annual Meeting of the Association for Computational Linguistics,14,https://www.semanticscholar.org/paper/82e0fd80ac234fbad07fd05058e4c2ab3256ca8a,https://aclanthology.org/2022.acl-srw.24.pdf,10.18653/v1/2022.acl-srw.24,"In this paper, we investigate the problem of vision and language navigation. To solve this problem, grounding the landmarks and spatial relations in the textual instructions into visual modality is important. We propose a neural agent named Explicit Object Relation Alignment Agent (EXOR),to explicitly align the spatial information in both instruction and the visual environment, including landmarks and spatial relationships between the agent and landmarks.Empirically, our proposed method surpasses the baseline by a large margin on the R2R dataset. We provide a comprehensive analysis to show our model’s spatial reasoning ability and explainability.",,Yes,,2025-11-11T00:14:11.169Z
exploringlengthgener-2022,Exploring Length Generalization in Large Language Models,Cem Anil; Yuhuai Wu; Anders Andreassen; Aitor Lewkowycz; Vedant Misra; V. Ramasesh; Ambrose Slone; Guy Gur-Ari; Ethan Dyer; Behnam Neyshabur,2022,Neural Information Processing Systems,196,https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a,http://arxiv.org/pdf/2207.04901,10.48550/arXiv.2207.04901,"The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.",arxiv:2207.04901,Yes,,2025-11-11T00:13:07.427Z
fcmforgetfulcausalma-2022,FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners,Hao Liu; Xinyang Geng; Lisa Lee; Igor Mordatch; S. Levine; Sharan Narang; P. Abbeel,2022,arXiv.org,5,https://www.semanticscholar.org/paper/75af74f4e371b371147fb1d1e81addd921449cdd,http://arxiv.org/pdf/2210.13432,10.48550/arXiv.2210.13432,,,Yes,,2025-11-11T00:15:14.026Z
flopsasadiscriminant-2022,FLOPs as a Discriminant for Dense Linear Algebra Algorithms,F. L'opez; L. Karlsson; P. Bientinesi,2022,International Conference on Parallel Processing,5,https://www.semanticscholar.org/paper/cfd35d1dd27436fd850ee34b532e806eaa7b7b17,https://dl.acm.org/doi/pdf/10.1145/3545008.3545072,10.1145/3545008.3545072,"Expressions that involve matrices and vectors, known as linear algebra expressions, are commonly evaluated through a sequence of invocations to highly optimised kernels provided in libraries such as BLAS and LAPACK. A sequence of kernels represents an algorithm, and in general, because of associativity, algebraic identities, and multiple kernels, one expression can be evaluated via many different algorithms. These algorithms are all mathematically equivalent (i.e., in exact arithmetic, they all compute the same result), but often differ noticeably in terms of execution time. When faced with a decision, high-level languages, libraries, and tools such as Julia, Armadillo, and Linnea choose by selecting the algorithm that minimises the FLOP count. In this paper, we test the validity of the FLOP count as a discriminant for dense linear algebra algorithms, analysing ”anomalies”: problem instances for which the fastest algorithm does not perform the least number of FLOPs. To do so, we focused on relatively simple expressions and analysed when and why anomalies occurred. We found that anomalies exist and tend to cluster into large contiguous regions. For one expression anomalies were rare, whereas for the other they were abundant. We conclude that FLOPs is not a sufficiently dependable discriminant even when building algorithms with highly optimised kernels. Plus, most of the anomalies remained as such even after filtering out the inter-kernel cache effects. We conjecture that combining FLOP counts with kernel performance models will significantly improve our ability to choose optimal algorithms.",arxiv:2207.02070,Yes,,2025-11-11T00:15:19.325Z
folionaturallanguage-2022,FOLIO: Natural Language Reasoning with First-Order Logic,Simeng Han; Hailey Schoelkopf; Yilun Zhao; Zhenting Qi; Martin Riddell; Luke Benson; Lucy Sun; E. Zubova; Yujie Qiao; Matthew Burtell; David Peng; Jonathan Fan; Yixin Liu; Brian Wong; Malcolm Sailor; Ansong Ni; Linyong Nan; Jungo Kasai; Tao Yu; Rui Zhang; Shafiq R. Joty; Alexander R. Fabbri; Wojciech Kryscinski; Xi Victoria Lin; Caiming Xiong; Dragomir R. Radev,2022,Conference on Empirical Methods in Natural Language Processing,144,https://www.semanticscholar.org/paper/5581bf85386737bd3378eec68189759a05280bea,http://arxiv.org/pdf/2209.00840,10.48550/arXiv.2209.00840,"Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.",arxiv:2209.00840,Yes,,2025-11-11T00:13:07.427Z
facilitatingautomate-2022,"Facilitating automated conversion of scientific knowledge into scientific simulation models with the Machine Assisted Generation, Calibration, and Comparison (MAGCC) Framework",Chase Cockrell; S. Christley; G. An,2022,arXiv.org,2,https://www.semanticscholar.org/paper/85808a4a7199dbc7890152caf84e9ecd73ae7563,http://arxiv.org/pdf/2204.10382,10.48550/arXiv.2204.10382,"The Machine Assisted Generation, Comparison, and Calibration (MAGCC) framework provides machine assistance and automation of recurrent crucial steps and processes in the development, implementation, testing, and use of scientific simulation models. MAGCC bridges systems for knowledge extraction via natural language processing or extracted from existing mathematical models and provides a comprehensive workflow encompassing the composition of scientific models and artificial intelligence (AI)-assisted code generation. MAGCC accomplishes this through: 1) the development of a comprehensively expressive formal knowledge representation knowledgebase, the Structured Scientific Knowledge Representation (SSKR) that encompasses all the types of information needed to make any simulation model, 2) the use of an artificially-intelligent logic-reasoning system, the Computational Modeling Assistant (CMA), that takes information from the SSKR and generates, in a traceable fashion, model specifications across a range of simulation modeling methods, and 3) the use of the CMA to generate compliable/executable code for a simulation model from those model specifications. The current MAGCC framework can be customized any scientific domain’s specific knowledgebase and existing mathematical/computational models, and future work will involve expanding the types of computational model representation that can be generated and integrating newly-developed code generating AI systems.",arxiv:2204.10382,Yes,,2025-11-11T00:15:16.543Z
facultymembersandlab-2022,Faculty Members and Labs in Department of Computer Science,Hiroshi Imai,2022,,0,https://www.semanticscholar.org/paper/67f3fd328281e803e6fb2a6909f0ec02e3985b17,,,,,Yes,,2025-11-11T00:15:19.325Z
faithfulreasoningusi-2022,Faithful Reasoning Using Large Language Models,Antonia Creswell; M. Shanahan,2022,arXiv.org,133,https://www.semanticscholar.org/paper/f0a0e8b6e84207f50db4d24cc4016e40601214ef,,,"Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.",arxiv:2208.14271,Yes,,2025-11-11T00:13:07.427Z
followupattentionane-2022,Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration,Matteo Paltenghi; Rahul Pandita; Austin Z. Henley; Albert Ziegler,2022,IEEE Transactions on Software Engineering,5,https://www.semanticscholar.org/paper/eff1130a44419b21971dc2228576066af36a20f8,https://doi.org/10.1109/tse.2024.3445338,10.1109/TSE.2024.3445338,"Recent neural models of code, such as OpenAI Codex and AlphaCode, have demonstrated remarkable proficiency at code generation due to the underlying attention mechanism. However, it often remains unclear how the models actually process code, and to what extent their reasoning and the way their attention mechanism scans the code matches the patterns of developers. A poor understanding of the model reasoning process limits the way in which current neural models are leveraged today, so far mostly for their raw prediction. To fill this gap, this work studies how the processed attention signal of three open large language models - CodeGen, InCoder and GPT-J - agrees with how developers look at and explore code when each answers the same sensemaking questions about code. Furthermore, we contribute an open-source eye-tracking dataset comprising 92 manually-labeled sessions from 25 developers engaged in sensemaking tasks. We empirically evaluate five heuristics that do not use the attention and ten attention-based post-processing approaches of the attention signal of CodeGen against our ground truth of developers exploring code, including the novel concept of follow-up attention which exhibits the highest agreement between model and human attention. Our follow-up attention method can predict the next line a developer will look at with 47% accuracy. This outperforms the baseline prediction accuracy of 42.3%, which uses the session history of other developers to recommend the next line. These results demonstrate the potential of leveraging the attention signal of pre-trained models for effective code exploration.",arxiv:2210.05506,Yes,,2025-11-11T00:15:14.026Z
formalmetatheoryofse-2022,Formal metatheory of second-order abstract syntax,M. Fiore; Dmitrij Szamozvancev,2022,Proc. ACM Program. Lang.,24,https://www.semanticscholar.org/paper/12f42f44024d47a9fbbad04d7d2701b6398e6f44,https://dl.acm.org/doi/pdf/10.1145/3498715,10.1145/3498715,"Despite extensive research both on the theoretical and practical fronts, formalising, reasoning about, and implementing languages with variable binding is still a daunting endeavour – repetitive boilerplate and the overly complicated metatheory of capture-avoiding substitution often get in the way of progressing on to the actually interesting properties of a language. Existing developments offer some relief, however at the expense of inconvenient and error-prone term encodings and lack of formal foundations. We present a mathematically-inspired language-formalisation framework implemented in Agda. The system translates the description of a syntax signature with variable-binding operators into an intrinsically-encoded, inductive data type equipped with syntactic operations such as weakening and substitution, along with their correctness properties. The generated metatheory further incorporates metavariables and their associated operation of metasubstitution, which enables second-order equational/rewriting reasoning. The underlying mathematical foundation of the framework – initial algebra semantics – derives compositional interpretations of languages into their models satisfying the semantic substitution lemma by construction.",arxiv:2201.03504,Yes,,2025-11-11T00:15:16.543Z
formalreasoningabout-2022,Formal reasoning about layered monadic interpreters,Irene Yoon; Yannick Zakowski; Steve Zdancewic,2022,Proc. ACM Program. Lang.,20,https://www.semanticscholar.org/paper/883f4010fcc3fd47068d0d17c838d19cad5fcbd7,https://dl.acm.org/doi/pdf/10.1145/3547630,10.1145/3547630,"Monadic computations built by interpreting, or handling, operations of a free monad are a compelling formalism for modeling language semantics and defining the behaviors of effectful systems. The resulting layered semantics offer the promise of modular reasoning principles based on the equational theory of the underlying monads. However, there are a number of obstacles to using such layered interpreters in practice. With more layers comes more boilerplate and glue code needed to define the monads and interpreters involved. That overhead is compounded by the need to define and justify the relational reasoning principles that characterize the equivalences at each layer. This paper addresses these problems by significantly extending the capabilities of the Coq interaction trees (ITrees) library, which supports layered monadic interpreters. We characterize a rich class of interpretable monads---obtained by applying monad transformers to ITrees---and show how to generically lift interpreters through them. We also introduce a corresponding framework for relational reasoning about ""equivalence of monads up to a relation R"". This collection of typeclasses, instances, new reasoning principles, and tactics greatly generalizes the existing theory of the ITree library, eliminating large amounts of unwieldy boilerplate code and dramatically simplifying proofs.",,Yes,,2025-11-11T00:14:11.169Z
formalspecificationa-2022,Formal specification and model checking of lattice-based key encapsulation mechanisms in Maude,Duong Dinh Tran; K. Ogata; Santiago Escobar; S. Akleylek; A. Otmani,2022,FAVPQC@ICFEM,4,https://www.semanticscholar.org/paper/8ad95fba177e51c46192510578dfa0178b3d7ea4,,,,,Yes,,2025-11-11T00:15:14.026Z
fromhumandaystomachi-2022,From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams,Iddo Drori; Sarah J. Zhang; Reece Shuttleworth; Sarah J. Zhang; Keith Tyser; Zad Chin; Pedro Lantigua; Saisamrit Surbehera; Gregory Hunter; Derek Austin; Leonard Tang; Yann Hicke; Sage Simhon; S. Karnik; Darnell Granberry; Madeleine Udell,2022,Knowledge Discovery and Data Mining,13,https://www.semanticscholar.org/paper/a3dc36cb2ad9920f35746f980f003d423883f97c,https://dl.acm.org/doi/pdf/10.1145/3580305.3599827,10.1145/3580305.3599827,"A final exam in machine learning at a top institution such as MIT, Harvard, or Cornell typically takes faculty days to write, and students hours to solve. We demonstrate that large language models pass machine learning finals at a human level on finals available online and automatically generate new human-quality final exam questions in seconds. Previous work has developed program synthesis and few-shot learning methods to solve university-level problem set questions in mathematics and STEM courses. In this work, we develop and compare methods that solve final exams, which differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We curate a dataset and benchmark of questions from machine learning final exams available online and code for answering these questions and generating new questions. We show how to generate new questions from other questions and course notes. For reproducibility and future research on this final exam benchmark, we use automatic checkers for multiple-choice, numeric, and questions with expression answers. A student survey comparing the quality, appropriateness, and difficulty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for final exams. We perform ablation studies comparing zero-shot learning with few-shot learning and chain-of-thought prompting using GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that few-shot learning methods perform best. We highlight the transformative potential of language models to streamline the writing and solution of large-scale assessments, significantly reducing the workload from human days to mere machine seconds. Our results suggest that rather than banning large language models such as ChatGPT in class, instructors should teach students to harness them by asking students meta-questions about correctness, completeness, and originality of the responses generated, encouraging critical thinking in academic studies.",arxiv:2206.05442,Yes,,2025-11-11T00:15:16.543Z
frominferenceprocess-2022,From inference processes to situations of misunderstanding,Alaric Kohler; Teuta Mehmeti,2022,Journal of Argumentation in Context,1,https://www.semanticscholar.org/paper/94fd6bf8a31a6856b7c037890d87f7c1b5b2d3b9,,10.1075/jaic.18010.koh,"
In this paper, we describe inferences on a school task, which are reconstructed by the mean of two perspectives from argumentation theory: The pragma-dialectical model and Grize’s natural logic. Both analyses focus on the same item of mathematics, issued from a PISA survey, in order to discuss their specific contribution in elucidating the actual reasoning involved in both the student's answer and the evaluator’s expectations. The mismatch between these two points of view allow us to discuss the potentiality of a situation of misunderstanding.
Investigating how specific tasks in particular contexts are interpreted provides a contribution to methodological approaches treating thinking processes as situated and socially negotiated from a diversity of points of views, as for example Inhelder’s (1962) microgenetic approach. In order to extend such analysis to interpretations of discourse, an interdisciplinary approach combining argumentation theory and socio-cognitive psychology is needed.
Here, we observed for instance that students may provide the expected answers and still interpret the question or problem differently from the task’s designers (or “teacher”). The meaning of language and other signs, such as graphs or mathematical symbols, cannot be taken for granted when several interlocutors are involved. This issue chiefly concerns argumentation theory, since it raises the question of the integration of specific contexts and points of view in the analysis of argumentation. Therefore, argumentation should be analysed also as a process, and not only as a product; For more detail on this distinction, see for instance Grize (1996) and Kuhn & Udell (2003, 2007).",,Yes,,2025-11-11T00:15:19.325Z
fusionbrainresearchp-2022,FusionBrain: Research Project in Multimodal and Multitask Learning,Dimitar I. Dimitrov; A. Kuznetsov; A. A. Mal’tseva; E. F. Goncharova,2022,Doklady. Mathematics,0,https://www.semanticscholar.org/paper/91694fc5f0bae350157f4fc565d0207ae12f7eb9,,10.1134/S1064562422060242,,,Yes,,2025-11-11T00:15:16.543Z
fuzzytissuelikepsyst-2022,Fuzzy tissue-like P systems with promoters and their application in power coordinated control of microgrid,Wenping Yu; Jieping Wu; Yufeng Chen; Yubo Wu,2022,Journal of Membrane Computing,6,https://www.semanticscholar.org/paper/b7440c855634668553dd5ceed5109667f4dd7f20,,10.1007/s41965-022-00109-2,,,Yes,,2025-11-11T00:15:16.543Z
gptneoforcommonsense-2022,GPT-Neo for commonsense reasoning-a theoretical and practical lens,Rohan Kashyap; Vivek Kashyap; Narendra C.P,2022,arXiv.org,8,https://www.semanticscholar.org/paper/d7a9b3c750c7e5f0c9af3864a796b7fcdc07f030,https://arxiv.org/pdf/2211.15593,10.48550/arXiv.2211.15593,"Recent work has demonstrated substantial gains in pre-training large-language models (LLMs) followed by supervised fine-tuning on the downstream task. In this paper, we evaluate the performance of the GPT-neo model using $6$ commonsense reasoning benchmark tasks. We aim to examine the performance of smaller models using the GPT-neo models against several larger model baselines such as GPT-$3$, Llama-$2$, MPT and Falcon. Upon fine-tuning with the appropriate set of hyperparameters, our model achieves competitive accuracy on several tasks. We also investigate and substantiate our results using attention-head visualization to better understand the model performance. Finally, we conduct various robustness tests using various methods to gauge the model performance under numerous settings.",arxiv:2211.15593,Yes,,2025-11-11T00:14:11.169Z
galacticaalargelangu-2022,Galactica: A Large Language Model for Science,Ross Taylor; Marcin Kardas; Guillem Cucurull; Thomas Scialom; A. Hartshorn; Elvis Saravia; Andrew Poulton; Viktor Kerkez; Robert Stojnic,2022,arXiv.org,892,https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1,,,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",arxiv:2211.09085,Yes,,2025-11-11T00:13:07.427Z
geodingeosciencebase-2022,GeoDIN - Geoscience-Based Deep Interaction Networks for Predicting Flow Dynamics in Reservoir Simulation Models,M. Maučec; Ridwan Jalali,2022,SPE Journal,11,https://www.semanticscholar.org/paper/e35997934ca17d4fffe1f35e410696279240d088,,10.2118/203952-pa,"
 Network graphs represent a general language for describing complex systems and a framework for knowledge discovery. Graph learning is a new concept with applications emerging in biomedicine, pharmacology, smart mobility, and physical reasoning. When applied to petroleum systems, such as reservoir models, graphs provide unique differentiators for the abstraction of reservoir connectivity to facilitate “reservoir-centric” machine learning (ML) applications.
 In this paper, we demonstrate, for the first time, the application of geoscience-based deep interaction networks (GeoDIN) to learn complex physics relationships from 3D reservoir models for fast and accurate prediction of subsurface spatio-temporal flow dynamics. We build the network graph with embedded subsurface and physics representations and train the ML model to “act like the reservoir simulator.”
 We use a simulation benchmark model for two-phase incompressible flow, with approximately 1.1 million grid size, one central injector, and four corner producers. Static 3D grid properties include porosity and permeability. We use full-physics simulation output to construct the interaction network (IN) graph, where graph nodes objects (nodes) represent reservoir grid cells. We embed the feature vector combining pore, oil and water volumes, and pressure and relative permeability. The graph objects representing wells are connected with well completion factors. The producing wells have embedded oil and water production rates, while the objects representing injecting wells have embedded water injection rates. We represent graph relations (edges) with bidirectional transmissibility of the source cell. To preprocess the data for ML, we scale the graph object attributes using “min-max” normalization and we normalize the graph relation attributes using Box-Cox transformation.
 We train the GeoDIN framework to predict oil and water saturation dynamics in space and time. When benchmarked with full-physics simulation, the INs ran on two V100 graphics processing units and substantially accelerated the prediction phase compared to the physics-based simulator running on 70 Intel Xeon E5 CPU cores. On average, the error in GeoDIN predicted spatio-temporal distribution of oil saturation remains within 5% of full-physics simulation for 90% of model grid cells, while the error in water saturation remains within 2.5% of full-physics simulation. The spatio-temporal propagation of pressure is more sensitive to local embeddings of INs, which communicate on node-to-node information transfer. This results in a larger prediction error of the GeoDIN model when benchmarked to full-physics simulation. On average, the error distribution suggests that the great majority (90 to 95%) of grid cells fall within 10 to 30% error bound relative to full-physics simulation.
 The presented GeoDIN approach to network learning carries a game-changing potential for the prediction of subsurface flow dynamics. As the way forward, we will investigate the implementation of graph neural networks with automated feature learning, generalization, and scaleup.",,Yes,,2025-11-11T00:15:16.543Z
geometryoflearningad-2022,Geometry of learning: A data-driven understanding with neighborhood and graph methods,Sarath Shekkizhar,2022,,1,https://www.semanticscholar.org/paper/59ec42d3f55d895a0f513ec406299dbf85d93f05,,,,,Yes,,2025-11-11T00:15:16.543Z
greaselmgraphreasoni-2022,GreaseLM: Graph REASoning Enhanced Language Models for Question Answering,Xikun Zhang; Antoine Bosselut; Michihiro Yasunaga; Hongyu Ren; Percy Liang; Christopher D. Manning; J. Leskovec,2022,International Conference on Learning Representations,251,https://www.semanticscholar.org/paper/4ab41d9780f1d1ac34d39fa7e527e73652507fcc,,,"Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.",arxiv:2201.08860,Yes,,2025-11-11T00:13:07.427Z
groundingvisualrepre-2022,Grounding Visual Representations with Texts for Domain Generalization,Seonwoo Min; Nokyung Park; Siwon Kim; Seunghyun Park; Jinkyu Kim,2022,European Conference on Computer Vision,40,https://www.semanticscholar.org/paper/08fe439561157188b076b5c4b5c45e7e72b38741,http://arxiv.org/pdf/2207.10285,10.48550/arXiv.2207.10285,"Reducing the representational discrepancy between source and target domains is a key component to maximize the model generalization. In this work, we advocate for leveraging natural language supervision for the domain generalization task. We introduce two modules to ground visual representations with texts containing typical reasoning of humans: (1) Visual and Textual Joint Embedder and (2) Textual Explanation Generator. The former learns the image-text joint embedding space where we can ground high-level class-discriminative information into the model. The latter leverages an explainable model and generates explanations justifying the rationale behind its decision. To the best of our knowledge, this is the first work to leverage the vision-and-language cross-modality approach for the domain generalization task. Our experiments with a newly created CUB-DG benchmark dataset demonstrate that cross-modality supervision can be successfully used to ground domain-invariant visual representations and improve the model generalization. Furthermore, in the large-scale DomainBed benchmark, our proposed method achieves state-of-the-art results and ranks 1st in average performance for five multi-domain datasets. The dataset and codes are available at https://github.com/mswzeus/GVRT.",arxiv:2207.10285,Yes,,2025-11-11T00:15:14.026Z
guesteditorialprefac-2022,Guest Editorial Preface,Jianhui Lv,2022,,0,https://www.semanticscholar.org/paper/b216a212533213429aef2e29ded888d53b05df2d,,,,,Yes,,2025-11-11T00:15:16.543Z
hiddenschemanetworks-2022,Hidden Schema Networks,Ramsés J. Sánchez; L. Conrads; Pascal Welke; K. Cvejoski; C. Ojeda,2022,Annual Meeting of the Association for Computational Linguistics,4,https://www.semanticscholar.org/paper/bcf185005b4741d6b57fb017c9620a7a704db1c1,http://arxiv.org/pdf/2207.03777,10.48550/arXiv.2207.03777,"Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g. topics or sentiments, and that (ii) GPT-2-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk “reasoning” models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks.",arxiv:2207.03777,Yes,,2025-11-11T00:15:16.543Z
hierarchicalannotati-2022,Hierarchical Annotation for Building A Suite of Clinical Natural Language Processing Tasks: Progress Note Understanding,Yanjun Gao; Dmitriy Dligach; Timothy Miller; S. Tesch; Ryan Laffin; M. Churpek; M. Afshar,2022,International Conference on Language Resources and Evaluation,22,https://www.semanticscholar.org/paper/6a3a75561c627c118778e4d056c080cd70056d21,,10.48550/arXiv.2204.03035,"Applying methods in natural language processing on electronic health records (EHR) data has attracted rising interests. Existing corpus and annotation focus on modeling textual features and relation prediction. However, there are a paucity of annotated corpus built to model clinical diagnostic thinking, a processing involving text understanding, domain knowledge abstraction and reasoning. In this work, we introduce a hierarchical annotation schema with three stages to address clinical text understanding, clinical reasoning and summarization. We create an annotated corpus based on a large collection of publicly available daily progress notes, a type of EHR that is time-sensitive, problem-oriented, and well-documented by the format of Subjective, Objective, Assessment and Plan (SOAP). We also define a new suite of tasks, Progress Note Understanding, with three tasks utilizing the three annotation stages. This new suite aims at training and evaluating future NLP models for clinical text understanding, clinical knowledge representation, inference and summarization.",arxiv:2204.03035,Yes,,2025-11-11T00:14:11.169Z
holisticevaluationof-2022,Holistic Evaluation of Language Models,Percy Liang; Rishi Bommasani; Tony Lee; Dimitris Tsipras; Dilara Soylu; Michihiro Yasunaga; Yian Zhang; Deepak Narayanan; Yuhuai Wu; Ananya Kumar; Benjamin Newman; Binhang Yuan; Bobby Yan; Ce Zhang; Christian Cosgrove; Christopher D. Manning; Christopher Ré; Diana Acosta-Navas; Drew A. Hudson; E. Zelikman; Esin Durmus; Faisal Ladhak; Frieda Rong; Hongyu Ren; Huaxiu Yao; Jue Wang; Keshav Santhanam; Laurel J. Orr; Lucia Zheng; Mert Yüksekgönül; Mirac Suzgun; Nathan Kim; Neel Guha; Niladri S. Chatterji; O. Khattab; Peter Henderson; Qian Huang; Ryan Chi; Sang Michael Xie; Shibani Santurkar; Surya Ganguli; Tatsunori Hashimoto; Thomas Icard; Tianyi Zhang; Vishrav Chaudhary; William Wang; Xuechen Li; Yifan Mai; Yuhui Zhang; Yuta Koreeda,2022,arXiv.org,7,https://www.semanticscholar.org/paper/29abcf865613287c661385c39401424f709a3fda,https://arxiv.org/pdf/2211.09110,10.48550/arXiv.2211.09110,"Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.",arxiv:2211.09110,Yes,,2025-11-11T00:15:14.026Z
housekeeptidyingvirt-2022,Housekeep: Tidying Virtual Households using Commonsense Reasoning,Yash Kant; Arun Ramachandran; Sriram Yenamandra; Igor Gilitschenski; Dhruv Batra; Andrew Szot; Harsh Agrawal,2022,European Conference on Computer Vision,79,https://www.semanticscholar.org/paper/7890ece03cfb88e0620f8e791105569bd7128c76,https://arxiv.org/pdf/2205.10712,10.48550/arXiv.2205.10712,"We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the home for embodied AI. In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged. Instead, the agent must learn from and is evaluated against human preferences of which objects belong where in a tidy house. Specifically, we collect a dataset of where humans typically place objects in tidy and untidy houses constituting 1799 objects, 268 object categories, 585 placements, and 105 rooms. Next, we propose a modular baseline approach for Housekeep that integrates planning, exploration, and navigation. It leverages a fine-tuned large language model (LLM) trained on an internet text corpus for effective planning. We show that our baseline agent generalizes to rearranging unseen objects in unknown environments. See our webpage for more details: https://yashkant.github.io/housekeep/",arxiv:2205.10712,Yes,,2025-11-11T00:14:11.169Z
howwelldoeschatgptdo-2022,How Well Does ChatGPT Do When Taking the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment,A. Gilson; C. Safranek; Ting Huang; V. Socrates; L. Chi; R. A. Taylor; David Chartash,2022,medRxiv,98,https://www.semanticscholar.org/paper/7d4867e28b02059eef4cb25bfcd304b2071b30a9,https://www.medrxiv.org/content/medrxiv/early/2022/12/26/2022.12.23.22283901.full.pdf,10.1101/2022.12.23.22283901,"Background: ChatGPT is a 175 billion parameter natural language processing model which can generate conversation style responses to user input. Objective: To evaluate the performance of ChatGPT on questions within the scope of United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as analyze responses for user interpretability. Methods: We used two novel sets of multiple choice questions to evaluate ChatGPT's performance, each with questions pertaining to Step 1 and Step 2. The first was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the userbase. The second, was the National Board of Medical Examiners (NBME) Free 120-question exams. After prompting ChatGPT with each question, ChatGPT's selected answer was recorded, and the text output evaluated across three qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results: On the four datasets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free- Step2, ChatGPT achieved accuracies of 44%, 42%, 64.4%, and 57.8%. The model demonstrated a significant decrease in performance as question difficulty increased (P=.012) within the AMBOSS- Step1 dataset. We found logical justification for ChatGPT's answer selection was present in 100% of outputs. Internal information to the question was present in >90% of all questions. The presence of information external to the question was respectively 54.5% and 27% lower for incorrect relative to correct answers on the NBME-Free-Step1 and NBME-Free-Step2 datasets (P<=.001). Conclusion: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at greater than 60% threshold on the NBME-Free- Step-1 dataset we show that the model is comparable to a third year medical student. Additionally, due to the dialogic nature of the response to questions, we demonstrate ChatGPT's ability to provide reasoning and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as a medical education tool.",,Yes,,2025-11-11T00:13:07.427Z
howtoassistdesigners-2022,How to assist designers to model learning games with Petri nets?,Mathieu Muratet; T. Carron; Amel Yessad,2022,International Conference on Foundations of Digital Games,1,https://www.semanticscholar.org/paper/beee1f469926964e4560ec2c666385c313af5569,https://hal.science/hal-03710508/file/Help_For_Petri_Nets.pdf,10.1145/3555858.3555937,"In previous research, we presented a methodological framework that provides players with adaptive feedback. The core of this framework relies on modeling the learning game with a Petri net. However, this modeling is a challenging task. Indeed, Petri nets are well adapted to model dynamic and complex systems but require a mastery of the underlying mathematical formalism to build them manually. In particular, when the learning game is characterized by a large freedom of action. In this paper, we present an authoring tool and its domain-specific language to assist designers to model learning games with Petri nets. We carried out a case study where our contribution was implemented. Results show that our contribution helps designers to build the Petri net in combination with classical Petri net editors which are still useful to visualize, to check and to validate the Petri nets built.",,Yes,,2025-11-11T00:15:14.026Z
humanlanguageunderst-2022,Human Language Understanding & Reasoning,Christopher D. Manning,2022,Daedalus,123,https://www.semanticscholar.org/paper/a0c87ee1b0903c1c9ac72809caf75b6b6997baa0,https://direct.mit.edu/daed/article-pdf/151/2/127/2060607/daed_a_01905.pdf,10.1162/daed_a_01905,"Abstract The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.",,Yes,,2025-11-11T00:13:07.427Z
humanlikeintuitivebe-2022,Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT,Thilo Hagendorff; Sarah Fabi; Michal Kosinski,2022,Nature Computational Science,202,https://www.semanticscholar.org/paper/0bfc05adcddd4fe5d1335d96cc313c41526d4558,https://www.nature.com/articles/s43588-023-00527-x.pdf,10.1038/s43588-023-00527-x,"We design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics. The reasoning capabilities of OpenAI’s generative pre-trained transformer family were tested using semantic illusions and cognitive reflection tests that are typically used in human studies. While early models were prone to human-like cognitive errors, ChatGPT decisively outperformed humans, avoiding the cognitive traps embedded in the tasks.",arxiv:2306.07622,Yes,,2025-11-11T00:13:07.427Z
identifyingexplanati-2022,Identifying Explanations Within Student-Tutor Chat Logs,Ethan Prihar,2022,Educational Data Mining,1,https://www.semanticscholar.org/paper/53ef2dccbd6ff2b521bff18702231d67de77749f,,,,,Yes,,2025-11-11T00:15:19.325Z
identifyingtheinflue-2022,Identifying the influence of transfer learning method in developing an end-to-end automatic speech recognition system with a low data level,O. Mamyrbayev; K. Alimhan; Dina Oralbekova; A. Bekarystankyzy; B. Zhumazhanov,2022,Eastern-European Journal of Enterprise Technologies,13,https://www.semanticscholar.org/paper/a58a42aefa22c59b188421752b9b3305e4ec1c27,https://journals.uran.ua/eejet/article/download/252801/250760,10.15587/1729-4061.2022.252801,"Ensuring the best quality and performance of modern speech technologies, today, is possible based on the widespread use of machine learning methods. The idea of this project is to study and implement an end-to-end system of automatic speech recognition using machine learning methods, as well as to develop new mathematical models and algorithms for solving the problem of automatic speech recognition for agglutinative (Turkic) languages.
Many research papers have shown that deep learning methods make it easier to train automatic speech recognition systems that use an end-to-end approach. This method can also train an automatic speech recognition system directly, that is, without manual work with raw signals. Despite the good recognition quality, this model has some drawbacks. These disadvantages are based on the need for a large amount of data for training. This is a serious problem for low-data languages, especially Turkic languages such as Kazakh and Azerbaijani. To solve this problem, various methods are needed to apply. Some methods are used for end-to-end speech recognition of languages belonging to the group of languages of the same family (agglutinative languages). Method for low-resource languages is transfer learning, and for large resources – multi-task learning. To increase efficiency and quickly solve the problem associated with a limited resource, transfer learning was used for the end-to-end model. The transfer learning method helped to fit a model trained on the Kazakh dataset to the Azerbaijani dataset. Thereby, two language corpora were trained simultaneously. Conducted experiments with two corpora show that transfer learning can reduce the symbol error rate, phoneme error rate (PER), by 14.23 % compared to baseline models (DNN+HMM, WaveNet, and CNC+LM). Therefore, the realized model with the transfer method can be used to recognize other low-resource languages.",,Yes,,2025-11-11T00:15:16.543Z
illuminatingproteins-2022,Illuminating protein space with a programmable generative model,John Ingraham; Max Baranov; Zak Costello; Vincent Frappier; Ahmed Ismail; Shan Tie; Wujie Wang; Vincent Xue; F. Obermeyer; Andrew L. Beam; G. Grigoryan,2022,bioRxiv,449,https://www.semanticscholar.org/paper/2cfc26b4af99f195b433fa7aa00f221b111c7cd4,https://www.nature.com/articles/s41586-023-06728-8.pdf,10.1038/s41586-023-06728-8,"Three billion years of evolution has produced a tremendous diversity of protein molecules^ 1 , but the full potential of proteins is likely to be much greater. Accessing this potential has been challenging for both computation and experiments because the space of possible protein molecules is much larger than the space of those likely to have functions. Here we introduce Chroma, a generative model for proteins and protein complexes that can directly sample novel protein structures and sequences, and that can be conditioned to steer the generative process towards desired properties and functions. To enable this, we introduce a diffusion process that respects the conformational statistics of polymer ensembles, an efficient neural architecture for molecular systems that enables long-range reasoning with sub-quadratic scaling, layers for efficiently synthesizing three-dimensional structures of proteins from predicted inter-residue geometries and a general low-temperature sampling algorithm for diffusion models. Chroma achieves protein design as Bayesian inference under external constraints, which can involve symmetries, substructure, shape, semantics and even natural-language prompts. The experimental characterization of 310 proteins shows that sampling from Chroma results in proteins that are highly expressed, fold and have favourable biophysical properties. The crystal structures of two designed proteins exhibit atomistic agreement with Chroma samples (a backbone root-mean-square deviation of around 1.0 Å). With this unified approach to protein design, we hope to accelerate the programming of protein matter to benefit human health, materials science and synthetic biology. Evolution has produced a range of diverse proteins, and now a generative model called Chroma can expand that set by allowing the user to design new proteins and protein complexes with desired properties and functions.",,Yes,,2025-11-11T00:15:14.026Z
impaktadatasetforope-2022,ImPaKT: A Dataset for Open-Schema Knowledge Base Construction,L. Vilnis; Zachary Kenneth Fisher; Bhargav Kanagal; Patrick C. Murray; Sumit K. Sanghai,2022,arXiv.org,3,https://www.semanticscholar.org/paper/18e1dd6604f98ebcdf3f281803a5c92763e3ffef,http://arxiv.org/pdf/2212.10770,10.48550/arXiv.2212.10770,"Large language models have ushered in a golden age of semantic parsing. The seq2seq paradigm allows for open-schema and abstractive attribute and relation extraction given only small amounts of finetuning data. Language model pretraining has simultaneously enabled great strides in natural language inference, reasoning about entailment and implication in free text. These advances motivate us to construct ImPaKT, a dataset for open-schema information extraction, consisting of around 2500 text snippets from the C4 corpus, in the shopping domain (product buying guides), professionally annotated with extracted attributes, types, attribute summaries (attribute schema discovery from idiosyncratic text), many-to-one relations between compound and atomic attributes, and implication relations. We release this data in hope that it will be useful in fine tuning semantic parsers for information extraction and knowledge base construction across a variety of domains. We evaluate the power of this approach by fine-tuning the open source UL2 language model on a subset of the dataset, extracting a set of implication relations from a corpus of product buying guides, and conducting human evaluations of the resulting predictions.",arxiv:2212.10770,Yes,,2025-11-11T00:15:14.026Z
imaginedversusrememb-2022,Imagined versus Remembered Stories: Quantifying Differences in Narrative Flow,Maarten Sap; A. Jafarpour; Yejin Choi; Noah A. Smith; J. Pennebaker; E. Horvitz,2022,,1,https://www.semanticscholar.org/paper/5b49f4a3b4cd6c3bed3441a927eaa721fd8d36dd,,,"Lifelong experiences and learned knowledge lead to shared expectations about how common situations tend to unfold. Such knowledge of narrative event flow enables people to weave together a story. However, comparable computational tools to evaluate the flow of events in narratives are limited. We quantify the differences between autobiographical and imagined stories by introducing sequentiality , a measure of narrative flow of events, drawing probabilistic inferences from a cutting-edge large language model (GPT-3). Sequentiality captures the flow of a narrative by comparing the probability of a sentence with and without its preceding story context. We applied our measure to study thousands of diary-like stories, collected from crowdworkers about either a recent remembered experience or an imagined story on the same topic. The results show that imagined stories have higher sequentiality than autobiographical stories and that the sequentiality of autobiographical stories increases when the memories are retold several months later. In pur-suit of deeper understandings of how sequentiality measures the flow of narratives, we explore proportions of major and minor events in story sentences, as annotated by crowdworkers. We find that lower sequentiality is associated with higher proportions of major events. The methods and results highlight opportunities to use cutting-edge computational analyses, such as sequentiality, on large corpora of matched imagined and autobiographical stories to investigate the influences of memory and reasoning on language generation processes.",arxiv:2201.02662,Yes,,2025-11-11T00:15:16.543Z
implementationofdeci-2022,Implementation of Decision Tree Algorithm Machine Learning in Detecting Covid-19 Virus Patients Using Public Datasets,Nadiah Nadiah; Sopian Soim; Sholihin Sholihin,2022,Indonesian Journal of Artificial Intelligence and Data Mining,5,https://www.semanticscholar.org/paper/f83d0e2fda2682b6e724c42771c73766e0bd7c62,http://ejournal.uin-suska.ac.id/index.php/IJAIDM/article/download/17054/pdf,10.24014/ijaidm.v5i1.17054,"The advancement of AI (Artificial Intelligence) technology has been widely implemented in numerous sectors of daily life. Machine Learning is one of the subfields of Artificial Intelligence. Using statistics, mathematics, and data mining, machine learning is developed so that machines may learn by assessing data without being reprogrammed. At this time the world is on alert for the spread of a popular virus, the corona virus. Coronaviruses are part of a family of viruses caused by diseases ranging from the flu. The disease caused by the coronavirus is known as Covid-19. Therefore, to help identify whether a somebody has coronavirus disease based on certain symptoms, a model is created that can classify people with the covid-19 virus using machine learning. The classification methods utilized in this study are decision trees and large-scale machine learning projects. The study employed Python 3.7 as its programming language and PyCharm as its Integrated Development Environment (IDE). Based on the results, the accuracy rate as expected after conducting various trials is 99%.",,Yes,,2025-11-11T00:15:16.543Z
improvingcommonsense-2022,Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles,Shuquan Ye; Yujia Xie; Dongdong Chen; Yichong Xu; Lu Yuan; Chenguang Zhu; Jing Liao,2022,Computer Vision and Pattern Recognition,17,https://www.semanticscholar.org/paper/111dadc41f7b0337235fb526bf0cd3a4ac23b98d,https://arxiv.org/pdf/2211.16504,10.1109/CVPR52729.2023.00259,"This paper focuses on analyzing and improving the commonsense ability of recent popular vision-language (VL) models. Despite the great success, we observe that existing VL-models still lack commonsense knowledge/reasoning ability (e.g., “Lemons are sour”), which is a vital component towards artificial general intelligence. Through our analysis, we find one important reason is that existing large-scale VL datasets do not contain much commonsense knowledge, which motivates us to improve the commonsense of VL-models from the data perspective. Rather than collecting a new VL training dataset, we propose a more scalable strategy, i.e., “Data Augmentation with kNowledge graph linearization for CommonsensE capability” (DANCE). It can be viewed as one type of data augmentation technique, which can inject commonsense knowledge into existing VL datasets on the fly during training. More specifically, we leverage the commonsense knowledge graph (e.g., ConceptNet) and create variants of text description in VL datasets via bidirectional sub-graph sequentialization. For better commonsense evaluation, we further propose the first retrieval-based commonsense diagnostic benchmark. By conducting extensive experiments on some representative VL-models, we demonstrate that our DANCE technique is able to significantly improve the commonsense ability while maintaining the performance on vanilla retrieval tasks. The code and data are available at https://github.com/pleaseconnectwifi/DANCE.",arxiv:2211.16504,Yes,,2025-11-11T00:14:11.169Z
improvingtheradioele-2022,Improving the Radioelectronic Device Simulation Quality by Using a Step Recovery Diode,Gleb M. Shevchenko; E. Semyonov,2022,International Siberian Conference on Control and Communications,0,https://www.semanticscholar.org/paper/c82c398ccec82924bb71a8a7e059cdf73fe186f0,,10.1109/SIBCON56144.2022.10003001,"Computer design of radio electronic facilities and systems is currently the main tool for their creation by radio engineers. Diodes are widespread among the electronic component base used in the design. Modern physical layer models of diodes describe their operation with good accuracy, however, equivalent circuits models are used in radio engineering computer-aided design systems. In the vast majority of cases, these are simplified quasi-static models developed in the 70s of the last century. So, the dynamics of the diode operation is observed with a large error, and some aspects of transient processes are not modeled at all. In this paper we consider a refined non-quasi-static model of a diode with the dependence of the lifetime of nonequilibrium charge carriers on the forward current, the mathematical apparatus of which is expressed in the language of equivalent circuits. Therefore, it can be implemented directly by engineers. Using the dependence between the lifetime of nonequilibrium charge carriers and the forward current at a high level of injection in the non-quasistatic diode model, the modeling error of the output voltage of the push-pull pulse sharper does no more than 5%. The standard quasi-static model gives a significantly larger modeling error for both waveform and position. It is shown that the delay between the experimental and model curve is reduced by a factor of half.",,Yes,,2025-11-11T00:15:16.543Z
innetworkfractionalc-2022,In-network fractional calculations using P4 for scientific computing workloads,Shivam Patel; Rigden Atsatsang; K. Tichauer; M. Wang; J. Kowalkowski; Nik Sultana,2022,EuroP4@CoNEXT,10,https://www.semanticscholar.org/paper/d2eced12f2499332e1e1ee800b362ff183653b1a,https://dl.acm.org/doi/pdf/10.1145/3565475.3569083,10.1145/3565475.3569083,"Recent P4 research has motivated the need for in-network fractional calculations to support functions in Networking (for calculations related to active queue management and load balancing) and in Machine Learning. The P4 language and ASICs do not natively support fractional types (e.g., float). Existing P4 techniques provide incomplete emulations of the IEEE-754 standard, which was designed as a generic approach that can benefit from dedicated hardware acceleration, but whose features are difficult to fully support in P4. This paper re-thinks the foundation of in-network fractional calculation and proposes a new approach that is more resource conscious and is straightforward to encode in P4. Instead of floating-point, it uses a fixed-point encoding of numerals; and instead of sampling functions into tables it uses Taylor Approximation to reduce data-plane calculations to simple arithmetic over pre-calculated coefficients, requiring constant space and linear time. The paper describes and evaluates a P4 code synthesis algorithm that allows users to trade-off switch resources for accuracy, grounded on an application of a well-understood mathematical theory. It describes how to encode π and various functions including cos, log and exp. This technique is being developed to support Scientific Computing (SC) applications which typically make heavy use of fractional approximations of Real numbers. The paper applies this technique in a novel P4 program that is being open-sourced: in-network Monte Carlo simulation of photon propagation that models the analysis that is carried out in a class of cancer treatments. This technique is also being used in ongoing work on another SC application: online event detection in a large-scale neutrino detection experiment.",,Yes,,2025-11-11T00:15:19.325Z
incrementalandaccura-2022,Incremental and accurate computation of machine learning models with smart data summarization,Sikder Tahsin Al-Amin; Carlos Ordonez,2022,Journal of Intelligence and Information Systems,4,https://www.semanticscholar.org/paper/83477adf53eb65408cc2938172111ecb89515551,,10.1007/s10844-021-00690-5,,,Yes,,2025-11-11T00:15:16.543Z
informationsystemfor-2022,Information system for converting audio in Ukrainian language into its textual representation using nlp methods and machine learning,Yurii Tyshchuk; V. Vysotska; Olha Vlasenko,2022,"Vìsnik Nacìonalʹnogo unìversitetu ""Lʹvìvsʹka polìtehnìka"". Serìâ Ìnformacìjnì sistemi ta merežì",1,https://www.semanticscholar.org/paper/f3bc69f32e76a5936ab4f833e0d912b333d057a0,https://science.lpnu.ua/sites/default/files/journal-paper/2023/jan/29741/221029maket-25-53.pdf,10.23939/sisn2022.12.023,"Speech recognition involves various models, methods and algorithms for analysing and processing the user’s recorded voice. This allows people to control different systems that support one type of speech recognition. A speech-to-text conversion system is a type of speech recognition that uses spoken data for further processing. It also provides several stages for processing an audio file, which uses electroacoustic means, filtering algorithms in the audio file to isolate relevant sounds, electronic data arrays for the selected language, as well as mathematical models that make up the most likely words from phonemes. Thanks to the conversion of speech to text, people whose professions are closely related to typing a large amount of text on the keyboard, significantly speed up and facilitate the work process, as well as reduce the amount of stress. In addition, such systems help businesses, because the concept of remote work is becoming more and more popular, and therefore companies need tools to record and systematize meetings in the form of written text. The object of the research is the process of converting the Ukrainian-language text into a written one based on NLP and machine learning methods. The subject of the research is file processing algorithms for extracting relevant sounds and recognizing phonemes, as well as mathematical models for recognizing an array of phonemes as specific words. The purpose of the work is to design and develop an information system for converting audio Ukrainian-language text into written text based on the Ukrainian Speech-to-text Web application, which is a technology for accurate and easy analysis of Ukrainian-language audio files and their subsequent transcription into text. The application supports downloading files from the file system and recording using the microphone, as well as saving the analysed data. The article also describes the stages of design and the general typical architecture of the corresponding system for converting audio Ukrainian-language text into written text. According to the results of the experimental testing of the developed system, it was found that the number of words does not affect the accuracy of the conversion algorithm, and the decrease in percentage is not large and occurred due to the complexity of the words and the low quality of the microphone, and therefore the recorded file.",,Yes,,2025-11-11T00:15:14.026Z
innermonologueembodi-2022,Inner Monologue: Embodied Reasoning through Planning with Language Models,Wenlong Huang; F. Xia; Ted Xiao; Harris Chan; Jacky Liang; Peter R. Florence; Andy Zeng; Jonathan Tompson; Igor Mordatch; Yevgen Chebotar; P. Sermanet; Noah Brown; Tomas Jackson; Linda Luu; S. Levine; Karol Hausman; Brian Ichter,2022,Conference on Robot Learning,1098,https://www.semanticscholar.org/paper/f3cf71c51b882fe3111d71c4bf104297d38197f8,http://arxiv.org/pdf/2207.05608,10.48550/arXiv.2207.05608,"Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.",arxiv:2207.05608,Yes,,2025-11-11T00:13:07.427Z
instancesequencereas-2022,Instance-sequence reasoning for video question answering,R. Liu; Yahong Han,2022,Frontiers of Computer Science,17,https://www.semanticscholar.org/paper/fc7398788a8e14ea0b0286bf8e66e3ce9ad3b07d,,10.1007/s11704-021-1248-1,,,Yes,,2025-11-11T00:14:11.169Z
interleavingretrieva-2022,Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions,H. Trivedi; Niranjan Balasubramanian; Tushar Khot; Ashish Sabharwal,2022,Annual Meeting of the Association for Computational Linguistics,664,https://www.semanticscholar.org/paper/f208ea909fa7f54fea82def9a92fd81dfc758c39,http://arxiv.org/pdf/2212.10509,10.48550/arXiv.2212.10509,"Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.",arxiv:2212.10509,Yes,,2025-11-11T00:14:11.169Z
istherearoleforthrom-2022,Is There a Role for Thromboprophylaxis in Selected Outpatients With COVID-19?-Reply.,Junqing Xie; D. Prieto-Alhambra,2022,JAMA Internal Medicine,0,https://www.semanticscholar.org/paper/ee20d54e70f9e9e3bcba2a060049dd55de5b443d,https://jamanetwork.com/journals/jamainternalmedicine/articlepdf/2799542/jamainternal_xie_2022_lr_220027_1674845299.23866.pdf,10.1001/jamainternmed.2022.5881,"In Reply We appreciate the opportunity to respond to the letter about our study1 on maternal prescriptions filled for antipsychotics during pregnancy and offsprings’ school performance. Dr Jingjing and colleagues suggest the implementation of propensity score matching over multivariable regression to balance the covariables. Propensity score matching is commonly used for control of confounding in pharmacoepidemiologic studies, especially when the outcomes are rare, many confounders are present, and there are systematic differences in the distribution of characteristics between groups.2 In the present study,1 outcomes were standardized test scores in language and mathematics (continuous variables), and our sample was sufficiently large to allow for the inclusion of all a priori identified confounders without overfitting the models. In this scenario, the propensity score approach would yield similar or slightly weaker associations to the traditional multivariable regression3 and would reduce precision and statistical power due to nonmatched observations being discarded. Furthermore, our conclusions across all 7 sensitivity analyses were consistent, including analyses of better-balanced populations (eg, sibling comparisons and children of mothers with antipsychotic prescription fills before vs during pregnancy).1 In addition, Dr Jingjing and colleagues remarked on the potential confounding by prepregnancy body mass index (BMI). Our thoughts are the following. Individuals who are overweight and obese are at higher risk of developing mental illnesses.4 Therefore, prepregnancy BMI could potentially be associated with the likelihood of receiving antipsychotic treatment during pregnancy. Our study1 was based on the linkages of Danish nationwide registers, and information on prepregnancy BMI was not included in the Danish Medical Birth Registry until 2003.5 We acknowledged potential residual confounding from any unmeasured factors in our discussion.1 However, we considered other covariates, eg, maternal age, smoking, and education, which would partly control for prepregnancy BMI, and we expect that further adjustment for prepregnancy BMI would not change the results substantially. To test this assumption, we conducted a subgroup analysis restricted to 296 306 children born from 2004 to 2009. In this additional analysis of 565 671 language tests and 329 328 mathematics tests, we found that maternal prescription fill for antipsychotics was not associated with test score performance in language (adjusted difference, 0.5 [95% CI, −1.3 to 2.2], and further adjustment for prepregnancy BMI, 0.7 [95% CI, −1.0 to 2.4]) or in mathematics (adjusted difference, 0.7 [95% CI, −1.4 to 2.7] and further adjustment for prepregnancy BMI, 0.9 [95% CI, −1.2 to 2.9]). These results completely align with our findings in the primary analyses in the full population (children born in 1997–2009, adjusted difference, 0.5 [95% CI, −0.8 to 1.7] for language and 0.4; [95% CI, −1.0 to 1.8] for mathematics). Therefore, further adjustment for prepregnancy BMI did not change the overall conclusion of the study; that is, maternal antipsychotic prescription during pregnancy did not appear to be associated with offspring standardized test scores.1",,Yes,,2025-11-11T00:15:19.325Z
isaquestiondecomposi-2022,Is a Question Decomposition Unit All We Need?,Pruthvi H. Patel; Swaroop Mishra; Mihir Parmar; Chitta Baral,2022,Conference on Empirical Methods in Natural Language Processing,55,https://www.semanticscholar.org/paper/a80f2102e5de3ead1b9689b440503f49383ddc94,http://arxiv.org/pdf/2205.12538,10.48550/arXiv.2205.12538,"Large Language Models (LMs) have achieved state-of-the-art performance on many Natural Language Processing (NLP) benchmarks. With the growing number of new benchmarks, we build bigger and more complex LMs. However, building new LMs may not be an ideal option owing to the cost, time and environmental impact associated with it. We explore an alternative route: can we modify data by expressing it in terms of the model’s strengths, so that a question becomes easier for models to answer? We investigate if humans can decompose a hard question into a set of simpler questions that are relatively easier for models to solve. We analyze a range of datasets involving various forms of reasoning and find that it is indeed possible to significantly improve model performance (24% for GPT3 and 29% for RoBERTa-SQuAD along with a symbolic calculator) via decomposition. Our approach provides a viable option to involve people in NLP research in a meaningful way. Our findings indicate that Human-in-the-loop Question Decomposition (HQD) can potentially provide an alternate path to building large LMs.",arxiv:2205.12538,Yes,,2025-11-11T00:15:14.026Z
jarvisaneurosymbolic-2022,JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents,Kai Zheng; KAI-QING Zhou; Jing Gu; Yue Fan; Jialu Wang; Zong-xiao Li; Xuehai He; X. Wang,2022,arXiv.org,47,https://www.semanticscholar.org/paper/961a1772f3b90d9dffd2b571c6996007a1d0ccd1,http://arxiv.org/pdf/2208.13266,10.48550/arXiv.2208.13266,"Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1\% to 15.8\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.",arxiv:2208.13266,Yes,,2025-11-11T00:14:11.169Z
jecccommonsensereaso-2022,JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions,Mo Yu; Xiaoxiao Guo; Yufei Feng; Yi Gu; Xiao-Dan Zhu; M. Greenspan; Murray Campbell; Chuang Gan,2022,Annual Meeting of the Association for Computational Linguistics,0,https://www.semanticscholar.org/paper/868f9bb603dfa8f6951787040fc6d62c909a15c2,http://arxiv.org/pdf/2210.15456,10.48550/arXiv.2210.15456,"Commonsense reasoning simulates the human ability to make presumptions about our physical world, and it is an essential cornerstone in building general AI systems. We propose a new commonsense reasoning dataset based on human's Interactive Fiction (IF) gameplay walkthroughs as human players demonstrate plentiful and diverse commonsense reasoning. The new dataset provides a natural mixture of various reasoning types and requires multi-hop reasoning. Moreover, the IF game-based construction procedure requires much less human interventions than previous ones. Different from existing benchmarks, our dataset focuses on the assessment of functional commonsense knowledge rules rather than factual knowledge. Hence, in order to achieve higher performance on our tasks, models need to effectively utilize such functional knowledge to infer the outcomes of actions, rather than relying solely on memorizing facts. Experiments show that the introduced dataset is challenging to previous machine reading models as well as the new large language models with a significant 20% performance gap compared to human experts.",arxiv:2210.15456,Yes,,2025-11-11T00:15:14.026Z
karamlintegratingkno-2022,KARaML: Integrating Knowledge-Based and Machine Learning Approaches to Solve the Winograd Schema Challenge,S. Hong; B. Bennett; Judith Clymo; Lucía Gómez Álvarez,2022,Make,2,https://www.semanticscholar.org/paper/5394454e442f0b2d0b8e4f93dea69d458511d17e,,,,,Yes,,2025-11-11T00:15:16.543Z
kmirabenchmarkforeva-2022,"KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models",Daniel Gao; Yantao Jia; Lei Li; Chengzhen Fu; Zhicheng Dou; Hao Jiang; Xinyu Zhang; Lei Chen; Zhao Cao,2022,arXiv.org,9,https://www.semanticscholar.org/paper/718343008a6cfca9e86ab6160caba353c52c17cf,,,"Previous works show the great potential of pre-trained language models (PLMs) for storing a large amount of factual knowledge. However, to figure out whether PLMs can be reliable knowledge sources and used as alternative knowledge bases (KBs), we need to further explore some critical features of PLMs. Firstly, knowledge memorization and identification abilities: traditional KBs can store various types of entities and relationships; do PLMs have a high knowledge capacity to store different types of knowledge? Secondly, reasoning ability: a qualified knowledge source should not only provide a collection of facts, but support a symbolic reasoner. Can PLMs derive new knowledge based on the correlations between facts? To evaluate these features of PLMs, we propose a benchmark, named Knowledge Memorization, Identification, and Reasoning test (KMIR). KMIR covers 3 types of knowledge, including general knowledge, domain-specific knowledge, and commonsense, and provides 184,348 well-designed questions. Preliminary experiments with various representative pre-training language models on KMIR reveal many interesting phenomenons: 1) The memorization ability of PLMs depends more on the number of parameters than training schemes. 2) Current PLMs are struggling to robustly remember the facts. 3) Model compression technology retains the amount of knowledge well, but hurts the identification and reasoning abilities. We hope KMIR can facilitate the design of PLMs as better knowledge sources.",arxiv:2202.13529,Yes,,2025-11-11T00:13:07.427Z
knifedistillingreaso-2022,KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales,Aaron Chan; Zhiyuan Zeng; Wyatt Lake; Brihi Joshi; Hanjie Chen; Xiang Ren,2022,,1,https://www.semanticscholar.org/paper/c6a47dec94ded708fe759ca0b71ad57ab72fec07,,,"Language models (LMs) have yielded impressive results on many language reasoning tasks, but their unexpected errors raise doubts about their reasoning abilities. In light of this, there is growing interest in finetuning/prompting LMs with both task instances and their associated free-text rationales (FTRs), which explain the correct reasoning process for predicting the correct task output (i.e., how to be""right for the right reasons""). However, existing finetuning methods fail to improve LM performance, while prompting needs prohibitively large (i.e.,>50B) LMs to work well. We propose KNIFE, which shows that reasoning knowledge can be effectively distilled from FTRs into a small (i.e.,<1B) LM and improve the LM's performance. First, KNIFE finetunes a teacher LM (given task input and FTR) to predict the task output, transferring reasoning knowledge from the FTRs to the teacher's hidden states. Second, KNIFE finetunes a student LM (given task input only) such that its hidden states are aligned with the teacher's. Thus, the student is endowed with reasoning knowledge but can be used for inference without direct FTR input. On two question-answering datasets, KNIFE outperforms various finetuning and prompting baselines in fully-supervised and low-resource settings. Also, we observe that FTR quality is crucial to KNIFE's performance.",arxiv:2212.09721,Yes,,2025-11-11T00:14:11.169Z
korelasiantarakemamp-2022,KORELASI ANTARA KEMAMPUAN BELAJAR MATEMATIKA DENGAN HASIL BELAJAR FISIKA SECARA ONLINE SISWA SMUN 1 TAKENGON,Richasanty Septima S; Yenni Tirtasari,2022,JURNAL RISET RUMPUN MATEMATIKA DAN ILMU PENGETAHUAN ALAM,0,https://www.semanticscholar.org/paper/bf2a23af7bc9b66f89b718922a837de1dcafa257,,10.55606/jurrimipa.v1i2.509,"This study aims to see and find out whether there is a relationship between the ability to learn mathematics and the results of learning physics online. Physics is a branch of science that studies the behavior of nature through experimental observations and quantitative measurements. In studying nature, Physics uses the language of mathematics to model natural phenomena in mathematical equations. In studying Physics, it is necessary to be able to The research method used is descriptive correlation analysis. The sample of this study was 60 students for three different classes. The research instrument is a report on student learning outcomes or a list of odd semester report cards for the 2020/2021 school year. The results of data analysis show that there is a significant relationship between mathematics learning ability and online physics learning outcomes with a large correlation coefficient (r) of 0.787 For class X-1, the value of the correlation coefficient is r = 0.734 for class X-2 and r = 0.661 For class X-3. This shows that if the ability to learn mathematics is high, the learning outcomes of physics will be high as well.",,Yes,,2025-11-11T00:15:16.543Z
kritknowledgereasoni-2022,KRIT: Knowledge-Reasoning Intelligence in vision-language Transformer,Kezhen Chen; Qiuyuan Huang; Daniel J. McDuff; Yonatan Bisk; Jianfeng Gao,2022,,3,https://www.semanticscholar.org/paper/5d47143c13591def061e203bf6dd6d97fd110631,,,,,Yes,,2025-11-11T00:13:07.427Z
kafspknowledgeawaref-2022,KaFSP: Knowledge-Aware Fuzzy Semantic Parsing for Conversational Question Answering over a Large-Scale Knowledge Base,Junzhuo Li; Deyi Xiong,2022,Annual Meeting of the Association for Computational Linguistics,12,https://www.semanticscholar.org/paper/98932fb1bd273b01bfe31339224398c2fd45c674,https://aclanthology.org/2022.acl-long.35.pdf,10.18653/v1/2022.acl-long.35,"In this paper, we study two issues of semantic parsing approaches to conversational question answering over a large-scale knowledge base: (1) The actions defined in grammar are not sufficient to handle uncertain reasoning common in real-world scenarios. (2) Knowledge base information is not well exploited and incorporated into semantic parsing. To mitigate the two issues, we propose a knowledge-aware fuzzy semantic parsing framework (KaFSP). It defines fuzzy comparison operations in the grammar system for uncertain reasoning based on the fuzzy set theory. In order to enhance the interaction between semantic parsing and knowledge base, we incorporate entity triples from the knowledge base into a knowledge-aware entity disambiguation module. Additionally, we propose a multi-label classification framework to not only capture correlations between entity types and relations but also detect knowledge base information relevant to the current utterance. Both enhancements are based on pre-trained language models. Experiments on a large-scale conversational question answering benchmark demonstrate that the proposed KaFSP achieves significant improvements over previous state-of-the-art models, setting new SOTA results on 8 out of 10 question types, gaining improvements of over 10% F1 or accuracy on 3 question types, and improving overall F1 from 83.01% to 85.33%. The source code of KaFSP is available at https://github.com/tjunlp-lab/KaFSP.",,Yes,,2025-11-11T00:14:11.169Z
knowledgegraphsandth-2022,Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372),Paul Groth; E. Simperl; M. Erp; Denny Vrandečić,2022,Dagstuhl Reports,10,https://www.semanticscholar.org/paper/6de46ecb3ea78ea7ec88bf03a3b70bb8a791784f,,10.4230/DagRep.12.9.60,,,Yes,,2025-11-11T00:15:16.543Z
knowledgegraphrepres-2022,Knowledge graph representation learning and graph neural networks for language understanding,Jing Huang,2022,GRADES-NDA@SIGMOD,3,https://www.semanticscholar.org/paper/1ec4ac0318daf526fa77cd710b03e602449a1620,,10.1145/3534540.3534710,"As AI technologies become mature in natural language processing, speech recognition and computer vision, ""intelligent"" user interfaces emerge to handle complex and diverse tasks that require human-like knowledge and reasoning capability. In Part 1, I will present our recent work on knowledge graph representation learning using Graph Neural Networks (GNNs): the first approach is called orthogonal transform embedding (OTE), which integrates graph context into the embedding distance scoring function and improves prediction accuracy on complex relations such as the difficult N-to-1, 1-to-N and N-to-N cases; the second approach is called multi-hop attention GNN (MAGNA), a principled way to incorporate multi-hop context information into every layer of attention computation. MAGNA uses a diffusion prior on attention values, to efficiently account for all paths between the pair of disconnected nodes. Experimental results on knowledge graph completion as well as node classification benchmarks show that MAGNA achieves state-of-the-art results. In Part 2, I will present how we take advantage of GNNs for language understanding and reasoning tasks. We show that combined with large pre-trained language models and knowledge graph embeddings, GNNs are proven effective in multi-hop reading comprehension across documents, improving time sensitivity for question answering over temporal knowledge graphs, and constructing robust syntactic information for aspect-level sentiment analysis.",,Yes,,2025-11-11T00:15:14.026Z
knowledgeispowersymb-2022,"Knowledge is Power: Symbolic Knowledge Distillation, Commonsense Morality, & Multimodal Script Knowledge",Yejin Choi,2022,Web Search and Data Mining,5,https://www.semanticscholar.org/paper/8703985fd545ba7498a75fcbcc27845a3b549833,,10.1145/3488560.3500242,,,Yes,,2025-11-11T00:15:19.325Z
lagcsemanticsofconcu-2022,LAGC Semantics of Concurrent Programming Languages,Crystal Chang Din; Reiner Hähnle; L. Henrio; E. Johnsen; Ka I Pun; S. L. T. Tarifa,2022,arXiv.org,12,https://www.semanticscholar.org/paper/770d32ad8346db10f2b96d312fd9b688be688622,,,"Formal, mathematically rigorous programming language semantics are the essential prerequisite for the design of logics and calculi that permit automated reasoning about concurrent programs. We propose a novel modular semantics designed to align smoothly with program logics used in deductive verification and formal specification of concurrent programs. Our semantics separates local evaluation of expressions and statements performed in an abstract, symbolic environment from their composition into global computations, at which point they are concretised. This makes incremental addition of new language concepts possible, without the need to revise the framework. The basis is a generalisation of the notion of a program trace as a sequence of evolving states that we enrich with event descriptors and trailing continuation markers. This allows to postpone scheduling constraints from the level of local evaluation to the global composition stage, where well-formedness predicates over the event structure declaratively characterise a wide range of concurrency models. We also illustrate how a sound program logic and calculus can be defined for this semantics.",arxiv:2202.12195,Yes,,2025-11-11T00:15:16.543Z
lambadabackwardchain-2022,LAMBADA: Backward Chaining for Automated Reasoning in Natural Language,Seyed Mehran Kazemi; Najoung Kim; Deepti Bhatia; Xinyuan Xu; Deepak Ramachandran,2022,Annual Meeting of the Association for Computational Linguistics,87,https://www.semanticscholar.org/paper/03fb95e6be583ca954c3d00812a9e9a40f118e51,http://arxiv.org/pdf/2212.13894,10.48550/arXiv.2212.13894,"Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.",arxiv:2212.13894,Yes,,2025-11-11T00:13:07.427Z
lepuspromptbasedunsu-2022,LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA,Muhammad Khalifa; Lajanugen Logeswaran; Moontae Lee; Honglak Lee; Lu Wang,2022,arXiv.org,3,https://www.semanticscholar.org/paper/6650e44f00f74c596b3981d01493b5f75cfa6f63,http://arxiv.org/pdf/2205.12650,10.48550/arXiv.2205.12650,,,Yes,,2025-11-11T00:15:14.026Z
lmpriorspretrainedla-2022,LMPriors: Pre-Trained Language Models as Task-Specific Priors,Kristy Choi; Chris Cundy; Sanjari Srivastava; Stefano Ermon,2022,arXiv.org,50,https://www.semanticscholar.org/paper/f663c1b574d0a56c2e1c8e7aee3d8caf90b52808,http://arxiv.org/pdf/2210.12530,10.48550/arXiv.2210.12530,"Particularly in low-data regimes, an outstanding challenge in machine learning is developing principled techniques for augmenting our models with suitable priors. This is to encourage them to learn in ways that are compatible with our understanding of the world. But in contrast to generic priors such as shrinkage or sparsity, we draw inspiration from the recent successes of large-scale language models (LMs) to construct task-specific priors distilled from the rich knowledge of LMs. Our method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task—such as variable names and descriptions—to encourage downstream model outputs to be consistent with the LM’s common-sense reasoning based on the metadata. Empirically, we demonstrate that LMPriors improve model performance in settings where such natural language descriptions are available, and perform well on several tasks that benefit from such prior knowledge, such as feature selection, causal inference, and safe reinforcement learning.",arxiv:2210.12530,Yes,,2025-11-11T00:14:11.169Z
lunalanguageundersta-2022,LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training,Hongwei Han; Jialiang Xu; Mengyuan Zhou; Yijia Shao; Shi Han; Dongmei Zhang,2022,arXiv.org,10,https://www.semanticscholar.org/paper/0e97b13624bc15e7f30c82402252eca4d1a8eeba,http://arxiv.org/pdf/2212.02691,10.48550/arXiv.2212.02691,"Transformers are widely used in NLP tasks. However, current approaches to leveraging transformers to understand language expose one weak spot: Number understanding. In some scenarios, numbers frequently occur, especially in semi-structured data like tables. But current approaches to rich-number tasks with transformer-based language models abandon or lose some of the numeracy information - e.g., breaking numbers into sub-word tokens - which leads to many number-related errors. In this paper, we propose the LUNA framework which improves the numerical reasoning and calculation capabilities of transformer-based language models. With the number plugin of NumTok and NumBed, LUNA represents each number as a whole to model input. With number pre-training, including regression loss and model distillation, LUNA bridges the gap between number and vocabulary embeddings. To the best of our knowledge, this is the first work that explicitly injects numeracy capability into language models using Number Plugins. Besides evaluating toy models on toy tasks, we evaluate LUNA on three large-scale transformer models (RoBERTa, BERT, TabBERT) over three different downstream tasks (TATQA, TabFact, CrediTrans), and observe the performances of language models are constantly improved by LUNA. The augmented models also improve the official baseline of TAT-QA (EM: 50.15 ->59.58) and achieve SOTA performance on CrediTrans (F1 = 86.17).",arxiv:2212.02691,Yes,,2025-11-11T00:15:14.026Z
languagemodelsmostly-2022,Language Models (Mostly) Know What They Know,Saurav Kadavath; Tom Conerly; Amanda Askell; T. Henighan; Dawn Drain; Ethan Perez; Nicholas Schiefer; Z. Dodds; Nova Dassarma; Eli Tran-Johnson; Scott Johnston; S. El-Showk; Andy Jones; Nelson Elhage; Tristan Hume; Anna Chen; Yuntao Bai; Sam Bowman; Stanislav Fort; Deep Ganguli; Danny Hernandez; Josh Jacobson; John Kernion; Shauna Kravec; Liane Lovitt; Kamal Ndousse; Catherine Olsson; Sam Ringer; Dario Amodei; Tom B. Brown; Jack Clark; Nicholas Joseph; Benjamin Mann; Sam McCandlish; Chris Olah; Jared Kaplan,2022,arXiv.org,1052,https://www.semanticscholar.org/paper/142ebbf4760145f591166bde2564ac70c001e927,http://arxiv.org/pdf/2207.05221,10.48550/arXiv.2207.05221,"We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability""P(True)""that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict""P(IK)"", the probability that""I know""the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.",arxiv:2207.05221,Yes,,2025-11-11T00:15:14.026Z
languagemodelsaregre-2022,Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,Abulhair Saparov; He He,2022,International Conference on Learning Representations,384,https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a,http://arxiv.org/pdf/2210.01240,10.48550/arXiv.2210.01240,"Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",arxiv:2210.01240,Yes,,2025-11-11T00:13:07.427Z
languagemodelsaremul-2022,Language Models are Multilingual Chain-of-Thought Reasoners,Freda Shi; Mirac Suzgun; Markus Freitag; Xuezhi Wang; Suraj Srivats; Soroush Vosoughi; Hyung Won Chung; Yi Tay; Sebastian Ruder; Denny Zhou; Dipanjan Das; Jason Wei,2022,International Conference on Learning Representations,459,https://www.semanticscholar.org/paper/62f0db3a5ad5c795ec18fc7a6e7b01836809df57,http://arxiv.org/pdf/2210.03057,10.48550/arXiv.2210.03057,"We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.",arxiv:2210.03057,Yes,,2025-11-11T00:14:11.169Z
languagemodelsofcode-2022,Language Models of Code are Few-Shot Commonsense Learners,Aman Madaan; Shuyan Zhou; Uri Alon; Yiming Yang; Graham Neubig,2022,Conference on Empirical Methods in Natural Language Processing,237,https://www.semanticscholar.org/paper/39e40821b7207125e54e6ed7112e55cd38c6f0c3,http://arxiv.org/pdf/2210.07128,10.48550/arXiv.2210.07128,"We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph.To employ large language models (LMs) for this task, existing approaches ‘serialize’ the output graph as a flat list of nodes and edges.Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all.We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (codex) outperforms natural-LMs that are fine-tuned on the target task (T5) and other strong LMs such as GPT-3 in the few-shot setting.",arxiv:2210.07128,Yes,,2025-11-11T00:14:11.169Z
languagemodelsshowhu-2022,Language models show human-like content effects on reasoning,Ishita Dasgupta; Andrew Kyle Lampinen; Stephanie C. Y. Chan; Antonia Creswell; D. Kumaran; James L. McClelland; Felix Hill,2022,arXiv.org,207,https://www.semanticscholar.org/paper/290732e9fb08a29af8892a7c1f73c9d2a1b9d7db,http://arxiv.org/pdf/2207.07051,10.48550/arXiv.2207.07051,"Reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable""content effects""; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large language models, as well as humans, and find that the language models reflect many of the same patterns observed in humans across these tasks $\unicode{x2014}$ like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected both in answer patterns, and in lower-level features like the relationship between model answer distributions and human response times. Our findings have implications for understanding both these cognitive effects in humans, and the factors that contribute to language model performance.",arxiv:2207.07051,Yes,,2025-11-11T00:13:07.427Z
largelanguagemodelsc-2022,Large Language Models Can Self-Improve,Jiaxin Huang; S. Gu; Le Hou; Yuexin Wu; Xuezhi Wang; Hongkun Yu; Jiawei Han,2022,Conference on Empirical Methods in Natural Language Processing,708,https://www.semanticscholar.org/paper/3fa70115248377c3d1517c9f978791a296fbc1dd,http://arxiv.org/pdf/2210.11610,10.48550/arXiv.2210.11610,"Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate""high-confidence""rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",arxiv:2210.11610,Yes,,2025-11-11T00:13:07.427Z
largelanguagemodelsa-2022,Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors,Mohammad Reza Taesiri; Finlay Macklon; Yihe Wang; Hengshuo Shen; C. Bezemer,2022,arXiv.org,18,https://www.semanticscholar.org/paper/55e3fe05598be7c3dd357d51166869f6571b824f,http://arxiv.org/pdf/2210.02506,10.48550/arXiv.2210.02506,"Video game testing requires game-specific knowledge as well as common sense reasoning about the events in the game. While AI-driven agents can satisfy the first requirement, it is not yet possible to meet the second requirement automatically. Therefore, video game testing often still relies on manual testing, and human testers are required to play the game thoroughly to detect bugs. As a result, it is challenging to fully automate game testing. In this study, we explore the possibility of leveraging the zero-shot capabilities of large language models for video game bug detection. By formulating the bug detection problem as a question-answering task, we show that large language models can identify which event is buggy in a sequence of textual descriptions of events from a game. To this end, we introduce the GameBugDescriptions benchmark dataset, which consists of 167 buggy gameplay videos and a total of 334 question-answer pairs across 8 games. We extensively evaluate the performance of six models across the OPT and InstructGPT large language model families on our benchmark dataset. Our results show promising results for employing language models to detect video game bugs. With the proper prompting technique, we could achieve an accuracy of 70.66%, and on some video games, up to 78.94%. Our code, evaluation data and the benchmark can be found on https://asgaardlab.github.io/LLMxBugs",arxiv:2210.02506,Yes,,2025-11-11T00:13:07.427Z
largelanguagemodelse-2022,Large language models encode clinical knowledge,K. Singhal; Shekoofeh Azizi; T. Tu; S. Mahdavi; Jason Wei; Hyung Won Chung; Nathan Scales; A. Tanwani; H. Cole-Lewis; S. Pfohl; P. Payne; Martin G. Seneviratne; P. Gamble; C. Kelly; Nathaneal Scharli; A. Chowdhery; P. A. Mansfield; B. A. Y. Arcas; D. Webster; Greg S. Corrado; Yossi Matias; K. Chou; Juraj Gottweis; Nenad Tomašev; Yun Liu; A. Rajkomar; J. Barral; Christopher Semturs; A. Karthikesalingam; Vivek Natarajan,2022,Nature,3053,https://www.semanticscholar.org/paper/6052486bc9144dc1730c12bf35323af3792a1fd0,https://www.nature.com/articles/s41586-023-06291-2.pdf,10.1038/s41586-023-06291-2,"Med-PaLM, a state-of-the-art large language model for medicine, is introduced and evaluated across several medical question answering tasks, demonstrating the promise of these models in this domain. Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model^ 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM^ 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA^ 3 , MedMCQA^ 4 , PubMedQA^ 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics^ 6 ), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.",arxiv:2212.13138,Yes,,2025-11-11T00:13:07.427Z
latviannationalcorpo-2022,Latvian National Corpora Collection – Korpuss.lv,Baiba Saulite; Roberts Darģis; Normunds Gruzitis; I. Auzina; K. Levane-Petrova; L. Pretkalnina; Laura Rituma; Peteris Paikens; Arturs Znotins; Laine Strankale; Kristīne Pokratniece; Ilmars Poikans; Guntis Barzdins; I. Skadina; Anda Baklāne; Valdis Saulespurēns; Jānis Ziediņš,2022,International Conference on Language Resources and Evaluation,7,https://www.semanticscholar.org/paper/ed3efcf1864e1580a81cfaa34f93f9f9505d80c6,,,,,Yes,,2025-11-11T00:15:19.325Z
learntoexplainmultim-2022,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,Pan Lu; Swaroop Mishra; Tony Xia; Liang Qiu; Kai-Wei Chang; Song-Chun Zhu; Oyvind Tafjord; Peter Clark; A. Kalyan,2022,Neural Information Processing Systems,1695,https://www.semanticscholar.org/paper/d3135733aa39dec20ce72aa138589dda27c8406d,http://arxiv.org/pdf/2209.09513,10.48550/arXiv.2209.09513,"When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https://scienceqa.github.io.",arxiv:2209.09513,Yes,,2025-11-11T00:13:07.427Z
learningcomplexnatur-2022,Learning Complex Natural Language Inferences with Relational Neural Models,Boris Rakovan,2022,,0,https://www.semanticscholar.org/paper/d3712e0fb82e281f48cbdeed580ee950f71e16c0,,,,,Yes,,2025-11-11T00:14:11.169Z
learningtoanswersema-2022,Learning to Answer Semantic Queries over Code,Surya Prakash Sahu; Madhurima Mandal; Shikhar Bharadwaj; Aditya Kanade; Petros Maniatis; S. Shevade,2022,arXiv.org,1,https://www.semanticscholar.org/paper/ff5b5a98571e54b1c948e6e954aa2f7c05772736,https://arxiv.org/pdf/2209.08372,10.48550/arXiv.2209.08372,,,Yes,,2025-11-11T00:15:16.543Z
learningtoreasonandm-2022,Learning to Reason and Memorize with Self-Questioning,Jack Lanchantin; Shubham Toshniwal; J. Weston; Arthur Szlam; Sainbayar Sukhbaatar,2022,,0,https://www.semanticscholar.org/paper/d21364775c881b9bb6c99885652c81df29d755e5,,,,,Yes,,2025-11-11T00:15:16.541Z
learningtoreasonwith-2022,Learning to Reason with a Scalable Probabilistic Logic,William Yang Wang,2022,,0,https://www.semanticscholar.org/paper/2e17c3b883f0cdfe72e4877cdd88d3cc98e9836c,,10.1184/r1/21652148.v1,,,Yes,,2025-11-11T00:15:19.325Z
leasttomostprompting-2022,Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,Denny Zhou; Nathanael Scharli; Le Hou; Jason Wei; Nathan Scales; Xuezhi Wang; D. Schuurmans; O. Bousquet; Quoc Le; Ed H. Chi,2022,International Conference on Learning Representations,1366,https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321,http://arxiv.org/pdf/2205.10625,10.48550/arXiv.2205.10625,"Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",arxiv:2205.10625,Yes,,2025-11-11T00:13:07.427Z
legalpromptingteachi-2022,Legal Prompting: Teaching a Language Model to Think Like a Lawyer,Fang Yu; Lee Quartey; Frank Schilder,2022,arXiv.org,81,https://www.semanticscholar.org/paper/cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3,http://arxiv.org/pdf/2212.01326,10.48550/arXiv.2212.01326,"Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought (CoT) prompts can improve arithmetic or common sense tasks significantly. We explore how such approaches fare with legal reasoning tasks and take the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning approaches. Our findings show that while CoT prompting and fine-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from specific legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.",arxiv:2212.01326,Yes,,2025-11-11T00:13:07.427Z
lilaaunifiedbenchmar-2022,Lila: A Unified Benchmark for Mathematical Reasoning,Swaroop Mishra; Pan Lu; A. Kalyan,2022,,0,https://www.semanticscholar.org/paper/a630c70aed27b52f6d04d1e772b153c5a7b6f6fe,,,"Mathematical reasoning skills are essential for general-purpose intelligent systems to perform tasks from grocery shopping to climate modeling. Towards evaluating and improving AI systems in this domain, we propose LILA, a unified mathematical reasoning benchmark consisting of 23 diverse tasks along four dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs, thereby obtaining explainable solutions in addition to the correct answer. We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation. Finally, we introduce BHASKARA, a general-purpose mathematical reasoning model trained on LILA. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83% F1 score vs. single-task models), while the best performing model only obtains 60.40%, indicating the room for improvement in general mathematical reasoning and understanding.",arxiv:2210.17517,Yes,,2025-11-11T00:13:07.427Z
limitationsoflanguag-2022,Limitations of Language Models in Arithmetic and Symbolic Induction,Jingu Qian; Hong Wang; Zekun Li; SHIYANG LI; Xifeng Yan,2022,Annual Meeting of the Association for Computational Linguistics,82,https://www.semanticscholar.org/paper/2a7ae3e98357569c41424dacd60c62d3df78a0db,http://arxiv.org/pdf/2208.05051,10.48550/arXiv.2208.05051,"Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models. However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs. Experimental results show that none of these techniques can solve the simplest addition induction problem completely. In the end, we introduce LMs with tutor, which demonstrates every single step of teaching. LMs with tutor is able to deliver 100% accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction.",arxiv:2208.05051,Yes,,2025-11-11T00:14:11.169Z
littleredridinghoodg-2022,Little Red Riding Hood Goes around the Globe: Crosslingual Story Planning and Generation with Large Language Models,E. Razumovskaia; Joshua Maynez; Annie Louis; Mirella Lapata; Shashi Narayan,2022,International Conference on Language Resources and Evaluation,5,https://www.semanticscholar.org/paper/30cc7ae95583ade1f05226c08c6f6609777aeedd,http://arxiv.org/pdf/2212.10471,10.48550/arXiv.2212.10471,"Previous work has demonstrated the effectiveness of planning for story generation exclusively in a monolingual setting focusing primarily on English. We consider whether planning brings advantages to automatic story generation across languages. We propose a new task of crosslingual story generation with planning and present a new dataset for this task. We conduct a comprehensive study of different plans and generate stories in several languages, by leveraging the creative and reasoning capabilities of large pretrained language models. Our results demonstrate that plans which structure stories into three acts lead to more coherent and interesting narratives, while allowing to explicitly control their content and structure.",arxiv:2212.10471,Yes,,2025-11-11T00:13:07.427Z
logiganlearninglogic-2022,LogiGAN: Learning Logical Reasoning via Adversarial Pre-training,Xinyu Pi; Wanjun Zhong; Yan Gao; Nan Duan; Jian-Guang Lou,2022,Neural Information Processing Systems,17,https://www.semanticscholar.org/paper/8b78827faf49277b8f9f4510a766cba30e5fbe20,http://arxiv.org/pdf/2205.08794,10.48550/arXiv.2205.08794,"We present LogiGAN, an unsupervised adversarial pre-training framework for improving logical reasoning abilities of language models. Upon automatic identifying logical reasoning phenomena in massive text corpus via detection heuristics, we train language models to predict the masked-out logical statements. Inspired by the facilitation effect of reflective thinking in human learning, we analogically simulate the learning-thinking process with an adversarial Generator-Verifier architecture to assist logic learning. LogiGAN implements a novel sequential GAN approach that (a) circumvents the non-differentiable challenge of the sequential GAN by leveraging the Generator as a sentence-level generative likelihood scorer with a learning objective of reaching scoring consensus with the Verifier; (b) is computationally feasible for large-scale pre-training with arbitrary target length. Both base and large size language models pre-trained with LogiGAN demonstrate obvious performance improvement on 12 datasets requiring general reasoning abilities, revealing the fundamental role of logic in broad reasoning, as well as the effectiveness of LogiGAN. Ablation studies on LogiGAN components reveal the relative orthogonality between linguistic and logic abilities and suggest that reflective thinking's facilitation effect might also generalize to machine learning.",arxiv:2205.08794,Yes,,2025-11-11T00:14:11.169Z
logicprobabilityandp-2022,"Logic, Probability, and Pragmatics in Syllogistic Reasoning",Michael Henry Tessler; J. Tenenbaum; Noah D. Goodman,2022,Topics in Cognitive Science,10,https://www.semanticscholar.org/paper/781d57e426c2e4e2e33e8d1d95edd0591b5c7137,,10.1111/tops.12593,"Syllogistic reasoning lies at the intriguing intersection of natural and formal reasoning of language and logic. Syllogisms comprise a formal system of reasoning yet make use of natural language quantifiers (e.g., all, some) and invite natural language conclusions. The conclusions people tend to draw from syllogisms, however, deviate substantially from the purely logical system. Are principles of natural language understanding to blame? We introduce a probabilistic pragmatic perspective on syllogistic reasoning: We decompose reasoning with natural language arguments into two subproblems: language comprehension and language production. We formalize models of these processes within the Rational Speech Act framework and explore the pressures that pragmatic reasoning places on the production of conclusions. We test our models on a recent, large data set of syllogistic reasoning and find that the selection process of conclusions from syllogisms are best modeled as a pragmatic speaker who has the goal of aligning the beliefs of a naive listener with those of their own. We compare our model to previously published models that implement two alternative theories-Mental Models and Probability Heuristics-finding that our model quantitatively predicts the full distributions of responses as well as or better than previous accounts, but with far fewer parameters. Our results suggest that human syllogistic reasoning may be best understood not as a poor approximation to ideal logical reasoning, but rather as rational probabilistic inference in support of natural communication.",,Yes,,2025-11-11T00:14:11.169Z
logicalfallacydetect-2022,Logical Fallacy Detection,A. Akbik; Tanja Bergmann; Duncan Blythe; Kashif; Stefan Rasul; Schweter Roland; Vollgraf; Tom B. Brown; Benjamin Mann; N. Ryder; Jared D Subbiah; Prafulla Kaplan; A. Dhariwal; P. Neelakantan; Girish Shyam; Amanda Sastry; Sandhini Askell; Ariel Agarwal; Herbert-Voss; Gretchen Krueger; T. Henighan; R. Child; Aditya Ramesh; Daniel M. Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; M. Sigler; Scott Litwin; Benjamin Gray; Chess; Alec Radford; I. Sutskever; Kevin Clark; Minh-Thang Luong; Quoc V. Le; Giovanni Da; San Martino; Alberto Barrón-Cedeño; Simona C Kaplan; A. Morrison; Thomas M Goldin; Richard G Olino; Heimberg; Lev Konstantinovskiy; Oliver Price; Mevan Babakar; Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut; Yuhao Peng Qi; Yuhui Zhang; Jason Zhang; Bolton; D. Luan,2022,,0,https://www.semanticscholar.org/paper/c21f978f8be7253bd12c254cd7f05064b0db4526,,,,,Yes,,2025-11-11T00:15:16.541Z
logicaltasksformeasu-2022,Logical Tasks for Measuring Extrapolation and Rule Comprehension,Ippei Fujisawa; R. Kanai,2022,arXiv.org,5,https://www.semanticscholar.org/paper/965e409a3e7b5670d609837fac9823b160d6639c,http://arxiv.org/pdf/2211.07727,10.48550/arXiv.2211.07727,"Logical reasoning is essential in a variety of human activities. A representative example of a logical task is mathematics. Recent large-scale models trained on large datasets have been successful in various fields, but their reasoning ability in arithmetic tasks is limited, which we reproduce experimentally. Here, we recast this limitation as not unique to mathematics but common to tasks that require logical operations. We then propose a new set of tasks, termed logical tasks, which will be the next challenge to address. This higher point of view helps the development of inductive biases that have broad impact beyond the solution of individual tasks. We define and characterize logical tasks and discuss system requirements for their solution. Furthermore, we discuss the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias. Finally, we provide directions for solving logical tasks.",arxiv:2211.07727,Yes,,2025-11-11T00:15:19.325Z
lostinthesupermarket-2022,"Lost in the Supermarket? A Commentary on Gries, Müller, and Jost",D. Osborne; Nicole Satherley; C. Sibley,2022,Psychological Inquiry,1,https://www.semanticscholar.org/paper/6f4169c24369c49ce4c7f12c27e75d790ba54388,,10.1080/1047840X.2022.2065132,"Scholars have long-debated how citizens come to adopt a political ideology. Whereas some suggest that material needs and/or self-interest motivate citizens to endorse the issue positions and ideological stances that maximize utility (see Chong, 2000; Chong & Mullinix, 2022; Sniderman, Glaser, & Griffin, 1991; Weeden & Kurzban, 2017), others argue that less rational—and even irrational—forces are at play and instead focus on the psychological needs met by (Jost, 2020, 2021; Jost, Glaser, Kruglanski, & Sulloway, 2003b), as well as symbolic attachments to (Jardina, 2019; Reny & Sears, 2020; Sears, 1993; Sears & Henry, 2005), specific ideologies. It seems that the extant literature is at an impasse over the antecedents to belief systems. Are citizens rational? Or are they not? Gries, M€ uller, and Jost (this issue) reconcile these contrasting perspectives by asserting that both rational and irrational processes motivate people’s ideological preferences. To these ends, the authors develop a comprehensive model of ideological choice that incorporates both (a) psychological and (b) consumption needs which are weighted by the importance assigned to them by the individual. On the other end of the production chain, ideological entrepreneurs supply ideologies that differentially reconcile these demands and disseminate them within a larger marketplace of beliefs. Although a formal mathematical model is used to identify the ideologies available within the frontier of options that best reconcile these dual needs, Gries et al. assert that, given the informational costs associated with becoming perfectly informed, most citizens simply “try out” different ideologies until they find one that satisfices their psychological and consumption needs. In seeking to resolve the perennial quandary over the determinants of ideology, Gries et al. (this issue) make multiple important contributions to the literature. First, in our view, much of the debate over mass belief systems entails discussions where both parties talk past one another. Those in the ideological purists camp (generally comprised of political scientists) define ideology in rigid terms focused on the presence of a stable and coherent belief system as articulated by Converse (1964) and others, whereas those in the ideological minimalists camp (often comprised of psychologists) have resuscitated the competence of the average voter by treating ideology as a self-defined/identity-based concept present in the vast majority of people (Jost, 2006, 2021). Gries et al. bridge this divide by acknowledging that ideologies are comprised of a “network of attitudes and beliefs... [that are] linked together logically and/or psychologically” (p. 65). Such a compromise brings both sides of this seemingly intractable conflict together and provides the foundations for a promising resolution to one of the most enduring debates in political psychology. In a similar manner, Gries et al. (this issue) help to reconcile the debate over rational and irrational approaches to political ideology by recognizing that both play a role in shaping people’s issue positions. Whereas there is a longstanding tradition of scholars pitting symbolic and self-interested approaches against each other when explaining political attitudes (e.g., Sears, Hensler, & Speer, 1979; Sears, Lau, Tyler, & Allen, 1980; Weeden & Kurzban, 2017), Gries et al. develop a sophisticated model that acknowledges that psychological and consumption needs collectively motivate people to adopt an ideology that best-satisfies these needs. Specifically, the weighting factor within their model recognizes that people will assign different levels of importance to reconciling these distinct needs. For some, an ideology that partially satisfies a highly valued consumption need will be more appealing than a competing ideology that fully satisfies epistemic needs for certainty. Conversely, others will choose an ideology that fulfills their need to belong even if it conflicts with their consumption needs. In this sense, Gries et al.’s model help explains how both psychological and consumption needs motivate people to adopt a given ideology from the larger marketplace of ideas. Gries et al.’s (this issue) mathematical model of ideological choice also helps to reduce the ambiguity inherent in variables measured within the behavioral sciences (but see our discussion on the falsifiability of the model below). Indeed, as noted by the authors, “ordinary language is inherently ambiguous” (p. 70). Nebulous concepts like “self-interest,” “rationality,” and “epistemic needs” belie direct measurement and render explicit hypothesis testing difficult or near impossible. By explicitly quantifying, a priori, the relationships that psychological and consumption needs have with ideologies and their respective (perceived) abilities to resolve these needs, the authors provide a useful tool for evaluating the rationality of citizens. That is, the rationality of the public can be assessed within Gries et al.’s model by calculating the multivariate distance between people’s ideological choice and the array of (weighted) psychological and consumption needs that motivate their beliefs—the further",,Yes,,2025-11-11T00:15:19.325Z
measuretheoreticsetr-2022,M EASURE -T HEORETIC S ET R EPRESENTATION L EARNING,Michael Boratko; Shib Dhruvesh Patel; Sankar Dasgupta; Andrew McCallum,2022,,0,https://www.semanticscholar.org/paper/bd4caa6a5210e7b4fe24735dc7b5169c5284670b,,,,,Yes,,2025-11-11T00:15:19.325Z
mapskbamillionscalep-2022,MAPS-KB: A Million-scale Probabilistic Simile Knowledge Base,Qi He; Xintao Wang; Jiaqing Liang; Yanghua Xiao,2022,AAAI Conference on Artificial Intelligence,4,https://www.semanticscholar.org/paper/83ab9f2fdb2d7445f8b72b732fe17f9e21603082,http://arxiv.org/pdf/2212.05254,10.48550/arXiv.2212.05254,"The ability to understand and generate similes is an imperative step to realize human-level AI. However, there is still a considerable gap between machine intelligence and human cognition in similes, since deep models based on statistical distribution tend to favour high-frequency similes. Hence, a large-scale symbolic knowledge base of similes is required, as it contributes to the modeling of diverse yet unpopular similes while facilitating additional evaluation and reasoning. To bridge the gap, we propose a novel framework for large-scale simile knowledge base construction, as well as two probabilistic metrics which enable an improved understanding of simile phenomena in natural language. Overall, we construct MAPS-KB, a million-scale probabilistic simile knowledge base, covering 4.3 million triplets over 0.4 million terms from 70 GB corpora. We conduct sufficient experiments to justify the effectiveness and necessity of the methods of our framework. We also apply MAPS-KB on three downstream tasks to achieve state-of-the-art performance, further demonstrating the value of MAPS-KB. Resources of MAPS-KB are publicly available at https://github.com/Abbey4799/MAPS-KB.",arxiv:2212.05254,Yes,,2025-11-11T00:15:16.543Z
mentionmemoryincorpo-2022,MENTION MEMORY : INCORPORATING TEXTUAL KNOWLEDGE INTO TRANSFORMERS THROUGH ENTITY MENTION ATTENTION,Michiel de Jong; Yury Zemlyanskiy; Nicholas FitzGerald; Fei Sha; W. Cohen,2022,International Conference on Learning Representations,50,https://www.semanticscholar.org/paper/7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7,,,"Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge with `mention memory', a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining.",arxiv:2110.06176,Yes,,2025-11-11T00:15:14.026Z
mrklsystemsamodularn-2022,"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",Ehud Karpas; Omri Abend; Yonatan Belinkov; Barak Lenz; Opher Lieber; Nir Ratner; Y. Shoham; Hofit Bata; Yoav Levine; Kevin Leyton-Brown; Dor Muhlgay; N. Rozen; Erez Schwartz; Gal Shachaf; Shai Shalev-Shwartz; A. Shashua; Moshe Tenenholtz,2022,arXiv.org,93,https://www.semanticscholar.org/paper/1bcde55995a957b3e8a595d536b816cb8989cf1d,http://arxiv.org/pdf/2205.00445,10.48550/arXiv.2205.00445,"Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced""miracle"") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.",arxiv:2205.00445,Yes,,2025-11-11T00:13:07.427Z
murmurmodularmultist-2022,MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation,Swarnadeep Saha; Xinyan Velocity Yu; Mohit Bansal; Ramakanth Pasunuru; Asli Celikyilmaz,2022,Annual Meeting of the Association for Computational Linguistics,13,https://www.semanticscholar.org/paper/5791c2b41dd23310c53d6738a4c0d587107c2dc8,http://arxiv.org/pdf/2212.08607,10.48550/arXiv.2212.08607,"Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR, a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26% more logically consistent summaries on LogicNLG, compared to direct prompting.",arxiv:2212.08607,Yes,,2025-11-11T00:14:11.169Z
machinelearningandlo-2022,Machine Learning and Logical Reasoning: The New Frontier (Dagstuhl Seminar 22291),Sébastien Bardin; S. Jha; Vijay Ganesh,2022,Dagstuhl Reports,2,https://www.semanticscholar.org/paper/642f92924f88ad5d91a30f293783442d86f87bb7,,10.4230/DagRep.12.7.80,,,Yes,,2025-11-11T00:15:14.026Z
machinereadingfastan-2022,"Machine Reading, Fast and Slow: When Do Models “Understand” Language?",Sagnik Ray Choudhury; Anna Rogers; Isabelle Augenstein,2022,International Conference on Computational Linguistics,19,https://www.semanticscholar.org/paper/2a215364e3fbe5a4c45b61dc5bd869399fa82661,http://arxiv.org/pdf/2209.07430,10.48550/arXiv.2209.07430,"Two of the most fundamental issues in Natural Language Understanding (NLU) at present are: (a) how it can established whether deep learning-based models score highly on NLU benchmarks for the ”right” reasons; and (b) what those reasons would even be. We investigate the behavior of reading comprehension models with respect to two linguistic ”skills”: coreference resolution and comparison. We propose a definition for the reasoning steps expected from a system that would be ”reading slowly”, and compare that with the behavior of five models of the BERT family of various sizes, observed through saliency scores and counterfactual explanations. We find that for comparison (but not coreference) the systems based on larger encoders are more likely to rely on the ”right” information, but even they struggle with generalization, suggesting that they still learn specific lexical patterns rather than the general principles of comparison.",arxiv:2209.07430,Yes,,2025-11-11T00:15:14.026Z
makinglanguagemodels-2022,Making Language Models Better Reasoners with Step-Aware Verifier,Yifei Li; Zeqi Lin; Shizhuo Zhang; Qiang Fu; B. Chen; Jian-Guang Lou; Weizhu Chen,2022,Annual Meeting of the Association for Computational Linguistics,271,https://www.semanticscholar.org/paper/e826ac71dad8c4ce36d82fb7add43e3d306bb7e1,https://aclanthology.org/2023.acl-long.291.pdf,10.18653/v1/2023.acl-long.291,"Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",arxiv:2206.02336,Yes,,2025-11-11T00:14:11.169Z
mathclmmathematicalc-2022,MathCLM: Mathematical Cognitive Learning Model Based on the Evolution of Knowledge Graph,Gongqi Lin; Xiuqin Zhong; Hongguang Fu,2022,"International Conference on Control, Automation, Robotics and Vision",0,https://www.semanticscholar.org/paper/cdc45b158dea0d58c47fead1105356d6bb174937,,10.1109/ICARCV57592.2022.10004260,"In real-world applications, the effective integration of learning and reasoning in a cognitive agent model is a challenging mission. However, such integration may lead to a better understanding, practice, and construction of more realistic models, especially for mathematical learning. Unfortunately, existing models are either oversimplified or require much processing time, which is unsuitable for online learning and education. Therefore, we propose a novel cognitive learning model, called Mathematical Cognitive Learning Model (MathCLM) based on the evolution of knowledge graph, for online mathematical learning that seeks to effectively represent, learn, and reason in online learning environments. The model's architecture combines cognitive learning with symbolic knowledge representation based on natural language processing (NLP). We introduce the mathematical instance concept to build the strategies by mathematical knowledge, such as theorems, axioms, etc., and infer new custom instances based on the learning knowledge. Furthermore, it can deal with uncertainty and errors from instances recommendation using a graph matching model and displays the inference progressing with different combinations of instances. We build a platform to promote and validate our model. The validation of the model on the real-world platform and the results presented here indicate the promise of the approach when performing online learning and reasoning in real-world scenarios, with possible applications in various areas.",,Yes,,2025-11-11T00:14:11.169Z
mathematicallanguage-2022,Mathematical language shapes how we understand the economy,W. Arthur,2022,,0,https://www.semanticscholar.org/paper/51aae5840a02bafe5368753e31b4168eeacdf841,,,,,Yes,,2025-11-11T00:13:07.427Z
mathematicaltoolsfor-2022,Mathematical tools for making sense of a global pandemic,Felipe Munoz-Rubke; Felipe Almuna; Jaclyn Duemler; Eloísa Velásquez,2022,"International Journal of Science Education, Part B",3,https://www.semanticscholar.org/paper/8ee28890f6cddb1f0ecd255af33e7b1968c0e445,,10.1080/21548455.2022.2100941,"ABSTRACT
 The COVID-19 pandemic revealed that many countries have failed to provide the general population with the cognitive tools to thoroughly understand and cope with a global health crisis. While scientists and leaders worldwide have struggled to discover ways to contain the spread of the virus, this difficult task has become overwhelming due to the limited ability of many citizens to grasp the urgency of the situation. Although in today’s digitized world we have endless access to data and more ways to represent information and statistics than ever before, numerous incidents have demonstrated that the frequent misapprehension of data can cause confusion rather than clarity. This opinion paper examines how issues such as the misunderstanding of large quantities, fractions, probabilities, and mathematical modeling may be affecting the way people view the current pandemic. Finally, we also discuss how numeracy can act as a protective factor against motivated reasoning, which often affects how we consume information related to the pandemic.",,Yes,,2025-11-11T00:14:11.169Z
measurementevaluatio-2022,"Measurement, Evaluation, and Model Construction of Mathematical Literacy Based on IoT and PISA",Yunfeng Chen,2022,Mathematical Problems in Engineering,4,https://www.semanticscholar.org/paper/da5c08f5175374114940aec48ddc6dcba494dab3,https://downloads.hindawi.com/journals/mpe/2022/3278401.pdf,10.1155/2022/3278401,"“Mathematics Curriculum Standard for Ordinary Senior High School” points out that mathematical modelling literacy is the literacy of abstracting real problems mathematically, expressing problems with mathematical language, and building models with mathematical methods to solve problems. It assesses the amount to which students who are about to finish compulsory schooling have the knowledge and aptitude to deal with future life issues, based on the notion of lifelong learning. The usage of a local integrated development environment requires a number of complex processes, including installation and setup, as well as the procurement of necessary hardware. In reality, in order to nurture students, the core literacy of mathematics is to completely execute quality education, further identify the training and development direction of talents, and infiltrate the core literacy material into junior high school mathematics classroom instruction. For such a large-scale international education measurement project, the research and development of test questions is very important. Therefore, detailed technical consideration has been carried out and test volumes have been designed that are relatively suitable for different countries. The test questions are closely related to life, while there are few questions related to real situational problems in the test, which are mathematically processed first. Due to the international background of evaluation, it is a great challenge for researchers to realize the localization of evaluation on the basis of adapting to the domestic situation. At the same time, there is no scientific and perfect mathematical modelling literacy evaluation system in China’s basic education research. How can middle school front-line teachers better evaluate students’ mathematical modelling literacy? As a result, it is critical to develop a scientific and quantitative mathematical literacy measurement model that will aid in the teaching and research of mathematical modelling by middle school front-line teachers, as well as contributing theoretically to the evaluation and research of mathematical modelling literacy in China. Therefore, based on mathematics literacy evaluation, this paper studies the hierarchy of the IoT and the measurement and evaluation model of mathematics literacy based on the IoT, as well as its enlightenment to the compilation of mathematics academic test questions in China.",,Yes,,2025-11-11T00:13:07.427Z
mechanizationoflagcs-2022,Mechanization of LAGC Semantics in Isabelle,Niklas Heidler,2022,arXiv.org,1,https://www.semanticscholar.org/paper/6da3c8e830a4f4398da15ddd16afca0fae962b76,,,"Formal programming language semantics are imperative when trying to verify properties of programs in an automated manner. Using a new approach, Din et al. strengthen the ability of reasoning about concurrent programs by proposing a modular trace semantics, which can flexibly adapt to the most prominent imperative programming language paradigms. These semantics decouple the evaluation in the local environments from the evaluation in the global environment by generating abstract, symbolic traces for the individual, local systems. The traces are then composed and concretized, resulting in global traces for the global system. Hence, these semantics are called Locally Abstract, Globally Concrete (LAGC). In this work, we present a formalization of the LAGC semantics in the popular theorem proving environment Isabelle/HOL. The given model is based on the prior work on the theory of LAGC semantics by Din et al. and includes formalizations of the basic theorems, the LAGC semantics for the While Language (WL), as well as the LAGC semantics for an extended version of the While Language (WLEXT). We furthermore use our Isabelle model in order to provide formal proofs for several advanced properties of the LAGC semantics, which have not been analyzed in the original paper. Whilst the main goal of the work was to formalize the LAGC semantics in a mathematically rigorous manner, we also achieve a high level of proof automatization and manage to contribute an efficient code-generation for the computation of program traces. As the formalization of the semantics is highly modular, the given theories could in the future be extended with even more sophisticated programming language paradigms.",arxiv:2202.08017,Yes,,2025-11-11T00:15:16.543Z
mechanizationofalarg-2022,Mechanization of a Large DSML: An Experiment with AADL and Coq,J. Hugues; L. Wrage; J. Hatcliff; D. Stewart,2022,International Conference on Formal Methods and Models for Co-Design,1,https://www.semanticscholar.org/paper/b02e3df037c35e15bff81e1cd80a545b5dbbcfc3,,10.1109/MEMOCODE57689.2022.9954589,"Domain-Specific Modeling Languages (DSMLs) rely on model-based techniques to deliver tailored languages to meet specific needs, such as system modeling, formal verification, and code generation. A DSML has specific static and dynamic behavior rules that must be properly assessed before processing the model. The definition of these rules remains a challenge. Meta-modeling techniques usually lack the foundational elements required to fully express behavioral semantics. In this context, using an interactive theorem prover provides a mathematical foundation with which the semantics of a DSML can be defined. This includes an abstract syntax tree, typing rules, and derivation of an executable simulator. In this paper, we report on an ongoing effort to capture the SAE AADL language using Coq along with specific analysis capabilities. Our contribution provides an unambiguous semantics for a large set of the language and can be used as a foundation to build rich analysis capabilities.",,Yes,,2025-11-11T00:14:11.169Z
mechanizingmatchingl-2022,Mechanizing Matching Logic in Coq,Péter Bereczky; Xiaohong Chen; D'aniel Horp'acsi; Tam'as B'alint Mizsei; Lucas Peña; Jan Tusil,2022,FROM,4,https://www.semanticscholar.org/paper/70f0ea217682d704f618fb2b09d55cbe9cd66701,https://arxiv.org/pdf/2201.05716,10.4204/EPTCS.369.2,"Matching logic is a formalism for specifying, and reasoning about, mathematical structures, using patterns and pattern matching. Growing in popularity, it has been used to define many logical systems such as separation logic with recursive definitions and linear temporal logic. In addition, it serves as the logical foundation of the K semantic framework, which was used to build practical verifiers for a number of real-world languages. Despite being a fundamental formal system accommodating substantial theories, matching logic lacks a general-purpose, machine-checked formalization. Hence, we formalize matching logic using the Coq proof assistant. Specifically, we create a new representation of matching logic that uses a locally nameless encoding, and we formalize the syntax, semantics, and proof system of this representation in the Coq proof assistant. Crucially, we prove the soundness of the formalized proof system and provide a means to carry out interactive matching logic reasoning in Coq. We believe this work provides a previously unexplored avenue for reasoning about matching logic, its models, and the proof system.",arxiv:2201.05716,Yes,,2025-11-11T00:15:16.543Z
medmcqaalargescalemu-2022,MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering,Ankit Pal; Logesh Kumar Umapathi; Malaikannan Sankarasubbu,2022,"ACM Conference on Health, Inference, and Learning",461,https://www.semanticscholar.org/paper/741776172685b9717159a9fcd21841461bb33b14,http://arxiv.org/pdf/2203.14371,10.48550/arXiv.2203.14371,"This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS \&NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects \&topics. A detailed explanation of the solution, along with the above information, is provided in this study.",arxiv:2203.14371,Yes,,2025-11-11T00:13:07.427Z
miqaabenchmarkforinf-2022,MiQA: A Benchmark for Inference on Metaphorical Questions,I. Comsa; Julian Martin Eisenschlos; S. Narayanan,2022,AACL,12,https://www.semanticscholar.org/paper/777683db4795ff691533c2c4be3244fabd826842,http://arxiv.org/pdf/2210.07993,10.48550/arXiv.2210.07993,"We propose a benchmark to assess the capability of large language models to reason with conventional metaphors. Our benchmark combines the previously isolated topics of metaphor detection and commonsense reasoning into a single task that requires a model to make inferences by accurately selecting between the literal and metaphorical register. We examine the performance of state-of-the-art pre-trained models on binary-choice tasks and find a large discrepancy between the performance of small and very large models, going from chance to near-human level. We also analyse the largest model in a generative setting and find that although human performance is approached, careful multiple-shot prompting is required.",arxiv:2210.07993,Yes,,2025-11-11T00:15:14.026Z
mindseyegroundedlang-2022,Mind's Eye: Grounded Language Model Reasoning through Simulation,Ruibo Liu; Jason Wei; S. Gu; Te-Yen Wu; Soroush Vosoughi; Claire Cui; Denny Zhou; Andrew M. Dai,2022,International Conference on Learning Representations,89,https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8,http://arxiv.org/pdf/2210.05359,10.48550/arXiv.2210.05359,"Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.",arxiv:2210.05359,Yes,,2025-11-11T00:13:07.427Z
miningdistributeddat-2022,Mining Distributed Data using Vertical Federated Learning Review,Manaaf Abdulredha; DR Yassen; Lamia AbedNoor; Muhammed,2022,,0,https://www.semanticscholar.org/paper/7d76acadc9712d229687c91cbd752ccb86d6c2f4,,,,,Yes,,2025-11-11T00:15:19.325Z
modalspecificpseudoq-2022,Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval,Minjoon Jung; Seongho Choi; Joo-Kyung Kim; Jin-Hwa Kim; Byoung-Tak Zhang,2022,Conference on Empirical Methods in Natural Language Processing,11,https://www.semanticscholar.org/paper/42d0c43d016b1cc1b8bf0fb01cbad17bdbb16400,http://arxiv.org/pdf/2210.12617,10.48550/arXiv.2210.12617,"Video corpus moment retrieval (VCMR) is the task to retrieve the most relevant video moment from a large video corpus using a natural language query.For narrative videos, e.g., drama or movies, the holistic understanding of temporal dynamics and multimodal reasoning are crucial.Previous works have shown promising results; however, they relied on the expensive query annotations for the VCMR, i.e., the corresponding moment intervals.To overcome this problem, we propose a self-supervised learning framework: Modal-specific Pseudo Query Generation Network (MPGN).First, MPGN selects candidate temporal moments via subtitle-based moment sampling.Then, it generates pseudo queries exploiting both visualand textual information from the selected temporal moments.Through the multimodal information in the pseudo queries, we show that MPGN successfully learns to localize the video corpus moment without any explicit annotation.We validate the effectiveness of MPGN on TVR dataset, showing the competitive results compared with both supervised models and unsupervised setting models.",arxiv:2210.12617,Yes,,2025-11-11T00:15:16.541Z
modelindependentdesi-2022,Model-Independent Design of Knowledge Graphs - Lessons Learnt From Complex Financial Graphs,Luigi Bellomarini; Andrea Gentili; Eleonora Laurenza; Emanuel Sallinger,2022,International Conference on Extending Database Technology,2,https://www.semanticscholar.org/paper/c3e145773ae5ef9ffa3107f21d0e5f5636f8734a,,10.48786/edbt.2022.46,,,Yes,,2025-11-11T00:15:14.026Z
modelingcovid19disea-2022,Modeling COVID-19 disease processes by remote elicitation of causal Bayesian networks from medical experts,S. Mascaro; Yue Wu; Owen Woodberry; Erik P. Nyberg; Ross Pearson; J. Ramsay; A. Mace; David Foley; T. Snelling; A. Nicholson,2022,BMC Medical Research Methodology,13,https://www.semanticscholar.org/paper/1df6c7e235460d5beeed23fbd45f350dd739645e,https://bmcmedresmethodol.biomedcentral.com/counter/pdf/10.1186/s12874-023-01856-1,10.1186/s12874-023-01856-1,"Background COVID-19 is a new multi-organ disease causing considerable worldwide morbidity and mortality. While many recognized pathophysiological mechanisms are involved, their exact causal relationships remain opaque. Better understanding is needed for predicting their progression, targeting therapeutic approaches, and improving patient outcomes. While many mathematical causal models describe COVID-19 epidemiology, none have described its pathophysiology. Methods In early 2020, we began developing such causal models. The SARS-CoV-2 virus’s rapid and extensive spread made this particularly difficult: no large patient datasets were publicly available; the medical literature was flooded with sometimes conflicting pre-review reports; and clinicians in many countries had little time for academic consultations. We used Bayesian network (BN) models, which provide powerful calculation tools and directed acyclic graphs (DAGs) as comprehensible causal maps. Hence, they can incorporate both expert opinion and numerical data, and produce explainable, updatable results. To obtain the DAGs, we used extensive expert elicitation (exploiting Australia’s exceptionally low COVID-19 burden) in structured online sessions. Groups of clinical and other specialists were enlisted to filter, interpret and discuss the literature and develop a current consensus. We encouraged inclusion of theoretically salient latent (unobservable) variables, likely mechanisms by extrapolation from other diseases, and documented supporting literature while noting controversies. Our method was iterative and incremental: systematically refining and validating the group output using one-on-one follow-up meetings with original and new experts. 35 experts contributed 126 hours face-to-face, and could review our products. Results We present two key models, for the initial infection of the respiratory tract and the possible progression to complications, as causal DAGs and BNs with corresponding verbal descriptions, dictionaries and sources. These are the first published causal models of COVID-19 pathophysiology. Conclusions Our method demonstrates an improved procedure for developing BNs via expert elicitation, which other teams can implement to model emergent complex phenomena. Our results have three anticipated applications: (i) freely disseminating updatable expert knowledge; (ii) guiding design and analysis of observational and clinical studies; (iii) developing and validating automated tools for causal reasoning and decision support. We are developing such tools for the initial diagnosis, resource management, and prognosis of COVID-19, parameterized using the ISARIC and LEOSS databases.",,Yes,,2025-11-11T00:15:16.543Z
modelingandreasoning-2022,Modeling and reasoning about uncertainty in goal models: a decision-theoretic approach,S. Liaskos; Shakil M. Khan; M. Soutchanski; John; Mylopoulos,2022,Journal of Software and Systems Modeling,6,https://www.semanticscholar.org/paper/3a28969d4d4c6c59da63d36f26368966cbf33386,,10.1007/s10270-021-00968-w,,,Yes,,2025-11-11T00:15:14.026Z
modellingrolesofmath-2022,Modelling Roles of Mathematics in Physics,E. Palmgren; Tapio Rasa,2022,Science Education,9,https://www.semanticscholar.org/paper/2de4ebee2bcbe09f66f6d3367830b18460f50130,https://link.springer.com/content/pdf/10.1007/s11191-022-00393-5.pdf,10.1007/s11191-022-00393-5,"Modelling roles of mathematics in physics has proved to be a difficult task, with previous models of the interplay between the two disciplines mainly focusing on mathematical modelling and problem solving. However, to convey a realistic view of physics as a field of science to our students, we need to do more than train them to become fluent in modelling and problem solving. In this article, we present a new characterisation of the roles mathematics plays in physics and physics education, taking as a premise that mathematics serves as a constitutive structure in physics analogous to language. In doing so, we aim to highlight how mathematics affects the way we conceptualise physical phenomena. To contextualise our characterisation, we examine some of the existing models and discuss aspects of the interplay between physics and mathematics that are missing in them. We then show how these aspects are incorporated in our characterisation in which mathematics serves as a foundation upon which physical theories are built, and on which we may build mathematical representations of physical information that in turn serve as a basis for further reasoning and modifications. Through reasoning processes mathematics also aids in generating new information and explanations. We have elucidated each of these roles with an example from the historical development of quantum physics. To conclude, we discuss how our new characterisation may aid the development of physics education and physics education research.",,Yes,,2025-11-11T00:15:19.325Z
modellingoffuzzyexpe-2022,Modelling of Fuzzy Expert System for an Assessment of Security Information Management System UIS (University Information System),Ljilja Šikman; T. Latinovic,2022,Tehnički Vjesnik,7,https://www.semanticscholar.org/paper/5350b60583818e81d02c19fa4613fdb0b3ffea6e,https://hrcak.srce.hr/file/390869,10.17559/tv-20200721154801,": Several methodologies based on the international standard ISO/IEC 27001 have been developed for modelling information security management systems within higher education. This paper transformed the ISO/IEC 27001 standard into a questionnaire, which was sent digitally to about 100 universities in Bosnia and Herzegovina, and to the EU, Norway and the USA. The questions are arranged by levels, and the levels have their numerical weights, derived from individual questions in the levels themselves. Otherwise, the questions are asked with Yes or No and thus are reduced to binary variables. The rules necessary for the functioning of the system have been calculated. The fuzzy logic method represents a new approach to the problems of managing complex systems, which is very difficult to describe with a certain mathematical model, as well as in systems with a large number of inputs and outputs where there are unclear interactions. Risk assessment is a major part of the ISMS process. Traditional risk calculation models are based on the application of probability and classical set theory. Here, we have converted the risk assessment into a system rating of 5 to 10 numerically or from five to ten descriptively. We perform fuzzy optimization by finding the values of the input parameters of a complex simulated system, which results in the desired output. We use the fuzzy logic controller to execute fuzzy inference rules from the fuzzy rule database in determining congestion parameters, obtaining warning information and appropriate action. Simulating the situation of an advanced system that evaluates the protection quality of such a system with fuzzy logic, we use MATLAB. The paper combines the original Visual Basic programming language and MATLAB's Fuzzy Toolbox, to solve the complex problem of assessing compliance with the ISO/IEC 27001 standard, as one of the main standards for information systems security modelling. University information systems were used, but it is also applicable to all other information systems. The evaluation has been done for several universities and it has been proven that the system evaluates correctly, but these universities must not be publicly named. There was no such approach in the use of fuzzy logic and on such systems, and that is the originality of this work.",,Yes,,2025-11-11T00:15:16.543Z
modulecosoftwareform-2022,"Moduleco, software for macroeconomic modelization",P. Nepomiastchy,2022,Mausam,0,https://www.semanticscholar.org/paper/ccf4c20249a44483c49be8f94901d225bead8aa6,https://mausamjournal.imd.gov.in/index.php/MAUSAM/article/download/1834/1648,10.54302/mausam.v36i2.1834,"The paper describes the Moduleco system which is designed to facilitate the construction and the use of large scale (1000 equations or more) dynamic and non-linear macroecenomic models. 
The Moduleco system will include a software for the management of the time-series data base, a special modeling language for the model equations Input, a special common language to active the tasks, several tools of formal computation and an interactive language for easy data input-output and for easy scenario generation. 
The paper describes also the mathematical algorithms which are to be included in the Moduleco system. Indeed, we have noticed that most of the macroeconomic models can be put in a quasi triangular form: possibly after renumbering of the variables and equations, there exist a small set of variables, called loop variables, such as for given values of them, the remaining model is triangular and can be solved directly. As we have shown that quasi-triangular models can be simulated and optimized. much faster than general ones, the Moduleco system will include methods for automatic renumbering of variables and equations in order to minimize the number of loop variables. 
The simulation and optimization algorithms will then be adapted to take into account this quasi triangularity. Experiments made on 4 concrete macroeconomic models have shown the efficiency of the proposed methods. 
Moreover, the adjoint variable technique, well known in optimal control theory, has been adapted to the structure of macroeconomic models. On the example of the French STAR model (139 equations), it is shown that this technique is 106 times faster to compute the gradient than the finite difference technique generally used by economists.",,Yes,,2025-11-11T00:15:16.543Z
muragmultimodalretri-2022,MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text,Wenhu Chen; Hexiang Hu; Xi Chen; Pat Verga; William W. Cohen,2022,Conference on Empirical Methods in Natural Language Processing,208,https://www.semanticscholar.org/paper/38b0803b59e4973f09018ce942164b02be4b8bc9,http://arxiv.org/pdf/2210.02928,10.48550/arXiv.2210.02928,"While language Models store a massive amount of world knowledge implicitly in their parameters, even very large models often fail to encode information about rare entities and events, while incurring huge computational costs. Recently, retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated world knowledge into language generation by leveraging an external non-parametric index and have demonstrated impressive performance with constrained model sizes. However, these methods are restricted to retrieving only textual knowledge, neglecting the ubiquitous amount of knowledge in other modalities like images – much of which contains information not covered by any text. To address this limitation, we propose the first Multimodal Retrieval-Augmented Transformer (MuRAG), which accesses an external non-parametric multimodal memory to augment language generation. MuRAG is pre-trained with a mixture of large-scale image-text and text-only corpora using a joint contrastive and generative loss. We perform experiments on two different datasets that require retrieving and reasoning over both images and text to answer a given query: WebQA, and MultimodalQA. Our results show that MuRAG achieves state-of-the-art accuracy, outperforming existing models by 10-20% absolute on both datasets and under both distractor and full-wiki settings.",arxiv:2210.02928,Yes,,2025-11-11T00:15:14.026Z
multistepdeductivere-2022,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,Qiming Bao; A. Peng; Tim Hartill; N. Tan; Zhenyun Deng; M. Witbrock; Jiamou Liu,2022,International Workshop on Neural-Symbolic Learning and Reasoning,16,https://www.semanticscholar.org/paper/4a52399e66da3fb1406132ecedf274925b5f7972,http://arxiv.org/pdf/2207.14000,10.48550/arXiv.2207.14000,"Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gated attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datasets, we develop PARARULE-Plus, a large dataset with more examples that require deeper reasoning steps. Experimental results show that the addition of PARARULE-Plus can increase the model's performance on examples requiring deeper reasoning depths. The source code and data are available at https://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.",arxiv:2207.14000,Yes,,2025-11-11T00:13:07.427Z
multihopquestionansw-2022,Multi-hop Question Answering,Vaibhav Mavi; Anubhav Jangra; A. Jatowt,2022,Foundations and Trends in Information Retrieval,55,https://www.semanticscholar.org/paper/63e939b0606207941673def2b69c6240d549d198,https://arxiv.org/pdf/2204.09140,10.1561/1500000102,"The task of Question Answering (QA) has attracted significant research interest for long. Its relevance to language understanding and knowledge retrieval tasks, along with the simple setting makes the task of QA crucial for strong AI systems. Recent success on simple QA tasks has shifted the focus to more complex settings. Among these, Multi-Hop QA (MHQA) is one of the most researched tasks over the recent years. In broad terms, MHQA is the task of answering natural language questions that involve extracting and combining multiple pieces of information and doing multiple steps of reasoning. An example of a multi-hop question would be""The Argentine PGA Championship record holder has won how many tournaments worldwide?"". Answering the question would need two pieces of information:""Who is the record holder for Argentine PGA Championship tournaments?""and""How many tournaments did [Answer of Sub Q1] win?"". The ability to answer multi-hop questions and perform multi step reasoning can significantly improve the utility of NLP systems. Consequently, the field has seen a surge with high quality datasets, models and evaluation strategies. The notion of 'multiple hops' is somewhat abstract which results in a large variety of tasks that require multi-hop reasoning. This leads to different datasets and models that differ significantly from each other and makes the field challenging to generalize and survey. We aim to provide a general and formal definition of the MHQA task, and organize and summarize existing MHQA frameworks. We also outline some best practices for building MHQA datasets. This book provides a systematic and thorough introduction as well as the structuring of the existing attempts to this highly interesting, yet quite challenging task.",arxiv:2204.09140,Yes,,2025-11-11T00:15:16.543Z
multilingualeventlin-2022,Multilingual Event Linking to Wikidata,Adithya Pratapa; Rishubh Gupta; T. Mitamura,2022,MIA,7,https://www.semanticscholar.org/paper/e20531a7084a6b2c2bfee7c8cf911752841e5910,https://aclanthology.org/2022.mia-1.5.pdf,10.18653/v1/2022.mia-1.5,"We present a task of multilingual linking of events to a knowledge base. We automatically compile a large-scale dataset for this task, comprising of 1.8M mentions across 44 languages referring to over 10.9K events from Wikidata. We propose two variants of the event linking task: 1) multilingual, where event descriptions are from the same language as the mention, and 2) crosslingual, where all event descriptions are in English. On the two proposed tasks, we compare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and multilingual adaptations of the biencoder and crossencoder architectures from BLINK (Wu et al., 2020). In our experiments on the two task variants, we find both biencoder and crossencoder models significantly outperform the BM25+ baseline. Our results also indicate that the crosslingual task is in general more challenging than the multilingual task. To test the out-of-domain generalization of the proposed linking systems, we additionally create a Wikinews-based evaluation set. We present qualitative analysis highlighting various aspects captured by the proposed dataset, including the need for temporal reasoning over context and tackling diverse event descriptions across languages.",arxiv:2204.06535,Yes,,2025-11-11T00:15:19.325Z
multiscalemodelingat-2022,Multiscale Modeling at the Interface of Molecular Mechanics and Natural Language through Attention Neural Networks.,M. Buehler,2022,Accounts of Chemical Research,19,https://www.semanticscholar.org/paper/832529dd12d1e5ec6db62291280fca231d6d17f4,,10.1021/acs.accounts.2c00330,"ConspectusHumans are continually bombarded with massive amounts of data. To deal with this influx of information, we use the concept of attention in order to perceive the most relevant input from vision, hearing, touch, and others. Thereby, the complex ensemble of signals is used to generate output by querying the processed data in appropriate ways. Attention is also the hallmark of the development of scientific theories, where we elucidate which parts of a problem are critical, often expressed through differential equations. In this Account we review the emergence of attention-based neural networks as a class of approaches that offer many opportunities to describe materials across scales and modalities, including how universal building blocks interact to yield a set of material properties. In fact, the self-assembly of hierarchical, structurally complex, and multifunctional biomaterials remains a grand challenge in modeling, theory, and experiment. Expanding from the process by which material building blocks physically interact to form a type of material, in this Account we view self-assembly as both the functional emergence of properties from interacting building blocks as well as the physical process by which elementary building blocks interact and yield structure and, thereby, functions. This perspective, integrated through the theory of materiomics, allows us to solve multiscale problems with a first-principles-based computational approach based on attention-based neural networks that transform information to feature to property while providing a flexible modeling approach that can integrate theory, simulation, and experiment. Since these models are based on a natural language framework, they offer various benefits including incorporation of general domain knowledge via general-purpose pretraining, which can be accomplished without labeled data or large amounts of lower-quality data. Pretrained models then offer a general-purpose platform that can be fine-tuned to adapt these models to make specific predictions, often with relatively little labeled data. The transferrable power of the language-based modeling approach realizes a neural olog description, where mathematical categorization is learned by multiheaded attention, without domain knowledge in its formulation. It can hence be applied to a range of complex modeling tasks─such as physical field predictions, molecular properties, or structure predictions, all using an identical formulation. This offers a complementary modeling approach that is already finding numerous applications, with great potential to solve complex assembly problems, enabling us to learn, build, and utilize functional categorization of how building blocks yield a range of material functions. In this Account, we demonstrate the approach in various application areas, including protein secondary structure prediction and prediction of normal-mode frequencies as well as predicting mechanical fields near cracks. Unifying these diverse problem areas is the building block approach, where the models are based on a universally applicable platform that offers benefits ranging from transferability, interpretability, and cross-domain pollination of knowledge as exemplified through a transformer model applied to predict how musical compositions infer de novo protein structures. We discuss future potentialities of this approach for a variety of material phenomena across scales, including the use in multiparadigm modeling schemes.",,Yes,,2025-11-11T00:15:14.026Z
nlxgptamodelfornatur-2022,NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks,Fawaz Sammani; Tanmoy Mukherjee; Nikos Deligiannis,2022,Computer Vision and Pattern Recognition,72,https://www.semanticscholar.org/paper/bc64190d42d9dc34077b6a096d9053bb88deaa3a,https://arxiv.org/pdf/2203.05081,10.1109/CVPR52688.2022.00814,"Natural language explanation (NLE) models aim at explaining the decision-making process of a black box system via generating natural language sentences which are human-friendly, high-level and fine-grained. Current NLE models11Throughout this paper, we refer to NLE models as Natural Language Explanation models aimed for vision and vision-language tasks. explain the decision-making process of a vision or vision-language model (a.k.a., task model), e.g., a VQA model, via a language model (a.k.a., explanation model), e.g., GPT. Other than the additional memory resources and inference time required by the task model, the task and explanation models are completely independent, which disassociates the explanation from the reasoning process made to predict the answer. We introduce NLX-GPT, a general, compact and faithful language model that can simultaneously predict an answer and explain it. We first conduct pre-training on large scale data of image-caption pairs for general understanding of images, and then formulate the answer as a text prediction task along with the explanation. Without region proposals nor a task model, our resulting overall framework attains better evaluation scores, contains much less parameters and is 15× faster than the current SoA model. We then address the problem of evaluating the explanations which can be in many times generic, data-biased and can come in several forms. We therefore design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based attack, a selfevaluation framework that requires no labels. Code is at: https://github.com/fawazsammani/nlxgpt.",arxiv:2203.05081,Yes,,2025-11-11T00:13:07.427Z
naturallanguageproce-2022,Natural Language Processing and Parallel Computing for Information Retrieval from Electronic Health Records,Ali Abu Salimeh; Najah Al-shanableh; M. Alzyoud,2022,ITM Web of Conferences,2,https://www.semanticscholar.org/paper/6bb946c85a31de264306bf565bd11e88f89e312a,https://www.itm-conferences.org/articles/itmconf/pdf/2022/02/itmconf_icacs2022_01013.pdf,10.1051/itmconf/20224201013,"In this paper, we review the literature to find suitable information retrieval techniques for EHealth. Also discussed NLP techniques that have been proved their capability to extract valuable information in unstructured data from EHR. One of the best NLP techniques used for searching free text is LSI, due to its capability of finding semantic terms and in rich the search results by finding the hidden relations between terms. LSI uses a mathematical model called SVD, which is not scalable for large amounts of data due to its complexity and exhausts the memory, and a review for recent applications of LSI was discussed.",,Yes,,2025-11-11T00:14:11.169Z
naturalprovergrounde-2022,NaturalProver: Grounded Mathematical Proof Generation with Language Models,S. Welleck; Jiacheng Liu; Ximing Lu; Hannaneh Hajishirzi; Yejin Choi,2022,Neural Information Processing Systems,85,https://www.semanticscholar.org/paper/d593b9b8d63426f0d6a795dd7f2294619bc03610,https://arxiv.org/pdf/2205.12910,10.48550/arXiv.2205.12910,"Theorem proving in natural mathematical language - the mixture of symbolic and natural language used by humans - plays a central role in mathematical advances and education, and tests aspects of reasoning that are core to intelligence. Yet it has remained underexplored with modern generative models. We study large-scale language models on two new generation tasks: suggesting the next step in a mathematical proof, and full proof generation. We develop NaturalProver, a language model that generates proofs by conditioning on background references (e.g. theorems and definitions that are either retrieved or human-provided), and optionally enforces their presence with constrained decoding. On theorems from the NaturalProofs benchmark, NaturalProver improves the quality of next-step suggestions and generated proofs over fine-tuned GPT-3, according to human evaluations from university-level mathematics students. NaturalProver is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40% of the time, which is to our knowledge the first demonstration of these capabilities using neural language models.",arxiv:2205.12910,Yes,,2025-11-11T00:13:07.427Z
neuralcollaborativef-2022,Neural Collaborative Filtering with Ontologies for Integrated Recommendation Systems,Rana Alaa El-deen Ahmed; M. Fernández-Veiga; M. Gawich,2022,Italian National Conference on Sensors,7,https://www.semanticscholar.org/paper/c56ab563455e1571f4653ed738244041d5998a5e,https://www.mdpi.com/1424-8220/22/2/700/pdf?version=1642488203,10.3390/s22020700,"Machine learning (ML) and especially deep learning (DL) with neural networks have demonstrated an amazing success in all sorts of AI problems, from computer vision to game playing, from natural language processing to speech and image recognition. In many ways, the approach of ML toward solving a class of problems is fundamentally different than the one followed in classical engineering, or with ontologies. While the latter rely on detailed domain knowledge and almost exhaustive search by means of static inference rules, ML adopts the view of collecting large datasets and processes this massive information through a generic learning algorithm that builds up tentative solutions. Combining the capabilities of ontology-based recommendation and ML-based techniques in a hybrid system is thus a natural and promising method to enhance semantic knowledge with statistical models. This merge could alleviate the burden of creating large, narrowly focused ontologies for complicated domains, by using probabilistic or generative models to enhance the predictions without attempting to provide a semantic support for them. In this paper, we present a novel hybrid recommendation system that blends a single architecture of classical knowledge-driven recommendations arising from a tailored ontology with recommendations generated by a data-driven approach, specifically with classifiers and a neural collaborative filtering. We show that bringing together these knowledge-driven and data-driven worlds provides some measurable improvement, enabling the transfer of semantic information to ML and, in the opposite direction, statistical knowledge to the ontology. Moreover, the novel proposed system enables the extraction of the reasoning recommendation results after updating the standard ontology with the new products and user behaviors, thus capturing the dynamic behavior of the environment of our interest.",,Yes,,2025-11-11T00:15:19.325Z
neurosymboliccausall-2022,Neuro-Symbolic Causal Language Planning with Commonsense Prompting,Yujie Lu; Weixi Feng; Wanrong Zhu; Wenda Xu; X. Wang; Miguel P. Eckstein; William Yang Wang,2022,arXiv.org,12,https://www.semanticscholar.org/paper/71fd336f1ca337a638dffb236b432c29cdd19f3d,http://arxiv.org/pdf/2206.02928,10.48550/arXiv.2206.02928,,,Yes,,2025-11-11T00:14:11.169Z
newperspectivesforfu-2022,New Perspectives for Fuzzy Datalog (Extended Abstract),Matthias Lanzinger; Stefano Sferrazza; G. Gottlob,2022,Datalog,2,https://www.semanticscholar.org/paper/937753fb585974890f923444d7197e4f997aeeb5,,,,,Yes,,2025-11-11T00:15:19.325Z
newreasoningmodelsim-2022,New Reasoning Models: Improving Optimisation and Decision Support with the Management of Uncertainty and Constraints,A. Valls; César Fernández; Mateu Villaret,2022,International Journal of Computational Intelligence Systems,0,https://www.semanticscholar.org/paper/3c6cc5252a1ec483f95229ae255e02c561bd9cc0,https://link.springer.com/content/pdf/10.1007/s44196-022-00151-z.pdf,10.1007/s44196-022-00151-z,"• Management of Uncertainty In this area, we have four papers that cover the following topics: The paper by Ojeda-Hernández et al. extends the standard Formal Concept Analysis model (based on lattices) by incorporating mixed attributes that model positive and negative information in the same framework. It has been applied to a Mathematical course to define learning paths and study the knowledge space of the students. Alsinet et al. propose the use of graph isomorphism networks to approximately compute the polarization degree of arguments in conversational systems. Experiments are done in the Reddit debate tool. Pascual-Fontanilles et al. present a method for using incoming sets of new examples to adapt a classification model, which is based on fuzzy logic. It is tested in the problem of diagnosis of diabetic retinopathy. The paper of Zhu et al. proposes a linguistic multiple criteria decision aiding model for large groups of decision makers, to be used in evaluation of opinions in complex livelihood projects. They use uncertain linguistic values based on intervals and define different aggregation and consensus operations for this kind of information. • Constraints Satisfaction In this category, we have six papers. The one by Akbay et al. provides an adaptative version of the hybrid combinatorial optimization generic algorithm Construct, merge, solve and adapt. This version is able to selfadapt, avoiding an intensive parameter tuning process while still being state-of-the-art in the minimum positive influence dominating set problem. The one by Zhou et al. addresses the Budgeted Maximum Coverage Problem with a variable depth local search 1 algorithm that is further improved with neighbour structures. Their proposed method outperforms the best existing heuristics and the exact solver CPLEX.The other four papers deal with SAT and its optimization version, MaxSAT. The paper by Almagro-Blanco and Giráldez-Cru analyzes the accuracy of several machine learning techniques to estimate the hardness of realistic pseudo-industrial SAT instances, generated with the Popularity-Similarity SAT model. Their experimental results show that ensemble methods (e.g., Random Forest) achieve the best performance, remaining robust to perturbations in the training phase. The paper by Bofill et al. explores the impact of using implied constraints in a MaxSAT model for the problem of scheduling business-to-business meetings. The experimental results clearly show that variable selection by the SAT solver is significantly affected depending on the implied constraints used in the encoding. Nurcahyadi et al. describe an ant colony optimization solver with negative learning for solving MaxSAT. The experimental results indicate that the proposed approach can be used to improve the results of existing solvers by internally using them to solve smaller sub-instances. Finally, Li et al. tackle the problem of reducing non-clausal MaxSAT and MinSAT to clausal MaxSAT and MinSAT. They define three cost-preserving transformations and report on an empirical comparison that provides evidence that nonclausal MaxSAT and MinSAT can be effectively solved * Aida Valls aida.valls@urv.cat",,Yes,,2025-11-11T00:14:11.169Z
newseditsadatasetofn-2022,NewsEdits : A Dataset of News Article Revision Histories and a Novel Document-Level Reasoning Challenge,T. Afrin; Elaine Lin Wang; Diane Litman; Lind-659 say; Clare Matsumura; Richard Correnti; Norm Goldstein; Irshad Bhat; Talita Anthonio; D. Blei; Andrew Y. Ng; Ryan L Boyd; Kate G Blackburn; James W Pen-685; Felipe Bravo-Marquez; Manuel Manriquez; Lynn Carlson; Daniel Marcu; Mary Ellen; Sarah Cohen; James T Hamilton; Roman Grundkiewicz; Marcin Junczys-Dowmunt; Kathleen A. Hansen; Jean Ward; J. L. Conners; Tatsunori Hashimoto; Kelvin Guu; Yonatan Oren; Xiaoqi Jiao; Yichun Yin; Lifeng Shang; Xin Jiang; Erik W Johnson; Jonathan P. Schreiner; C. Leacock; M. Chodorow; Michael Gamon; Joel Tetreault. 2010; N. Mostafazadeh; Michael Roth; Annie Louis; Nanyun Peng; Marjan Ghazvininejad; Jonathan May; J. Pustejovsky; Patrick Hanks; R. Saurí; R. Gaizauskas; Dragomir Andrea Setzer; F. M. Zanzotto; Fan Zhang; H. Hashemi; Rebecca Hwa,2022,,0,https://www.semanticscholar.org/paper/fdfa68ef25d640a76640ef3162404c47b23f47a4,,,,,Yes,,2025-11-11T00:14:11.169Z
newseditsanewsarticl-2022,NewsEdits: A News Article Revision Dataset and a Novel Document-Level Reasoning Challenge,Alexander Spangher; Xiang Ren; Jonathan May; Nanyun Peng,2022,North American Chapter of the Association for Computational Linguistics,27,https://www.semanticscholar.org/paper/4ef0484cf6898a74a6d17bc5f2efa4e287722471,https://arxiv.org/pdf/2206.07106,10.48550/arXiv.2206.07106,"News article revision histories provide clues to narrative and factual evolution in news articles. To facilitate analysis of this evolution, we present the first publicly available dataset of news revision histories, NewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million articles with 4.6 million versions from over 22 English- and French-language newspaper sources based in three countries, spanning 15 years of coverage (2006-2021).We define article-level edit actions: Addition, Deletion, Edit and Refactor, and develop a high-accuracy extraction algorithm to identify these actions. To underscore the factual nature of many edit actions, we conduct analyses showing that added and deleted sentences are more likely to contain updating events, main content and quotes than unchanged sentences. Finally, to explore whether edit actions are predictable, we introduce three novel tasks aimed at predicting actions performed during version updates. We show that these tasks are possible for expert humans but are challenging for large NLP models. We hope this can spur research in narrative framing and help provide predictive tools for journalists chasing breaking news.",arxiv:2206.07106,Yes,,2025-11-11T00:14:11.169Z
notasstraightforward-2022,Not as Straightforward as It Appears: Undergraduates Leverage Areas to Find Definite Integrals,I. Kontorovich; Tianqing Li,2022,International Journal of Science and Mathematics Education,5,https://www.semanticscholar.org/paper/bbed03b5026e35cfc10c0639ba170854d823c0df,,10.1007/s10763-022-10339-6,,,Yes,,2025-11-11T00:15:19.325Z
novelcomputationalma-2022,Novel computational mathematical algorithms for structural optimization using graph-theoretical methods,Farzad Shafiei Dizaji; Mehrdad Shafiei Dizaji,2022,Engineering computations,0,https://www.semanticscholar.org/paper/ea5ecd4af90ac120cf121ddf50e8d751ccbb13ba,,10.1108/ec-09-2021-0547,"PurposeThe purpose is to reduce round-off errors in numerical simulations. In the numerical simulation, different kinds of errors may be created during analysis. Round-off error is one of the sources of errors. In numerical analysis, sometimes handling numerical errors is challenging. However, by applying appropriate algorithms, these errors are manageable and can be reduced. In this study, five novel topological algorithms were proposed in setting up a structural flexibility matrix, and five different examples were used in applying the proposed algorithms. In doing so round-off errors were reduced remarkably.Design/methodology/approachFive new algorithms were proposed in order to optimize the conditioning of structural matrices. Along with decreasing the size and duration of analyses, minimizing analytical errors is a critical factor in the optimal computer analysis of skeletal structures. Appropriate matrices with a greater number of zeros (sparse), a well structure and a well condition are advantageous for this objective. As a result, a problem of optimization with various goals will be addressed. This study seeks to minimize analytical errors such as rounding errors in skeletal structural flexibility matrixes via the use of more consistent and appropriate mathematical methods. These errors become more pronounced in particular designs with ill-suited flexibility matrixes; structures with varying stiffness are a frequent example of this. Due to the usage of weak elements, the flexibility matrix has a large number of non-diagonal terms, resulting in analytical errors. In numerical analysis, the ill-condition of a matrix may be resolved by moving or substituting rows; this study examined the definition and execution of these modifications prior to creating the flexibility matrix. Simple topological and algebraic features have been mostly utilized in this study to find fundamental cycle bases with particular characteristics. In conclusion, appropriately conditioned flexibility matrices are obtained, and analytical errors are reduced accordingly.Findings(1) Five new algorithms were proposed in order to optimize the conditioning of structural flexibility matrices. (2) A JAVA programming language was written for all five algorithms and a friendly GUI software tool is developed to visualize sub-optimal cycle bases. (3) Topological and algebraic features of the structures were utilized in this study.Research limitations/implicationsThis is a multi-objective optimization problem which means that sparsity and well conditioning of a matrix cannot be optimized simultaneously. In conclusion, well-conditioned flexibility matrices are obtained, and analytical errors are reduced accordingly.Practical implicationsEngineers always finding mathematical modeling of real-world problems and make them as simple as possible. In doing so, lots of errors will be created and these errors could cause the mathematical models useless. Applying decent algorithms could make the mathematical model as precise as possible.Social implicationsErrors in numerical simulations should reduce due to the fact that they are toxic for real-world applications and problems.Originality/valueThis is an original research. This paper proposes five novel topological mathematical algorithms in order to optimize the structural flexibility matrix.",,Yes,,2025-11-11T00:15:14.026Z
numglueasuiteoffunda-2022,NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,Swaroop Mishra; Arindam Mitra; Neeraj Varshney; Bhavdeep Singh Sachdeva; Peter Clark; Chitta Baral; A. Kalyan,2022,Annual Meeting of the Association for Computational Linguistics,118,https://www.semanticscholar.org/paper/39238a92de090c104936a4f78375b95600e42ce5,http://arxiv.org/pdf/2204.05660,10.48550/arXiv.2204.05660,"Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.",arxiv:2204.05660,Yes,,2025-11-11T00:13:07.427Z
numericalcorrelation-2022,Numerical Correlation in Text,Daniel M. Spokoyny; Chien-Sheng Wu; Caiming Xiong,2022,MATHNLP,0,https://www.semanticscholar.org/paper/ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9,https://aclanthology.org/2022.mathnlp-1.5.pdf,10.18653/v1/2022.mathnlp-1.5,"Evaluation of quantitative reasoning of large language models is an important step towards understanding their current capabilities and limitations. We propose a new task, Numerical Correlation in Text, which requires models to identify the correlation between two numbers in a sentence. To this end, we introduce a new dataset, which contains over 2,000 Wikipedia sentences with two numbers and their correlation labels. Using this dataset we are able to show that recent numerically aware pretraining methods for language models do not help generalization on this task posing a challenge for future work in this area.",,Yes,,2025-11-11T00:15:16.543Z
numericalsimulationo-2022,Numerical Simulation of Well Type Optimization in Tridimensional Development of Multi-Layer Shale Gas Reservoir,Tao Huang; Xin-Yi Liao; Zhaoqin Huang; F. Song; Renyi Wang,2022,Energies,2,https://www.semanticscholar.org/paper/86e51d3deabb7a6bfb685741f1d3754908249d45,https://www.mdpi.com/1996-1073/15/18/6529/pdf?version=1662545494,10.3390/en15186529,"Aimed at the development of shale gas reservoirs with large reservoir thickness and multiple layers, this paper carried out a numerical simulation study on the optimization of three different well types: horizontal well, deviated well, and vertical well. To make the model more in line with the characteristics of shale gas reservoirs, a two-phase gas–water seepage mathematical model of shale gas reservoirs was established, considering the adsorption and desorption of shale gas, Knudsen diffusion effect, and stress sensitivity effect. The embedded discrete fracture model was used to describe hydraulic fracture and natural fracture. Based on Fortran language, a numerical simulator for multi-layer development of shale gas reservoirs was compiled, and the calculation results were compared with the actual production data of Barnett shale gas reservoirs to verify the reliability of the numerical simulator. The spread range of hydraulic fractures in the reservoir with different natural fracture densities is calculated by the simulation to determine well spacing and fracture spacing. The orthogonal experimental design method is then used to optimize the best combination of well spacing and fracture spacing for different well types. The results show that the well productivity of the high-density (0.012 m/m2) natural fractures reservoir > the well productivity of the medium-density (0.006 m/m2) natural fractures reservoir > the well productivity of the low-density (0.001 m/m2) natural fractures reservoir. According to the design of the orthogonal test, it can be seen that the most significant factor affecting the productivity of horizontal wells is the fracture spacing in the Y direction. For deviated wells and vertical wells, the X-direction well spacing has the greatest impact on its productivity.",,Yes,,2025-11-11T00:15:16.543Z
numericalanalysisofb-2022,Numerical analysis of blood flow in abdominal aortic aneurysm using finite volume method,A. Fatahillah; Azza Liarista Anggraini; S. Setiawani,2022,Desimal: Jurnal Matematika,1,https://www.semanticscholar.org/paper/6391f7520b13664d268596b3e91fba6019a1a7bb,http://ejournal.radenintan.ac.id/index.php/desimal/article/download/9928/5583,10.24042/djm.v5i2.9928,"There is a deadly cardiovascular disease that can cause swelling of the Abdominal Aorta. This disease is known as Abdominal Aortic Aneurysm (AAA). AAA is believed to be a degenerative process caused by genetic factors, gender, body weight, and age. Changes in collagen and elastin in the aortic wall are the cause of the degeneration process. Therefore, it will cause dilatation of the aortic wall. Swelling of the aortic blood vessels will affect the blood flow velocity in the aortic blood vessels. This research aims to analyze the velocity of blood flow in the Abdominal Aortic Aneurysm based on swelling diameter, proximal neck length, and aneurysm channel length using Computational Fluids Dynamics (CFD). The blood flow velocity was modeled using mathematical language based on mass continuity equations and momentum equations.  Then the finite volume method was one method to solve the mathematical model. MATLAB and ANSYS FLUENT software were used to simulate the velocity of blood flow analysis. The results of the research were shown that the larger the diameter and swelling channel length, the smaller the velocity of blood flow produced. Then, the greater the length of the proximal neck, the faster the resulting blood flow will be.",,Yes,,2025-11-11T00:15:16.543Z
oconceitodemedidaoco-2022,"O conceito de medida, o continuum e o discreto",J. Magossi; Vania Rosa Izidoro,2022,Revista Professor de Matemática On line,0,https://www.semanticscholar.org/paper/53c3ab853327a016bee163da3fc67850a9ccb03b,https://doi.org/10.21711/2319023x2022/pmo1028,10.21711/2319023x2022/pmo1028,"The word “measure” has been used throughout the history of humanity in almost every sector of human activity. It is not surprising that any changes in the way things are measured, also cause impacts in the developent of science and technologies. The refinements in the measurement criteria, in a broad sense, occur thanks to existing technologies, or they emerge from some well-defined rule, written in some language, with some scientific or practical purpose. Whereas, in a remote past, the diameter of the Earth was measured based on similarities of triangles, in modern times technologies made these measures much more precise. Even so a consensus is not yet reached, given that the mathematical continuum imposes restrictions on measurement reality. For example, there is no way to use in its fullness in laboratories, since approximations are necessary, taking into account that it is an irrational number with infinite decimal places. This characterizes a seesaw, in which, on one hand, there are the practical measures in the reality we live in, and, on the other, the theoretical measures. The goal in this article is to expose that, on one hand, from the perspective of mathematics, some examples characterize the relation between continuum and the discrete, in the measured aspect. On the other hand, we show that this relationship can indicate contradictions, in a stage of interactions between the practical and the theoretical world, if no careful reading happens. Apart from a historical digression with examples, it is shown that something similar occurs with the concept of measure when it is seen as an amount of information, called entropy by C. E. Shannon. There is also care to be taken regarding entropy, seen from the point of view of discrete models, and their extension to continuous models, differential entropy. While on the discrete side the amount of information is positive, the differential entropy, on the continuous side, can be negative, positive or arbitrarily large.",,Yes,,2025-11-11T00:15:19.325Z
olgaanontologyandlst-2022,OLGA : An Ontology and LSTM-based approach for generating Arithmetic Word Problems (AWPs) of transfer type,Suresh Kumar; P. S. Kumar,2022,arXiv.org,1,https://www.semanticscholar.org/paper/94d5aa5c39fef504e0eddf2464cb2285459fc744,https://arxiv.org/pdf/2211.12164,10.48550/arXiv.2211.12164,"Machine generation of Arithmetic Word Problems (AWPs) is challenging as they express quantities and mathematical relationships and need to be consistent. ML-solvers require a large annotated training set of consistent problems with language variations. Exploiting domain-knowledge is needed for consistency checking whereas LSTM-based approaches are good for producing text with language variations. Combining these we propose a system, OLGA, to generate consistent word problems of TC (Transfer-Case) type, involving object transfers among agents. Though we provide a dataset of consistent 2-agent TC-problems for training, only about 36% of the outputs of an LSTM-based generator are found consistent. We use an extension of TC-Ontology, proposed by us previously, to determine the consistency of problems. Among the remaining 64%, about 40% have minor errors which we repair using the same ontology. To check consistency and for the repair process, we construct an instance-specific representation (ABox) of an auto-generated problem. We use a sentence classifier and BERT models for this task. The training set for these LMs is problem-texts where sentence-parts are annotated with ontology class-names. As three-agent problems are longer, the percentage of consistent problems generated by an LSTM-based approach drops further. Hence, we propose an ontology-based method that extends consistent 2-agent problems into consistent 3-agent problems. Overall, our approach generates a large number of consistent TC-type AWPs involving 2 or 3 agents. As ABox has all the information of a problem, any annotations can also be generated. Adopting the proposed approach to generate other types of AWPs is interesting future work.",arxiv:2211.12164,Yes,,2025-11-11T00:15:16.543Z
optimlscalinglanguag-2022,OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization,S. Iyer; Xi Victoria Lin; Ramakanth Pasunuru; Todor Mihaylov; Daniel Simig; Ping Yu; Kurt Shuster; Tianlu Wang; Qing Liu; Punit Singh Koura; Xian Li; Brian O'Horo; Gabriel Pereyra; Jeff Wang; Christopher Dewan; Asli Celikyilmaz; Luke S. Zettlemoyer; Veselin Stoyanov,2022,arXiv.org,291,https://www.semanticscholar.org/paper/e965e93e76a9e6c4e4863d145b5c007b540d575d,,,"Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.",arxiv:2212.12017,Yes,,2025-11-11T00:13:07.427Z
onrealityandthelimit-2022,On Reality and the Limits of Language Data,Nigel Collier; Fangyu Liu; Ehsan Shareghi,2022,arXiv.org,6,https://www.semanticscholar.org/paper/4217467e747182b9ad8035e8a2d657d2ce80af07,http://arxiv.org/pdf/2208.11981,10.48550/arXiv.2208.11981,,,Yes,,2025-11-11T00:15:14.026Z
onsecondthoughtletsn-2022,"On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",Omar Shaikh; Hongxin Zhang; William B. Held; Michael Bernstein; Diyi Yang,2022,Annual Meeting of the Association for Computational Linguistics,229,https://www.semanticscholar.org/paper/b1b8c3e47f44158d22fb70bb453d2494ed013b70,http://arxiv.org/pdf/2212.08061,10.48550/arXiv.2212.08061,"Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model’s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.",arxiv:2212.08061,Yes,,2025-11-11T00:13:07.427Z
ontheadvanceofmaking-2022,On the Advance of Making Language Models Better Reasoners,Yifei Li; Zeqi Lin; Shizhuo Zhang; Qiang Fu; Bei Chen; Jian-Guang Lou; Weizhu Chen,2022,arXiv.org,155,https://www.semanticscholar.org/paper/760561c57f68044e2f1d089088df1da6c627b09a,http://arxiv.org/pdf/2206.02336,10.48550/arXiv.2206.02336,,,Yes,,2025-11-11T00:14:11.169Z
openingremarksofthea-2022,Opening Remarks of the AI Journey Team,,2022,Doklady. Mathematics,0,https://www.semanticscholar.org/paper/294a22d68a7caa844e1f5c5278972c4f7cb2ac51,https://doi.org/10.1134/s1064562422060254,10.1134/S1064562422060254,,,Yes,,2025-11-11T00:15:19.325Z
optimalquadraticbind-2022,Optimal Quadratic Binding for Relational Reasoning in Vector Symbolic Neural Architectures,Naoki Hiratani; H. Sompolinsky,2022,Neural Computation,6,https://www.semanticscholar.org/paper/606512e75059aa0bde761748bf8b4f349539ef19,https://arxiv.org/pdf/2204.07186,10.1162/neco_a_01558,"Abstract Binding operation is fundamental to many cognitive processes, such as cognitive map formation, relational reasoning, and language comprehension. In these processes, two different modalities, such as location and objects, events and their contextual cues, and words and their roles, need to be bound together, but little is known about the underlying neural mechanisms. Previous work has introduced a binding model based on quadratic functions of bound pairs, followed by vector summation of multiple pairs. Based on this framework, we address the following questions: Which classes of quadratic matrices are optimal for decoding relational structures? And what is the resultant accuracy? We introduce a new class of binding matrices based on a matrix representation of octonion algebra, an eight-dimensional extension of complex numbers. We show that these matrices enable a more accurate unbinding than previously known methods when a small number of pairs are present. Moreover, numerical optimization of a binding operator converges to this octonion binding. We also show that when there are a large number of bound pairs, however, a random quadratic binding performs, as well as the octonion and previously proposed binding methods. This study thus provides new insight into potential neural mechanisms of binding operations in the brain.",arxiv:2204.07186,Yes,,2025-11-11T00:14:11.169Z
optimalcentroidsmode-2022,Optimal centroids model approach for many-feature data structure prediction,Le Thi Cam Binh; Pham Van Nha,2022,Evolutionary Intelligence,0,https://www.semanticscholar.org/paper/43a6bb7e8d8915168647f309c5de189402a58b2f,,10.1007/s12065-022-00747-6,,,Yes,,2025-11-11T00:15:14.026Z
optimizationoflogist-2022,Optimization Of Logistics,,2022,,0,https://www.semanticscholar.org/paper/2661f6ad22389c7980232f05af4c520ff01613e2,,,,,Yes,,2025-11-11T00:15:19.325Z
optimizinglanguagemo-2022,Optimizing Language Models for Argumentative Reasoning,Luke Thorburn; Ariel Kruger,2022,ArgML@COMMA,15,https://www.semanticscholar.org/paper/ae3a6bbe22ea136280e2927807775b3ac8356440,,,,,Yes,,2025-11-11T00:13:07.427Z
outofdistributiongen-2022,Out-of-Distribution Generalization in Algorithmic Reasoning Through Curriculum Learning,A. Nam; Mustafa Abdool; Trevor Maxfield; James L. McClelland,2022,arXiv.org,2,https://www.semanticscholar.org/paper/a6874229b18c793baa94f72ae65bd750dcceb4a6,http://arxiv.org/pdf/2210.03275,10.48550/arXiv.2210.03275,,,Yes,,2025-11-11T00:14:11.169Z
overcomingbarriersto-2022,Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic,Mandar Sharma; N. Muralidhar; Naren Ramakrishnan,2022,arXiv.org,6,https://www.semanticscholar.org/paper/1ec4ae64ded99846bcd8a1a3b16fe2875f55c7c1,http://arxiv.org/pdf/2211.02098,10.48550/arXiv.2211.02098,"Through their transfer learning abilities, highly-parameterized large pre-trained language models have dominated the NLP landscape for a multitude of downstream language tasks. Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning. However, as we illustrate in this paper, building a general purpose language model that also happens to be proficient in mathematical reasoning is not as straight-forward as training it on a numeric dataset. In this work, we develop a novel framework that enables language models to be mathematically proficient while retaining their linguistic prowess. Specifically, we offer information-theoretic interventions to overcome the catastrophic forgetting of linguistic skills that occurs while injecting non-linguistic skills into language models.",arxiv:2211.02098,Yes,,2025-11-11T00:13:07.427Z
p3problemandmagnolia-2022,P3 problem and Magnolia language: Specializing array computations for emerging architectures,Benjamin Chetioui; Marius Kleppe Larnøy; Jaakko Järvi; M. Haveraaen; L. Mullin,2022,Frontiers of Computer Science,2,https://www.semanticscholar.org/paper/50ac3b6bb980016189c142f67e35406a2b623b08,https://www.frontiersin.org/articles/10.3389/fcomp.2022.931312/pdf,10.3389/fcomp.2022.931312,"The problem of producing portable high-performance computing (HPC) software that is cheap to develop and maintain is called the P3 (performance, portability, productivity) problem. Good solutions to the P3 problem have been achieved when the performance profiles of the target machines have been similar. The variety of HPC architectures is, however, large and can be expected to grow larger. Software for HPC therefore needs to be highly adaptable, and there is a pressing need to provide developers with tools to produce software that can target machines with vastly different profiles. Multi-dimensional array manipulation constitutes a core component of numerous numerical methods, such as finite difference solvers of Partial Differential Equations (PDEs). The efficiency of these computations is tightly connected to traversing and distributing array data in a hardware-friendly way. The Mathematics of Arrays (MoA) allows for formally reasoning about array computations and enables systematic transformations of array-based programs, e.g., to use data layouts that fit to a specific architecture. This paper presents a programming methodology aimed for tackling the P3 problem in domains that are well-explored using Magnolia, a language designed to embody generic programming. The Magnolia programmer can restrict the semantic properties of abstract generic types and operations by defining so-called axioms. Axioms can be used to produce tests for concrete implementations of specifications, for formal verification, or to perform semantics-preserving program transformations. We leverage Magnolia's semantic specification facilities to extend the Magnolia compiler with a term rewriting system. We implement MoA's transformation rules in Magnolia, and demonstrate through a case study on a finite difference solver of PDEs how our rewriting system allows exploring the space of possible optimizations.",,Yes,,2025-11-11T00:15:14.026Z
palprogramaidedlangu-2022,PAL: Program-aided Language Models,Luyu Gao; Aman Madaan; Shuyan Zhou; Uri Alon; Pengfei Liu; Yiming Yang; Jamie Callan; Graham Neubig,2022,International Conference on Machine Learning,567,https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7,http://arxiv.org/pdf/2211.10435,10.48550/arXiv.2211.10435,"Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (""few-shot prompting""). Much of this success can be attributed to prompting methods such as""chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .",arxiv:2211.10435,Yes,,2025-11-11T00:14:11.169Z
pastaadatasetformode-2022,PASTA: A Dataset for Modeling PArticipant STAtes in Narratives,Sayontan Ghosh; Mahnaz Koupaee; Isabella Chen; Francis Ferraro; Nathanael Chambers; Niranjan Balasubramanian,2022,Transactions of the Association for Computational Linguistics,6,https://www.semanticscholar.org/paper/e894fb15054d3bc9659060406a12dfd1055ae32e,https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00600/2173956/tacl_a_00600.pdf,10.1162/tacl_a_00600,"Abstract The events in a narrative are understood as a coherent whole via the underlying states of their participants. Often, these participant states are not explicitly mentioned, instead left to be inferred by the reader. A model that understands narratives should likewise infer these implicit states, and even reason about the impact of changes to these states on the narrative. To facilitate this goal, we introduce a new crowdsourced English-language, Participant States dataset, PASTA. This dataset contains inferable participant states; a counterfactual perturbation to each state; and the changes to the story that would be necessary if the counterfactual were true. We introduce three state-based reasoning tasks that test for the ability to infer when a state is entailed by a story, to revise a story conditioned on a counterfactual state, and to explain the most likely state change given a revised story. Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual).1",arxiv:2208.00329,Yes,,2025-11-11T00:15:16.541Z
pengembanganbahanaja-2022,PENGEMBANGAN BAHAN AJAR MATEMATIKA BERBASIS MULTIMODAL PADA MATERI BARISAN DAN DERET,Nurul Kurnia; Subhan Ajiz Awalludin,2022,"EduMatSains : Jurnal Pendidikan, Matematika dan Sains",0,https://www.semanticscholar.org/paper/5af2401442c0936f54daeddf04e0ea734458d4ce,http://ejournal.uki.ac.id/index.php/edumatsains/article/download/3934/2282,10.33541/edumatsains.v7i1.3934,"The purpose of this study is to develop multimodal-based mathematics teaching materials on row and series materials using assistance media in the form of Power Point, iSpring Suite 10 and Website 2 apk builder. The method used in this study is the Waterfall model. A tool used to analyze compilers using Unified Modeling Language (UML). The result of this research is an android application from the development of mathematics teaching materials on multimodal-based lineups and series materials. Multimodal is a way of learning by combining several techniques, be it sound, visual and also writing. Testing this application uses a black box test, which is testing an application, which can be tested on anyone and the android media used in this test is the Samsung Galaxy A51. Based on the test results, all the buttons in the application work as intended. Then it can be concluded that the application of mathematics to the material of rows and series can be used to train students' mathematical reasoning skills, especially in row and series materials. For this application, it is hoped that it can be developed again in various ways, be it animation, design, features, practice questions, various and more varied quiz questions and more effective programming so that it becomes more interesting.",,Yes,,2025-11-11T00:15:16.543Z
pengembanganlembarke-2022,PENGEMBANGAN LEMBAR KERJA SISWA DENGAN PENDEKATAN SAINTIFIK PADA MATERI BILANGAN KELAS VII MTs MUHAMMADIYAH 1 MALANG,Abner Alosius Ama Tondo; N. Irianti; R. Setiawan,2022,PRISMATIKA Jurnal Pendidikan dan Riset Matematika,0,https://www.semanticscholar.org/paper/fc900df9d33b59fd31907fd89636643ea05f2269,https://doi.org/10.33503/prismatika.v5i1.1885,10.33503/prismatika.v5i1.1885,"The development of LKS can help teaching and learning activities in the classroom especially during the covid-19 pandemic. Worksheets that are made in an attractive and systematic manner can help students to learn to be more active in learning mathematics. Choice of approach and model Learning is also something that plays a very important role in supporting development Interesting and systematic worksheets. The purpose of this study was to determine the feasibility of developing LKS with a scientific approach on the number material for class VII MTs Muhammadiyah 1 Malang. This research was a development research using the ADDIE development model which consists of five main stages, namely Analysis, Design, Development, Implementation, and Evaluation. The LKS developed was validated by three validators, namely two mathematics education lecturers and a seventh grade mathematics teacher. The quality of the LKS with a scientific approach that was developed was assessed from the aspects of media, language and material with an average validation result of 83.12%. The subject of the experiment was carried out on class VII A students of MTs Muhammadiyah 1 Malang with a total of 23 students. Data collection techniques were carried out by interviews, validation, tests and questionnaires. Based on the results of the small group trial, an average percentage score of 80.45% showed that the LKS was very valid, in the large group the LKS that the researchers developed got a very interesting response with an average percentage score of 81.13%. So that the LKS compiled with a scientific approach is feasible to use.",,Yes,,2025-11-11T00:15:19.325Z
pengembanganmodulmat-2022,PENGEMBANGAN MODUL MATEMATIKA DENGAN MODEL CONSTRUCTIVIST LEARNING DESIGN PADA MATERI RELASI DAN FUNGSI UNTUK KELAS VIII SMP,Silvia Kusumaningrum,2022,SCIENCE : Jurnal Inovasi Pendidikan Matematika dan IPA,1,https://www.semanticscholar.org/paper/d47e8f98efb4423f13d1ddc21ea15c53eaf9d193,https://jurnalp4i.com/index.php/science/article/download/1267/1241,10.51878/science.v2i2.1267,"This research aims to develop teaching materials in the form of mathematics modules for class VIII junior high schools. Based on the needs analysis, the material developed in this module is relation and functions using the Constructivist Learning Design (CLD) model that has six stages, namely situations, groupings, bridges, questions, exhibits, and reflections. The method used in this research is research and development. This research and development procedure consists of five stages, namely conducting needs analysis, initial product development, expert validation, small-scale field trials, and large-scale field trials and module feasibility trials for teachers. The student's math modules are developed according to the Constructivist Learning Design (CLD) model, and the language used in the modules is by with Ejaan Yang Disempurnakan (EYD). Based on the validation results of media experts, the average percentage of the entire questionnaire was 89.79%, so the category was obtained very well. The presentation of the module and the graphic design of the module are proportional. Based on the results of small-scale field trials, the average percentage of the entire questionnaire was 81.11%, so the category was obtained very well. Based on the results of large-scale field trials, the average percentage of the entire questionnaire was 82.39%, so the category was obtained very well. Students feel the benefits of modules and are interested in the use of modules. Based on the results of the module feasibility trial for teachers, the average percentage of the entire questionnaire was 93.88%, so an excellent category was obtained. The developed modules can be used and understood easily by students. Therefore, it can be concluded that the mathematics module developed meets the feasibility of being used in the learning of relation and function materials.
ABSTRAKPenelitian ini bertujuan untuk mengembangkan bahan ajar berupa modul matematika untuk kelas VIII SMP. Berdasarkan analisis kebutuhan, materi yang dikembangkan dalam modul ini adalah relasi dan fungsi dengan menggunakan model Constructivist Learning Design (CLD) yang mempunyai enam tahapan yaitu situations, groupings, bridge, questions, exhibit, dan reflections. Keenam tahapan tersebut terdapat pada bagian-bagian di dalam modul. Metode yang digunakan pada penelitian ini adalah penelitian dan pengembangan (research and development). Prosedur penelitian dan pengembangan ini terdiri dari lima tahap, yaitu melakukan analisis kebutuhan, pengembangan produk awal, validasi ahli, uji coba lapangan skala kecil, dan uji coba lapangan skala besar serta uji coba kelayakan modul kepada guru. Berdasarkan hasil validasi ahli materi dan bahasa, persentase rata-rata keseluruhan angket sebesar 89,46% maka diperoleh kategori sangat baik. Modul yang dikembangkan sesuai dengan model Constructivist Learning Design (CLD), dan bahasa yang digunakan dalam modul sesuai dengan kaidah Ejaan Yang Disempurnakan (EYD). Berdasarkan hasil validasi ahli media, persentase rata-rata keseluruhan angket sebesar 89,79% maka diperoleh kategori sangat baik. Penyajian modul dan desain grafis modul sudah proporsional. Berdasarkan hasil uji coba lapangan skala kecil, persentase rata-rata keseluruhan angket sebesar 81,11% maka diperoleh kategori sangat baik. Berdasarkan hasil uji coba lapangan skala besar, persentase rata-rata keseluruhan angket sebesar 82,39% maka diperoleh kategori sangat baik. Siswa merasakan adanya manfaat modul dan tertarik dengan adanya penggunaan modul. Berdasarkan hasil uji coba kelayakan modul kepada guru, persentase rata-rata keseluruhan angket sebesar 93,88% maka diperoleh kategori sangat baik. Modul yang dikembangkan dapat digunakan dan dipahami dengan mudah oleh siswa. Dengan demikian, dapat disimpulkan bahwa modul matematika yang dikembangkan memenuhi kelayakan untuk digunakan pada pembelajaran materi relasi dan fungsi.",,Yes,,2025-11-11T00:15:14.026Z
pengembanganmodulpem-2022,"PENGEMBANGAN MODUL PEMBELAJARAN ILMU PENGETAHUAN ALAM BERBASIS STEM (SCIENCE, TECHNOLOGY, ENGINEERING, AND MATHEMATICS) UNTUK MENINGKATKAN BERPIKIR KRITIS SISWA SD/MI",I. Anisa; Retno Triwoelandari; Yono Yono,2022,Refleksi Edukatika,2,https://www.semanticscholar.org/paper/5ec98e8c54046704edddc47d3016eb48e8069ecf,https://jurnal.umk.ac.id/index.php/RE/article/download/6840/pdf,10.24176/re.v12i2.6840,"The purpose of this study is to determine the effectiveness of developing natural science learning modules based on STEM (Science, Technology, Engineering, and Mathematics) to improve critical thinking and suitable for use for learning natural sciences in grade IV SD/MI. This research method uses research and development or is called Research and Development (RD) which refers to the ASSURE development model. The subject of this research is class IV SDIT Khoiru Ummah. This learning module goes through the stages of expert validation. The result show based on the results of the validation of the learning module, it was declared feasible to use, seen from the results of design validation, which obtained 76.31%, language validation 87.5% and material validation 71.5%. In addition, the increase in students' critical thinking was declared effective. Based on the results of the large group which was divided into 2, namely the experimental class got greater results than the control class. From the results presented, the STEM-based natural science learning module (Science, Technology, Engineering, and Mathematics) is suitable for use by fourth graders and is effective in improving critical thinking of fourth grade elementary/MI students.",,Yes,,2025-11-11T00:15:16.543Z
pengembangansoalkema-2022,PENGEMBANGAN SOAL KEMAMPUAN PENALARAN MATEMATIS UNTUK SISWA SMA,Y. Astuti; Ristontowi,2022,Jurnal Math-UMB.EDU,3,https://www.semanticscholar.org/paper/a974957e5465e8364b7ca150ecbc5b3eaeab15df,http://jurnal.umb.ac.id/index.php/math/article/download/2508/2176,10.36085/mathumbedu.v9i2.2508,"Abstrak
Tujuan penelitian ini mengembangkan soal-soal kemampuan penalaran matematis yang valid dan praktis. Metode penelitian yang digunakan research and development dengan model Tessmer (modifikasi Zulkardi, 2006) yang terdiri dari tahap preliminary, self evaluation, expert review dan one-to-one, small group dan field test. Penelitian ini hanya sampai  pada tahap small group. Subjek penelitian ini adalah 30 orang siswa kelas X. Penelitian ini menggunakan instrumen berupa dokumen, lembar validasi, lembar komentar/saran dan prototype. Kevalidan soal dilihat dari hasil analisis penilaian validator pada lembar validasi yang menyatakan soal-soal dikembangkan baik berdasarkan konstruk, konten dan bahasa. Keterbacaan soal dilihat dari tahap one-to-one. Setelah selesai pada tahap one-to-one dilanjutkan pada tahap small group yaitu uji coba prototype II kepada 30 orang siswa SMA kelas X. Penelitian  ini  menghasilkan soal-soal  penalaran  matematis yang  valid  dan  praktis.  
Kata kunci: Pengembangan Soal Kemampuan Penalaran Matematis.
 
Abstract
The purpose of this study is to develop problems of mathematical reasoning skills that are valid and practical. Research methods used research and development with tessmer model (modification Zulkardi, 2006) consisting of preliminary stages, self evaluation, expert review and one-to-one, small group and field test. This research only reached the small group stage. The subjects of this study were 30 students of class X. This study used instruments in the form of documents, validation sheets, comment sheets / suggestions and prototypes. The validity of the problem is seen from the results of the validator assessment analysis on the validation sheet that states the problems are developed both based on construct, content and language. The readability of the question is viewed from the one-to-onestage. After completion in the one-to-one stage continued in the small group stage, namely the prototype II trial to 30 students of high school class X. This research produces valid and practical mathematical reasoning problems.
Keywords: Development of Mathematical Reasoning Skills.",,Yes,,2025-11-11T00:15:19.325Z
pentatronpersonalize-2022,PENTATRON: PErsonalized coNText-Aware Transformer for Retrieval-based cOnversational uNderstanding,Niranjan Uma Naresh; Ziyan Jiang; Ankit; Sungjin Lee; Jie Hao; Xing Fan; Chenlei Guo,2022,Conference on Empirical Methods in Natural Language Processing,6,https://www.semanticscholar.org/paper/ecdee4c3e7c6a5ce0c25c4d24bbfa363e1bbb5aa,http://arxiv.org/pdf/2210.12308,10.48550/arXiv.2210.12308,"Conversational understanding is an integral part of modern intelligent devices. In a large fraction of the global traffic from customers using smart digital assistants, frictions in dialogues may be attributed to incorrect understanding of the entities in a customer's query due to factors including ambiguous mentions, mispronunciation, background noise and faulty on-device signal processing. Such errors are compounded by two common deficiencies from intelligent devices namely, (1) the device not being tailored to individual customers, and (2) the device responses being unaware of the context in the conversation session. Viewing this problem via the lens of retrieval-based search engines, we build and evaluate a scalable entity correction system, PENTATRON. The system leverages a parametric transformer-based language model to learn patterns from in-session customer-device interactions coupled with a non-parametric personalized entity index to compute the correct query, which aids downstream components in reasoning about the best response. In addition to establishing baselines and demonstrating the value of personalized and context-aware systems, we use multitasking to learn the domain of the correct entity. We also investigate the utility of language model prompts. Through extensive experiments, we show a significant upward movement of the key metric (Exact Match) by up to 500.97% (relative to the baseline).",arxiv:2210.12308,Yes,,2025-11-11T00:15:14.026Z
perancangangameeduka-2022,PERANCANGAN GAME EDUKASI “THINKING MATH” UNTUK MELATIH KEMAMPUAN PENALARAN MATEMATIS,Alaya Diwimuri; Joko Soebagyo,2022,"EduMatSains : Jurnal Pendidikan, Matematika dan Sains",1,https://www.semanticscholar.org/paper/664918791769ba61f9ece018cac3b5867d5a9cef,http://ejournal.uki.ac.id/index.php/edumatsains/article/download/3916/2279,10.33541/edumatsains.v7i1.3916,"The purpose of this study is to design and build an educational game based on android thinking math using the Kodular web-based tool with block programming language. This educational game is needed as an effort to train one's mathematical reasoning skills, and eliminate boredom and fear for those who want to learn mathematics. The results of five studies related to mathematics educational games, generally stated that mathematics educational games could increase one's interest in learning mathematics which were used effectively and interestingly. The method used is the Waterfall educational game development method. The tool used to analyze the compiler uses the Unified Modeling Language (UML). The result of the study is an Android-based math learning game. Based on the test results, it can be concluded that the implementation of the thinking math educational game to train mathematical reasoning skills was successfully carried out. For this educational game itself, it is hoped that it can be further developed in terms of animation, design, features, music, various quiz questions with more varied subject matter and more effective programming so that it becomes more interesting.",,Yes,,2025-11-11T00:15:16.543Z
pp88bayesianjointmod-2022,"PP88 Bayesian Joint Models For Cost-Effectiveness Analyses Based On Clustered Participant Data, With Implementation In Stan",Jonas Esser; Anita Varga; M. E. Alili; J. V. van Dongen; J. Bosmans,2022,International Journal of Technology Assessment in Health Care,0,https://www.semanticscholar.org/paper/e041df5638268762afd2ace56130263b631b7020,https://www.cambridge.org/core/services/aop-cambridge-core/content/view/9F40AB69AE90642F1BD4ECBFE1AEAC9B/S026646232200215Xa.pdf/div-class-title-pp88-bayesian-joint-models-for-cost-effectiveness-analyses-based-on-clustered-participant-data-with-implementation-in-stan-div.pdf,10.1017/S026646232200215X,"Introduction Cost-effectiveness analyses of empirical participant data are frequently complicated by irregularly distributed and correlated observations, which are not well approximated by normal distributions. Things get even more difficult when observations are clustered within higher level units (for example, hospitals) or the participant (that is, multiple measurements at different timepoints). Therefore, we developed a flexible Bayesian approach to jointly model costs and effects of two competing interventions with a multilevel structure. Methods Our new model is presented in mathematical form and discussed in detail. We model costs and Quality-Adjusted Life-Years effects through Gamma and Beta distributions, and account for the dependency between costs and effects by adding the effects as a predictor for the costs. We further include hurdle models to account for costs of for the presence of zero costs and perfect health scores. The full model is implemented in the probabilistic programming language Stan. To compare the performance of our Bayesian model to a frequentist approach (linear mixed model combined with bootstrapping), we simulate 1000 datasets consisting of 400 participants and 20 clusters. Performance of both models is assessed in terms of variance, bias and coverage probability with respect to the costs and effects defined in the simulation. Results We ran a preliminary simulation with high intraclass correlation, strong negative correlation for patient-level costs and effects, and positive correlation of cluster effects on both outcomes. The analysis shows that the Bayesian model exhibits a slightly larger bias for estimated costs, but smaller errors and higher coverage probability compared to the frequentist alternative. We will explore different scenarios where we vary the parameters of the simulations and assess whether the results are robust to change. Conclusions It is very important that economic evaluations in health care produce precise and reliable results. Our Bayesian approach is able to handle multiple statistical complexities at once and performs better than a comparable frequentist model. Whether this conclusion holds for different simulation scenarios will be explored in further stages of this study.",,Yes,,2025-11-11T00:15:16.543Z
palmscalinglanguagem-2022,PaLM: Scaling Language Modeling with Pathways,A. Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; P. Barham; Hyung Won Chung; Charles Sutton; Sebastian Gehrmann; Parker Schuh; Kensen Shi; Sasha Tsvyashchenko; Joshua Maynez; Abhishek Rao; Parker Barnes; Yi Tay; Noam M. Shazeer; Vinodkumar Prabhakaran; Emily Reif; Nan Du; Ben Hutchinson; Reiner Pope; James Bradbury; Jacob Austin; M. Isard; Guy Gur-Ari; Pengcheng Yin; Toju Duke; Anselm Levskaya; S. Ghemawat; Sunipa Dev; H. Michalewski; Xavier García; Vedant Misra; Kevin Robinson; L. Fedus; Denny Zhou; Daphne Ippolito; D. Luan; Hyeontaek Lim; Barret Zoph; A. Spiridonov; Ryan Sepassi; David Dohan; Shivani Agrawal; Mark Omernick; Andrew M. Dai; Thanumalayan Sankaranarayana Pillai; Marie Pellat; Aitor Lewkowycz; Erica Moreira; R. Child; Oleksandr Polozov; Katherine Lee; Zongwei Zhou; Xuezhi Wang; Brennan Saeta; Mark Díaz; Orhan Firat; Michele Catasta; Jason Wei; K. Meier-Hellstern; D. Eck; J. Dean; Slav Petrov; Noah Fiedel,2022,Journal of machine learning research,7113,https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb,,,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",arxiv:2204.02311,Yes,,2025-11-11T00:13:07.427Z
parselaunifiednatura-2022,Parsel: A Unified Natural Language Framework for Algorithmic Reasoning,E. Zelikman; Qian Huang; Gabriel Poesia; Noah D. Goodman; Nick Haber,2022,arXiv.org,16,https://www.semanticscholar.org/paper/239b5649b12f28fd610de036afba41b9246db6c9,http://arxiv.org/pdf/2212.10561,10.48550/arXiv.2212.10561,,,Yes,,2025-11-11T00:13:07.427Z
parselalgorithmicrea-2022,Parsel🦆: Algorithmic Reasoning with Language Models by Composing Decompositions,E. Zelikman; Qian Huang; Gabriel Poesia; Noah D. Goodman; Nick Haber,2022,Neural Information Processing Systems,67,https://www.semanticscholar.org/paper/e325fe41c8c1d547ccd102ac82be3ec8b23960f2,,,"Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\% to 85\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel",arxiv:2212.10561,Yes,,2025-11-11T00:13:07.427Z
pathlanguagemodeling-2022,Path Language Modeling over Knowledge Graphsfor Explainable Recommendation,Shijie Geng; Zuohui Fu; Juntao Tan; Yingqiang Ge; Gerard de Melo; Yongfeng Zhang,2022,The Web Conference,80,https://www.semanticscholar.org/paper/75f4423820a6d2de93535fda5f80e17ae051dc47,,10.1145/3485447.3511937,"To facilitate human decisions with credible suggestions, personalized recommender systems should have the ability to generate corresponding explanations while making recommendations. Knowledge graphs (KG), which contain comprehensive information about users and products, are widely used to enable this. By reasoning over a KG in a node-by-node manner, existing explainable models provide a KG-grounded path for each user-recommended item. Such paths serve as an explanation and reflect the historical behavior pattern of the user. However, not all items can be reached following the connections within the constructed KG under finite hops. Hence, previous approaches are constrained by a recall bias in terms of existing connectivity of KG structures. To overcome this, we propose a novel Path Language Modeling Recommendation (PLM-Rec) framework, learning a language model over KG paths consisting of entities and edges. Through path sequence decoding, PLM-Rec unifies recommendation and explanation in a single step and fulfills them simultaneously. As a result, PLM-Rec not only captures the user behaviors but also eliminates the restriction to pre-existing KG connections, thereby alleviating the aforementioned recall bias. Moreover, the proposed technique makes it possible to conduct explainable recommendation even when the KG is sparse or possesses a large number of relations. Experiments and extensive ablation studies on three Amazon e-commerce datasets demonstrate the effectiveness and explainability of the PLM-Rec framework.",,Yes,,2025-11-11T00:14:11.169Z
pengaruhketerampilan-2022,Pengaruh Keterampilan Bahasa Guru terhadap Penalaran Siswa,W. Wahyuni,2022,Jurnal Ilmiah Pendidikan Matematika Al Qalasadi,0,https://www.semanticscholar.org/paper/a94e6b475cbd2d110cc65e76283cbcf7b1e20812,https://journal.iainlangsa.ac.id/index.php/qalasadi/article/download/4970/2338,10.32505/qalasadi.v6i2.4970,"Understanding the vocabulary that is part of the body of language contributes significantly to overall understanding in many areas of content, such as mathematical reasoning. Teachers are expected to be able to pay attention to the way children communicate their reasoning in order to respond appropriately to improve children's reasoning and communication in their mathematical thinking. This study explores specifically the effects on teachers' language skills in demonstration lessons while teaching mathematics in the classroom and the learning objectives of improving students' reasoning ability. The study involved teachers from three different schools in Langsa City. The large number of teachers does not seem to pay special attention in the course of demonstrations of the broader pattern of justification-based discourse being formed. Only with the introduction and understanding of these specific difficulties can teachers then begin to address the instructional needs of their students from a language perspective. Designing and delivering vocabulary instructions effectively is a necessary action. Failure to design, convey, and understand children's language will have an effect on the growth of their mathematical alignment. In an effort to improve the overall mathematical performance of students, teachers need to recognize the importance of language to children's mathematical reasoning.",,Yes,,2025-11-11T00:15:16.543Z
perspectivesandchall-2022,Perspectives and Challenges of AI Techniques in the Field of Social Sciences and Communication,Raúl Ramos Pollán,2022,Journal of Autonomous Intelligence,1,https://www.semanticscholar.org/paper/ef3721038c3bf4b3924f0ad2f5b4722523f82f85,https://doi.org/10.32629/jai.v5i1.504,10.32629/jai.v5i1.504,"In the past decade, the methods and technologies of artificial intelligence (AI) have made great progress. In many cases, they have become part of the usual landscape of solving new or old problems in different fields of human knowledge. In this progress, there are several aspects, especially three aspects: the availability and universality of data in many fields of human activities; a deeper understanding of the mathematics of the basic control algorithm; and the availability and capability of hardware and computing which allows a wide range and a large number of data experiments. Considering these aspects, the key challenge for each problem and application area is to understand how to use these technologies, to what extent they may reach, and what constraints need to be overcome in order to obtain beneficial results (in terms of production cost, value, etc.). This challenge includes identifying data sources and their integration and recovery requirements, the necessity and cost of acquiring or constructing tag data sets, volume data required for measurement, verifying its feasibility, technical method of data analysis task and its consistency with the final application goal, and social and communication sciences are no exception. The knowledge in these fields is related to artificial intelligence, but they do have particularities that define the most appropriate type of artificial intelligence technology and method (i.e. natural language processing). The successful use of AI technology in these disciplines involves not only technical knowledge, but also the establishment of a viable application environment, including the availability of data, the appropriate complexity of tasks to be performed, and verification procedures with experts in the field. This paper introduces the methodology of generating artificial intelligence model, summarizes the artificial intelligence methods and services most likely to be used in social and communication sciences, and finally gives some application examples to illustrate the practical and technical considerations in this regard.",,Yes,,2025-11-11T00:15:19.325Z
planbenchanextensibl-2022,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,Karthik Valmeekam; Alberto Olmo; S. Sreedharan; Subbarao Kambhampati,2022,Neural Information Processing Systems,306,https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc,,,"Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.",arxiv:2206.10498,Yes,,2025-11-11T00:13:07.427Z
plansformergeneratin-2022,Plansformer: Generating Symbolic Plans using Transformers,Vishal Pallagani; Bharath Muppasani; K. Murugesan; F. Rossi; L. Horesh; Biplav Srivastava; F. Fabiano; A. Loreggia,2022,arXiv.org,44,https://www.semanticscholar.org/paper/ae441f7305dc2cd58c708528b3ecee3501cc5c46,http://arxiv.org/pdf/2212.08681,10.48550/arXiv.2212.08681,"Large Language Models (LLMs) have been the subject of active research, significantly advancing the field of Natural Language Processing (NLP). From BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural language tasks such as question answering, summarization, and text generation. Many ongoing efforts focus on understanding LLMs' capabilities, including their knowledge of the world, syntax, and semantics. However, extending the textual prowess of LLMs to symbolic reasoning has been slow and predominantly focused on tackling problems related to the mathematical field. In this paper, we explore the use of LLMs for automated planning - a branch of AI concerned with the realization of action sequences (plans) to achieve a goal, typically executed by intelligent agents, autonomous robots, and unmanned vehicles. We introduce Plansformer; an LLM fine-tuned on planning problems and capable of generating plans with favorable behavior in terms of correctness and length with reduced knowledge-engineering efforts. We also demonstrate the adaptability of Plansformer in solving different planning domains with varying complexities, owing to the transfer learning abilities of LLMs. For one configuration of Plansformer, we achieve ~97% valid plans, out of which ~95% are optimal for Towers of Hanoi - a puzzle-solving domain.",arxiv:2212.08681,Yes,,2025-11-11T00:15:14.026Z
plugandplayvqazerosh-2022,Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training,A. Tiong; Junnan Li; Boyang Albert Li; S. Savarese; S. Hoi,2022,Conference on Empirical Methods in Natural Language Processing,121,https://www.semanticscholar.org/paper/26fd105d0b5a458979c012cddb3ba2de943388c4,http://arxiv.org/pdf/2210.08773,10.48550/arXiv.2210.08773,"Visual question answering (VQA) is a hallmark of vision and language reasoning and a challenging task under the zero-shot setting. We propose Plug-and-Play VQA (PNP-VQA), a modular framework for zero-shot VQA. In contrast to most existing works, which require substantial adaptation of pretrained language models (PLMs) for the vision modality, PNP-VQA requires no additional training of the PLMs. Instead, we propose to use natural language and network interpretation as an intermediate representation that glues pretrained models together. We first generate question-guided informative image captions, and pass the captions to a PLM as context for question answering. Surpassing end-to-end trained baselines, PNP-VQA achieves state-of-the-art results on zero-shot VQAv2 and GQA. With 11B parameters, it outperforms the 80B-parameter Flamingo model by 8.5% on VQAv2. With 738M PLM parameters, PNP-VQA achieves an improvement of 9.1% on GQA over FewVLM with 740M PLM parameters. Code is released at https://github.com/salesforce/LAVIS/tree/main/projects/pnp-vqa",arxiv:2210.08773,Yes,,2025-11-11T00:14:11.169Z
powersystemterminalc-2022,Power system terminal continuous trust evaluation model based on fine-grained data flow analysis,Ming Xie,2022,Other Conferences,0,https://www.semanticscholar.org/paper/20947d6808a8b14b3fa8cf25f8efd366c07fe081,,10.1117/12.2626908,"With the wide application of new power services, the continuous strengthening of plant network coordination and interaction, resulting in a large extension of data network, network security protection is more difficult. We propose a power system terminal continuous trust evaluation model based on fine-grained data flow analysis, which effectively solves the problem of weak anti-jamming of traditional trust evaluation and unstable trust evaluation results through the analysis of the context behavior of the access subject, evidence reasoning, and identification of intent of confidence propagation. Innovative application of natural language processing (NLP) technology to Web application traffic intrusion detection, multi-level, multi-grained traffic depth analysis, dynamic intelligent correlation and drilling analysis for network traffic data, reduce the traditional feature-based and reputation detection technology leakage rate, the location, tracking and traceability of abnormal traffic, the accuracy of the detection results reached 96.59 percent.",,Yes,,2025-11-11T00:15:14.026Z
practicalmaththirded-2022,Practical Math Third Edition A Answers,,2022,,0,https://www.semanticscholar.org/paper/e6effd2e3f6a98e35bf8b3d2e2a14df0437d5218,,,,,Yes,,2025-11-11T00:15:19.325Z
predictivemaintenanc-2022,Predictive Maintenance Beyond Prediction Of Failures,,2022,,0,https://www.semanticscholar.org/paper/cbd373cb11d8a7549f1e100f2ce910013cf1177e,,,,,Yes,,2025-11-11T00:15:19.325Z
predictivequeryingfo-2022,Predictive Querying for Autoregressive Neural Sequence Models,Alex Boyd; Samuel Showalter; S. Mandt; Padhraic Smyth,2022,Neural Information Processing Systems,4,https://www.semanticscholar.org/paper/de7d334a543d077f4162ebcd8da7eee843b7b10a,http://arxiv.org/pdf/2210.06464,10.48550/arXiv.2210.06464,"In reasoning about sequential events it is natural to pose probabilistic queries such as""when will event A occur next""or""what is the probability of A occurring before B"", with applications in areas such as user modeling, medicine, and finance. However, with machine learning shifting towards neural autoregressive models such as RNNs and transformers, probabilistic querying has been largely restricted to simple cases such as next-event prediction. This is in part due to the fact that future querying involves marginalization over large path spaces, which is not straightforward to do efficiently in such models. In this paper we introduce a general typology for predictive queries in neural autoregressive sequence models and show that such queries can be systematically represented by sets of elementary building blocks. We leverage this typology to develop new query estimation methods based on beam search, importance sampling, and hybrids. Across four large-scale sequence datasets from different application domains, as well as for the GPT-2 language model, we demonstrate the ability to make query answering tractable for arbitrary queries in exponentially-large predictive path-spaces, and find clear differences in cost-accuracy tradeoffs between search and sampling methods.",arxiv:2210.06464,Yes,,2025-11-11T00:15:19.325Z
principledapproaches-2022,Principled Approaches Applications Interpretability Robustness Efficiency Deep Learning Systems Natural Language Processing Computer Vision Mathematical Modeling,T. Nguyen,2022,,0,https://www.semanticscholar.org/paper/56f32a568a3be4122c3399d79803c4976246d45b,,,,,Yes,,2025-11-11T00:14:11.169Z
proqastructuralpromp-2022,ProQA: Structural Prompt-based Pre-training for Unified Question Answering,Wanjun Zhong; Yifan Gao; Ning Ding; Yujia Qin; Zhiyuan Liu; Ming Zhou; Jiahai Wang; Jian Yin; Nan Duan,2022,North American Chapter of the Association for Computational Linguistics,37,https://www.semanticscholar.org/paper/c963c505ffc4cc8b33315eb967784d0a466b3910,http://arxiv.org/pdf/2205.04040,10.48550/arXiv.2205.04040,"Question Answering (QA) is a longstanding challenge in natural language processing. Existing QA works mostly focus on specific question types, knowledge domains, or reasoning skills. The specialty in QA research hinders systems from modeling commonalities between tasks and generalization for wider applications. To address this issue, we present ProQA, a unified QA paradigm that solves various tasks through a single model. ProQA takes a unified structural prompt as the bridge and improves the QA-centric ability by structural prompt-based pre-training. Through a structurally designed prompt-based input schema, ProQA concurrently models the knowledge generalization for all QA tasks while keeping the knowledge customization for every specific QA task. Furthermore, ProQA is pre-trained with structural prompt-formatted large-scale synthesized corpus, which empowers the model with the commonly-required QA ability. Experimental results on 11 QA benchmarks demonstrate that ProQA consistently boosts performance on both full data fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore, ProQA exhibits strong ability in both continual learning and transfer learning by taking the advantages of the structural prompt.",arxiv:2205.04040,Yes,,2025-11-11T00:15:14.026Z
probingcommonsensekn-2022,Probing Commonsense Knowledge in Pre-trained Language Models with Sense-level Precision and Expanded Vocabulary,Daniel Loureiro; A. Jorge,2022,arXiv.org,1,https://www.semanticscholar.org/paper/3cd9e5de457662f5e3c268f75341a93a16254e55,http://arxiv.org/pdf/2210.06376,10.48550/arXiv.2210.06376,"Progress on commonsense reasoning is usually measured from performance improvements on Question Answering tasks designed to require commonsense knowledge. However, fine-tuning large Language Models (LMs) on these specific tasks does not directly evaluate commonsense learned during pre-training. The most direct assessments of commonsense knowledge in pre-trained LMs are arguably cloze-style tasks targeting commonsense assertions (e.g., A pen is used for [MASK].). However, this approach is restricted by the LM's vocabulary available for masked predictions, and its precision is subject to the context provided by the assertion. In this work, we present a method for enriching LMs with a grounded sense inventory (i.e., WordNet) available at the vocabulary level, without further training. This modification augments the prediction space of cloze-style prompts to the size of a large ontology while enabling finer-grained (sense-level) queries and predictions. In order to evaluate LMs with higher precision, we propose SenseLAMA, a cloze-style task featuring verbalized relations from disambiguated triples sourced from WordNet, WikiData, and ConceptNet. Applying our method to BERT, producing a WordNet-enriched version named SynBERT, we find that LMs can learn non-trivial commonsense knowledge from self-supervision, covering numerous relations, and more effectively than comparable similarity-based approaches.",arxiv:2210.06376,Yes,,2025-11-11T00:15:14.026Z
problematicissuesofm-2022,Problematic issues of management of the Federal fire service and other types of fire protection,K. Vlasov,2022,Technology of technosphere safety,0,https://www.semanticscholar.org/paper/8695da5df3ef8c9452cbc98ee31092d3b98eef0f,,10.25257/tts.2022.3.97.131-143,"Introduction. The activities of fire and rescue units can be conditionally divided into extinguishing a large number of ordinary fires and single large fires. Considering these categories of fires from the point of view of economic efficiency and ensuring the level of combat readiness of fire departments, a decision tree model is proposed to identify the most promising ways to develop the organization. The functioning of fire and rescue units within the garrison is studied from the standpoint of assessing the long-term cyclical development of the organization, taking into account various external and internal factors. The tasks of the research are to identify problematic issues of management of fire and rescue units of the Federal Fire Service and fire protection units of other types stationed on the territory of the fire and rescue garrison, taking into account the scale of fires and evaluating the effectiveness of the organization of activities. Methods. Big Data technologies based on the methods of mathematical statistics of the software modules Panda and NumPy of the high-level programming language Python 3 are applied. The ""Decision Tree"" method implemented by means of the sklearn Python module and the packages CHAID and rpart of the programming language for statistical data processing R is used. Results and discussion. The frequency of occurrence of large fires, when the involvement of almost all available forces and means of the fire and rescue garrison is required, is about one case for an interval of 4-5 years. The number of ordinary fires during the same time is approximately 100-130 thousand cases. Such a ratio of ordinary and large fires is a source of significant contradiction and leads to a number of problematic issues related to the organization of management of Federal Fire Service units, as well as units of other types of fire protection. To study the proposed issues in the context of comparing the categories of economic efficiency and the level of combat readiness of units, the ""decision tree"" method was applied. Conclusions. The analysis of the results of the functioning of the fire and rescue garrison at the limit of permissible possibilities arising in the process of eliminating a large fire shows that all the management structures of the garrison are affected and, based on the results of the study of the actions of the garrison units, objective prerequisites for a leap (qualitative) reform of the management system can be formed. Keywords: large fire, operational activity, busy time, mobile fire and rescue equipment, fire extinguishing devices, histogram, decision tree.",,Yes,,2025-11-11T00:15:16.543Z
programyesworkshop20-2022,"Program YES Workshop 2022 Optimal Transport, Statistics, Machine Learning and moving in between",G. Jin; Valentina Masarotto,2022,,0,https://www.semanticscholar.org/paper/8cde92023b267ed8490ce125795a3979cdfd624f,,,,,Yes,,2025-11-11T00:15:19.325Z
projectbasedlearning-2022,Project-Based Learning: Rewards and Challenges,Linda Astriani; Sasnia Akmalia,2022,Book of Proceedings 2022,5,https://www.semanticscholar.org/paper/bb619e58a3c425497bfed0172bf7d23d6e386606,,10.23918/vesal2022a4,"The purpose of this study is to evaluate the accuracy and usefulness of the project-based learning-based spatial and statistical modules. In this study, the five steps of the ADDIE development model-Analysis, Design, Development, Implementation, and Evaluation are used. The study's participants were 5th graders at SD Negeri 1 Cisantana in the Kuningan Regency's Cigugur District during the academic year 2021–2022. Analyses of descriptive, qualitative, and quantitative data were used in data gathering approaches. The study's findings show that 1) the language validity test is very valid, scoring an average of 96%; 2) the material validity test is very valid, scoring an average of 96%; 3) the media validity test is very valid, scoring an average of 98%; and 4) the practicality test is very valid, scoring an average of 98%. 3) The media validity test has a very high average score of 98%, however the practicality test has a lower average score. 4) The teacher's results of the practicality test received a score of 96% in the very practical area, 5) the small group's results of the practicality test received a score of 96% in the the practical category, and 6) the large group's results of the practicality test received a score of 97% in the very practical category. As a result, it can be said that the Spatial Structure and Statistics Module, which was created utilizing the ADDIE model, is legitimate and useful for use in the even semester 5th grade elementary school mathematics curriculum.",,Yes,,2025-11-11T00:15:19.325Z
promptandrerankameth-2022,Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models,Mirac Suzgun; Luke Melas-Kyriazi; Dan Jurafsky,2022,Conference on Empirical Methods in Natural Language Processing,70,https://www.semanticscholar.org/paper/0d6bb585493e34975f0437faa3179db3a02f6ae8,https://arxiv.org/pdf/2205.11503,10.48550/arXiv.2205.11503,"We propose a method for arbitrary textual style transfer (TST)—the task of transforming a text into any given style—utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent components: textual similarity, target style strength, and fluency. Our method uses zero-shot or few-shot prompting to obtain a set of candidate generations in the target style, and then re-ranks them according to the three components. Our method enables small pre-trained language models to perform on par with state-of-the-art large-scale models while using two orders of magnitude less compute and memory. We also investigate the effect of model size and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on style transfer quality across seven diverse textual style transfer datasets, finding, among other things, that delimiter-pair choice has a large impact on performance, and that models have biases on the direction of style transfer.",arxiv:2205.11503,Yes,,2025-11-11T00:14:11.169Z
promptbasedconservat-2022,Prompt-based Conservation Learning for Multi-hop Question Answering,Zhenyun Deng; Yonghua Zhu; Yang Chen; Qianqian Qi; M. Witbrock; P. Riddle,2022,International Conference on Computational Linguistics,5,https://www.semanticscholar.org/paper/432b1611029cb9c8ff7a632bcef0f47f0b879004,http://arxiv.org/pdf/2209.06923,10.48550/arXiv.2209.06923,"Multi-hop question answering (QA) requires reasoning over multiple documents to answer a complex question and provide interpretable supporting evidence. However, providing supporting evidence is not enough to demonstrate that a model has performed the desired reasoning to reach the correct answer. Most existing multi-hop QA methods fail to answer a large fraction of sub-questions, even if their parent questions are answered correctly. In this paper, we propose the Prompt-based Conservation Learning (PCL) framework for multi-hop QA, which acquires new knowledge from multi-hop QA tasks while conserving old knowledge learned on single-hop QA tasks, mitigating forgetting. Specifically, we first train a model on existing single-hop QA tasks, and then freeze this model and expand it by allocating additional sub-networks for the multi-hop QA task. Moreover, to condition pre-trained language models to stimulate the kind of reasoning required for specific multi-hop questions, we learn soft prompts for the novel sub-networks to perform type-specific reasoning. Experimental results on the HotpotQA benchmark show that PCL is competitive for multi-hop QA and retains good performance on the corresponding single-hop sub-questions, demonstrating the efficacy of PCL in mitigating knowledge loss by forgetting.",arxiv:2209.06923,Yes,,2025-11-11T00:15:16.541Z
promptcappromptguide-2022,PromptCap: Prompt-Guided Task-Aware Image Captioning,Yushi Hu; Hang Hua; Zhengyuan Yang; Weijia Shi; Noah A. Smith; Jiebo Luo,2022,arXiv.org,119,https://www.semanticscholar.org/paper/a5cb8f26acb71edd77ff9a143d3ddaab2367eb40,https://arxiv.org/pdf/2211.09699,10.48550/arXiv.2211.09699,"Knowledge-based visual question answering (VQA) involves questions that require world knowledge beyond the image to yield the correct answer. Large language models (LMs) like GPT-3 are particularly helpful for this task because of their strong knowledge retrieval and reasoning capabilities. To enable LM to understand images, prior work uses a captioning model to convert images into text. However, when summarizing an image in a single caption sentence, which visual entities to describe are often underspecified. Generic image captions often miss visual details essential for the LM to answer visual questions correctly. To address this challenge, we propose PromptCap (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs. Different from generic captions, PromptCap takes a natural-language prompt to control the visual entities to describe in the generated caption. The prompt contains a question that the caption should aid in answering. To avoid extra annotation, PromptCap is trained by examples synthesized with GPT-3 and existing datasets. We demonstrate PromptCap's effectiveness on an existing pipeline in which GPT-3 is prompted with image captions to carry out VQA. PromptCap outperforms generic captions by a large margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks (60.4% on OK-VQA and 59.6% on A-OKVQA). Zero-shot results on WebQA show that PromptCap generalizes well to unseen domains.",arxiv:2211.09699,Yes,,2025-11-11T00:15:14.026Z
promptinggpt3toberel-2022,Prompting GPT-3 To Be Reliable,Chenglei Si; Zhe Gan; Zhengyuan Yang; Shuohang Wang; Jianfeng Wang; Jordan L. Boyd-Graber; Lijuan Wang,2022,International Conference on Learning Representations,327,https://www.semanticscholar.org/paper/c8d594f09413b1555970f43e68847c211235d60f,http://arxiv.org/pdf/2210.09150,10.48550/arXiv.2210.09150,"Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.",arxiv:2210.09150,Yes,,2025-11-11T00:15:14.026Z
proofnetabenchmarkfo-2022,ProofNet: A Benchmark for Autoformalizing and Formally Proving Undergraduate-Level Mathematics Problems,Zhangir Azerbayev; Bartosz Piotrowski; J. Avigad,2022,,14,https://www.semanticscholar.org/paper/d9c05c32b7935dc8f7a048f79c2ce2f45558ddc8,,,,,Yes,,2025-11-11T00:15:16.543Z
propositionalreasoni-2022,Propositional Reasoning via Neural Transformer Language Models,Oscar J. Romero; A. Tomasic; A. Steinfeld; John Zimmerman,2022,International Workshop on Neural-Symbolic Learning and Reasoning,3,https://www.semanticscholar.org/paper/ca68b7b6a6f062da58453a48898e1f14b4200a27,,,,,Yes,,2025-11-11T00:13:07.427Z
quant40engineeringqu-2022,"Quant 4.0: engineering quantitative investment with automated, explainable, and knowledge-driven artificial intelligence",Jian Guo; Sai Wang; L. Ni; H. Shum,2022,Frontiers of Information Technology & Electronic Engineering,12,https://www.semanticscholar.org/paper/497a1accfd0be6cad1be4f2b6fa88078dae7414a,,10.1631/FITEE.2300720,"Quantitative investment (abbreviated as “quant” in this paper) is an interdisciplinary field combining financial engineering, computer science, mathematics, statistics, etc. Quant has become one of the mainstream investment methodologies over the past decades, and has experienced three generations: quant 1.0, trading by mathematical modeling to discover mis-priced assets in markets; quant 2.0, shifting the quant research pipeline from small “strategy workshops” to large “alpha factories”; quant 3.0, applying deep learning techniques to discover complex nonlinear pricing rules. Despite its advantage in prediction, deep learning relies on extremely large data volume and labor-intensive tuning of “black-box” neural network models. To address these limitations, in this paper, we introduce quant 4.0 and provide an engineering perspective for next-generation quant. Quant 4.0 has three key differentiating components. First, automated artificial intelligence (AI) changes the quant pipeline from traditional hand-crafted modeling to state-of-the-art automated modeling and employs the philosophy of “algorithm produces algorithm, model builds model, and eventually AI creates AI.” Second, explainable AI develops new techniques to better understand and interpret investment decisions made by machine learning black boxes, and explains complicated and hidden risk exposures. Third, knowledge-driven AI supplements data-driven AI such as deep learning and incorporates prior knowledge into modeling to improve investment decisions, in particular for quantitative value investing. Putting all these together, we discuss how to build a system that practices the quant 4.0 concept. We also discuss the application of large language models in quantitative finance. Finally, we propose 10 challenging research problems for quant technology, and discuss potential solutions, research directions, and future trends.",arxiv:2301.04020,Yes,,2025-11-11T00:15:19.325Z
quantitativetradingm-2022,Quantitative Trading Method based on Neural Network Machine Learning,W.-S. Weng,2022,"2022 Asia Conference on Algorithms, Computing and Machine Learning (CACML)",0,https://www.semanticscholar.org/paper/bbf0ed0223e9faa16789a2ca0f1439517145d339,,10.1109/CACML55074.2022.00107,"Quantitative trading plays an essential role in the investment field with its advanced mathematical models for computer-aided trading of investment strategies. The artificial neural network algorithm is the trading algorithm with the largest amount of funds managed in the world. Due to the short history of quantitative trading research in China, large-scale funds have not been reported to be managed by the neural network algorithm. The results of tests on financial derivatives using neural networks with different structures demonstrate that the neural network strategies all have positive expected return. Within a considerable range of changes in structure. In this paper, the python language is majorly used to design a model implementation plan for a quantitative trading system reading currently widely recognized stock technical indicators, such as MA, MACD, KDJ, and BOLL. Additionally, position management strategies are optimized. Furthermore, a quantitative trading method based on neural network machine learning is constructed and verified with examples.",,Yes,,2025-11-11T00:15:16.543Z
quantitativeandmetri-2022,"Quantitative and Metric Rewriting: Abstract, Non-Expansive, and Graded Systems",Francesco Gavazzo; Cecilia Di Florio,2022,arXiv.org,4,https://www.semanticscholar.org/paper/aac084f99fcc73b4afe9f80d20c3da8fd1ddfd0d,https://arxiv.org/pdf/2206.13610,10.48550/arXiv.2206.13610,"Modern mathematics begins with symbolic manipulation. The central role of signs and symbols per se is one of the main achievement of the Medieval culture [88] leading, among others, to the development of elementary or symbolic algebra. Starting from the latter, the syntactic manipulation of symbols more or less independently of their meaning — i.e. to what symbols stand for — has become an essential part of mathematical reasoning, not to say of reasoning in general. Today, symbolic manipulation is not just a pillar of mathematics, but it is at the very hearth of computation. Indeed, the symbolic manipulations of elementary algebra carry a computational content and, vice versa, computational processes can be fully described symbolically. Rewriting theory [94, 25] is the discipline that studies (the computational content of) symbolic manipulation in general. As such, rewriting has its origin both in symbolic algebra as the study of the algorithmic properties of equational reasoning, and in computability and programming language theory, where rewriting systems have been used to define symbolic models of computation — such as the _-calculus [17] and combinatory logic [34, 74] — as well as the (operational) semantics and implementation of programming languages [7]. In both cases, rewriting is motivated by the need to define operational notions of equality revealing the computational content of equational deductions. Remarkably, operationality is ultimately achieved by making equality asymmetric, so that the aforementioned computational content can be fully uncovered by orienting equations. Nowadays, these oriented equations (and the evolution thereof) are known as rewriting — or reduction — relations. All of that highlights a crucial trait of rewriting theory, namely its deep connection with equational reasoning. In fact, rewriting does not actually focus on arbitrary symbolic transformations, but with equality-preserving ones: a rewriting relation refines equality by making the latter operational, and it is thus contained in it.",arxiv:2206.13610,Yes,,2025-11-11T00:15:16.541Z
rarrresearchingandre-2022,"RARR: Researching and Revising What Language Models Say, Using Language Models",Luyu Gao; Zhuyun Dai; Panupong Pasupat; Anthony Chen; Arun Tejasvi Chaganty; Yicheng Fan; Vincent Zhao; N. Lao; Hongrae Lee; Da-Cheng Juan; Kelvin Guu,2022,Annual Meeting of the Association for Computational Linguistics,273,https://www.semanticscholar.org/paper/66242baf48b0f6b828e7547ac39ffaa5e1b2cb3e,https://aclanthology.org/2023.acl-long.910.pdf,10.18653/v1/2023.acl-long.910,"Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search.",arxiv:2210.08726,Yes,,2025-11-11T00:14:11.169Z
revelframeworktomeas-2022,REVEL Framework to measure Local Linear Explanations for black-box models: Deep Learning Image Classification case of study,Iván Sevillano-García; Juli'an Luengo-Mart'in; Francisco Herrera,2022,International Journal of Intelligent Systems,9,https://www.semanticscholar.org/paper/f1a48aa95cdbe3e54e381e3a20bed981061fcc86,https://downloads.hindawi.com/journals/ijis/2023/8068569.pdf,10.1155/2023/8068569,"Explainable artificial intelligence is proposed to provide explanations for reasoning performed by artificial intelligence. There is no consensus on how to evaluate the quality of these explanations, since even the definition of explanation itself is not clear in the literature. In particular, for the widely known local linear explanations, there are qualitative proposals for the evaluation of explanations, although they suffer from theoretical inconsistencies. The case of image is even more problematic, where a visual explanation seems to explain a decision while detecting edges is what it really does. There are a large number of metrics in the literature specialized in quantitatively measuring different qualitative aspects, so we should be able to develop metrics capable of measuring in a robust and correct way the desirable aspects of the explanations. Some previous papers have attempted to develop new measures for this purpose. However, these measures suffer from lack of objectivity or lack of mathematical consistency, such as saturation or lack of smoothness. In this paper, we propose a procedure called REVEL to evaluate different aspects concerning the quality of explanations with a theoretically coherent development which do not have the problems of the previous measures. This procedure has several advances in the state of the art: it standardizes the concepts of explanation and develops a series of metrics not only to be able to compare between them but also to obtain absolute information regarding the explanation itself. The experiments have been carried out on four image datasets as benchmark where we show REVEL’s descriptive and analytical power.",arxiv:2211.06154,Yes,,2025-11-11T00:15:16.543Z
rgfgmlxmertanimprove-2022,RGFGM-LXMERT-An Improve Architecture Based On LXMERT,Renjie Yu,2022,International Conferences on Computing and Pattern Recognition,0,https://www.semanticscholar.org/paper/5209fef6918535a2f243ed47653c22cc6077879d,,10.1145/3581807.3581879,"LXMERT (Learning Cross-Modality Encoder Representations from Transformers) is a two-stream cross-modality pre-trained model that performs well in different downstream tasks which contain two visual question answering datasets and a challenging visual-reasoning task (i.e., VQA, GQA, and NLVR). But the large-scale model still has a lot of room for progress. That is, the model accuracy is very low, the generalization ability is weak, and it is easy to be attacked by adversarial attacks. Furthermore, training the LXMERT model takes a lot of time and money, so there is an urgent need to improve. Thus, I try to improve the training speed, generalization ability, and accuracy of the model by enhancing both the training method and the model structure. In the training method, FGM (Fast Gradient Method) adversarial training is introduced in the finetune phase of the model by adding the disturbances in both the language embedding layer's and visual feature linear layer's weights, which effectively improves the model accuracy and generalization ability. In the model structure, a residual block with weight is used to improve the training speed by 1.6% in the pre-training phase of this model without losing the model performance. Next, t the most important structure, the Encoder, is redesigned to make the model more convergent. The Encoder's FFN (Feed-Forward Neural Network) is replaced by GLU (Gated Linear Unit), which also improves the ability of model fitting and model performance. The improved model performs better on the VQA task than the benchmark (i.e., LXMERT). In the end, detailed ablation studies prove that my enhancement strategies are effective for LXMERT and observe the effectiveness of different measures on the model.",,Yes,,2025-11-11T00:15:16.543Z
rhoreducinghallucina-2022,RHO ($ρ$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding,Ziwei Ji; Zihan Liu; Nayeon Lee; Tiezheng Yu; Bryan Wilie; Mini Zeng; Pascale Fung,2022,Annual Meeting of the Association for Computational Linguistics,65,https://www.semanticscholar.org/paper/4e53b481beabba42aac027e5a8c69fed26ab4062,http://arxiv.org/pdf/2212.01588,10.48550/arXiv.2212.01588,"Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA).",arxiv:2212.01588,Yes,,2025-11-11T00:15:14.026Z
roscoeasuiteofmetric-2022,ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning,O. Yu. Golovneva; Moya Chen; Spencer Poff; Martin Corredor; Luke Zettlemoyer; Maryam Fazel-Zarandi; Asli Celikyilmaz,2022,arXiv.org,189,https://www.semanticscholar.org/paper/391246ce9c59d61c94cca3f8bef56c95542a4708,https://arxiv.org/pdf/2212.07919,10.48550/arXiv.2212.07919,"Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics.",arxiv:2212.07919,Yes,,2025-11-11T00:13:07.427Z
ratemlacodegeneratio-2022,RateML: A Code Generation Tool for Brain Network Models,Michiel van der Vlag; M. Woodman; J. Fousek; Sandra Diaz-Pier; Aarón Pérez Martín; Viktor Jirsa ; A. Morrison,2022,Frontiers in Network Physiology,8,https://www.semanticscholar.org/paper/0cadee0c50f3992f4cb595feaf2effce2896c33f,https://www.frontiersin.org/articles/10.3389/fnetp.2022.826345/pdf,10.3389/fnetp.2022.826345,"Whole brain network models are now an established tool in scientific and clinical research, however their use in a larger workflow still adds significant informatics complexity. We propose a tool, RateML, that enables users to generate such models from a succinct declarative description, in which the mathematics of the model are described without specifying how their simulation should be implemented. RateML builds on NeuroML’s Low Entropy Model Specification (LEMS), an XML based language for specifying models of dynamical systems, allowing descriptions of neural mass and discretized neural field models, as implemented by the Virtual Brain (TVB) simulator: the end user describes their model’s mathematics once and generates and runs code for different languages, targeting both CPUs for fast single simulations and GPUs for parallel ensemble simulations. High performance parallel simulations are crucial for tuning many parameters of a model to empirical data such as functional magnetic resonance imaging (fMRI), with reasonable execution times on small or modest hardware resources. Specifically, while RateML can generate Python model code, it enables generation of Compute Unified Device Architecture C++ code for NVIDIA GPUs. When a CUDA implementation of a model is generated, a tailored model driver class is produced, enabling the user to tweak the driver by hand and perform the parameter sweep. The model and driver can be executed on any compute capable NVIDIA GPU with a high degree of parallelization, either locally or in a compute cluster environment. The results reported in this manuscript show that with the CUDA code generated by RateML, it is possible to explore thousands of parameter combinations with a single Graphics Processing Unit for different models, substantially reducing parameter exploration times and resource usage for the brain network models, in turn accelerating the research workflow itself. This provides a new tool to create efficient and broader parameter fitting workflows, support studies on larger cohorts, and derive more robust and statistically relevant conclusions about brain dynamics.",,Yes,,2025-11-11T00:15:19.325Z
reactsynergizingreas-2022,ReAct: Synergizing Reasoning and Acting in Language Models,Shunyu Yao; Jeffrey Zhao; Dian Yu; Nan Du; Izhak Shafran; Karthik Narasimhan; Yuan Cao,2022,International Conference on Learning Representations,4452,https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d,,,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",arxiv:2210.03629,Yes,,2025-11-11T00:13:07.427Z
reasoningcircuitsfew-2022,Reasoning Circuits: Few-shot Multi-hop Question Generation with Structured Rationales,Saurabh Kulshreshtha; Anna Rumshisky,2022,NLRSE,4,https://www.semanticscholar.org/paper/2a0953e6aa8a8c4b88928957338e93f8636ebe84,https://arxiv.org/pdf/2211.08466,10.48550/arXiv.2211.08466,"Multi-hop Question Generation is the task of generating questions which require the reader to reason over and combine information spread across multiple passages employing several reasoning steps. Chain-of-thought rationale generation has been shown to improve performance on multi-step reasoning tasks and make model predictions more interpretable. However, few-shot performance gains from including rationales have been largely observed only in +100B language models, and otherwise require large-scale manual rationale annotation. In this paper, we introduce a new framework for applying chain-of-thought inspired structured rationale generation to multi-hop question generation under a very low supervision regime (8- to 128-shot). We propose to annotate a small number of examples following our proposed multi-step rationale schema, treating each reasoning step as a separate task to be performed by a generative language model. We show that our framework leads to improved control over the difficulty of the generated questions and better performance compared to baselines trained without rationales, both on automatic evaluation metrics and in human evaluation. Importantly, we show that this is achievable with a modest model size.",arxiv:2211.08466,Yes,,2025-11-11T00:14:11.169Z
reconstructingaction-2022,Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors,Xi Wang; Gengyan Li; Yen-Ling Kuo; Muhammed Kocabas; Emre Aksan; Otmar Hilliges,2022,International Conference on 3D Vision,34,https://www.semanticscholar.org/paper/bb26db1a4af5b3199d4b9a4767fa12c23507b40f,https://www.research-collection.ethz.ch/bitstream/20.500.11850/587514/7/rhoi.pdf,10.1109/3DV57658.2022.00047,"We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories.",arxiv:2209.02485,Yes,,2025-11-11T00:15:14.026Z
relationleakageineli-2022,Relation Leakage in Elicited Natural Language Inference Datasets,Michael Stephen Saxon; Xinyi Wang; Wenda Xu; William Yang Wang,2022,,0,https://www.semanticscholar.org/paper/5a6f6f44a2e05709d81245526786f8dc8f8ab263,,,,,Yes,,2025-11-11T00:14:11.169Z
researchfrontiersofp-2022,Research frontiers of pre-training mathematical models based on BERT,Guang Li; Wennan Wang; Liukai Zhu; Jun Peng; Xujia Li; Ruijie Luo,2022,"2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)",1,https://www.semanticscholar.org/paper/b55b85c3868060ed5de3b403d4691d84fd4229f4,,10.1109/EEBDA53927.2022.9744791,"Natural language processing (NLP) is a popular technology after the rise of big data and machine learning in recent years. With the development of deep learning, the field of natural language processing has also undergone a landmark transformation, including the emergence of the BERT large-scale language training model. The emergence of this model makes text mining a qualitative leap, meets more practical needs, and solves the related problems of feature vectorization of unstructured data. This article will sort out the connotation, task application, and main optimization and improvement methods of the BERT pre-training model released by Google, and provide a reference for subsequent related research and development based on BERT.",,Yes,,2025-11-11T00:14:11.169Z
researchonemotionalc-2022,Research on emotional content recognition of music video based on support vector machine,Xi Liu,2022,International Symposium on Parameterized and Exact Computation,0,https://www.semanticscholar.org/paper/caec30c92ef52b10130e65768e1e04d314966dd9,,10.1145/3544109.3544350,"As one of the main ways of mass entertainment, music video itself also has rich emotional connotation and strong emotional regulation function to meet the emotional needs of the audience. At present, the feelings of music videos are mostly based on the evaluation of music videos by listeners or experts. Its workload is quite large, and people with different cognitive levels have different evaluations of the same music video. The emotional content of music video is the emotional intensity and emotional type that users are expected to be induced in the process of watching music video. At present, many scholars are engaged in the research of emotional content of music video, mainly through pattern classifier, rule reasoning and other methods to establish the mapping relationship between low-level feature space and basic emotion type space, and identify the emotion types of music video according to this relationship. However, emotion belongs to the category of psychology, which has typical uncertainty and unknowness. Different audiences have great differences in their recognition of the emotional content of the same music video. Other similar studies focus on the identification of emotional content, but do not consider the uncertainty of emotion. Based on the theory of unascertained mathematics, a new emotion content recognition algorithm based on unascertained measure is proposed to identify the emotion types in music video. Based on the statistics of support vector machine model, it is found that music video emotion is often expressed and rendered in a specific form. It can be concluded that there must be an internal relationship between emotional content and some low-level features of music video. We choose scene brightness, scene rhythm and color energy as three low-level feature indexes to analyze the emotional content of music video. Music video scene segments are selected as the basic unit to study emotional content. The mirror head segmentation algorithm is used to segment the shots of music video clips, and the support vector machine algorithm is used to segment and extract music video scenes. This algorithm opens up a new perspective for the study of music video emotion type recognition, and believes that with the in-depth research and system integration of music video feature emotion attributes, the performance of music video emotion content recognition based on deterministic measure will be improved in the future.",,Yes,,2025-11-11T00:15:19.325Z
responsiblereasoning-2022,Responsible Reasoning with Large Language Models and the Impact of Proper Nouns,Sumit Kumar Jha,2022,,6,https://www.semanticscholar.org/paper/a70fcd82d8d26bf4c8b0bdde88e96b6ce2c80ecf,,,,,Yes,,2025-11-11T00:13:07.427Z
rethinkingwithretrie-2022,Rethinking with Retrieval: Faithful Large Language Model Inference,Hangfeng He; Hongming Zhang; D. Roth,2022,arXiv.org,191,https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1,http://arxiv.org/pdf/2301.00303,10.48550/arXiv.2301.00303,"Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.",arxiv:2301.00303,Yes,,2025-11-11T00:13:07.427Z
retrievalaugmentedan-2022,Retrieval-Augmented and Knowledge-Grounded Language Models for Faithful Clinical Medicine,Fenglin Liu; Bang-ju Yang; Chenyu You; Xian Wu; Shen Ge; Zhangdaihong Liu; Xunhu Sun; Yang Yang; D. Clifton,2022,,2,https://www.semanticscholar.org/paper/6bff251e4503607cd439295770be1907eebb6700,,,"Language models (LMs), including large language models (such as ChatGPT), have the potential to assist clinicians in generating various clinical notes. However, LMs are prone to produce ``hallucinations'', i.e., generated content that is not aligned with facts and knowledge. In this paper, we propose the Re$^3$Writer method with retrieval-augmented generation and knowledge-grounded reasoning to enable LMs to generate faithful clinical texts. We demonstrate the effectiveness of our method in generating patient discharge instructions. It requires the LMs not to only understand the patients' long clinical documents, i.e., the health records during hospitalization, but also to generate critical instructional information provided both to carers and to the patient at the time of discharge. The proposed Re$^3$Writer imitates the working patterns of physicians to first \textbf{re}trieve related working experience from historical instructions written by physicians, then \textbf{re}ason related medical knowledge. Finally, it \textbf{re}fines the retrieved working experience and reasoned medical knowledge to extract useful information, which is used to generate the discharge instructions for previously-unseen patients. Our experiments show that, using our method, the performance of five representative LMs can be substantially boosted across all metrics. Meanwhile, we show results from human evaluations to measure the effectiveness in terms of fluency, faithfulness, and comprehensiveness.",arxiv:2210.12777,Yes,,2025-11-11T00:14:11.169Z
romqaabenchmarkforro-2022,"RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering",Victor Zhong; Weijia Shi; Wen-tau Yih; Luke Zettlemoyer,2022,Conference on Empirical Methods in Natural Language Processing,28,https://www.semanticscholar.org/paper/b09a0e0398023683da479afc31df31440abb8f3e,https://arxiv.org/pdf/2210.14353,10.48550/arXiv.2210.14353,"We introduce RoMQA, the first benchmark for robust, multi-evidence, multi-answer question answering (QA). RoMQA contains clusters of questions that are derived from related constraints mined from the Wikidata knowledge graph. RoMQA evaluates robustness of QA models to varying constraints by measuring worst-case performance within each question cluster. Compared to prior QA datasets, RoMQA has more human-written questions that require reasoning over more evidence text and have, on average, many more correct answers. In addition, human annotators rate RoMQA questions as more natural or likely to be asked by people. We evaluate state-of-the-art large language models in zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is challenging: zero-shot and few-shot models perform similarly to naive baselines, while supervised retrieval methods perform well below gold evidence upper bounds. Moreover, existing models are not robust to variations in question constraints, but can be made more robust by tuning on clusters of related questions. Our results show that RoMQA is a challenging benchmark for large language models, and provides a quantifiable test to build more robust QA methods.",arxiv:2210.14353,Yes,,2025-11-11T00:15:14.026Z
robusttrafficrulesan-2022,Robust Traffic Rules and Knowledge Representation for Conflict Resolution in Autonomous Driving,K. Manas; Stefan Zwicklbauer; A. Paschke,2022,RuleML+RR,6,https://www.semanticscholar.org/paper/ac18b4cf109c57018d0bfeb93077a98daf5f66a5,,,,,Yes,,2025-11-11T00:15:19.325Z
rozwizywaniezadateks-2022,Rozwiązywanie zadań tekstowych przez studentów – przyszłych nauczycieli edukacji wczesnoszkolnej,Beata Bugajska-Jaszczołt; M. Czajkowska,2022,Problemy Wczesnej Edukacji,0,https://www.semanticscholar.org/paper/83fe695fd3c0295508ff40139ecfcab223b12a57,https://czasopisma.bg.ug.edu.pl/index.php/pwe/article/download/7714/6858,10.26881/pwe.2022.55.09,"The results of the research aimed at diagnosing the ability to solve textual tasks by the students – candidates for lower primary teachers are presented in the article. The research was carried out between the years 2017 and 2019. It comprised 392 students of pedagogy with teaching specialization. The main research method was a competency test, and the technique was document analysis. The research shows that the students have well-mastered calculational algorithms and techniques but they have difficulties in presenting their reasoning in the language of mathematics. A large group of students strive to write down the solution in the form of a ready mathematical formula, without representing the situation on a drawing/diagram or manipulating on concretes to help them solve the problem.",,Yes,,2025-11-11T00:15:16.543Z
rumedbencharussianme-2022,RuMedBench: A Russian Medical Language Understanding Benchmark,Pavel Blinov; A. Reshetnikova; A. Nesterov; Galina Zubkova; V. Kokh,2022,Conference on Artificial Intelligence in Medicine in Europe,16,https://www.semanticscholar.org/paper/11a348120b115ddeb4bcee18c876a60a07852355,,10.1007/978-3-031-09342-5_38,"The paper describes the open Russian medical language understanding benchmark covering several task types (classification, question answering, natural language inference, named entity recognition) on a number of novel text sets. Given the sensitive nature of the data in healthcare, such a benchmark partially closes the problem of Russian medical dataset absence. We prepare the unified format labeling, data split, and evaluation metrics for new tasks. The remaining tasks are from existing datasets with a few modifications. A single-number metric expresses a model's ability to cope with the benchmark. Moreover, we implement several baseline models, from simple ones to neural networks with transformer architecture, and release the code. Expectedly, the more advanced models yield better performance, but even a simple model is enough for a decent result in some tasks. Furthermore, for all tasks, we provide a human evaluation. Interestingly the models outperform humans in the large-scale classification tasks. However, the advantage of natural intelligence remains in the tasks requiring more knowledge and reasoning.",arxiv:2201.06499,Yes,,2025-11-11T00:15:14.026Z
soalkemampuanpenalar-2022,SOAL KEMAMPUAN PENALARAN MATEMATIS MATERI BANGUN RUANG SISI DATAR BERKONTEKS BENGKULU,Etika Budiarti; Nyayu Masyita Ariani; Adi Asmara,2022,Jurnal Math-UMB.EDU,3,https://www.semanticscholar.org/paper/63e1ef7f20c25220bdc890c8d221b91126a11e9e,http://jurnal.umb.ac.id/index.php/math/article/download/3582/2525,10.36085/mathumbedu.v9i3.3582,"Tujuan penelitian ini untuk menghasilkan soal-soal kemampuan penalaran matematis materi bangun ruang sisi datar berkonteks Bengkulu yang terstandar: yang valid, memiliki keterbacaan baik,  memiliki tingkat kesukaran dan indeks daya beda baik. Penelitian ini yang dilaksanakan pada tahun ajaran 2020/2021 ini mengikuti alur  Tessmer. Subjek ujicoba adalah siswa kelas VIII SMP di Bengkulu. Sebelum prototipe soal dibuat, dilakukan analisis siswa, kurikulum, materi dan konteks. Telaah dan revisi soal dilakukan pada tahap experts review. Soal ditelaah oleh 3 orang validator dari sisi konten, konstruk, dan bahasa. Komentar dan saran validator pada proses validasi menjadi acuan untuk revisi soal. Hasil analisis validasi akhir para validator menunjukkan bahwa soal telah valid. Proses one to one pada 3 orang siswa kelas VIII yang  masing-masing berkemampuan tinggi, sedang, dan rendah, Tergambar dari proses one to one bahwa maksud soal dapat terbaca oleh siswa. Bukti empirik mengenai tingkat kesukaran dan indeks daya beda butir soal diperoleh pada tahap small group, dengan cara mengujicobakan soal yang telah valid dan memiliki keterbacaan yang baik kepada 31 siswa kelas VIII SMP di Bengkulu.  Skor siswa terhadap soal tersebut dianalisis secara kuantitatif. Hasil analisis menunjukkan bahwa 8 soal dikembangkan dikategorikan terstandar. Soal-soal ini dapat digunakan untuk melatih kemampuan penalaran matematis siswa SMP.
Kata Kunci: Soal matematika, kemampuan penalaran matematis, konteks Bengkulu, bangun ruang sisi datar
 Abstract
The purpose of this research was to produce standardized items on mathematical reasoning abilities in the Bengkulu context of flat-sided geometry: those that were valid, had good clarity appeal, had a difficulty level and a good discrimination index. This research, which was conducted in the 2020/2021 academic year, follows the Tessmer’ model. The test subjects were students of class VIII SMP in Bengkulu. Before the items prototype was made, an analysis of students, curriculum, material and context was carried out. The review and revision of the items were carried out at the experts review stage. The items were reviewed by 3 validators in terms of content, construct, and language. The validator's comments and suggestions in the validation process become a reference for revision of the questions. The results of the final validation analysis of the validators, showed that the questions were valid. One to one process for 3 students of class VIII, each of which has high, medium, and low abilities. It is illustrated from the one to one process that the meaning of the questions can be read by students. Empirical evidence regarding the level of difficulty and discrimination index of items was obtained at the small group stage, by testing items that were valid and had good readability to 31 grade VIII SMP students in Bengkulu. Student scores on these items were analyzed quantitatively. The results of the analysis showed that the 8 items developed were categorized as standardized. These items can be used to exercise the mathematical reasoning skills of junior high school students.
Keywords: Item test of mathematics, mathematical reasoning ability, Bengkulu context, flat-sided geometry",,Yes,,2025-11-11T00:15:19.325Z
stmoedesigningstable-2022,ST-MoE: Designing Stable and Transferable Sparse Expert Models,Barret Zoph; Irwan Bello; Sameer Kumar; Nan Du; Yanping Huang; J. Dean; Noam M. Shazeer; W. Fedus,2022,,281,https://www.semanticscholar.org/paper/1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3,,,"Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).",arxiv:2202.08906,Yes,,2025-11-11T00:15:14.026Z
studyontheinfluenceo-2022,STUDY ON THE INFLUENCE OF NETWORK FACTORS ON THE LOYALTY AND EMOTIONAL BEHAVIOR OF CHINESE PROFESSIONAL FOOTBALL CLUB FANS,Shaoyong Liu; Wenlang Huang,2022,International Journal of Neuropsychopharmacology,1,https://www.semanticscholar.org/paper/0f7b40c92fa6410cad7c1b30f78bf8e1b353589e,https://academic.oup.com/ijnp/article-pdf/25/Supplement_1/A78/44558553/pyac032.107.pdf,10.1093/ijnp/pyac032.107,"Abstract Background China Professional Football League has been in operation for nearly 30 years and has attracted more and more attention from the public and society. China Professional Football League has promoted the professional and commercial development of Chinese football and attracted a large number of fans. At present, Chinese professional football clubs are struggling, but they still get the support of many fans. The development of any club is inseparable from the support of loyal fans, who are the basis of the development of the club. Only with many loyal fans can the club have long-term and strong vitality. Only by making outstanding achievements, excavating the economic benefits of the club, establishing and maintaining the club culture and cultivating a large number of loyal fans, can the club bring profound heritage and solid economic benefits to the club. Football club has a large number of loyal fans, which is the cornerstone of the sustainable and vigorous development of professional football league. However, due to many factors, the loyalty of fans has decreased, which is related to the evaluation of pursuit by the Internet. Subjects and Methods Based on this, this paper studies the loyalty of Chinese football fans, takes the fans of China Henan sslm football club as the research object, and analyzes the current situation of China Henan sslm football club and its fans. Through the research, it is found that with the continuous development of the Internet, public emotional behavior will have an important impact on football public opinion. There are two directions: one is the top-down impact, the other is the bottom-up impact; It is mainly manifested in two typical ways: social mobilization and emotional social struggle in football public opinion. The main expressions of their emotional behavior are: weakness, anger, sadness and anger, etc. On the basis of combing the collective behavior and emotional struggle, the research finds that the communication framework of emotional behavior mainly includes the communication paths of discourse co meaning, identity co meaning and emotional co meaning; Functional analysis includes target function, attribution function and ideographic function. From the tendency of public sentiment, football public opinion shows criticism, populism, nationalism, pragmatism, patriotism and justice. The social expression of public sentiment in microblog public opinion includes the spiral phenomenon of silence, butterfly effect, herd effect, resentment and so on. From the perspective of psychology, the public emotions in football public opinion are fear, anxiety, anger and sadness, while in terms of expression, the public express their feelings through direct expression and folk language. Results Using the methods of literature review, questionnaire survey and mathematical statistics, this paper analyzes the dimension of fan loyalty and constructs the fan loyalty model of sslm football club in Henan Province. Conclusion According to the research, fan participation is divided into three dimensions: pleasure participation, symbolic participation and central participation; Product attributes related to the club, such as team performance, star players, head coach and team management, are positively correlated with club fan loyalty; Logo design, stadium, game quality and team tradition are also positively correlated with club fan loyalty; There is a significant positive correlation between fans' attitude loyalty and fans' behavior loyalty. The author believes that club brand communication is an important factor for fans to participate in the development of football clubs. It is necessary for football clubs to strengthen the publicity of fans, let more fans know the real situation of the club, increase the exposure of football clubs and improve the satisfaction of fans. Secondly, the importance of the performance of the club team in the correlation of product related attributes is self-evident. The club needs to constantly improve the performance of the team in the game and win more attention from the outside world. Therefore, the club must actively train star players, select excellent coaches and lead the team to achieve better results. The club needs to strengthen the development of Lenovo products again, which have attributes unrelated to products. The design of club logo, the construction and maintenance of competition venues, the quality of competition and the tradition of the team are the basis of influencing the loyalty of fans. The club needs to continuously strengthen the product research and development of Lenovo's non product related attributes. Work can not only bring economic benefits to the club, but also shape the club's fan culture and further improve the club's fan loyalty. Finally, the club needs to provide a space for fans to communicate: there is a positive correlation between fans' escape, recognition, acceptance, nostalgia, regional glory and fans' loyalty. Let fans find suitable community organizations, form common interests and help improve the loyalty of club fans. Acknowledgements This research was supported by the general project of philosophy and Social Sciences Planning of Zhejiang Province (20NDJC184YB), and the Fundamental Research Funds for Zhejiang Provincial Universities and Research Institutes (2021R005).",,Yes,,2025-11-11T00:15:19.325Z
starbootstrappingrea-2022,STaR: Bootstrapping Reasoning With Reasoning,E. Zelikman; Yuhuai Wu; Noah D. Goodman,2022,,651,https://www.semanticscholar.org/paper/23dd78e424d32f6a48660dcd67ce994b8a7db8be,,,"Generating step-by-step""chain-of-thought""rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the""Self-Taught Reasoner""(STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.",arxiv:2203.14465,Yes,,2025-11-11T00:13:07.427Z
safetextabenchmarkfo-2022,SafeText: A Benchmark for Exploring Physical Safety in Language Models,Sharon Levy; Emily Allaway; Melanie Subbiah; Lydia B. Chilton; D. Patton; K. McKeown; William Yang Wang,2022,Conference on Empirical Methods in Natural Language Processing,46,https://www.semanticscholar.org/paper/2b6291eb76e2ff885238e94704bb795046d7d530,http://arxiv.org/pdf/2210.10045,10.48550/arXiv.2210.10045,"Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and requires additional commonsense knowledge to comprehend that it leads to physical harm. We create the first benchmark dataset, SafeText, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. We utilize SafeText to empirically study commonsense physical safety across various models designed for text generation and commonsense reasoning tasks. We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice. As a result, we argue for further studies of safety and the assessment of commonsense physical safety in models before release.",arxiv:2210.10045,Yes,,2025-11-11T00:14:11.169Z
scientificandcreativ-2022,Scientific and Creative Analogies in Pretrained Language Models,Tamara Czinczoll; H. Yannakoudakis; Pushkar Mishra; Ekaterina Shutova,2022,Conference on Empirical Methods in Natural Language Processing,9,https://www.semanticscholar.org/paper/933f60dda5847f208d9d3fd65e9b0df9cfed2403,https://arxiv.org/pdf/2211.15268,10.48550/arXiv.2211.15268,"This paper examines the encoding of analogy in large-scale pretrained language models, such as BERT and GPT-2. Existing analogy datasets typically focus on a limited set of analogical relations, with a high similarity of the two domains between which the analogy holds. As a more realistic setup, we introduce the Scientific and Creative Analogy dataset (SCAN), a novel analogy dataset containing systematic mappings of multiple attributes and relational structures across dissimilar domains. Using this dataset, we test the analogical reasoning capabilities of several widely-used pretrained language models (LMs). We find that state-of-the-art LMs achieve low performance on these complex analogy tasks, highlighting the challenges still posed by analogy understanding.",arxiv:2211.15268,Yes,,2025-11-11T00:15:14.026Z
selectioninferenceex-2022,Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning,Antonia Creswell; M. Shanahan; I. Higgins,2022,International Conference on Learning Representations,408,https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd,,,"Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",arxiv:2205.09712,Yes,,2025-11-11T00:13:07.427Z
selectiveannotationm-2022,Selective Annotation Makes Language Models Better Few-Shot Learners,Hongjin Su; Jungo Kasai; Chen Henry Wu; Weijia Shi; Tianlu Wang; Jiayi Xin; Rui Zhang; Mari Ostendorf; Luke Zettlemoyer; Noah A. Smith; Tao Yu,2022,International Conference on Learning Representations,289,https://www.semanticscholar.org/paper/86d0d3855f94105e25d81cab9f3d269c6062a9c4,http://arxiv.org/pdf/2209.01975,10.48550/arXiv.2209.01975,"Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9%/11.4% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks. Our code is available at https://github.com/HKUNLP/icl-selective-annotation.",arxiv:2209.01975,Yes,,2025-11-11T00:14:11.169Z
selfconsistencyimpro-2022,Self-Consistency Improves Chain of Thought Reasoning in Language Models,Xuezhi Wang; Jason Wei; D. Schuurmans; Quoc Le; Ed H. Chi; Denny Zhou,2022,International Conference on Learning Representations,4975,https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2,,,"Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",arxiv:2203.11171,Yes,,2025-11-11T00:13:07.427Z
selfpacedmultigraine-2022,Self-Paced Multi-Grained Cross-Modal Interaction Modeling for Referring Expression Comprehension,Peihan Miao; Wei Su; Gaoang Wang; Xuewei Li; Xi Li,2022,IEEE Transactions on Image Processing,12,https://www.semanticscholar.org/paper/09b85c61b99083a292b561b51a25b8a931dcd1b1,https://arxiv.org/pdf/2204.09957,10.1109/TIP.2023.3334099,"As an important and challenging problem in vision-language tasks, referring expression comprehension (REC) generally requires a large amount of multi-grained information of visual and linguistic modalities to realize accurate reasoning. In addition, due to the diversity of visual scenes and the variation of linguistic expressions, some hard examples have much more abundant multi-grained information than others. How to aggregate multi-grained information from different modalities and extract abundant knowledge from hard examples is crucial in the REC task. To address aforementioned challenges, in this paper, we propose a Self-paced Multi-grained Cross-modal Interaction Modeling framework, which improves the language-to-vision localization ability through innovations in network structure and learning mechanism. Concretely, we design a transformer-based multi-grained cross-modal attention, which effectively utilizes the inherent multi-grained information in visual and linguistic encoders. Furthermore, considering the large variance of samples, we propose a self-paced sample informativeness learning to adaptively enhance the network learning for samples containing abundant multi-grained information. The proposed framework significantly outperforms state-of-the-art methods on widely used datasets, such as RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame datasets, demonstrating the effectiveness of our method.",arxiv:2204.09957,Yes,,2025-11-11T00:15:16.541Z
semisupervisedground-2022,Semi-supervised Grounding Alignment for Multi-modal Feature Learning,Shih-Han Chou; Zicong Fan; J. Little; L. Sigal,2022,Canadian Conference on Computer and Robot Vision,8,https://www.semanticscholar.org/paper/3735b7ac2bc306a7345a678a215fdd158d3947d6,,10.1109/CRV55824.2022.00015,"Self-supervised transformer-based architectures, such as ViLBERT [1] and others, have recently emerged as dominant paradigms for multi-modal feature learning. Such architectures leverage large-scale datasets (e.g., Conceptual Captions [2]) and, typically, image-sentence pairings, for self-supervision. However, conventional multi-modal feature learning requires huge datasets and computing for both pre-training and fine-tuning to the target task. In this paper, we illustrate that more granular semi-supervised alignment at a region-phrase level is an additional useful cue and can further improve the performance of such representations. To this end, we propose a novel semi-supervised grounding alignment loss, which leverages an off-the-shelf pre-trained phrase grounding model for pseudo-supervision (by producing region-phrase alignments). This semi-supervised formulation enables better feature learning in the absence of any additional human annotations on the large-scale (Conceptual Captions) dataset. Further, it shows an even larger margin of improvement on smaller data splits, leading to effective data-efficient feature learning. We illustrate the superiority of the learned features by fine-tuning the resulting models to multiple vision-language downstream tasks: visual question answering (VQA), visual commonsense reasoning (VCR), and visual grounding. Experiments on the VQA, VCR, and grounding benchmarks demonstrate the improvement of up to 1.3% in accuracy (in visual grounding) with large-scale training; up to 5.9% (in VQA) with 1/8 of the data for pre-training and fine-tuning11We will release the code and all pre-trained models upon acceptance..",,Yes,,2025-11-11T00:15:19.325Z
setinterdependencetr-2022,Set Interdependence Transformer: Set-to-Sequence Neural Networks for Permutation Learning and Structure Prediction,Mateusz Jurewicz; Leon Derczynski,2022,International Joint Conference on Artificial Intelligence,3,https://www.semanticscholar.org/paper/d4740b0cdaf0e855fbc1f41364f9e707a76cc25e,https://arxiv.org/pdf/2206.03720,10.48550/arXiv.2206.03720,"The task of learning to map an input set onto a permuted sequence of its elements is challenging for neural networks. Set-to-sequence problems occur in natural language processing, computer vision and structure prediction, where interactions between elements of large sets define the optimal output. Models must exhibit relational reasoning, handle varying cardinalities and manage combinatorial complexity. Previous attention-based methods require n layers of their set transformations to explicitly represent n-th order relations. Our aim is to enhance their ability to efficiently model higher-order interactions through an additional interdependence component. We propose a novel neural set encoding method called the Set Interdependence Transformer, capable of relating the set's permutation invariant representation to its elements within sets of any cardinality. We combine it with a permutation learning module into a complete, 3-part set-to-sequence model and demonstrate its state-of-the-art performance on a number of tasks. These range from combinatorial optimization problems, through permutation learning challenges on both synthetic and established NLP datasets for sentence ordering, to a novel domain of product catalog structure prediction. Additionally, the network's ability to generalize to unseen sequence lengths is investigated and a comparative empirical analysis of the existing methods' ability to learn higher-order interactions is provided.",arxiv:2206.03720,Yes,,2025-11-11T00:15:16.543Z
singlestreammultilev-2022,Single-Stream Multi-Level Alignment for Vision-Language Pretraining,Zaid Khan; B. Vijaykumar; Xiang Yu; S. Schulter; Manmohan Chandraker; Y. Fu,2022,European Conference on Computer Vision,21,https://www.semanticscholar.org/paper/c10370810c8c5ccf12ae5a604b0f23601f90c4b2,http://arxiv.org/pdf/2203.14395,10.48550/arXiv.2203.14395,"Self-supervised vision-language pretraining from pure images and text with a contrastive loss is effective, but ignores fine-grained alignment due to a dual-stream architecture that aligns image and text representations only on a global level. Earlier, supervised, non-contrastive methods were capable of finer-grained alignment, but required dense annotations that were not scalable. We propose a single stream architecture that aligns images and language at multiple levels: global, fine-grained patch-token, and conceptual/semantic, using two novel tasks: symmetric cross-modality reconstruction (XMM) and a pseudo-labeled key word prediction (PSL). In XMM, we mask input tokens from one modality and use cross-modal information to reconstruct the masked token, thus improving fine-grained alignment between the two modalities. In PSL, we use attention to select keywords in a caption, use a momentum encoder to recommend other important keywords that are missing from the caption but represented in the image, and then train the visual encoder to predict the presence of those keywords, helping it learn semantic concepts that are essential for grounding a textual token to an image region. We demonstrate competitive performance and improved data efficiency on image-text retrieval, grounding, visual question answering/reasoning against larger models and models trained on more data. Code and models available at zaidkhan.me/SIMLA.",arxiv:2203.14395,Yes,,2025-11-11T00:15:14.026Z
smartercontractstopr-2022,Smarter Contracts to Predict using Deep-Learning Algorithms,Syed Badruddoja; R. Dantu; Yanyan He; Mark A. Thompson; Abiola Salau; Kritagya Upadhyay,2022,International Conference on Blockchain Computing and Applications,2,https://www.semanticscholar.org/paper/5fdc9c5a5fa205dfb97da54a7e3cb0ca0b6c877e,,10.1109/BCCA55292.2022.9922240,"Deep learning techniques can predict cognitive intelligence from large datasets involving complex computations with activation functions. However, the prediction output needs verification for trust and reliability. Moreover, these algorithms suffer from the model's provenance to keep track of model updates and developments. Blockchain smart contracts provide a trustable ledger with consensus-based decisions that assure integrity and verifiability. In addition, the immutability feature of blockchain also supports the provenance of data that can help deep learning algorithms. Nevertheless, smart contract languages cannot predict due to the absence of floating-point operations required by activation functions of neural networks. In this paper, we derive a novel method using the Taylor series expansion to compute the floating-point equivalent output for activation functions. We train the deep learning model off-chain using a standard Python programming language. Moreover, we store models and predict on-chain with blockchain smart contracts to produce a trusted forecast. Our experiment and analysis achieved an accuracy (99%) similar to popular Keras Python library models for the MNIST dataset. Furthermore, any blockchain platform can reproduce the activation function using our derived method. Last but not least, other deep learning algorithms can reuse the mathematical model to predict on-chain.",,Yes,,2025-11-11T00:15:16.543Z
smoothquantaccuratea-2022,SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models,Guangxuan Xiao; Ji Lin; Mickael Seznec; Julien Demouth; Song Han,2022,International Conference on Machine Learning,1083,https://www.semanticscholar.org/paper/2c994fadbb84fb960d8306ee138dbeef41a5b323,http://arxiv.org/pdf/2211.10438,10.48550/arXiv.2211.10438,"Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.",arxiv:2211.10438,Yes,,2025-11-11T00:13:07.427Z
snoopyanonlineinterf-2022,Snoopy: An Online Interface for Exploring the Effect of Pretraining Term Frequencies on Few-Shot LM Performance,Yasaman Razeghi; R. Mekala; IV RobertL.Logan; Matt Gardner; Sameer Singh,2022,Conference on Empirical Methods in Natural Language Processing,4,https://www.semanticscholar.org/paper/9715be0a94c9b05bafe299cbfb4f846453bfd2ab,https://aclanthology.org/2022.emnlp-demos.39.pdf,10.18653/v1/2022.emnlp-demos.39,"Current evaluation schemes for large language models often fail to consider the impact of the overlap between pretraining corpus and test data on model performance statistics. Snoopy is an online interface that allows researchers to study this impact in few-shot learning settings. Our demo provides term frequency statistics for the Pile, which is an 800 GB corpus, ac-companied by the precomputed performance of EleutherAI/GPT models on more than 20 NLP benchmarks, including numerical, commonsense reasoning, natural language understanding, and question-answering tasks. Snoopy allows a user to interactively align specific terms in test instances with their frequency in the Pile, enabling exploratory analysis of how term frequency is related to the accuracy of the models, which are hard to discover through au-tomated means. A user can look at correla-tions over various model sizes and numbers of in-context examples and visualize the re-sult across multiple (potentially aggregated) datasets. Using Snoopy , we show that a re-searcher can quickly replicate prior analyses for numerical tasks, while simultaneously allowing for much more expansive exploration that was previously challenging. Snoopy is available at https://nlp.ics.uci.edu/snoopy .",,Yes,,2025-11-11T00:15:14.026Z
socraticmodelscompos-2022,Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language,Andy Zeng; Adrian S. Wong; Stefan Welker; K. Choromanski; F. Tombari; Aveek Purohit; M. Ryoo; Vikas Sindhwani; Johnny Lee; Vincent Vanhoucke; Peter R. Florence,2022,International Conference on Learning Representations,645,https://www.semanticscholar.org/paper/ada81a4de88a6ce474df2e2446ad11fea480616e,,,"Large pretrained (e.g.,""foundation"") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",arxiv:2204.00598,Yes,,2025-11-11T00:13:07.427Z
solvingmathwordprobl-2022,Solving Math Word Problem via Cooperative Reasoning induced Language Models,Xinyu Zhu; Junjie Wang; Lin Zhang; Yuxiang Zhang; Ruyi Gan; Jiaxing Zhang; Yujiu Yang,2022,arXiv.org,24,https://www.semanticscholar.org/paper/01f7bb1f9c611b5e849558e445fdccb98a3a3040,http://arxiv.org/pdf/2210.16257,10.48550/arXiv.2210.16257,,,Yes,,2025-11-11T00:13:07.427Z
solvingquantitativer-2022,Solving Quantitative Reasoning Problems with Language Models,Aitor Lewkowycz; Anders Andreassen; David Dohan; Ethan Dyer; H. Michalewski; V. Ramasesh; Ambrose Slone; Cem Anil; Imanol Schlag; Theo Gutman-Solo; Yuhuai Wu; Behnam Neyshabur; Guy Gur-Ari; Vedant Misra,2022,Neural Information Processing Systems,1192,https://www.semanticscholar.org/paper/ab0e3d3e4d42369de5933a3b4c237780b41c0d77,http://arxiv.org/pdf/2206.14858,10.48550/arXiv.2206.14858,"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",arxiv:2206.14858,Yes,,2025-11-11T00:13:07.427Z
solvingtctypeawpsusi-2022,Solving TC-type AWPs using external knowledge & learning,Suresh Kumar; P. S. Kumar,2022,COMAD/CODS,2,https://www.semanticscholar.org/paper/5f34bd0d2206d1a55645706b88e5429f27be729a,,10.1145/3493700.3493744,"Arithmetic Word Problems(AWPs) are mathematical numerical problems expressed in natural language like English, and they provide a natural representation for many quantitative reasoning situations, such as understanding epidemic facts, finance & sports news, etc. Transfer-Case(TC) problems are specific AWPs involving the transfer of objects from one agent to another. In the current modeling, we assume that TC-type AWPs involve a single transfer of a quantity. A TC-type AWP consists of one or more sentences and they essentially contain the information of four types: beforetransfer(BT), transfer(TR), after-transfer(AT), question(QS). The state-of-the-art approaches adopt either statistical, treebased, or template-based modelings. Tree-based approaches[1, 2, 4, 7] leverage the idea of transforming an arithmetic expression to an equivalent tree-structure whereas template-based approach[6] focuses on predicting the appropriate template for the given AWP.We try solving the TC-type AWPs using existing approaches and experiment with number-of-sentences, question-template, complexity-ofthe-sentences, etc; and observe the followings: previous approaches are less efficient in processing the compound or longer sentences of the problem-text, and are less robust to even small template variations. Additionally, they rely on extensive manually-given annotations. Moreover, existing approaches are missing relevant background knowledge(humans solve AWPs efficiently, as they have required knowledge). Knowledge-based systems are considered imperative for building intelligent AI systems. They have a wide variety of applications in various domains like program analysis, natural language understanding, decision making, search & analysis, and knowledge-infused learning, etc. Therefore, in the proposed work1, we focus on solving TC-type AWPs using external knowledge and learning. In Figure 1, we present an example TC-type AWP. SystemOverview(Figure 2): Broadly, the proposed framework has three components; classifier, TC-Ontology, and SWRL module.",,Yes,,2025-11-11T00:15:16.543Z
speakersatthehellose-2022,Speakers at the Hello Session,Anson Macdonald; Brock D Sherlock; Dilshan Wijesena; Hongzhi Liao; Josef I. Bisits; J. Connor; Joshua Graham; Kevin Pan; Ryan Seelig; Samuel Mason; Stuart-James M. Burney; Yerlan Nessipbayev,2022,,0,https://www.semanticscholar.org/paper/a1d089feab0eaa0f1dcfe6e839e768428a99c2e7,,,,,Yes,,2025-11-11T00:15:19.325Z
specialissueonaibase-2022,Special issue on AI-based web information processing,Chuanchao Huang; Shuren Zhou,2022,Neural computing & applications (Print),1,https://www.semanticscholar.org/paper/d53f097c6255c542345cf958fbcb99eb67f2526e,https://link.springer.com/content/pdf/10.1007/s00521-022-07342-x.pdf,10.1007/s00521-022-07342-x,,,Yes,,2025-11-11T00:15:19.325Z
specicationandtopdow-2022,Speciﬁcation and top-down design of distributed systems,,2022,,0,https://www.semanticscholar.org/paper/65c11dceacaa587c9fd0fc3f0b0b3160ee9fa445,,,,,Yes,,2025-11-11T00:15:19.325Z
statisticalrelationa-2022,Statistical Relational Extension of Answer Set Programming,Joohyung Lee; Zhun Yang,2022,RW,1,https://www.semanticscholar.org/paper/a7cf30fd9ba609fb8db4575a04dd90b009f3dd52,,10.1007/978-3-031-31414-8_4,,,Yes,,2025-11-11T00:15:16.543Z
structuredflexiblean-2022,"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",K. M. Collins; Catherine Wong; Jiahai Feng; Megan Wei; J. Tenenbaum,2022,Annual Meeting of the Cognitive Science Society,66,https://www.semanticscholar.org/paper/7ef9aafc68511afab5b287e62b754576ea37b4ce,http://arxiv.org/pdf/2205.05718,10.48550/arXiv.2205.05718,"Human language offers a powerful window into our thoughts -- we tell stories, give explanations, and express our beliefs and goals through words. Abundant evidence also suggests that language plays a developmental role in structuring our learning. Here, we ask: how much of human-like thinking can be captured by learning statistical patterns in language alone? We first contribute a new challenge benchmark for comparing humans and distributional large language models (LLMs). Our benchmark contains two problem-solving domains (planning and explanation generation) and is designed to require generalization to new, out-of-distribution problems expressed in language. We find that humans are far more robust than LLMs on this benchmark. Next, we propose a hybrid Parse-and-Solve model, which augments distributional LLMs with a structured symbolic reasoning module. We find that this model shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.",arxiv:2205.05718,Yes,,2025-11-11T00:13:07.427Z
studentscriticalthin-2022,Students Critical Thinking Skills Through Project-Based Learning Models in Solving Pythagoras Theorem Problems,Liliyani Sameth; Djaffar Lessy,2022,Indo-MathEdu Intellectuals Journal,1,https://www.semanticscholar.org/paper/6b017a47f524b2eaf635551e3aaac3a9b78dda84,https://ejournal.indo-intellectual.id/index.php/imeij/article/download/35/20,10.54373/imeij.v3i1.35,"Mathematics is a symbolic language of its characteristics is to use deductive reasoning and inductive reasoning methods. In the teaching of mathematics each subject is a unit so that the mastery of one subject is a support to learn the next subject. Similarly, the ability of students in carrying out calculating operations in mathematics lessons in developing students' critical thinking skills. To perform calculating operations on the right learning requires the right learning model and teaching materials. Therefore, the right learning model in cultivating students' critical thinking skills is a project-based learning model. This research aims to find out the ability to think critically through project-based learning models on the material of the theorem theorem phytagoras of students in class VIII of State Junior High School 5 West Leihitu District. The type of research used in this study is qualitative descriptive. The process of taking subjects is based on the highest value of group work on LKS. From the results of the group's work, 3 students were taken from the total number of students, namely 30 people. The instruments used in this study were researchers, tests and interviews. The data analysis techniques used follow concepts developed by Miles and Huberman, namely data reduction, data presentation, and inference. The results of the data analysis obtained that the critical thinking skills possessed by each student vary. From the results of interviews with 3 subjects, it is known that sl subjects meet 4 indicators of critical thinking skills used in this study, namely providing simple explanations, building basic abilities, making conclusions, and providing advanced explanations. The AH subject meets two indicators: providing simple explanations and building basic skills. While the subject of RK only met one indicator used in this study, namely building basic capabilities in the problem-solving process.",,Yes,,2025-11-11T00:15:16.543Z
subordinationalgebra-2022,Subordination Algebras as Semantic Environment of Input/Output Logic,Andrea De Domenico; A. Farjami; Krishna Manoorkar; A. Palmigiano; Mattia Panettiere; Xiaolong Wang,2022,"Workshop on Logic, Language, Information and Computation",2,https://www.semanticscholar.org/paper/4aa1e5936529d87829247415b1569d7f7e224e20,https://arxiv.org/pdf/2205.13903,10.48550/arXiv.2205.13903,"We establish a novel connection between two research areas in non-classical logics which have been developed independently of each other so far: on the one hand, input/output logic, introduced within a research program developing logical formalizations of normative reasoning in philosophical logic and AI; on the other hand, subordination algebras, investigated in the context of a research program integrating topological, algebraic, and duality-theoretic techniques in the study of the semantics of modal logic. Specifically, we propose that the basic framework of input/output logic, as well as its extensions, can be given formal semantics on (slight generalizations of) subordination algebras. The existence of this interpretation brings benefits to both research areas: on the one hand, this connection allows for a novel conceptual understanding of subordination algebras as mathematical models of the properties and behaviour of norms; on the other hand, thanks to the well developed connection between subordination algebras and modal logic, the output operators in input/output logic can be given a new formal representation as modal operators, whose properties can be explicitly axiomatised in a suitable language, and be systematically studied by means of mathematically established and powerful tools.",arxiv:2205.13903,Yes,,2025-11-11T00:15:16.543Z
successivepromptingf-2022,Successive Prompting for Decomposing Complex Questions,Dheeru Dua; Shivanshu Gupta; Sameer Singh; Matt Gardner,2022,Conference on Empirical Methods in Natural Language Processing,130,https://www.semanticscholar.org/paper/c90151f00b1ac4abf1cc353849b453aa21cc2df3,https://arxiv.org/pdf/2212.04092,10.48550/arXiv.2212.04092,"Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce “Successive Prompting” where, we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate synthetic dataset which can be used to bootstrap model’s ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement in F1 of ~5% when compared with a state-of-the-art model with synthetic augmentations and few-shot version of the DROP dataset.",arxiv:2212.04092,Yes,,2025-11-11T00:15:14.026Z
symbolicdataaugmenta-2022,Symbolic Data Augmentation for Assisted Neural Reasoning,Muhan Li,2022,,0,https://www.semanticscholar.org/paper/7865ae1c4d7ba0e3ba48aef54270c3fdf61ec11d,,,,,Yes,,2025-11-11T00:14:11.169Z
symbolicmathreasonin-2022,Symbolic Math Reasoning with Language Models,Vedant Gaur; Nikunj Saunshi,2022,2022 IEEE MIT Undergraduate Research Technology Conference (URTC),12,https://www.semanticscholar.org/paper/f557f3a32d309373e7d31bb93ca1b80b4a6e39e7,,10.1109/URTC56832.2022.10002218,"The emergence of large language models (LLMs) such as OpenAI’s GPT-3, Google’s LaMDA, Meta’s OPT [2, 3, 7, 10] etc. have revolutionized the field of natural language processing (NLP). These models with upwards of hundreds of billions of parameters are trained on large unlabeled text corpora and can subsequently solve downstream tasks with little to no labeled data. While these models are increasingly versatile in their abilities, e.g., solving math word problems, the larger question of their ability to reason remains. Using and modifying the SVAMP dataset, we find that GPT-3’s davinci-002 model, in addition to having good performance on numerical math word problems, also performs well on the potentially harder symbolic version of the same problems. Furthermore, adopting a two-step approach (solve symbolically and then substitute numerical values) leads to better accuracy on the numerical test set in the zero-shot regime. Additionally, we find that the use of specific prompting techniques pushes the model, in many cases, to actively describe its thought process and aid in the final answer output when faced with a complex, multi-step problem, aligning with recent observations.",,Yes,,2025-11-11T00:13:07.427Z
systematicanalysisof-2022,Systematic Analysis of Image Schemas in Natural Language through Explainable Multilingual Neural Language Processing,Lennart Wachowiak; Dagmar Gromann,2022,International Conference on Computational Linguistics,13,https://www.semanticscholar.org/paper/964b613686e1c533c8994c78a952ffe823101578,,,,,Yes,,2025-11-11T00:14:11.169Z
systematictransforma-2022,Systematic Transformation Method from UML to Event-B,Xue Geng; Sheng-rong Zou; Junpeng Yao,2022,"IEEE International Conference on Software Quality, Reliability and Security Companion",0,https://www.semanticscholar.org/paper/028323a772a952e38bab6b028b8c096ab5a52456,,10.1109/QRS-C57518.2022.00127,"In object-oriented software development, UML has become a de facto modeling standard. However, although UML is easy to understand and apply, it has inaccurate semantics, and UML is a semi-formal modeling language, which cannot be formally verified. Event-B is a formal method based on a large number of mathematical predicate logic, which is precise but difficult to understand and apply. Therefore, how to combine the advantages of UML diagram and Event-B method is the focus of the research. The previous transformation methods are based on the transformation from UML scatter diagram to Event-B, which is prone to conflict and inconsistency. Therefore, we propose a systematic transformation method that can realize the corresponding unification of elements in UML and those in Event-B. The general software system is a medium-sized system. We believe that the medium-sized system can be clearly expressed by using use case diagram, class diagram, state diagram and sequence diagram. In this paper, the transformation methods from these four diagrams to Event-B are given respectively. The transformation method of the system is applied to the elevator control system which requires high safety and reliability. The system transformation method from UML to Event-B not only improves the accuracy of UML and is easy for software practitioners to use, but also enhances the comprehensibility of formal methods and is conducive to the promotion and application of formal methods.",,Yes,,2025-11-11T00:15:16.543Z
systematicityingpt3s-2022,Systematicity in GPT-3's Interpretation of Novel English Noun Compounds,Siyan Li; Riley Carlson; Christopher Potts,2022,Conference on Empirical Methods in Natural Language Processing,15,https://www.semanticscholar.org/paper/74be37384adc9b643b0c0a2d3b26c1361c5d779b,http://arxiv.org/pdf/2210.09492,10.48550/arXiv.2210.09492,"Levin et al. (2019) show experimentally that the interpretations of novel English noun compounds (e.g., stew skillet), while not fully compositional, are highly predictable based on whether the modifier and head refer to artifacts or natural kinds. Is the large language model GPT-3 governed by the same interpretive principles? To address this question, we first compare Levin et al.'s experimental data with GPT-3 generations, finding a high degree of similarity. However, this evidence is consistent with GPT3 reasoning only about specific lexical items rather than the more abstract conceptual categories of Levin et al.'s theory. To probe more deeply, we construct prompts that require the relevant kind of conceptual reasoning. Here, we fail to find convincing evidence that GPT-3 is reasoning about more than just individual lexical items. These results highlight the importance of controlling for low-level distributional regularities when assessing whether a large language model latently encodes a deeper theory.",arxiv:2210.09492,Yes,,2025-11-11T00:15:14.026Z
tcgeventeffectivetas-2022,TCG-Event: Effective Task Conditioning for Generation-based Event Extraction,Fatemeh Shiri; Tongtong Wu; Yuan-Fang Li; Gholamreza Haffari,2022,Australasian Language Technology Association Workshop,3,https://www.semanticscholar.org/paper/cd23ff769057416e1d1a702210ff019db8d4763f,,,,,Yes,,2025-11-11T00:15:14.026Z
tgea20alargescaledia-2022,TGEA 2.0: A Large-Scale Diagnostically Annotated Dataset with Benchmark Tasks for Text Generation of Pretrained Language Models,Huibin Ge; Xiaohu Zhao; Chuang Liu; Yulong Zeng; Qun Liu; Deyi Xiong,2022,Neural Information Processing Systems,1,https://www.semanticscholar.org/paper/44eca0cf8397b536fcf82c2e249eb05ed19b0ce4,,,,,Yes,,2025-11-11T00:14:11.169Z
tacubeprecomputingda-2022,TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data,Fan Zhou; Mengkang Hu; Haoyu Dong; Zhoujun Cheng; Shi Han; Dongmei Zhang,2022,Conference on Empirical Methods in Natural Language Processing,31,https://www.semanticscholar.org/paper/52b3087525b262f6f467453e22fdfa843353d40c,https://arxiv.org/pdf/2205.12682,10.48550/arXiv.2205.12682,"Existing auto-regressive pre-trained language models (PLMs) like T5 and BART, have been well applied to table question answering by UNIFIEDSKG and TAPEX, respectively, and demonstrated state-of-the-art results on multiple benchmarks. However, auto-regressive PLMs are challenged by recent emerging numerical reasoning datasets, such as TAT-QA, due to the error-prone implicit calculation. In this paper, we present TaCube, to pre-compute aggregation/arithmetic results for the table in advance, so that they are handy and readily available for PLMs to answer numerical reasoning questions. TaCube systematically and comprehensively covers a collection of computational operations over table segments. By simply concatenating TaCube to the input sequence of PLMs, it shows significant experimental effectiveness. TaCube promotes the F1 score from 49.6% to 66.2% on TAT-QA and achieves new state-of-the-art results on WikiTQ (59.6% denotation accuracy). TaCube’s improvements on numerical reasoning cases are even more notable: on TAT-QA, TaCube promotes the exact match accuracy of BART-large by 39.6% on sum, 52.5% on average, 36.6% on substraction, and 22.2% on division. We believe that TaCube is a general and portable pre-computation solution that can be potentially integrated to various numerical reasoning frameworks",arxiv:2205.12682,Yes,,2025-11-11T00:14:11.169Z
tacklingmathwordprob-2022,Tackling Math Word Problems with Fine-to-Coarse Abstracting and Reasoning,Ai Li; Xueyao Jiang; Bang Liu; Jiaqing Liang; Yanghua Xiao,2022,arXiv.org,3,https://www.semanticscholar.org/paper/a4901628fc09ab1edbeaee7ebe771f442c41d006,https://arxiv.org/pdf/2205.08274,10.48550/arXiv.2205.08274,"Math Word Problems (MWP) is an important task that requires the ability of understanding and reasoning over mathematical text. Existing approaches mostly formalize it as a generation task by adopting Seq2Seq or Seq2Tree models to encode an input math problem in natural language as a global representation and generate the output mathematical expression. Such approaches only learn shallow heuristics and fail to capture fine-grained variations in inputs. In this paper, we propose to model a math word problem in a fine-to-coarse manner to capture both the local fine-grained information and the global logical structure of it. Instead of generating a complete equation sequence or expression tree from the global features, we iteratively combine low-level operands to predict a higher-level operator, abstracting the problem and reasoning about the solving operators from bottom to up. Our model is naturally more sensitive to local variations and can better generalize to unseen problem types. Extensive evaluations on Math23k and SVAMP datasets demonstrate the accuracy and robustness of our method.",arxiv:2205.08274,Yes,,2025-11-11T00:14:11.169Z
takinganassetbasedap-2022,Taking an Asset-Based Approach in the Use of a Culturally Located Task to Construct Functional Reasoning,R. Hunter; Jodie Hunter; Bronwyn Gibbs,2022,Teachers College Record,5,https://www.semanticscholar.org/paper/2a1341e7d5001aeeabdf87ab9fbbf55a2d60a847,,10.1177/01614681221103958,"Background: Algebra has traditionally been seen as a site of inequity and a gatekeeper for marginalized learners within both national and international studies. However, considerable research has shown that taking an asset-based approach in mathematics teaching improves learning for marginalized students, including Māori and Pāsifika learners. Purpose: The purpose of this research was to explore how a culturally located task set with a familiar Pāsifika context and within an algebraic frame was mathematized to support rich understandings. The focus was placed on how Pāsifika and Māori students drew on multimodal forms of communication (gesture, drawings, language, and symbols) to collectively make sense of a culturally located growing pattern task designed to develop functional reasoning. Research Design: The 12 student participants in the study were aged between 11 and 12 years and were of Māori and Pāsifika nations ethnicity, as were their teacher and the two researchers. The lesson reported on in this study was one of eight lessons and was representative of all lessons as the teacher and students drew on a variety of multimodal means of communication to construct functional reasoning. The design drew on both qualitative case study and design research. Data collection tools included interviews, video-recorded classroom observations, field notes, and photographs of work samples. The research design and analysis were informed through use of the Ula model as appropriate to all participants’ Pāsifika ethnicity. Similarly, storying was used, given that it is a traditional form of sharing used by indigenous peoples. Findings: Clearly evident in the results was how use of a culturally located task within a known context and the teacher’s asset-based approach supported students to engage in a challenging algebraic task. Multimodal forms of representation as reasoning and communication tools supported them to access more sophisticated forms of algebraic understandings as they began to generalize recursive, covarying, and correspondence relationships in increasingly sophisticated ways. Use of body language and gesturing was central to their communication, as was use of their first language and natural language. Conclusions/Recommendations: To change the gatekeeping role of algebra, teachers need to take a strength-based perspective and draw on what students bring to school as valued knowledge. For all students to access powerful ways of reasoning algebraically, the key role of many multimodal forms of communication needs to be recognized and affirmed.",,Yes,,2025-11-11T00:15:14.026Z
taskcompassscalingmu-2022,Task Compass: Scaling Multi-task Pre-training with Task Prefix,Zhuosheng Zhang; Shuo Wang; Yichong Xu; Yuwei Fang; W. Yu; Yang Liu; H. Zhao; Chenguang Zhu; Michael Zeng,2022,Conference on Empirical Methods in Natural Language Processing,19,https://www.semanticscholar.org/paper/0979695b5d74016e97ab8f306f632114e98bd6d9,http://arxiv.org/pdf/2210.06277,10.48550/arXiv.2210.06277,"Leveraging task-aware annotated data as supervised signals to assist with self-supervised learning on large-scale unlabeled data has become a new trend in pre-training language models. Existing studies show that multi-task learning with large-scale supervised tasks suffers from negative effects across tasks. To tackle the challenge, we propose a task prefix guided multi-task pre-training framework to explore the relationships among tasks. We conduct extensive experiments on 40 datasets, which show that our model can not only serve as the strong foundation backbone for a wide range of tasks but also be feasible as a probing tool for analyzing task relationships. The task relationships reflected by the prefixes align transfer learning performance between tasks. They also suggest directions for data augmentation with complementary tasks, which help our model achieve human-parity results on commonsense reasoning leaderboards. Code is available at https://github.com/cooelf/CompassMTL",arxiv:2210.06277,Yes,,2025-11-11T00:15:14.026Z
teachingalgorithmicr-2022,Teaching Algorithmic Reasoning via In-context Learning,Hattie Zhou; Azade Nova; H. Larochelle; Aaron C. Courville; Behnam Neyshabur; Hanie Sedghi,2022,arXiv.org,126,https://www.semanticscholar.org/paper/4d17732d90440682b0500f4e209c6cc4fac20e0e,http://arxiv.org/pdf/2211.09066,10.48550/arXiv.2211.09066,"Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.",arxiv:2211.09066,Yes,,2025-11-11T00:13:07.427Z
teachingsmalllanguag-2022,Teaching Small Language Models to Reason,Lucie Charlotte Magister; Jonathan Mallinson; Jakub Adamek; Eric Malmi; A. Severyn,2022,Annual Meeting of the Association for Computational Linguistics,301,https://www.semanticscholar.org/paper/126a4776ff8315fd506766cb8f3c722cf746ad9e,http://arxiv.org/pdf/2212.08410,10.48550/arXiv.2212.08410,"Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% and 18.42% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.",arxiv:2212.08410,Yes,,2025-11-11T00:14:11.169Z
tensimplerulesforpri-2022,Ten simple rules for principled simulation modelling,L. Fogarty; Madeleine Ammar; Thomas Holding; Adam Powell; A. Kandler,2022,PLoS Comput. Biol.,9,https://www.semanticscholar.org/paper/4c3d680f2431316ea290de3b6376ed9da6d66c5f,https://journals.plos.org/ploscompbiol/article/file?id=10.1371/journal.pcbi.1009917&type=printable,10.1371/journal.pcbi.1009917,"ed counterpart (Fig 2 in [13]). However, it is an updated version of this second map that is given to millions of tourists travelling across London every year. The inclusion of only important information, abstraction, and omission of extraneous geographical detail has made it one of the most famously useful graphics in history. Models of complex systems should be the same—as detailed as needed but no more detailed than that. So, if you shouldn’t model the system you know in complete detail, what should you model instead? This depends to a great extent on the type of model that you have decided to make (Rule 1). Assuming that some mechanistic insight is the aim, based on knowledge of the system and precise formulation of the research question, a good strategy is to identify putative interactions or mechanisms that could underlie the behaviour or phenomenon under investigation and model those mechanisms. In other words, it is sensible to propose an engine that could be driving the phenomenon of interest and then make characterising and investigating that engine the central focus of the model. Anything in the system irrelevant to that engine is irrelevant to the model. Part of the art of modelling is identifying putative mechanisms and ensuring that the logical thread between mechanism and behaviour is unbroken. Rule 3: Be rigorous, and allow the time to be rigorous Perhaps this is an obvious rule—cience and rigour usually go hand in hand. Successful modelling requires considerable attention to mathematical and computational detail. It is vital that all concepts and all code used in a simulation model are clear to the author and rigorously tested—from their construction to their downstream implications (see Rule 6). Using convenient mathematical or statistical concepts with little or no knowledge about their inner workings will likely lead to their misuse and maybe to serious fundamental errors. Similarly, it can be dangerous to use, for example, software packages or useful-looking chunks of code as untested black boxes (but see Rule 6). The assumptions of the methods implemented in the code, and the implementation itself, must be clear and consistent with the assumptions and aims of your model. Alongside the mathematical and technical details of each part of the model, careful consideration should be given to the order in which these parts or processes are executed and what effect that ordering (or scheduling) has on the output. All of this is to say that modelling decisions need careful thought and, as with empirical data analysis, designing a model that describes your system in adequate (but not excessive) detail, and deriving robust inferences from it, can take significant time. This is time which needs to be budgeted for in any simulation modelling project from the outset. Rule 4: Have a plan for analysis The aim of this rule is to help you to avoid a surprisingly common and deeply frustrating situation: creating a monster simulation with so many moving parts that it just can’t be analysed meaningfully—a situation where you could say to yourself something like “I’ve finished coding this simulation, but how on earth can I analyse so many interactions and so many outputs?” Reaching this point probably means that something has gone wrong in the implementation of Rules 1 or 2—or that the model has been coded without a good plan for its analysis. So, perhaps you know the question you want to answer, you know or have guessed the relevant aspects of the system, but you didn’t plan how the model can answer the question. In general, it is important to have an idea of the analysis steps necessary to answer aspects of the central question. These plans are flexible and develop throughout a project, of course, but an initial plan is crucial. Your exact plan for analysis will depend on your aim and the kind of model you have constructed. For example, if you are interested in unravelling mechanistic relationships, you might use simulated data to explore the workings of the model and to analyse exactly how PLOS COMPUTATIONAL BIOLOGY PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1009917 March 31, 2022 3 / 8 different parameters or processes affect the outcome. To do this successfully, the number of parameters and processes should be manageable (see Rule 2), and parameter explorations should be principled (see Rule 8). Or, if you are interested in fitting your model to observed data, you may need to consider exactly how the model output maps to existing and future data and identify appropriate statistical techniques to robustly test between models and estimate parameters. Here, it will be important to be aware of the limitations and assumptions of the statistical techniques in addition to those of the model itself (see Rule 3). Rule 5: Be kind to your future self (and your readers), and minimise opportunities for errors You will need to revisit your code, sometimes many months after the analysis has been done and the paper has been written. This can either be a difficult exercise for your future self or a simple matter of glancing at your structured, documented, and organised code. Of course, it is best to aim for the latter. This means that code needs to be well structured and well commented so that you are able to easily reconstruct what has been done. To this end, this rule advises a number of good coding practices that are particularly important for simulation models, which can become large and unwieldy projects very quickly. All of these practices will benefit the readers of your model as much as they benefit your future self. First, the names of variables, functions, and parameters should be intuitive, consistent, and immediately intelligible. This makes the code more readable and minimises unnecessary comments. Similarly, adopting consistent formatting conventions (such as indentation and other whitespace), much like punctuation in a sentence, improves code readability. Commenting should be done with both your future self and your future readers in mind. For larger-scale multiresearcher and multiyear projects, more detailed documentation should be created and developed in tandem with the code. As a project grows, it becomes difficult to keep track of the interactions and dependencies between the different parts of the code. This complexity can be made more manageable by giving your code a well-defined and modular structure, meaning that sections of code should be divided into logically related units and the interfaces between these units of code should be well defined. Different programming languages offer different tools to achieve this, and the following advice applies equally to all of them. We’ll use the concept of functions as an example because this is the most fundamental and most commonly used. The aim here is to identify units of self-contained logic and create functions that encapsulate this logic. Functions should have a single clear purpose so, if you find you’re writing a function which performs multiple conceptual actions, consider breaking it up into 2 or more functions. Encapsulating logic in this way creates higher-order building blocks that can be reused and avoid code duplication. This reduces the opportunity for errors, not only because there will be less code to begin with, but also because each logical unit can be easily tested individually. In fact, it is good practice to test functions as you write them—this helps to avoid bugs in “low-level” functions that often manifest as difficult to diagnose problems downstream. The ubiquitously useful coding mantra “fail early and fail loudly” posits that it is better to raise an error as soon as it could feasibly be detected. So, for example, it is best to have functions check that their inputs are valid rather than allowing them to blindly compute and pass nonsensical output downstream. Despite taking these steps to manage the complexity of your project, you will write bugs. Everyone does and it is unavoidable. But, if you have written modular, well-structured code, it will be much easier to test the code, find the bugs, and squash them. PLOS COMPUTATIONAL BIOLOGY PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1009917 March 31, 2022 4 / 8 With a similar aim in mind, it is now common practice and explicitly required by some journals that model code is published alongside the paper it generates. This has a number of important benefits. It facilitates open and reproducible science. It also benefits you as a modeller. Bugs and coding issues can be identified by reviewers and readers who go through the code. This can be an important source of error checking. Finally, but equally importantly, you will need to test that the (now hopefully bug free) code actually does what it is supposed to in terms of the model’s dynamics. The full code, and each constituent part, should be put through its paces using simple test cases for which you have either good intuitions or solid analytical expectations to which the model outcomes should conform. Rule 6: Write practical code Practical code doesn’t have to be perfect, fully optimised, or even beautifully written, but it is fit for purpose, well tested, and clearly documented so that others (and you) can have confidence in it (see Rule 5). Practical coding also means using your coding time efficiently. Investing some time in learning to use the debugging tools available to your programming language will quickly pay for itself in time saved identifying and fixing bugs more quickly. There are a great many well-tested and already optimised scientific libraries/packages, and there is usually nothing to be gained by reimplementing functionality that already exists. Making use of good quality libraries will speed up development and reduce your opportunities to write bugs (see Rule 5), although it is critical to ensure that the methods used are appropriate for your context (see the note abou",,Yes,,2025-11-11T00:15:19.325Z
testinglargelanguage-2022,Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment,Lorenzo Bertolini; Julie Weeds; David Weir,2022,International Conference on Computational Linguistics,15,https://www.semanticscholar.org/paper/1606793daaa20d4a4a78e859c2fd6b4f7535680c,,,,,Yes,,2025-11-11T00:13:07.427Z
textgraphs2022shared-2022,TextGraphs 2022 Shared Task on Natural Language Premise Selection,Marco Valentino; Deborah Ferreira; Mokanarangan Thayaparan; André Freitas; Dmitry Ustalov,2022,Workshop on Graph-based Methods for Natural Language Processing,12,https://www.semanticscholar.org/paper/43845c54af0246baab17d572953e671487e1dcd1,,,,,Yes,,2025-11-11T00:13:07.427Z
theeffectsoftheactiv-2022,The Effects of the ACTIVE VALUES Program on Psychosocial Aspects and Executive Functions,José Francisco Jiménez-Parra; Noelia Belando-Pedreño; A. Valero-valenzuela,2022,International Journal of Environmental Research and Public Health,10,https://www.semanticscholar.org/paper/41e975b03d0f10c24340702e03191483ace7f27e,https://www.mdpi.com/1660-4601/20/1/595/pdf?version=1672318279,10.3390/ijerph20010595,"The main objective of this study was to implement an educational program named ACTIVE VALUES and to analyse the psychosocial and cognitive effects of its application. It is a quasi-experimental repeated measures research with a non-randomised experimental group (EG) and a control group (CG). The sample consisted of 102 students in the 6th grade of primary school, aged between 11 and 13 years (M = 11.59; SD = 0.60), and 4 teachers aged between 27 and 52 years (M = 38.5). The intervention program lasted 4 months, in which the EG implemented a teaching methodology based on the incorporation of classroom-based physical activity (CB-PA) in the structure of the Teaching for Personal and Social Responsibility (TPSR) model to develop personal and social values in students, as well as to reduce children’s sedentary behaviour in the classroom in different educational areas (e.g., mathematics, Spanish language, social sciences and natural sciences), while the CG used a conventional methodology based on direct instruction. The main results found show significant improvements in intrinsic motivation variables (including intrinsic motivation for achievement, stimulating experiences and knowledge), self-determination index, autonomy, relatedness, psychological mediators index, personal and social responsibility, teacher climate, intention to be physically active and executive functions in the EG, while amotivation values increased in the CG. In conclusion, interdisciplinary educational programs based on the combination of pedagogical models and active methodologies are postulated as methodological alternatives to achieve an integral and multilateral development of children and adolescents, as well as to improve the different learning domains of physical education, such as cognitive, social and motor. It is recommended that future research should consider longitudinal designs with mixed methods and follow-up data to assess learning retention, as well as larger samples and the measurement of a greater number of executive functions (e.g., inhibitory control and attention).",,Yes,,2025-11-11T00:15:19.325Z
theunreliabilityofex-2022,The Unreliability of Explanations in Few-Shot In-Context Learning,Xi Ye; Greg Durrett,2022,arXiv.org,36,https://www.semanticscholar.org/paper/e811e771f5950d86eafe50655c0d1e5b571e19b6,http://arxiv.org/pdf/2205.03401,10.48550/arXiv.2205.03401,,,Yes,,2025-11-11T00:15:14.026Z
themightyforcestatis-2022,The mighty force: statistical inference and high-dimensional statistics,E. Aurell; Jean Barbier; A. Decelle; R. Mulet,2022,arXiv.org,2,https://www.semanticscholar.org/paper/603977d9f5a6222ff926044f3089af679c985496,http://arxiv.org/pdf/2205.00750,10.48550/arXiv.2205.00750,"Inference is an English noun formed on the verb infer, from the Latin inferre, meaning to carry (fero) in or into (in-) something. That originally concrete meaning can still be felt in the portal quote of this chapter. In modern non-technical use the meaning of inference is more abstract, and rendered either as “A conclusion reached on the basis of evidence and reasoning” or as “The process of reaching such a conclusion” [1]. In scientific language these translate into characteristics of a phenomenon that are not observed directly, but which are arrived at (inferred) from observations with the help of mathematical and/or statistical methods, and those methods themselves. We will discuss three prominent examples of inference in both senses of modern usage, and how they naturally open up new perspectives and possibilities. Statistical physics is played out on the terrain between individual items and distributions over properties of items. The canonical example is the Langevin equation which describes the motion of a Brownian particle interacting with a thermal reservoir, and the Fokker-Planck equation which describes the evolution of the distribution of possible positions and velocities of the particle. In inference the goal can analogously be to reach one conclusion or retrieve one object, or to establish characteristics of a distribution over objects. The second kind of inference is also called statistical inference. We will here discuss inference in this sense. In the “big-data era”, statistical inference of different kinds is routinely performed based on data sets containing millions or even billions of samples, which themselves may live in spaces of tremendously large dimensionality. In this realm, classical statistical wisdom and tools fail: new mathematics and algorithms able to tackle the phenomena emerging in this regime are necessary. In the very same way, phase transitions were understood to emerge from the complexity (i.e. high-dimensionality) of physical systems more than a century ago, whose understanding required to develop statistical mechanics. It turns out that this is more than an analogy as the theory and methods to perform high-dimensional inference are directly connected to statistical mechanics as we will see in this chapter (and others in the book [2]). High-dimensional inference itself is part of a broader statistical theory of complex systems, referred to as high-dimensional statistics, a very active research field at the crossroads of (statistical) physics, computer science, information theory and machine learning, and which is the powerhouse of modern information processing systems.",arxiv:2205.00750,Yes,,2025-11-11T00:15:16.543Z
theoreticalandpaleoc-2022,Theoretical and paleoclimatic evidence for abrupt transitions in the Earth system,N. Boers; M. Ghil; T. Stocker,2022,Environmental Research Letters,64,https://www.semanticscholar.org/paper/8ffbb35aa8fae7df82b860417e8ab13d807413fa,https://doi.org/10.1088/1748-9326/ac8944,10.1088/1748-9326/ac8944,"Specific components of the Earth system may abruptly change their state in response to gradual changes in forcing. This possibility has attracted great scientific interest in recent years, and has been recognized as one of the greatest threats associated with anthropogenic climate change. Examples of such components, called tipping elements, include the Atlantic Meridional Overturning Circulation, the polar ice sheets, the Amazon rainforest, as well as the tropical monsoon systems. The mathematical language to describe abrupt climatic transitions is mainly based on the theory of nonlinear dynamical systems and, in particular, on their bifurcations. Applications of this theory to nonautonomous and stochastically forced systems are a very active field of climate research. The empirical evidence that abrupt transitions have indeed occurred in the past stems exclusively from paleoclimate proxy records. In this review, we explain the basic theory needed to describe critical transitions, summarize the proxy evidence for past abrupt climate transitions in different parts of the Earth system, and examine some candidates for future abrupt transitions in response to ongoing anthropogenic forcing. Predicting such transitions remains difficult and is subject to large uncertainties. Substantial improvements in our understanding of the nonlinear mechanisms underlying abrupt transitions of Earth system components are needed. We argue that such an improved understanding requires combining insights from (a) paleoclimatic records; (b) simulations using a hierarchy of models, from conceptual to comprehensive ones; and (c) time series analysis of recent observation-based data that encode the dynamics of the present-day Earth system components that are potentially prone to tipping.",,Yes,,2025-11-11T00:15:16.543Z
thinksumprobabilisti-2022,ThinkSum: Probabilistic reasoning over sets using large language models,Batu Mehmet Ozturkler; Nikolay Malkin; Zhen Wang; N. Jojic,2022,Annual Meeting of the Association for Computational Linguistics,23,https://www.semanticscholar.org/paper/370cea8b4220917f45a69358c0303df71f5063c7,http://arxiv.org/pdf/2210.01293,10.48550/arXiv.2210.01293,"Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think – retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum – probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.",arxiv:2210.01293,Yes,,2025-11-11T00:13:07.427Z
tobeornottobeaninteg-2022,To be or not to be an Integer? Encoding Variables for Mathematical Text,Deborah Ferreira; Mokanarangan Thayaparan; Marco Valentino; Julia Rozanova; André Freitas,2022,Findings,14,https://www.semanticscholar.org/paper/970d9ffcff27e1e1ce3f45734d59e9f99bef23cc,https://aclanthology.org/2022.findings-acl.76.pdf,10.18653/v1/2022.findings-acl.76,"The application of Natural Language Inference (NLI) methods over large textual corpora can facilitate scientific discovery, reducing the gap between current research and the available large-scale scientific knowledge. However, contemporary NLI models are still limited in interpreting mathematical knowledge written in Natural Language, even though mathematics is an integral part of scientific argumentation for many disciplines. One of the fundamental requirements towards mathematical language understanding, is the creation of models able to meaningfully represent variables. This problem is particularly challenging since the meaning of a variable should be assigned exclusively from its defining type, i.e., the representation of a variable should come from its context. Recent research has formalised the variable typing task, a benchmark for the understanding of abstract mathematical types and variables in a sentence. In this work, we propose VarSlot, a Variable Slot-based approach, which not only delivers state-of-the-art results in the task of variable typing, but is also able to create context-based representations for variables.",,Yes,,2025-11-11T00:14:11.169Z
towardgeneratingsyst-2022,Toward Generating System Architecture and Formal Functional Description in the Architecture Analysis & Design Language (AADL) With Structured Natural Language,Anshumaan Chauhan; Parth Ganeriwala; Chiradeep Sen; S. Bhattacharyya,2022,Conference on Computability in Europe,3,https://www.semanticscholar.org/paper/765631db254ee8c5df7f8db074ec35a513270728,,10.1115/detc2022-90002,"
 Model based engineering has enabled automated analytical reasoning early in the design phase. As a result, inconsistencies and design errors can be captured early in the development lifecycle. But there is still a gap in the natural language-based specifications and its actual implementation. This is because the formal method-based tools utilize mathematical principles and theories of computation that require specific skills, thus reducing the usability of model-based engineering. Natural language is the most widely used method to represent specifications. So, it is intuitive to utilize natural language-based representation to generate system and formal annotations such that it will enable automated architectural analysis with much wider acceptance leading to a much broader impact. In our paper we focus on designing the above-mentioned approach that integrates representation of the specifications in a subset of English language which can then be used to generate system architecture in Architecture Analysis and Design Language along with the generation of functional specifications. We illustrate our approach by validating it with use cases from the aerospace and electromechanical domains.",,Yes,,2025-11-11T00:14:11.169Z
towardabroadai-2022,Toward a broad AI,Sepp Hochreiter,2022,Communications of the ACM,17,https://www.semanticscholar.org/paper/4510b0c02aca42b7c7ed9931d1c265301c5f5ebd,https://dl.acm.org/doi/pdf/10.1145/3512715,10.1145/3512715,"I M A G E B Y C H R I S T O P H B U R G S T E D T particular aims at a new level of AI—a “broad AI”—with considerably enhanced and broader capabilities for skill acquisition and problem solving.3 We contrast “broad AI” to “narrow AI,” which are the AI systems currently applied. A broad AI considerably surpasses a narrow AI in the following essential properties: when learned models must quickly adapt to new situations, for new customers, new products, new processes, new workflows, or new sensory inputs. With the advent of large corpora of unlabeled data in vision and language, selfsupervised learning based on contrastive learning became very popular. Either views of images are contrasted with views of other images or text descriptions of images are contrasted with text descriptions of other images. Contrastive Language-Image Pretraining (CLIP)10 yielded very impressive results at zero-shot transfer learning. The CLIP model has the potential to become one of the most important foundation models.2 A model with high zero-shot transfer learning knowledge transfer and interaction, adaptability and robustness, abstraction and advanced reasoning, and efficiency (as illustrated in the accompanying figure). A broad AI is a sophisticated and adaptive system, which successfully performs any cognitive task by virtue of its sensory perception, previous experience, and learned skills. To improve adaptability and robustness, a broad AI utilizes few-shot learning, self-supervised learning with contrastive learning, and processes sensory inputs using context and memory. Few-shot learning trains models with a small amount of data using prior knowledge or previous experience. Few-shot learning has a plethora of real-world applications, for example, D E S P I T E B I G",,Yes,,2025-11-11T00:15:16.541Z
towardsidentifyingso-2022,"Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks",Jingyan Zhou; Jiawen Deng; Fei Mi; Yitong Li; Yasheng Wang; Minlie Huang; Xin Jiang; Qun Liu; Helen M. Meng,2022,arXiv.org,19,https://www.semanticscholar.org/paper/6626dadc76d1af9d19fc4c2a4fa3a4cf414e62e0,,,,,Yes,,2025-11-11T00:15:16.543Z
towardsmultihopopend-2022,Towards Multi-Hop Open-Domain Question Answering by Dense Retrieval,Zhao Meng Prof; Dr. Roger Wattenhofer,2022,,0,https://www.semanticscholar.org/paper/bc5def224c0afbd5d32b8542e43dfd774d650202,,,,,Yes,,2025-11-11T00:15:16.543Z
towardsreasoninginla-2022,Towards Reasoning in Large Language Models: A Survey,Jie Huang; K. Chang,2022,Annual Meeting of the Association for Computational Linguistics,758,https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,http://arxiv.org/pdf/2212.10403,10.48550/arXiv.2212.10403,"Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.",arxiv:2212.10403,Yes,,2025-11-11T00:13:07.427Z
towardssimplifyingan-2022,Towards Simplifying and Formalizing UML Class Diagram Generalization/Specialization Relationship with Mathematical Set Theory,Kruti Shah; Emanuel S. Grant,2022,International Conference on Information System and Data Mining,4,https://www.semanticscholar.org/paper/5b2b3ec307e1075e9218919e7e635d5ac37120c3,,10.1145/3546157.3546171,"The Unified Modeling Language (UML) is considered the de facto standard for object-oriented software model development. This makes it appropriate to be used in academia courses at both the graduate and undergraduate levels of education. Some challenges to using the UML is academia are its large number of model concepts and the imprecise semantic of some of these concepts. These challenges are daunting for students who are being introduced to the UML. One approach that can be taken in teaching UML towards addressing these concerns is to limit the number of UML concepts taught and recognize that students may not be able to develop correct UML system models. This approach leads to research work that develop a limited set of UML model concepts that are fewer in number and have more precise semantics. In this paper, we present a new approach to resolve an aspect of this problem by simplifying the generalization/specialization semantics of the class diagram through the application of mathematical formality to usage of these class diagram concepts. This research work derives a core set of concepts suitable for graduate and undergraduate comprehension of UML modeling and defines more precise semantics for those modeling concepts. The applicable mathematical principles applied in this work are from the domains of set theory and predicate logic. This approach is particularly relevant for the pedagogy of software engineering and the development of software systems that require a high level of reliability.",,Yes,,2025-11-11T00:14:11.169Z
towardsunderstanding-2022,Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters,Boshi Wang; Sewon Min; Xiang Deng; Jiaming Shen; You Wu; Luke Zettlemoyer; Huan Sun,2022,Annual Meeting of the Association for Computational Linguistics,302,https://www.semanticscholar.org/paper/35922cd0d6b17e45320917338e9f98cb5c1a4f6f,http://arxiv.org/pdf/2212.10001,10.48550/arXiv.2212.10001,"Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs’ capability to learn to reason in context.",arxiv:2212.10001,Yes,,2025-11-11T00:15:14.026Z
towardsverifyingumlc-2022,Towards Verifying UML Class Diagram and Formalizing Generalization/Specialization Relationship with Mathematical Set Theory,Kruti Shah; Emanuel S. Grant,2022,Journal of Software,0,https://www.semanticscholar.org/paper/d635df2ad885d6a273a32ae5f4d970b1c7f2f8e3,https://doi.org/10.17706/jsw.17.6.292-303,10.17706/jsw.17.6.292-303,"The Unified Modeling Language (UML) is considered the de facto standard for object-oriented software model development. This makes it appropriate to be used in academia courses at both the graduate and undergraduate levels of education. Some challenges to using the UML is academia are its large number of model concepts and the imprecise semantic of some of these concepts. These challenges are daunting for students who are being introduced to the UML. One approach that can be taken in teaching UML towards addressing these concerns is to limit the number of UML concepts taught and recognize that students may not be able to develop correct UML system models. This approach leads to research work that develop a limited set of UML model concepts that are fewer in number and have more precise semantics. In this paper, we present a new approach to resolve an aspect of this problem by simplifying the generalization/specialization semantics of the class diagram through the application of mathematical formality to usage of these class diagram concepts. Along with that, we discuss the progress of research in the area of verification of UML class models. This research work derives a core set of concepts suitable for graduate and undergraduate comprehension of UML modeling and defines more precise semantics for those modeling concepts. The applicable mathematical principles applied in this work are from the domains of set theory and predicate logic. This approach is particularly relevant for the pedagogy of software engineering and the development of software systems that require a high level of reliability.",,Yes,,2025-11-11T00:14:11.169Z
towardsamathematicsf-2022,Towards a Mathematics Formalisation Assistant using Large Language Models,Ayush Agrawal; Siddhartha Gadgil; Navin Goyal; Ashvni Narayanan; Anand Tadipatri,2022,arXiv.org,18,https://www.semanticscholar.org/paper/b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce,https://arxiv.org/pdf/2211.07524,10.48550/arXiv.2211.07524,"Mathematics formalisation is the task of writing mathematics (i.e., definitions, theorem statements, proofs) in natural language, as found in books and papers, into a formal language that can then be checked for correctness by a program. It is a thriving activity today, however formalisation remains cumbersome. In this paper, we explore the abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover. We find that with careful inputdependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75% accuracy for 120 theorem statements. For proofs quantitative analysis is infeasible and we undertake a detailed case study. We choose a diverse set of 13 theorems at undergrad level with proofs that fit in two-three paragraphs. We show that with a new prompting strategy Codex can formalise these proofs in natural language with at least one out of twelve Codex completion being easy to repair into a complete proof. This is surprising as essentially no aligned data exists for formalised mathematics, particularly for proofs. These results suggest that large language models are a promising avenue towards fully or partially automating formalisation.",arxiv:2211.07524,Yes,,2025-11-11T00:13:07.427Z
transcendingscalingl-2022,Transcending Scaling Laws with 0.1% Extra Compute,Yi Tay; Jason Wei; Hyung Won Chung; Vinh Q. Tran; David R. So; Siamak Shakeri; Xavier García; H. Zheng; J. Rao; A. Chowdhery; Denny Zhou; Donald Metzler; Slav Petrov; N. Houlsby; Quoc V. Le; Mostafa Dehghani,2022,Conference on Empirical Methods in Natural Language Processing,71,https://www.semanticscholar.org/paper/1bb6d5761903c7ac978188ae36e2648905e95dc5,https://arxiv.org/pdf/2210.11399,10.48550/arXiv.2210.11399,"Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving $\sim$4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.",arxiv:2210.11399,Yes,,2025-11-11T00:15:14.026Z
transferlearningwith-2022,Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning,Roshanak Mirzaee; Parisa Kordjamshidi,2022,Conference on Empirical Methods in Natural Language Processing,42,https://www.semanticscholar.org/paper/e3cd9f01f87a601b274b4ef6513a84c8cde03214,http://arxiv.org/pdf/2210.16952,10.48550/arXiv.2210.16952,"Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first dataset is synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL). Compared to previous SQA datasets, we include a larger variety of spatial relation types and spatial expressions. Our data generation process is easily extendable with new spatial expression lexicons. The second one is a real-world SQA dataset with human-generated questions built on an existing corpus with SPRL annotations. This dataset can be used to evaluate spatial language processing models in realistic situations. We show pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.",arxiv:2210.16952,Yes,,2025-11-11T00:14:11.169Z
triplefactretrievera-2022,Triple-Fact Retriever: An explainable reasoning retrieval model for multi-hop QA problem,Cheng Wu; Enrui Hu; Ke Zhan; Lan Luo; Xinyu Zhang; Hao Jiang; Qirui Wang; Zhao Cao; Fan Yu; Lei Chen,2022,IEEE International Conference on Data Engineering,8,https://www.semanticscholar.org/paper/dd77bff42060109c9b2e3e7d87cc9342309c3952,,10.1109/icde53745.2022.00095,"Nowadays, multi-hop question answer (QA) problem is challenging and not well solved in the QA community. The dominant bottleneck of the multi-hop QA problem is the need for a reasoning retriever to fetch a document path from an open-domain corpus (e.g., Wikipedia). A reasoning retriever aims to collect an evidence document from large corpora at one hop retrieval and aggregate the evidence for subsequent hop retrieval, which yields a document path after multi-hop retrieval. There exist two challenges, (1) to fetch the evidence document in an efficient and explainable way at one hop retrieval and (2) to update the question information by aggregating the evidence from the retrieved document after each hop retrieval. To address these two challenges, we propose a triple-fact-based retrieval model to effectively retrieve a related document path in an explainable way for each question. We extract a structured representation from the unstructured document and utilize the knowledge of pre-trained language model (PLM) to do the semantic-level matching between the question and document. We evaluate the proposed Triple-fact Retriever model on the recently proposed open-domain multi-hop QA dataset, HotpotQA, and a cross-document multi-step Reading Comprehension dataset, Wikihop. The results11The source code is available on our website: https://github.com/Rebaccamin/triple_retriever. demonstrate that the Triple-fact retriever outperforms the existing baseline retrieval works.",,Yes,,2025-11-11T00:13:07.427Z
truedetectiveachalle-2022,True Detective: A Challenging Benchmark for Deep Abductive Reasoning in Foundation Models,Maksym Del; Mark Fishel,2022,arXiv.org,0,https://www.semanticscholar.org/paper/b1054e448186822bfe9445bb6f4533d157e3da5e,http://arxiv.org/pdf/2212.10114,10.48550/arXiv.2212.10114,,,Yes,,2025-11-11T00:14:11.169Z
truedetectiveadeepab-2022,True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4,Maksym Del; Mark Fishel,2022,STARSEM,26,https://www.semanticscholar.org/paper/256ef1f8d0ea2982cc50d3e85e5f1b4920f037fe,https://aclanthology.org/2023.starsem-1.28.pdf,10.18653/v1/2023.starsem-1.28,"Large language models (LLMs) have demonstrated solid zero-shot reasoning capabilities, which is reflected in their performance on the current test tasks. This calls for a more challenging benchmark requiring highly advanced reasoning ability to be solved. In this paper, we introduce such a benchmark, consisting of 191 long-form (1200 words on average) mystery narratives constructed as detective puzzles. Puzzles are sourced from the “5 Minute Mystery” platform and include a multiple-choice question for evaluation. Only 47% of humans solve a puzzle successfully on average, while the best human solvers achieve over 80% success rate. We show that GPT-3 models barely outperform random on this benchmark (with 28% accuracy) while state-of-the-art GPT-4 solves only 38% of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area. Our work introduces a challenging benchmark for future studies on reasoning in language models and contributes to a better understanding of the limits of LLMs’ abilities.",arxiv:2212.10114,Yes,,2025-11-11T00:14:11.169Z
tutorialneurosymboli-2022,Tutorial: Neuro-symbolic AI for Mental Healthcare,Kaushik Roy; Usha Lokala; Manas Gaur; Amit P. Sheth,2022,International Conference on AI-ML-Systems,10,https://www.semanticscholar.org/paper/2545393a22c688acb3a6c12098479eb6a04ab7d3,https://mdsoar.org/bitstreams/4e99ba2e-5237-421c-b19c-983124bbced7/download,10.1145/3564121.3564817,"Artificial Intelligence (AI) systems for mental healthcare (MHCare) have been ever-growing after realizing the importance of early interventions for patients with chronic mental health (MH) conditions. Social media (SocMedia) emerged as the go-to platform for supporting patients seeking MHCare. The creation of peer-support groups without social stigma has resulted in patients transitioning from clinical settings to SocMedia supported interactions for quick help. Researchers started exploring SocMedia content in search of cues that showcase correlation or causation between different MH conditions to design better interventional strategies. User-level Classification-based AI systems were designed to leverage diverse SocMedia data from various MH conditions, to predict MH conditions. Subsequently, researchers created classification schemes to measure the severity of each MH condition. Such ad-hoc schemes, engineered features, and models not only require a large amount of data but fail to allow clinically acceptable and explainable reasoning over the outcomes. To improve Neural-AI for MHCare, infusion of clinical symbolic knowledge that clinicans use in decision making is required. An impactful use case of Neural-AI systems in MH is conversational systems. These systems require coordination between classification and generation to facilitate humanistic conversation in conversational agents (CA). Current CAs with deep language models lack factual correctness, medical relevance, and safety in their generations, which intertwine with unexplainable statistical classification techniques. This lecture-style tutorial will demonstrate our investigations into Neuro-symbolic methods of infusing clinical knowledge to improve the outcomes of Neural-AI systems to improve interventions for MHCare:(a) We will discuss the use of diverse clinical knowledge in creating specialized datasets to train Neural-AI systems effectively. (b) Patients with cardiovascular disease express MH symptoms differently based on gender differences. We will show that knowledge-infused Neural-AI systems can identify gender-specific MH symptoms in such patients. (c) We will describe strategies for infusing clinical process knowledge as heuristics and constraints to improve language models in generating relevant questions and responses.",,Yes,,2025-11-11T00:15:19.325Z
understandingnatural-2022,Understanding Natural Language in Context,Avichai Levy; E. Karpas,2022,International Conference on Automated Planning and Scheduling,1,https://www.semanticscholar.org/paper/b13cb83f9a9e8cea529c527f76ab2b50ab879bbc,https://arxiv.org/pdf/2205.12691,10.48550/arXiv.2205.12691,"Recent years have seen an increasing number of applications that have a natural language interface, either in the form of chatbots or via personal assistants such as Alexa (Amazon), Google Assistant, Siri (Apple), and Cortana (Microsoft). To use these applications, a basic dialog between the assistant and the human is required. While this kind of dialog exists today mainly within static robots that do not make any movement in the household space, the challenge of reasoning about the information conveyed by the environment increases significantly when dealing with robots that can move and manipulate objects in our home environment.
In this paper, we focus on cognitive robots, which have some knowledge-based models of the world and operate by reasoning and planning with this model. Thus, when the robot and the human communicate, there is already some formalism they can use -- the robot’s knowledge representation formalism. In this paper we describe an approach for translating natural language directives into the robot's formalism, allowing much more complicated household tasks to be completed. We do so by combining off-the-shelf SoTA large language models, planning tools, and the robot knowledge of the state of the world and of its own model. This results in much more accurate interpretation of directives in natural language.",arxiv:2205.12691,Yes,,2025-11-11T00:15:14.026Z
unigeounifyinggeomet-2022,UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression,Jiaqi Chen; Tong Li; Jinghui Qin; Pan Lu; Liang Lin; Chongyu Chen; Xiaodan Liang,2022,Conference on Empirical Methods in Natural Language Processing,133,https://www.semanticscholar.org/paper/72fce949725b20428e5f56247fef5c6bd1ce6154,https://arxiv.org/pdf/2212.02746,10.48550/arXiv.2212.02746,"Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning capability on multiple math tasks. However, in essence, these two tasks have similar problem representations and overlapped math knowledge which can improve the understanding and reasoning ability of a deep model on both two tasks. Therefore, we construct a large-scale Unified Geometry problem benchmark, UniGeo, which contains 4,998 calculation problems and 9,543 proving problems. Each proving problem is annotated with a multi-step proof with reasons and mathematical expressions. The proof can be easily reformulated as a proving sequence that shares the same formats with the annotated program sequence for calculation problems. Naturally, we also present a unified multi-task Geometric Transformer framework, Geoformer, to tackle calculation and proving problems simultaneously in the form of sequence generation, which finally shows the reasoning ability can be improved on both two tasks by unifying formulation. Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that aims to predict the mathematical expressions in the problem solution, thus improving the Geoformer model. Experiments on the UniGeo demonstrate that our proposed Geoformer obtains state-of-the-art performance by outperforming task-specific model NGS with over 5.6% and 3.2% accuracies on calculation and proving problems, respectively.",arxiv:2212.02746,Yes,,2025-11-11T00:13:07.427Z
unikgqaunifiedretrie-2022,UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph,Jinhao Jiang; Kun Zhou; Wayne Xin Zhao; Ji-rong Wen,2022,International Conference on Learning Representations,122,https://www.semanticscholar.org/paper/2d01da2c9ece0969d6ec56d22f78caf57050fc03,http://arxiv.org/pdf/2212.00959,10.48550/arXiv.2212.00959,"Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the answer entities that are multiple hops away from the topic entities mentioned in a natural language question on a large-scale Knowledge Graph (KG). To cope with the vast search space, existing work usually adopts a two-stage approach: it first retrieves a relatively small subgraph related to the question and then performs the reasoning on the subgraph to find the answer entities accurately. Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence. In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning. For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs. For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies. Compared with previous studies, our approach is more unified, tightly relating the retrieval and reasoning stages. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our method on the multi-hop KGQA task. Our codes and data are publicly available at~\url{https://github.com/RUCAIBox/UniKGQA}.",arxiv:2212.00959,Yes,,2025-11-11T00:13:07.427Z
unitedmonoidsfinding-2022,United Monoids: Finding Simplicial Sets and Labelled Algebraic Graphs in Trees,A. Mokhov,2022,"The Art, Science, and Engineering of Programming",3,https://www.semanticscholar.org/paper/9af58dfedb0306ff4b5071d3bc64e8c969e5cabb,https://arxiv.org/pdf/2202.09230v1,10.22152/programming-journal.org/2022/6/12,"Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the “algebraic graphs” approach (2017). It uses an algebraic data type to represent graphs and has attracted users, including from industry, due to its emphasis on equational reasoning and making a common class of bugs impossible by eliminating internal invariants. The previous formulation of algebraic graphs did not support edge labels, which was a serious practical limitation. In this paper, we redesign the main algebraic data type and remove this limitation. We follow a fairly standard approach of parameterising a data structure with a semiring of edge labels. The new formulation is both more general and simpler: the two operations for composing graphs used in the previous work can now be obtained from a single operation by fixing the semiring parameter to zero and one, respectively. By instantiating the new data type with different semirings, and working out laws for interpreting the resulting expression trees, we discover an unusual algebraic structure, which we call “united monoids”, that is, a pair of monoids whose unit elements coincide. We believe that it is worth studying united monoids in their full generality, going beyond the graphs which prompted their discovery. To that end, we characterise united monoids with a minimal set of axioms, prove a few basic theorems, and discuss several notable examples. We validate the presented approach by implementing it in the open-source algebraic-graphs library. Our theoretical contributions are supported by proofs that are included in the paper and have also been machinechecked in Agda. By extending algebraic graphs with support for edge labels, we make them suitable for a much larger class of possible applications. By studying united monoids, we provide a theoretical foundation for further research in this area. ACM CCS 2012 Mathematics of computing→ Graph theory;",arxiv:2202.09230,Yes,,2025-11-11T00:15:19.325Z
unpackinglargelangua-2022,Unpacking Large Language Models with Conceptual Consistency,Pritish Sahu; Michael Cogswell; Yunye Gong; Ajay Divakaran,2022,arXiv.org,18,https://www.semanticscholar.org/paper/4f4a80148cb8f328aeaee68b34f9797cfb5ea150,http://arxiv.org/pdf/2209.15093,10.48550/arXiv.2209.15093,"If a Large Language Model (LLM) answers""yes""to the question""Are mountains tall?""then does it know what a mountain is? Can you rely on it responding correctly or incorrectly to other questions about mountains? The success of Large Language Models (LLMs) indicates they are increasingly able to answer queries like these accurately, but that ability does not necessarily imply a general understanding of concepts relevant to the anchor query. We propose conceptual consistency to measure a LLM's understanding of relevant concepts. This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are. To compute it we extract background knowledge by traversing paths between concepts in a knowledge base and then try to predict the model's response to the anchor query from the background knowledge. We investigate the performance of current LLMs in a commonsense reasoning setting using the CSQA dataset and the ConceptNet knowledge base. While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency. Our analysis also shows significant variation in conceptual consistency across different kinds of relations, concepts, and prompts. This serves as a step toward building models that humans can apply a theory of mind to, and thus interact with intuitively.",arxiv:2209.15093,Yes,,2025-11-11T00:13:07.427Z
usingcognitivepsycho-2022,Using cognitive psychology to understand GPT-3,Marcel Binz; Eric Schulz,2022,Proceedings of the National Academy of Sciences of the United States of America,578,https://www.semanticscholar.org/paper/fa3609e00f9f422a309c621a35394c4a38f88687,https://doi.org/10.1073/pnas.2218523120,10.1073/pnas.2218523120,"Significance Language models are trained to predict the next word for a given text. Recently, it has been shown that scaling up these models causes them to not only generate language but also to solve challenging reasoning problems. The present article lets a large language model (GPT-3) do experiments from the cognitive psychology literature. We find that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books. We additionally utilize analysis tools from the cognitive psychology literature to demystify how GPT-3 solves different tasks and use the thereby acquired insights to make recommendations for how to improve future model iterations.",arxiv:2206.14576,Yes,,2025-11-11T00:14:11.169Z
utilizingbackgroundk-2022,Utilizing Background Knowledge for Robust Reasoning over Traffic Situations,Jiarui Zhang; Filip Ilievski; Aravinda Kollaa; Jonathan M Francis; Kaixin Ma; A. Oltramari,2022,arXiv.org,2,https://www.semanticscholar.org/paper/fb591c46afb5f1f2fc380a5a5a55f9cf9e485edb,http://arxiv.org/pdf/2212.07798,10.48550/arXiv.2212.07798,"Understanding novel situations in the traffic domain requires an intricate combination of domain-specific and causal commonsense knowledge. Prior work has provided sufficient perception-based modalities for traffic monitoring, in this paper, we focus on a complementary research aspect of Intelligent Transportation: traffic understanding. We scope our study to text-based methods and datasets given the abundant commonsense knowledge that can be extracted using language models from large corpus and knowledge graphs. We adopt three knowledge-driven approaches for zero-shot QA over traffic situations, based on prior natural language inference methods, commonsense models with knowledge graph self-supervision, and dense retriever-based models. We constructed two text-based multiple-choice question answering sets: BDD-QA for evaluating causal reasoning in the traffic domain and HDT-QA for measuring the possession of domain knowledge akin to human driving license tests. Among the methods, Unified-QA reaches the best performance on the BDD-QA dataset with the adaptation of multiple formats of question answers. Language models trained with inference information and commonsense knowledge are also good at predicting the cause and effect in the traffic domain but perform badly at answering human-driving QA sets. For such sets, DPR+Unified-QA performs the best due to its efficient knowledge extraction.",arxiv:2212.07798,Yes,,2025-11-11T00:14:11.169Z
vaultaugmentingthevi-2022,VAuLT: Augmenting the Vision-and-Language Transformer with the Propagation of Deep Language Representations,Georgios Chochlakis; Tejas Srinivasan; Jesse Thomason; Shrikanth S. Narayanan,2022,arXiv.org,11,https://www.semanticscholar.org/paper/26bf29a039c09211c854da0419cd17238fce579b,http://arxiv.org/pdf/2208.09021,10.48550/arXiv.2208.09021,,,Yes,,2025-11-11T00:14:11.169Z
vgstoreamultimodalex-2022,VGStore: A Multimodal Extension to SPARQL for Querying RDF Scene Graph,Yanzeng Li; Zilong Zheng; Wenjuan Han; Lei Zou,2022,International Workshop on the Semantic Web,2,https://www.semanticscholar.org/paper/73bedd5ea4b4342e1a7389055a051e9f3e2e85af,http://arxiv.org/pdf/2209.02981,10.48550/arXiv.2209.02981,"Semantic Web technology has successfully facilitated many RDF models with rich data representation methods. It also has the potential ability to represent and store multimodal knowledge bases such as multimodal scene graphs. However, most existing query languages, especially SPARQL, barely explore the implicit multimodal relationships like semantic similarity, spatial relations, etc. We first explored this issue by organizing a large-scale scene graph dataset, namely Visual Genome, in the RDF graph database. Based on the proposed RDF-stored multimodal scene graph, we extended SPARQL queries to answer questions containing relational reasoning about color, spatial, etc. Further demo (i.e., VGStore) shows the effectiveness of customized queries and displaying multimodal data.",arxiv:2209.02981,Yes,,2025-11-11T00:15:19.325Z
vquadvideoquestionan-2022,VQuAD: Video Question Answering Diagnostic Dataset,Vikrant Gupta; Badri N. Patro; Hemant Parihar; Vinay P. Namboodiri,2022,2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),11,https://www.semanticscholar.org/paper/234619e9eb3706c0c794e1d31ce75cb07dde39a2,https://purehost.bath.ac.uk/ws/files/239901811/VQuAD_Dataset.pdf,10.1109/WACVW54805.2022.00034,"In this paper, we investigate the task of Video based Question Answering. We provide a diagnostic dataset that can be used to evaluate the extent of reasoning abilities of various methods for solving this task. Previous datasets proposed for this task do not have this ability. Our dataset is large scale (around 1.3 million questions jointly for train and test) and evaluates both the spatial and temporal properties and the relationship between various objects for these properties. We evaluate state of the art language model (BERT) as a baseline to understand the extent of correlation based on language features alone. Other existing networks are then used to combine video features along with language features for solving this task. Unfortunately, we observe that the currently prevalent systems do not perform significantly better than the language baseline. We hypothesise that this is due to our efforts in ensuring that no obvious biases exist in this dataset and the dataset is balanced. To make progress, the learning techniques needs to obtain an ability to reason, going beyond basic correlation of biases. This is an interesting and significant challenge provided through our work. We release our dataset and source code for our baseline modules in the following webpage https://delta-lab-iitk.github.io/vquad/.",,Yes,,2025-11-11T00:15:14.026Z
valueaddedscoresshow-2022,Value-added scores show limited stability over time in primary school,Valentin Emslander; J. Levy; Ronny Scherer; Antoine Fischbach,2022,PLoS ONE,0,https://www.semanticscholar.org/paper/9197aac8ee8d6f26a8998a93ec4e037cf8986313,https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0279255&type=printable,10.1371/journal.pone.0279255,"Value-added (VA) models are used for accountability purposes and quantify the value a teacher or a school adds to their students’ achievement. If VA scores lack stability over time and vary across outcome domains (e.g., mathematics and language learning), their use for high-stakes decision making is in question and could have detrimental real-life implications: teachers could lose their jobs, or a school might receive less funding. However, school-level stability over time and variation across domains have rarely been studied together. In the present study, we examined the stability of VA scores over time for mathematics and language learning, drawing on representative, large-scale, and longitudinal data from two cohorts of standardized achievement tests in Luxembourg (N = 7,016 students in 151 schools). We found that only 34–38% of the schools showed stable VA scores over time with moderate rank correlations of VA scores from 2017 to 2019 of r = .34 for mathematics and r = .37 for language learning. Although they showed insufficient stability over time for high-stakes decision making, school VA scores could be employed to identify teaching or school practices that are genuinely effective—especially in heterogeneous student populations.",,Yes,,2025-11-11T00:15:19.325Z
verificationofradarp-2022,Verification of radar precipitation nowcasting of significant areas using the generalized Pareto distribution. Part 1: Elements of theory and methods for estimating parameters,A. Muravev; A. Bundel; D. Kiktev; A. Smirnov,2022,Hydrometeorological research and forecasting,2,https://www.semanticscholar.org/paper/4a341336ed762e7d8c33f465305367d0ccea07bb,https://cran.r-project.org/web/packages/SpatialVx/SpatialVx.pdf,10.37162/2618-9631-2022-3-6-41,"The assessments of nowcasting of large precipitation areas accumulated in the last few years at the Hydrometeorological Research Center of the Russian Federation are presented in two parts complemented by a discussion of methodological problems in the first part and application problems in the second part of the paper. The division is largely due to the sharp distinction between the theoretical modeling of extremes with a relatively free choice of assumptions and the statistical analysis of the distribution ""tails"" in rapidly ""impoverishing"" samples. The contrast between these parts is exacerbated by the responsibility we attribute to the statistical inference relating to extreme and, as a rule, dangerous events. The first part deals with the description of two classical models of the extreme value theory for independent one-dimensional random variables (""block maxima"") and for threshold exceedances in stationary time series (""peaks over threshold""). The article explores problems arising from violation of the theoretical results and carries a brief overview of the methods of addressing such problems when extremes are modeled using real data, including those from the field of meteorology. Special attention is given to the distributions with ""heavy"" tails. Methods and formulas for estimating important characteristics, including the parameters of limiting distributions, are discussed that are borrowed from the references in the documentation of computational mathematical packages of the R language repository. Keywords: precipitation nowcasting, extreme value theory, statistical modeling of extremes, heavy distribution tails, mathematical packages for fitting extreme value distributions",,Yes,,2025-11-11T00:15:19.325Z
verifyingdynamictrai-2022,Verifying Dynamic Trait Objects in Rust,Alexa VanHattum; Daniel Schwartz-Narbonne; Nathan Chong; Adrian Sampson,2022,2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),40,https://www.semanticscholar.org/paper/1ff44db7ee219174273efba0e4a42bf24c1807cf,https://dl.acm.org/doi/pdf/10.1145/3510457.3513031,10.1145/3510457.3513031,"Rust has risen in prominence as a systems programming language in large part due to its focus on reliability. The language's advanced type system and borrow checker eliminate certain classes of memory safety violations. But for critical pieces of code, teams need assurance beyond what the type checker alone can provide. Verification tools for Rust can check other properties, from memory faults in unsafe Rust code to user-defined correctness assertions. This paper particularly focuses on the challenges in reasoning about Rust's dynamic trait objects, a feature that provides dynamic dispatch for function abstractions. While the explicit dyn keyword that denotes dynamic dispatch is used in 37% of the 500 most-downloaded Rust libraries (crates), dynamic dispatch is implicitly linked into 70%. To our knowledge, our open-source Kani Rust Verifier is the first symbolic modeling checking tool for Rust that can verify correctness while supporting the breadth of dynamic trait objects, including dynamically dispatched closures. We show how our system uses semantic trait information from Rust's Mid-level Intermediate Representation (an advantage over targeting a language-agnostic level such as LLVM) to improve verification performance by 5%–15× for examples from open-source virtualization software. Finally, we share an open-source suite of verification test cases for dynamic trait objects.",,Yes,,2025-11-11T00:15:14.026Z
videoaslearningtools-2022,Video as learning tools for middle school mathematics: A case of probability using realistic mathematics education,Meiliasari Meiliasari; D. A. Wijayanti; Dear Noer,2022,AIP Conference Proceedings,0,https://www.semanticscholar.org/paper/a4fa100a2467f7694ca4cb2c24a9db282a7fc00b,https://doi.org/10.1063/5.0105119,10.1063/5.0105119,"The Coronavirus Disease (Covid-19) pandemic in Indonesia has an impact for various sectors, one of which is education. Ministry of Education and Culture provides a policy for school to carry out the learning process from home through online learning or distance learning as an effort to prevent the spread of Covid-19. Videos can be used to support teachers and students in learning mathematics during distance learning. This study aims to develop videos as learning tools of probability in middle school with realistic mathematics education approach. This methodology used is a Research and Development with Brog and Gall model five stages, namely product analysis, product design, product evaluation and revision, small group field test and revision, large group field test and final product. The participants of this study were 26 students of grade 8th. The prototype videos was evaluated by content and language experts obtained a score 83%, and 84% by media experts with criteria very feasible. The student questionnaire average for this video product increase from 75% in small group became 82% in the large group. This shows that the probability learning video for middle school with realistic mathematics education approach is feasible to be used as mathematics media learning. © 2022 American Institute of Physics Inc.. All rights reserved.",,Yes,,2025-11-11T00:15:16.543Z
visualabductivereaso-2022,Visual Abductive Reasoning,Chen Liang; Wenguan Wang; Tianfei Zhou; Yi Yang,2022,Computer Vision and Pattern Recognition,45,https://www.semanticscholar.org/paper/838a2297b94f7bad96c4f8370a5f58487f194f44,https://arxiv.org/pdf/2203.14040,10.1109/CVPR52688.2022.01512,"Abductive reasoning seeks the likeliest possible explanation for partial observations. Although abduction is frequently employed in human daily reasoning, it is rarely explored in computer vision literature. In this paper, we propose a new task and dataset, Visual Abductive Reasoning (VAR), for examining abductive reasoning ability of machine intelligence in everyday visual situations. Given an incomplete set of visual events, AI systems are required to not only describe what is observed, but also infer the hypothesis that can best explain the visual premise. Based on our large-scale VAR dataset, we devise a strong baseline model, REASONER (causal-and-cascaded reasoning Transformer). First, to capture the causal structure of the observations, a contextualized directional position embedding strategy is adopted in the encoder, that yields discriminative represen-tations for the premise and hypothesis. Then, multiple de-coders are cascaded to generate and progressively refine the premise and hypothesis sentences. The prediction scores of the sentences are used to guide cross-sentence information flow in the cascaded reasoning procedure. Our VAR bench-marking results show that REASONER surpasses many famous video-language models, while still being far behind human performance. This work is expected to foster future efforts in the reasoning-beyond-observation paradigm.",arxiv:2203.14040,Yes,,2025-11-11T00:14:11.169Z
visualknowledgelearn-2022,Visual Knowledge Learning,Xinlei Chen,2022,,0,https://www.semanticscholar.org/paper/b76225abf969f2ccbae2d5a96c8ba1e690547ec3,,10.1184/r1/21637757.v1,,,Yes,,2025-11-11T00:15:19.325Z
visualprogrammingcom-2022,Visual Programming: Compositional visual reasoning without training,Tanmay Gupta; Aniruddha Kembhavi,2022,Computer Vision and Pattern Recognition,524,https://www.semanticscholar.org/paper/af1c871282ec122869d03f5420ef5d9143358a91,https://arxiv.org/pdf/2211.11559,10.1109/CVPR52729.2023.01436,"We present Visprog, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. Visprog avoids the need for any task-specific training. Instead, it uses the incontext learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing subroutines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of VIsPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like Visprog are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform.",arxiv:2211.11559,Yes,,2025-11-11T00:14:11.169Z
visualspatialreasoni-2022,Visual Spatial Reasoning,Fangyu Liu; Guy Edward Toh Emerson; Nigel Collier,2022,Transactions of the Association for Computational Linguistics,238,https://www.semanticscholar.org/paper/354b48677e314ef2f47512c5a81723cfd17dd05d,https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00566/2138360/tacl_a_00566.pdf,10.1162/tacl_a_00566,"Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (e.g., under, in front of, facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs’ by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.1",arxiv:2205.00363,Yes,,2025-11-11T00:13:07.427Z
visuallyaugmentedlan-2022,Visually-Augmented Language Modeling,Weizhi Wang; Li Dong; Hao Cheng; Haoyu Song; Xiaodong Liu; Xifeng Yan; Jianfeng Gao; Furu Wei,2022,International Conference on Learning Representations,22,https://www.semanticscholar.org/paper/6d02cc3e66330fc170a5bde44be7b358149b9c0a,http://arxiv.org/pdf/2205.10178,10.48550/arXiv.2205.10178,"Human language is grounded on multimodal knowledge including visual knowledge like colors, sizes, and shapes. However, current large-scale pre-trained language models rely on text-only self-supervised training with massive text data, which precludes them from utilizing relevant visual information when necessary. To address this, we propose a novel pre-training framework, named VaLM, to Visually-augment text tokens with retrieved relevant images for Language Modeling. Specifically, VaLM builds on a novel latent text-image alignment method via an image retrieval module to fetch corresponding images given a textual context. With the visually-augmented context, VaLM uses a visual knowledge fusion layer to enable multimodal grounded language modeling by attending to both text context and visual knowledge in images. We evaluate VaLM on various visual knowledge-intensive commonsense reasoning tasks, which require visual information to excel. The experimental results illustrate that VaLM outperforms all strong language-only and vision-language baselines with substantial gains in reasoning object commonsense including color, size, and shape. Our code is available at https://github.com/Victorwz/VaLM.",arxiv:2205.10178,Yes,,2025-11-11T00:14:11.169Z
wanliworkerandaicoll-2022,WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation,Alisa Liu; Swabha Swayamdipta; Noah A. Smith; Yejin Choi,2022,Conference on Empirical Methods in Natural Language Processing,242,https://www.semanticscholar.org/paper/5e8d3c2dc0fc53949794fc00600e25558c4a2441,https://aclanthology.org/2022.findings-emnlp.508.pdf,10.18653/v1/2022.findings-emnlp.508,"A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process.",arxiv:2201.05955,Yes,,2025-11-11T00:14:11.169Z
warmforwinterinferri-2022,Warm (for Winter): Inferring Comparison Classes in Communication,Michael Henry Tessler; Noah D. Goodman,2022,Cognitive Sciences,4,https://www.semanticscholar.org/paper/d4c4fb5b6902999f5de9c66ca5b4192f0ef9b6f0,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9286384,10.1111/cogs.13095,"Abstract The meanings of natural language utterances depend heavily on context. Yet, what counts as context is often only implicit in conversation. The utterance it's warm outside signals that the temperature outside is relatively high, but the temperature could be high relative to a number of different comparison classes: other days of the year, other weeks, other seasons, etc. Theories of context sensitivity in language agree that the comparison class is a crucial variable for understanding meaning, but little is known about how a listener decides upon the comparison class. Using the case study of gradable adjectives (e.g., warm), we extend a Bayesian model of pragmatic inference to reason flexibly about the comparison class and test its qualitative predictions in a large‐scale free‐production experiment. We find that human listeners infer the comparison class by reasoning about the kinds of observations that would be remarkable enough for a speaker to mention, given the speaker and listener's shared knowledge of the world. Further, we quantitatively synthesize the model and data using Bayesian data analysis, which reveals that usage frequency and a preference for basic‐level categories are two main factors in comparison class inference. This work presents new data and reveals the mechanisms by which human listeners recover the relevant aspects of context when understanding language.",,Yes,,2025-11-11T00:15:16.541Z
weaklysupervisedform-2022,Weakly Supervised Formula Learner for Solving Mathematical Problems,Yuxuan Wu; Hideki Nakayama,2022,International Conference on Computational Linguistics,2,https://www.semanticscholar.org/paper/6b212f99aa29c464207a40c26c529ec552185092,,,,,Yes,,2025-11-11T00:14:11.169Z
welcometotheeraofvag-2022,Welcome to the era of vague news: a study of the demands of statistical and mathematical products in the COVID-19 pandemic media,I. Gal; V. Geiger,2022,Educational Studies in Mathematics,21,https://www.semanticscholar.org/paper/31b44679789c98fffb43ffef879a3957dc3de4bf,https://link.springer.com/content/pdf/10.1007/s10649-022-10151-7.pdf,10.1007/s10649-022-10151-7,"In this article, we report on a typology of the demands of statistical and mathematical products (StaMPs) embedded in media items related to the COVID-19 (coronavirus) pandemic. The typology emerged from a content analysis of a large purposive sample of diverse media items selected from digital news sources based in four countries. The findings encompass nine categories of StaMPs: (1) descriptive quantitative information, (2) models, predictions, causality and risk, (3) representations and displays, (4) data quality and strength of evidence, (5) demographics and comparative thinking, (6) heterogeneity and contextual factors, (7) literacy and language demands, (8) multiple information sources, and (9) critical demands. We illustrate these categories via selected media items, substantiate them through relevant research literature, and point to categories that encompass new or enhanced types of demands. Our findings offer insights into the rich set of capabilities that citizens (including both young people and adults) must possess in order to engage these mass media demands, critically analyze statistical and mathematical information in the media, evaluate the meaning and credibility of news reports, understand public policies, and make evidenced-informed judgments. Our conclusions point to the need to revise current curricular frameworks and conceptual models (e.g., regarding statistical and probability literacy, adult numeracy), to better incorporate notions such as blended knowledge, vagueness, risk, strength of evidence, and criticality. Furthermore, more attention is needed to the literacy and language demands of media items involving statistical and mathematical information. Implications for further research and educational practice are discussed.",,Yes,,2025-11-11T00:14:11.169Z
whatgoesbeyondmultim-2022,What Goes beyond Multi-modal Fusion in One-stage Referring Expression Comprehension: An Empirical Study,Gen Luo; Yiyi Zhou; Jiamu Sun; Shubin Huang; Xiaoshuai Sun; Qixiang Ye; Yongjian Wu; Rongrong Ji,2022,arXiv.org,8,https://www.semanticscholar.org/paper/73879fcedf89ccf9db86ee55f57db223593c9871,https://arxiv.org/pdf/2204.07913,10.48550/arXiv.2204.07913,,,Yes,,2025-11-11T00:15:16.541Z
whatdollmsknowaboutf-2022,What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis,Xiang Deng; Vasilisa Bashlovkina; Feng Han; Simon Baumgartner; Michael Bendersky,2022,The Web Conference,55,https://www.semanticscholar.org/paper/52136f813243ac3de8e277906112a41590a376d4,http://arxiv.org/pdf/2212.11311,10.1145/3543873.3587324,"Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. Instead, we approach this problem using semi-supervised learning with a large language model (LLM). Our pipeline generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model’s competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.",arxiv:2212.11311,Yes,,2025-11-11T00:15:14.026Z
whatdolargelanguagem-2022,What do Large Language Models Learn beyond Language?,Avinash Madasu; Shashank Srivastava,2022,Conference on Empirical Methods in Natural Language Processing,5,https://www.semanticscholar.org/paper/5efab88c0cdb11c795fa8f44a5d31b40e2a1c261,http://arxiv.org/pdf/2210.12302,10.48550/arXiv.2210.12302,"Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful `inductive biases' for non-linguistic reasoning. On a set of 19 diverse non-linguistic tasks involving quantitative computations, recognizing regular expressions and reasoning over strings. We find that pretrained models significantly outperform comparable non-pretrained neural models. This remains true also in experiments with training non-pretrained models with fewer parameters to account for model regularization effects. We further explore the effect of text domain on LMs by pretraining models from text from different domains and provenances. Our experiments surprisingly reveal that the positive effects of pre-training persist even when pretraining on multi-lingual text or computer code, and even for text generated from synthetic languages. Our findings suggest a hitherto unexplored deep connection between pre-training and inductive learning abilities of language models.",arxiv:2210.12302,Yes,,2025-11-11T00:13:07.427Z
whentomakeexceptions-2022,When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment,Zhijing Jin; Sydney Levine; Fernando Gonzalez; Ojasv Kamal; Maarten Sap; Mrinmaya Sachan; Rada Mihalcea; J. Tenenbaum; B. Scholkopf,2022,Neural Information Processing Systems,113,https://www.semanticscholar.org/paper/1c1ca2392155ddf30408a442e6b504b5d60d4f2a,https://arxiv.org/pdf/2210.01478,10.48550/arXiv.2210.01478,"AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the flexibility of the human moral mind -- the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule-breaking -- inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MORALCOT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. MORALCOT outperforms seven existing LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using RBQA. Our data is open-sourced at https://huggingface.co/datasets/feradauto/MoralExceptQA and code at https://github.com/feradauto/MoralCoT",arxiv:2210.01478,Yes,,2025-11-11T00:14:11.169Z
wikiwhyansweringande-2022,WikiWhy: Answering and Explaining Cause-and-Effect Questions,Matthew Ho; Aditya Sharma; Justin Chang; Michael Stephen Saxon; Sharon Levy; Yujie Lu; William Yang Wang,2022,International Conference on Learning Representations,24,https://www.semanticscholar.org/paper/8345d757e9127eff382d5285fef99312eaf283cd,https://arxiv.org/pdf/2210.12152,10.48550/arXiv.2210.12152,"As large language models (LLMs) grow larger and more sophisticated, assessing their""reasoning""capabilities in natural language grows more challenging. Recent question answering (QA) benchmarks that attempt to assess reasoning are often limited by a narrow scope of covered situations and subject matters. We introduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining why an answer is true in natural language. WikiWhy contains over 9,000""why""question-answer-rationale triples, grounded on Wikipedia facts across a diverse set of topics. Each rationale is a set of supporting statements connecting the question to the answer. WikiWhy serves as a benchmark for the reasoning capabilities of LLMs because it demands rigorous explicit rationales for each answer to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized. GPT-3 baselines achieve only 38.7% human-evaluated correctness in the end-to-end answer&explain condition, leaving significant room for future improvements.",arxiv:2210.12152,Yes,,2025-11-11T00:15:14.026Z
workingwithdynamiccr-2022,Working With Dynamic Crop Models Second Edition Methods Tools And Examples For Agriculture And Environment,,2022,,0,https://www.semanticscholar.org/paper/7037dc4215b25742c6935bc72a8c80b4b4c7b89e,,,,,Yes,,2025-11-11T00:15:19.325Z
zeroshotlearnersforn-2022,Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective,Ping Yang; Junjie Wang; Ruyi Gan; Xinyu Zhu; Lin Zhang; Ziwei Wu; Xinyu Gao; Jiaxing Zhang; Tetsuya Sakai,2022,Conference on Empirical Methods in Natural Language Processing,29,https://www.semanticscholar.org/paper/8862ed012fe06a794fda3ceae3f471a0c2a40fbe,http://arxiv.org/pdf/2210.08590,10.48550/arXiv.2210.08590,"We propose a new paradigm for zero-shot learners that is format agnostic, i.e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis. Zero-shot learning aims to train a model on a given task such that it can address new learning tasks without any additional training. Our approach converts zero-shot learning into multiple-choice tasks, avoiding problems in commonly used large-scale generative models such as FLAN. It not only adds generalization ability to models but also significantly reduces the number of parameters. Our method shares the merits of efficient training and deployment. Our approach shows state-of-the-art performance on several benchmarks and produces satisfactory results on tasks such as natural language inference and text classification. Our model achieves this success with only 235M parameters, which is substantially smaller than state-of-the-art models with billions of parameters. The code and pre-trained models are available at https://github.com/IDEA-CCNL/Fengshenbang-LM/tree/main/fengshen/examples/unimc .",arxiv:2210.08590,Yes,,2025-11-11T00:14:11.169Z
zeroshotpromptingfor-2022,Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning,Hui-Chi Kuo; Yun-Nung (Vivian) Chen,2022,Annual Meeting of the Association for Computational Linguistics,11,https://www.semanticscholar.org/paper/2a4b6fdf4fd74429431a730c14d0087e00b2a4fa,http://arxiv.org/pdf/2210.05901,10.48550/arXiv.2210.05901,"Intelligent virtual assistants are currently designed to perform tasks or services explicitly mentioned by users, so multiple related domains or tasks need to be performed one by one through a long conversation with many explicit intents. Instead, human assistants are capable of reasoning (multiple) implicit intents based on user utterances via commonsense knowledge, reducing complex interactions and improving practicality. Therefore, this paper proposes a framework of multi-domain dialogue systems, which can automatically infer implicit intents based on user utterances and then perform zero-shot prompting using a large pre-trained language model to trigger suitable single task-oriented bots. The proposed framework is demonstrated effective to realize implicit intents and recommend associated bots in a zero-shot manner.",arxiv:2210.05901,Yes,,2025-11-11T00:14:11.169Z
zeroshotmathematical-2022,Zero-shot Mathematical Problem Solving via Generative Pre-trained Transformers,F. Galatolo; M. Cimino; G. Vaglini,2022,International Conference on Enterprise Information Systems,2,https://www.semanticscholar.org/paper/8a4dce5735a101ff8f64c2b676afb8c24950a5d8,https://doi.org/10.5220/0011032400003179,10.5220/0011032400003179,": Mathematics is an effective testbed for measuring the problem-solving ability of machine learning models. The current benchmark for deep learning-based solutions is grade school math problems: given a natural language description of a problem, the task is to analyse the problem, exploit heuristics generated from a very large set of solved examples, and then generate an answer. In this paper, a descendant of the third generation of Generative Pre-trained Transformer Networks (GPT-3) is used to develop a zero-shot learning approach, to solve this problem. The proposed approach shows that coding based problem-solving is more effective than the natural language reasoning based one. Specifically, the architectural solution is built upon OpenAI Codex, a descendant of GPT-3 for programming tasks, trained on public GitHub repositories, the world’s largest source code hosting service. Experimental results clearly show the potential of the approach: by exploiting the Python as programming language, proposed pipeline achieves the 18.63% solve rate against the 6.82% of GPT-3. Finally, by using a fine-tuned verifier, the correctness of the answer can be ranked at runtime, and then improved by generating a predefined number of trials. With this approach, for 10 trials and an ideal verifier, the proposed pipeline achieves 54.20% solve rate.",,Yes,,2025-11-11T00:14:11.169Z
andsimulationofpower-2022,and simulation of power electronics system and simulation of electrical machines and electromagnetic,O. Lefranc; H. Schneider; C. Turpin; O. Rallières; Maël; Durand; Fernanda Vendrame; Nicolas Damay; H. Rabab; C. Forgez; Marie Sayegh; E. Monmasson; Brian Ospina Agudelo,2022,,0,https://www.semanticscholar.org/paper/b6ceb6ee67bbf9e96b2dd5f3383cbd943d8c2d8c,,,,,Yes,,2025-11-11T00:15:19.325Z
technicalreviewonkno-2022,technical review on knowledge intensive NLP for pre-trained language development,Snehal Awachat; Shwetal Raipure; Kavita B. Kalambe,2022,International Journal of Health Sciences,0,https://www.semanticscholar.org/paper/991a4434b906fce5381d701c6e794aa8a5cc3fc9,https://sciencescholar.us/journal/index.php/ijhs/article/download/7510/3788,10.53730/ijhs.v6ns2.7510,"In today’s world where data plays the very important role, we have various sources of pre-data like online books, equation analysis, encyclopedia, common-sense reasoning, common-sense knowledge, etc. The increasing capacity of pre-training language models have given knowledge intensive natural language processing (KI-NLP) a new boost for advanced functionalities for establishing a stable, flexible, robust and efficient model. Though pre-trained models have its own drawback for handling the KI-NLP tasks, we are here to discuss the challenges faced in this field. A wide variety of pre-trained language models enhanced with external knowledge sources have been proposed and are in rapid development to meet this difficulty. In this research we have also discusses the challenges in NLP in terms of generation of knowledge intensive models. We have also defined some mathematical model and its framework dependability for pre-training different language in NLP. Finally, we have also discussed about variety of literature reviews based on we intend to describe the present progress of pre-trained language model-based knowledge-enhanced models (PLMKEs) in this work by deconstructing their three key elements: information sources, knowledge-intensive NLP tasks, and knowledge fusion methods.",,Yes,,2025-11-11T00:14:11.169Z
utomaticconstructing-2022,Аutomatic Constructing the NURBS Surface of a Ship's Hull,E. Popov; I. Shorkina,2022,Proceedings of the 32nd International Conference on Computer Graphics and Vision,0,https://www.semanticscholar.org/paper/af5cbf15a1fad612cbd1bcebeac81866cc4bc352,https://doi.org/10.20948/graphicon-2022-162-169,10.20948/graphicon-2022-162-169,"The article describes an automated algorithm for the formation of a geometric model of the ship's hull surface according to an electronic theoretical drawing. An analysis of the traditional technology for the formation of ship surfaces based on the Russian CAD system (Sea Solution) is presented. The main disadvantages of traditional technology are determined. The automated algorithm for generating a geometric surface model is designed to reduce the complexity of manual design. The mathematical apparatus for constructing NURBS curves and surfaces serves as the instrumental basis of the algorithm, which provides a simple user control of forms. On the basis of a theoretical drawing generated by the CAD system, a cloud of approximation points was created to build the surface of the ship's hull from them. A general scheme of automatic construction of elements of the surface model in the initial approximation is given. To solve this problem, a software product based on HTML5, JavaScript and Three JS and Verb libraries was created. The use of the JavaScript language is due to its versatility, a large amount of information in the public domain. WebGL is used to implement 3D graphics rendering pipeline. Functions from the Three JS libraries made it possible to implement methods used in analytical geometry. The operability of the developed algorithm is demonstrated on the example of building a 3D model of the hull surface of a fishing trawler.",,Yes,,2025-11-11T00:15:19.325Z
johnis50yearsoldcanh-2022,"“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility",Himanshu Gupta; Neeraj Varshney; Swaroop Mishra; Kuntal Kumar Pal; Saurabh Arjun Sawant; Kevin Scaria; Siddharth Goyal; Chitta Baral,2022,Conference of the European Chapter of the Association for Computational Linguistics,16,https://www.semanticscholar.org/paper/9a3f1a51ab2b3816655e4c4f58a022421ea7b34b,http://arxiv.org/pdf/2210.07471,10.48550/arXiv.2210.07471,"In current NLP research, large-scale language models and their abilities are widely being discussed. Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities. This work focuses on a simple commonsense ability, reasoning about when an action (or its effect) is feasible. To this end, we introduce FeasibilityQA, a question-answering dataset involving binary classification (BCQ) and multi-choice multi-correct questions (MCQ) that test understanding of feasibility. We show that even state-of-the-art models such as GPT-3, GPT-2, and T5 struggle to answer the feasibility questions correctly. Specifically, on (MCQ, BCQ) questions, GPT-3 achieves accuracy of just (19%, 62%) and (25%, 64%) in zero-shot and few-shot settings, respectively. We also evaluate models by providing relevant knowledge statements required to answer the question and find that the additional knowledge leads to a 7% gain in performance, but the overall performance still remains low. These results make one wonder how much commonsense knowledge about action feasibility is encoded in state-of-the-art models and how well they can reason about it.",arxiv:2210.07471,Yes,,2025-11-11T00:15:14.026Z
workshoponproofsandf-2022,"“Workshop on Proofs and Formalization in Logic, Mathematics and Philosophy”",,2022,,0,https://www.semanticscholar.org/paper/f7f2276f043ff090c911dc6239f27374fba1869a,,,,,Yes,,2025-11-11T00:15:19.325Z
