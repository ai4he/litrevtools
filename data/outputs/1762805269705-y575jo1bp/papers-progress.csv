ID,Title,Authors,Year,Venue,Citations,URL,PDF URL,DOI,Abstract,Keywords,Included,Exclusion Reason,Extracted At
modelingcomplexmathe-2023,Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent,Haoran Liao; Qinyi Du; Shaohua Hu; Hao He; Yanyan Xu; Jidong Tian; Yaohui Jin,2023,arXiv.org,2,https://www.semanticscholar.org/paper/7017c58e19f4db0c38040935cc9fb7b7090a466d,,10.48550/arXiv.2312.08926,"Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements, associate domain knowledge, perform compound logical reasoning, and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs, thus leading to confusion in generation. In this work, we explore the potential of enhancing LLMs with agents by meticulous decomposition and modeling of mathematical reasoning process. Specifically, we propose a formal description of the mathematical solving and extend LLMs with an agent-based zero-shot framework named $\bf{P}$lanner-$\bf{R}$easoner-$\bf{E}$xecutor-$\bf{R}$eflector (PRER). We further provide and implement two MathAgents that define the logical forms and inherent relations via a pool of actions in different grains and orientations: MathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with humankind. Experiments on miniF2F and MATH have demonstrated the effectiveness of PRER and proposed MathAgents, achieving an increase of $12.3\%$($53.9\%\xrightarrow{}66.2\%$) on the MiniF2F, $9.2\%$ ($49.8\%\xrightarrow{}59.0\%$) on MATH, and $13.2\%$($23.2\%\xrightarrow{}35.4\%$) for level-5 problems of MATH against GPT-4. Further analytical results provide more insightful perspectives on exploiting the behaviors of LLMs as agents.",arxiv:2312.08926,Yes,,2025-11-10T20:07:50.576Z
scalingrelationshipo-2023,Scaling Relationship on Learning Mathematical Reasoning with Large Language Models,Zheng Yuan; Hongyi Yuan; Cheng Li; Guanting Dong; Chuanqi Tan; Chang Zhou,2023,arXiv.org,262,https://www.semanticscholar.org/paper/91206346edbe28abb606d7b3425cd455d4019d4f,https://arxiv.org/pdf/2308.01825,10.48550/arXiv.2308.01825,"Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\% significantly.",arxiv:2308.01825,Yes,,2025-11-10T20:07:50.576Z
wizardmathempowering-2023,WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct,Haipeng Luo; Qingfeng Sun; Can Xu; Pu Zhao; Jian-Guang Lou; Chongyang Tao; Xiubo Geng; Qingwei Lin; Shifeng Chen; Dongmei Zhang,2023,International Conference on Learning Representations,578,https://www.semanticscholar.org/paper/dd18782960f9ee4c66b79e1518b342ad3f8d19e7,https://arxiv.org/pdf/2308.09583,10.48550/arXiv.2308.09583,"Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM",arxiv:2308.09583,Yes,,2025-11-10T20:07:50.576Z
dynamicpromptlearnin-2022,Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning,Pan Lu; Liang Qiu; Kai-Wei Chang; Y. Wu; Song-Chun Zhu; Tanmay Rajpurohit; Peter Clark; A. Kalyan,2022,International Conference on Learning Representations,365,https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4,http://arxiv.org/pdf/2209.14610,10.48550/arXiv.2209.14610,"Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.",arxiv:2209.14610,Yes,,2025-11-10T20:10:51.804Z
galacticaalargelangu-2022,Galactica: A Large Language Model for Science,Ross Taylor; Marcin Kardas; Guillem Cucurull; Thomas Scialom; A. Hartshorn; Elvis Saravia; Andrew Poulton; Viktor Kerkez; Robert Stojnic,2022,arXiv.org,892,https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1,,,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",arxiv:2211.09085,Yes,,2025-11-10T20:10:51.804Z
numglueasuiteoffunda-2022,NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,Swaroop Mishra; Arindam Mitra; Neeraj Varshney; Bhavdeep Singh Sachdeva; Peter Clark; Chitta Baral; A. Kalyan,2022,Annual Meeting of the Association for Computational Linguistics,118,https://www.semanticscholar.org/paper/39238a92de090c104936a4f78375b95600e42ce5,http://arxiv.org/pdf/2204.05660,10.48550/arXiv.2204.05660,"Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.",arxiv:2204.05660,Yes,,2025-11-10T20:10:51.804Z
