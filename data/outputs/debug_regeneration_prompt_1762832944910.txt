
You are an expert in writing PRISMA systematic literature reviews for academic publication.

CRITICAL REQUIREMENT: You MUST cite ALL papers (existing AND new) using LaTeX \cite{} commands throughout the regenerated paper.

=== ALL BIBTEX ENTRIES (INCLUDING NEW PAPERS) ===
@article{cohen2022thisunicorn,
  title={"This is my unicorn, Fluffy": Personalizing frozen vision-language representations},
  author={Niv Cohen and Rinon Gal and E. Meirom and Gal Chechik and Y. Atzmon},
  year={2022},
  booktitle={European Conference on Computer Vision},
  doi={10.48550/arXiv.2204.01694},
  url={https://www.semanticscholar.org/paper/0791a0441e1f672c43aecb2d6708fbc8725c8cad},
  abstract={Large Vision&Language models pretrained on web-scale data provide representations that are invaluable for numerous V&L problems. However, it is unclear how they can be used for reasoning about user-specific visual concepts in unstructured language. This problem arises in multiple domains, from personalized image retrieval to personalized interaction with smart devices. We introduce a new learning setup called Personalized Vision&Language (PerVL) with two new benchmark datasets for retrieving and segmenting user-specific"personalized"concepts"in the wild". In PerVL, one should learn personalized concepts (1) independently of the downstream task (2) allowing a pretrained model to reason about them with free language, and (3) does not require personalized negative examples. We propose an architecture for solving PerVL that operates by extending the input vocabulary of a pretrained model with new word embeddings for the new personalized concepts. The model can then reason about them by simply using them in a sentence. We demonstrate that our approach learns personalized visual concepts from a few examples and can effectively apply them in image retrieval and semantic segmentation using rich textual queries.}
}

@article{stolfo2022causalframework,
  title={A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models},
  author={Alessandro Stolfo and Zhijing Jin and Kumar Shridhar and B. Scholkopf and Mrinmaya Sachan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.12023},
  url={https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe},
  abstract={We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution.By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems.Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.}
}

@article{snchez2022clusteringapproach,
  title={A Clustering Approach for the Optimal Siting of Recharging Stations in the Electric Vehicle Routing Problem with Time Windows},
  author={Danny García Sánchez and Alejandra Tabares and L. Faria and Juan Carlos Rivera and J. Franco},
  year={2022},
  booktitle={Energies},
  doi={10.3390/en15072372},
  url={https://www.semanticscholar.org/paper/f1164514c7180331c3b059c19eab5169c9c921a7},
  abstract={Transportation has been incorporating electric vehicles (EVs) progressively. EVs do not produce air or noise pollution, and they have high energy efficiency and low maintenance costs. In this context, the development of efficient techniques to overcome the vehicle routing problem becomes crucial with the proliferation of EVs. The vehicle routing problem concerns the freight capacity and battery autonomy limitations in different delivery-service scenarios, and the challenge of best locating recharging stations. This work proposes a mixed-integer linear programming model to solve the electric location routing problem with time windows (E-LRPTW) considering the state of charge, freight and battery capacities, and customer time windows in the decision model. A clustering strategy based on the k-means algorithm is proposed to divide the set of vertices (EVs) into small areas and define potential sites for recharging stations, while reducing the number of binary variables. The proposed model for E-LRPTW was implemented in Python and solved using mathematical modeling language AMPL together with CPLEX. Performed tests on instances with 5 and 10 clients showed a large reduction in the time required to find the solution (by about 60 times in one instance). It is concluded that the strategy of dividing customers by sectors has the potential to be applied and generate solutions for larger geographical areas and numbers of recharging stations, and determine recharging station locations as part of planning decisions in more realistic scenarios.}
}

@misc{desogus2022contributionrelationship,
  title={A Contribution on Relationship Banking. Economic, Anthropological and Mathematical Reasoning, Empirical Evidence from Italy},
  author={Marco Desogus and Elisa Casu},
  year={2022},
  url={https://www.semanticscholar.org/paper/dd88cc9b1f6d71ef82631b4e1c98c077ccdf291a}
}

@article{wang2022hybridgenetic,
  title={A Hybrid Genetic Algorithm for Flexible Job Shop Scheduling Problem},
  author={Xianglong Wang and Changyi Liu},
  year={2022},
  booktitle={2022 5th World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM)},
  doi={10.1109/WCMEIM56910.2022.10021523},
  url={https://www.semanticscholar.org/paper/3cff420b5a41a06291b68f5b6600935c090f8ad8},
  abstract={Partially flexible job shop scheduling problem (P-FJSP) is a NP Hard problem more complex than fully flexi-ble job shop scheduling problem (T -FJSP). In this paper, the mathematical model of flexible job shop scheduling is established with the goal of minimizing the maximum completion time (makespan). It combines the local search ability of simu-lated annealing algorithm and the global search ability of ge-netic algorithm. In the process of chromosome decoding, greedy decoding method is used to get a better scheduling solution as far as possible. The hybrid scheduling algorithm is implemented based on Visual Studio and C # language. Finally, 8×8 classic scheduling instance are used for simulation scheduling experiments to verify that the hybrid genetic algorithm proposed in this paper is effective in solving large-scale FJSP.}
}

@article{zhang2022multilayerattention,
  title={A Multi-Layer Attention Network for Visual Commonsense Reasoning},
  author={Wenqi Zhang and Yongchao Gao and Heng Qian and Hongli Lyu},
  year={2022},
  booktitle={International Conference on Data Science and Information Technology},
  doi={10.1109/DSIT55514.2022.9943834},
  url={https://www.semanticscholar.org/paper/0e0f20f3af3650b5a97b0ec3f046ba8160b45279},
  abstract={Visual Commonsense Reasoning (VCR) is a challenging multimodal task involving several research fields such as vision, cognition, and reasoning, which combines images and natural language for reasoning. Existing VCR methods focus on global attention or use pre-training models, but these methods lack attention to local features of visual and language. In this paper, a multi-layer attention network is proposed for the VCR task, including an intra-modal attention module and an inter-modal attention module. The intra-modal attention module complements important features of visual and language modalities with fine-grained visual attention to improve the relevance of visual and language. The inter-modal attention module captures the internal dependencies between visual and language. Finally, the two modules are integrated into an end-to-end reasoning framework. Experiments on the VCR large-scale dataset show that the proposed method exhibits a decent improvement in the VCR task and illustrates the effectiveness of the method on three subtasks.}
}

@article{ricci2022petrinetbasedapproach,
  title={A Petri-Net-Based Approach for Enhancing Clinical Reasoning in Medical Education},
  author={F. Ricci and F. Consorti and F. Pecoraro and D. Luzi and Oscar Tamburis},
  year={2022},
  journal={IEEE Transactions on Learning Technologies},
  doi={10.1109/tlt.2022.3157391},
  url={https://www.semanticscholar.org/paper/98b0fb67a6fb222998e3449621f3f5eecaed758e},
  abstract={Medical students are called to acquire competence to manage disease in its dynamic evolution over time, learning to analyze how clinical conditions evolve in a patient's history and how each condition interferes with the evolution of the other coexisting conditions. In this article, the health issue network (HIN) approach is introduced as a formal language based on Petri nets (PNs) to model properties that are particularly apposite for the graphical representation of HIN evolutionary paths. Moreover, the PNs’ underlying mathematical model allows users to draw coherent and well-formed graphs representing rather complex clinical cases. Finally, HIN can be easily integrated into a simulation environment to support case-based learning activities and assessment. The examples of the exercises provided in this article show, on the one hand, the ways the introduced methodology is figured out and implemented; on the other hand, they outline the variety of learning questions that users may deal with when deploying the HIN approach.}
}

@article{ekong2022ratiocinativestudy,
  title={A Ratiocinative Study and Assessment of W. V. O. Quine’s “Criterion of Ontological Commitment”},
  author={Joseph T. Ekong},
  year={2022},
  journal={International Journal of Philosophy},
  doi={10.47941/ijp.1052},
  url={https://www.semanticscholar.org/paper/7b6955111d3bd91b13e7a9c7fbdfd75d43825c36},
  abstract={Purpose: This work has three main objectives: Firstly, it offers an elucidation of the notion of ontological commitment. Secondly, it assesses the adequacy of the criterion of ontological commitment for different languages. Thirdly, it offers some speculative and evaluative remarks regarding the significance of Quine’s criterion of ontological commitment. Many ontologists, within the analytic tradition, often appeal to Quine's criterion of ontological commitment, when debating whether an assertion or theory implies the existence of a certain entity. Regarding his goal in formulating this criterion, he says that the criterion does not aim to help us discover what it is that there is, but only what a theory says there is: “I look to variables and quantification for evidence as to what a theory says that there is, not for evidence as to what there is” (Quine, 1960: 225). Its most popular formulation, using textual evidence from Quine's oeuvre, is: “To be is to be the value of a bound variable,” (Quine, 1961: 15). However, this formulation is susceptible to gross misunderstanding, especially if one is influenced by the formalities and technical maneuvers of model theory. In mathematical logic, model theory is the study of the relationship between formal theories (a collection of sentences in a formal language expressing statements about a mathematical structure), and their models (those structures in which the statements of the theory hold). Model theory is a branch of mathematical logic where we study mathematical structures by considering the first-order sentences true in those structures and the sets definable by first-order formulas. Model theory studies the relations between sentences of a formal language and the interpretations (or ‘structures’) which make these sentences true or false. It offers precise definitions of truth, logical truth and consequence, meanings and modalities. 
Methodology: This work is expository, analytic, critical and evaluative in its methodology. Of course, there are familiar philosophical problems which are within the discursive framework of ‘ontology,’ often phrased by asking if something or some category of things are “real,” or whether “they exist,” concretely. An outstanding example is provided by the traditional problem of universals, which issues in the nominalist-realist controversy, as to the real existence of universals, or of abstract entities such as classes (in the mathematical sense) or propositions (in the abstract sense, referring to the content of an assertion in abstraction from the particular words used to convey it). 
Results: In as much as one might agree with Quine’s Criterion of Ontological Commitment, one might also opine that it is nonetheless a feature of first-order language (i.e. the language embodied in first-order logic; a symbolized reasoning process comprising relations, functions and constants, in which each sentence or statement is broken down into a subject and a predicate. In this regard, the predicate modifies or defines the properties of the subject) that there should be an exact correspondence between the ontological commitments carried by a sentence and the objects that must be counted among the values of the variables in order for the sentence to be true. However, this in itself is not a reason for thinking that such a feature will generalize beyond first-order languages. It is possible for Quine’s Criterion to degenerate, when the language contains atomic predicates expressing extrinsic properties. 
Unique Contribution to theory, practice and policy: Based on Quine’s analysis, a theory is committed to those and only those entities that in the last analysis serve as the values of its bound variables. Thus, ordinary first-order theory commits one to an ontology only of individuals (particulars), whereas higher order logic commits one to the existence of sets, i.e. of collections of definite and distinct entities (or, alternatively, of properties and relations). Likewise, if bound first-order variables are assumed to range over sets (as they do in set theory), a commitment to the existence of these sets is incurred. Admittedly, the precise import of Quine’s criterion of ontological commitment, however, is not completely clear, nor is it clear in what other sense one is perhaps committed by a theory to those entities that are named or otherwise referred to in it, but not quantified over in it. However, it despite its limitations, it has made is possible for one to measure the ontological cost of theories, an important component in deciding which theories to accept, thus offering a partial foundation for theory choice.}
}

@article{li2022scenariobasedexploration,
  title={A Scenario-based Exploration of Expected Usefulness, Privacy Concerns, and Adoption Likelihood of Learning Analytics},
  author={X. Li and M. Rosson and Jenay Robert},
  year={2022},
  booktitle={ACM Conference on Learning @ Scale},
  doi={10.1145/3491140.3528271},
  url={https://www.semanticscholar.org/paper/067b8489b028d931b751cb9413225b761e51dcf3},
  abstract={Learning analytics has become a robust research area in the last decade, as innovative analytic models of learning data have been created with the goal of enhancing teaching and learning. However, barriers to large scale adoption of such technologies in higher education still exist. In recent years, a strand of research has begun to investigate stakeholders' expectations of learning analytics, hoping to find ways to integrate the innovations into everyday teaching practices. For instance, studies have investigated instructors' ideas about how learning analytics might be helpful, as well as concerns about student data privacy. However, most studies have taken a general approach rather than considering instructors' day-to-day experiences. Using survey methods, we presented instructors with hypothetical scenarios of learning analytics in use across disciplines, class sizes, teaching activities, and types of student data. We asked for ratings of both usefulness and privacy concerns for each proposed teaching situation. Our respondents considered scenarios involving learning outcomes-related data (e.g. grades) to be more useful than those that involve student interactions (e.g. language, social activity). In contrast, privacy concerns were lower for outcomes-oriented scenarios than interactions-focused scenarios. An interesting new finding was a negative correlation of usefulness and privacy; we discuss this in the context of instructors' possible cost-benefit reasoning. We reflect on our findings with respect to future efforts in developing and fielding learning analytics tools.}
}

@article{lu2022surveydeep,
  title={A Survey of Deep Learning for Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Wenhao Yu and S. Welleck and Kai-Wei Chang},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10535},
  url={https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d},
  abstract={Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.}
}

@article{hu2022surveyknowledge,
  title={A Survey of Knowledge Enhanced Pre-Trained Language Models},
  author={Linmei Hu and Zeyi Liu and Ziwang Zhao and Lei Hou and Liqiang Nie and Juanzi Li},
  year={2022},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  doi={10.1109/TKDE.2023.3310002},
  url={https://www.semanticscholar.org/paper/a26623d52d24e03044a158cddad931ec5ab7304c},
  abstract={Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are categorized into KG-based and retrieval-based methods. Finally, we point out some promising future directions of KE-PLMs.}
}

@article{zhou2022surveyneural,
  title={A Survey on Neural Open Information Extraction: Current Status and Future Directions},
  author={Shaowen Zhou and Yu Bowen and Aixin Sun and Cheng Long and Jingyang Li and Haiyang Yu and Jianguo Sun},
  year={2022},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2205.11725},
  url={https://www.semanticscholar.org/paper/5de6ecf62f14c9263882f9f30d6448df9efd34e0},
  abstract={Open Information Extraction (OpenIE) facilitates domain-independent discovery of relational facts from large corpora. The technique well suits many open-world natural language understanding scenarios, such as automatic knowledge base construction, open-domain question answering, and explicit reasoning. Thanks to the rapid development in deep learning technologies, numerous neural OpenIE architectures have been proposed and achieve considerable performance improvement. In this survey, we provide an extensive overview of the state-of-the-art neural OpenIE models, their key design decisions, strengths and weakness. Then, we discuss limitations of current solutions and the open issues in OpenIE problem itself. Finally we list recent trends that could help expand its scope and applicability, setting up promising directions for future research in OpenIE. To our best knowledge, this paper is the first review on neural OpenIE.}
}

@article{wankmller2022comparisonapproaches,
  title={A comparison of approaches for imbalanced classification problems in the context of retrieving relevant documents for an analysis},
  author={Sandra Wankmüller},
  year={2022},
  journal={Journal of Computational Social Science},
  doi={10.1007/s42001-022-00191-7},
  url={https://www.semanticscholar.org/paper/a12e9a6863c8453787575172599389d2ddcd9f62},
  abstract={One of the first steps in many text-based social science studies is to retrieve documents that are relevant for an analysis from large corpora of otherwise irrelevant documents. The conventional approach in social science to address this retrieval task is to apply a set of keywords and to consider those documents to be relevant that contain at least one of the keywords. But the application of incomplete keyword lists has a high risk of drawing biased inferences. More complex and costly methods such as query expansion techniques, topic model-based classification rules, and active as well as passive supervised learning could have the potential to more accurately separate relevant from irrelevant documents and thereby reduce the potential size of bias. Yet, whether applying these more expensive approaches increases retrieval performance compared to keyword lists at all, and if so, by how much, is unclear as a comparison of these approaches is lacking. This study closes this gap by comparing these methods across three retrieval tasks associated with a data set of German tweets (Linder in SSRN, 2017. https://doi.org/10.2139/ssrn.3026393 ), the Social Bias Inference Corpus (SBIC) (Sap et al. in Social bias frames: reasoning about social and power implications of language. In: Jurafsky et al. (eds) Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, p 5477–5490, 2020. https://doi.org/10.18653/v1/2020.aclmain.486 ), and the Reuters-21578 corpus (Lewis in Reuters-21578 (Distribution 1.0). [Data set], 1997. http://www.daviddlewis.com/resources/testcollections/reuters21578/ ). Results show that query expansion techniques and topic model-based classification rules in most studied settings tend to decrease rather than increase retrieval performance. Active supervised learning, however, if applied on a not too small set of labeled training instances (e.g. 1000 documents), reaches a substantially higher retrieval performance than keyword lists.}
}

@article{wang2022comparisonthree,
  title={A comparison of three approaches to covariate effects on latent factors},
  author={Ze Wang},
  year={2022},
  booktitle={Large-scale Assessments in Education},
  doi={10.1186/s40536-022-00148-2},
  url={https://www.semanticscholar.org/paper/c40412109167ae57baab6505edf9b628efca6d3a},
  abstract={In educational and psychological research, it is common to use latent factors to represent constructs and then to examine covariate effects on these latent factors. Using empirical data, this study applied three approaches to covariate effects on latent factors: the multiple-indicator multiple-cause (MIMIC) approach, multiple group confirmatory factor analysis (MG-CFA) approach, and the structural equation model trees (SEM Trees) approach. The MIMIC approach directly models covariate effects on latent factors. The MG-CFA approach allows testing of measurement invariance before latent factor means could be compared. The more recently developed SEM Trees approach partitions the sample into homogenous subsets based on the covariate space; model parameters are estimated separately for each subgroup. We applied the three approaches using an empirical dataset extracted from the eighth-grade U.S. data from the Trends in International Mathematics and Science Study 2019 database. All approaches suggested differences among mathematics achievement categories for the latent factor of mathematics self-concept. In addition, language spoken at home did not seem to affect students’ mathematics self-concept. Despite these general findings, the three approaches provided different pieces of information regarding covariate effects. For all models, we appropriately considered the complex data structure and sampling weights following recent recommendations for analyzing large-scale assessment data.}
}

@misc{alemany2022methodologycharacterize,
  title={A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America},
  author={L. A. Alemany and Luciana Benotti and Hernán Maina and Luc'ia M. Gonz'alez and Mariela Rajngewerc and Lautaro Mart'inez and Jos'e L. S'anchez and M. Schilman and Guido Ivetta and Alexia Halvorsen and Amanda Rojo and M. Bordone and Beatriz Busaniche},
  year={2022},
  url={https://www.semanticscholar.org/paper/a82a08b5e6a11f4d6fdff95dd30177957ed7855e},
  abstract={Automated decision-making systems, especially those based on natural language processing, are pervasive in our lives. They are not only behind the internet search engines we use daily, but also take more critical roles: selecting candidates for a job, determining suspects of a crime, diagnosing autism and more. Such automated systems make errors, which may be harmful in many ways, be it because of the severity of the consequences (as in health issues) or because of the sheer number of people they affect. When errors made by an automated system affect a population more than others, we call the system \textit{biased}. Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to audit them. In this paper, we present a methodology that spells out how social scientists, domain experts, and machine learning experts can collaboratively explore biases and harmful stereotypes in word embeddings and large language models. Our methodology is based on the following principles: * focus on the linguistic manifestations of discrimination on word embeddings and language models, not on the mathematical properties of the models * reduce the technical barrier for discrimination experts%, be it social scientists, domain experts or other * characterize through a qualitative exploratory process in addition to a metric-based approach * address mitigation as part of the training process, not as an afterthought}
}

@article{kim2022novelmodular,
  title={A novel modular modeling approach for understanding different electromechanics between left and right heart in rat},
  author={Nari Kim and Julius D. Pronto and D. Nickerson and A. Taberner and Peter J. Hunter},
  year={2022},
  booktitle={Frontiers in Physiology},
  doi={10.3389/fphys.2022.965054},
  url={https://www.semanticscholar.org/paper/2fba0d7b1293e4b13180fb3bccf86ed52ddcaf70},
  abstract={While ion channels and transporters involved in excitation-contraction coupling have been linked and constructed as comprehensive computational models, validation of whether each individual component of a model can be reused has not been previously attempted. Here we address this issue while using a novel modular modeling approach to investigate the underlying mechanism for the differences between left ventricle (LV) and right ventricle (RV). Our model was developed from modules constructed using the module assembly principles of the CellML model markup language. The components of three existing separate models of cardiac function were disassembled as to create smaller modules, validated individually, and then the component parts were combined into a new integrative model of a rat ventricular myocyte. The model was implemented in OpenCOR using the CellML standard in order to ensure reproducibility. Simulated action potential (AP), Ca2+ transient, and tension were in close agreement with our experimental measurements: LV AP showed a prolonged duration and a more prominent plateau compared with RV AP; Ca2+ transient showed prolonged duration and slow decay in LV compared to RV; the peak value and relaxation of tension were larger and slower, respectively, in LV compared to RV. Our novel approach of module-based mathematical modeling has established that the ionic mechanisms underlying the APs and Ca2+ handling play a role in the variation in force production between ventricles. This simulation process also provides a useful way to reuse and elaborate upon existing models in order to develop a new model.}
}

@article{mi2022reviewdevelopment,
  title={A review: development of named entity recognition (NER) technology for aeronautical information intelligence},
  author={Baigang Mi and Fan Yi},
  year={2022},
  booktitle={Artificial Intelligence Review},
  doi={10.1007/s10462-022-10197-2},
  url={https://www.semanticscholar.org/paper/ca2da2420fd25c8633641542730d3f0867c50f60}
}

@article{poythress2022semioticanalysis,
  title={A semiotic analysis of multiple systems of logic: using tagmemic theory to assess the usefulness and limitations of formal logics, and to produce a mathematical lattice model including multiple systems of logic},
  author={V. Poythress},
  year={2022},
  journal={Semiotica: Journal of the International Association for Semiotic Studies},
  doi={10.1515/sem-2020-0051},
  url={https://www.semanticscholar.org/paper/606db29a9d5cad5cd06b8eeb1f8beee390c87ca4},
  abstract={Abstract Tagmemic theory as a semiotic theory can be used to analyze multiple systems of logic and to assess their strengths and weaknesses. This analysis constitutes an application of semiotics and also a contribution to understanding of the nature of logic within the context of human meaning. Each system of logic is best adapted to represent one portion of human rationality. Acknowledging this correlation between systems and their targets helps explain the usefulness of more than one system. Among these systems, the two-valued system of classical logic takes its place. All the systems of logic can be incorporated into a complex mathematical model that has a place for each system and that represents a larger whole in human reasoning. The model can represent why tight formal systems of logic can be applied in some contexts with great success, but in other contexts are not directly applicable. The result suggests that human reasoning is innately richer than any one formal system of logic.}
}

@misc{song2022thesissubmitted,
  title={A thesis submitted to the Faculty of Graduate and Postdoctoral Affairs in partial fulfillment of the requirements for the degree of Master of Arts},
  author={Charlene Song},
  year={2022},
  url={https://www.semanticscholar.org/paper/25be22274b72f1337e977d94d0c94026d13a67d0}
}

@article{markta2022accuracypupils,
  title={ACCURACY OF PUPILS´ SELF-ASSESSMENT},
  author={Švamberk Šauerová Markéta and Smetáčková Irena},
  year={2022},
  booktitle={EduPort},
  doi={10.21062/edp.2022.009},
  url={https://www.semanticscholar.org/paper/fcfabc1d551304cde28a4f0658ed20e36559e05f},
  abstract={In this study, we investigated the accuracy of pupils´ self-assessment in two main school domains – mathematics and Czech language. The analysis explores whether pupils are able to evaluate adequately their own results in the didactic tests and then use some individual parameters to explain the level of self-assessment. The aim of the study was to analyze whether groups of pupils with different self-assessments of school tasks in the Czech language and mathematics (significant underestimation, adequate self-assessment, significant overestimation) differ in some of the cognitive skills studied. Our study questions were as follows: (1) Do pupils assess their achievements in particular school tasks accurately, or inaccurately? (2) Do pupils´ self-assessments differ in mathematics and language? (3) Do the pupil´s self-assessment correlate with individual parameters? The main tool used in the study was a didactic test on mathematics and a didactic test on the Czech language based on the Czech National Curricula Document and created by an expert team. In addition, Raven's Color Progressive Matrices (CPM), Similarities from the Wechsler Intelligence (WISC-SIM), and the Rey-Osterrieth Complex Figure (ROCF) were used. Considering the nature of the data, the non-parametric Kruskal-Wallis ANOVA was used. The present study is a part of the larger research project, involving 29 primary school classes, 657 pupils in total. Based on the data obtained, it can be concluded that the accuracy of pupils' self-assessments is low, while the accuracy of pupils' self-assessments in mathematics and Czech language differs (in mathematics there are more children with more accurate estimates and more pupils who underestimate themselves, in Czech language there are more pupils who overestimate their performance. Statistically significant differences were observed in the domains of Raven's Color Progressive Matrices and Rey-Osterrieth Figure, and in terms of the focus of each test, it could be concluded that there are significant differences between the groups in the domain of non-verbal reasoning skills and in the domain of analytical and organizational perceptual activity and memory. In the area of verbal intellectual abilities, there were no significant differences between the groups.}
}

@article{ji2022afrbertattentionbased,
  title={AFR-BERT: Attention-based mechanism feature relevance fusion multimodal sentiment analysis model},
  author={Mingyu Ji and Jiawei Zhou and Wei Ning},
  year={2022},
  booktitle={PLoS ONE},
  doi={10.1371/journal.pone.0273936},
  url={https://www.semanticscholar.org/paper/918f34bd4274316d684dd6c267b13fe010a74a6e},
  abstract={Multimodal sentiment analysis is an essential task in natural language processing which refers to the fact that machines can analyze and recognize emotions through logical reasoning and mathematical operations after learning multimodal emotional features. For the problem of how to consider the effective fusion of multimodal data and the relevance of multimodal data in multimodal sentiment analysis, we propose an attention-based mechanism feature relevance fusion multimodal sentiment analysis model (AFR-BERT). In the data pre-processing stage, text features are extracted using the pre-trained language model BERT (Bi-directional Encoder Representation from Transformers), and the BiLSTM (Bi-directional Long Short-Term Memory) is used to obtain the internal information of the audio. In the data fusion phase, the multimodal data fusion network effectively fuses multimodal features through the interaction of text and audio information. During the data analysis phase, the multimodal data association network analyzes the data by exploring the correlation of fused information between text and audio. In the data output phase, the model outputs the results of multimodal sentiment analysis. We conducted extensive comparative experiments on the publicly available sentiment analysis datasets CMU-MOSI and CMU-MOSEI. The experimental results show that AFR-BERT improves on the classical multimodal sentiment analysis model in terms of relevant performance metrics. In addition, ablation experiments and example analysis show that the multimodal data analysis network in AFR-BERT can effectively capture and analyze the sentiment features in text and audio.}
}

@article{gulwani2022aiassistedprogramming,
  title={AI-assisted programming: applications, user experiences, and neuro-symbolic techniques (keynote)},
  author={Sumit Gulwani},
  year={2022},
  booktitle={ESEC/SIGSOFT FSE},
  doi={10.1145/3540250.3569444},
  url={https://www.semanticscholar.org/paper/11230f03465d8ab073815397717d8afa3f3dae1c}
}

@article{yu2022alertadapt,
  title={ALERT: Adapt Language Models to Reasoning Tasks},
  author={Ping Yu and Tianlu Wang and O. Yu. Golovneva and Badr AlKhamissi and Gargi Ghosh and Mona T. Diab and Asli Celikyilmaz},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.08286},
  url={https://www.semanticscholar.org/paper/95c11cc5820ba32c60d5f2671f6567b9914a4978},
  abstract={Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.To address this question, we introduce {pasted macro ‘OUR’}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro ‘OUR’}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro ‘OUR’}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.}
}

@article{2022algorithmmethod,
  title={ALGORITHM METHOD IN TEACHING RUSSIAN AT SECONDARY SCHOOL},
  author={Юлия Владимировна Подкина},
  year={2022},
  booktitle={Tomsk state pedagogical university bulletin},
  doi={10.23951/1609-624x-2022-6-80-87},
  url={https://www.semanticscholar.org/paper/ded393f5b3432f3d0b9258fff2b9db33b204bf84},
  abstract={Введение. Обучение русскому языку в средней школе, развитие речи и формирование орфографических и пунктуационных навыков – важная задача, которая сопряжена с рядом трудностей. Эффективному изучению русского языка в общеобразовательной школе зачастую препятствуют такие факторы, как плохая усидчивость, отсутствие интереса к предмету, билингвизм и другое. Метод алгоритмизированного представления правил русской орфографии и пунктуации способствует наилучшему усвоению учебного материала и позволяет повысить качество обучения русскому языку школьников среднего и старшего звена. Цель − обоснование эффективности метода алгоритма в обучении русскому языку детей общеобразовательных средних школ, рассмотрение примерных моделей обучающих алгоритмов. Материал и методы. В работе применялись теоретические методы (моделирование, анализ, синтез); эмпирические методы (наблюдение, сравнение, эксперимент). Результаты и обсуждение. Простое заучивание правил не всегда приводит к повышению грамотности учащихся. Метод алгоритма предусматривает совместное с учениками составление алгоритмизированных схем различных видов, которые иллюстрируют изучаемое правило, позволяют пошагово отработать механизм рассуждения при выполнении орфографических и пунктуационных заданий. Такой подход способствует достижению высокого качества знаний путем систематической отработки практических навыков с помощью схем, адаптируемых под потребности каждого ребенка. Обучающий алгоритм может иметь разные виды: от четко сформулированной схемы (похожей на математический пример) до красочной иллюстрации, которая будет понятна детям с творческими способностями. Заключение. Метод алгоритма применяют для изучения практически любого правила русской орфографии и пунктуации. В созданной совместно с учащимися схеме должно быть отведено место для исключений и для примеров, которые ребенок впишет самостоятельно. При создании обучающей схемы школьник является активным соавтором. Схема никогда не является замкнутой системой. Она дорабатывается и совершенствуется в процессе практической деятельности учащихся. У детей из одного класса схемы могут быть совершенно различны, так как усовершенствованы и доработаны самостоятельно под руководством учителя.
 Introduction. Teaching Russian in secondary school, speech development and the formation of spelling and punctuation skills is an important task that involves a number of difficulties. Effective study of the Russian language in a secondary school is often hindered by factors such as poor perseverance, lack of interest in the subject, bilingualism, and more. Russian Russian spelling rules algorithmized representation method is considered in this paper, which allows to improve the quality of teaching Russian to middle and senior school students. The purpose is to substantiate the effectiveness of the algorithm method in teaching the Russian language to children of secondary schools, to consider approximate models of training algorithms. Material and methods. Theoretical methods (modeling, analysis, synthesis) were used in the work; empirical methods (observation, comparison, experiment). Results and discussion. Simple memorizing of the rules does not always lead to increased literacy of students. The algorithm method provides for the joint compilation of algorithmic schemes of various types with students, which illustrate the rule being studied, allow you to work out the mechanism of reasoning step by step when performing spelling and punctuation tasks. This approach contributes to the achievement of a high quality of knowledge through the systematic development of practical skills with the help of schemes adapted to the needs of each child. The training algorithm can have different types: from a clearly formulated scheme (similar to a mathematical example) up to a colorful illustration that will be understandable to children with creative abilities. Conclusion. The algorithm method can be applied to study almost any rule of Russian spelling and punctuation. In the scheme created jointly with the students, there should be a place for exceptions and for examples that the child will enter independently. When creating a training scheme, the student is an active co-author. A circuit is never a closed system. It is being refined and improved in the process of practical activity of students. For children from the same class, the schemes can be completely different, as they have been improved and finalized independently.}
}

@article{mare2022updatethermal,
  title={AN UPDATE OF THERMAL ERROR COMPENSATION MODEL VIA ON-MACHINE MEASUREMENT},
  author={M. Mareš and O. Horejš and Michal Straka and J. Švéda and Tomáš Kozlok},
  year={2022},
  journal={MM Science Journal},
  doi={10.17973/mmsj.2022_12_2022150},
  url={https://www.semanticscholar.org/paper/796f47a4059604f27ad57c3760cc7ebea9f6a020},
  abstract={Software compensation is state-of-the-art technology used to reduce CNC machine tool thermal errors, and it belongs to a key intelligent functions of modern machine tools. However, a pretrained and nonadaptive model may not be accurate and robust enough for long-term application. This research presents a transfer function based thermal error compensation model updated via on-machine measurement. A mathematical model is implemented into the machine management software of a large horizontal machining centre to compensate for thermal errors in real time using C#/C++ programming language. The results show that after the thermal error compensation model is updated via on-machine measurement, the prediction accuracy, measured as peak-to-peak values, and the normalized root mean squared error are significantly improved. The prediction accuracy of the compensation model updated via on-machine measurement strongly depends on the sampling interval of the on machine measurements.}
}

@misc{nam2022achievingunderstanding,
  title={Achieving and Understanding Out-of-Distribution Generalization in Systematic Reasoning in Small-Scale Transformers},
  author={A. Nam and Mustafa Abdool and Trevor Maxfield and James L. McClelland},
  year={2022},
  url={https://www.semanticscholar.org/paper/8283064365ae7594d891e8b7daf36fd37ca809b0},
  abstract={Out-of-distribution generalization (OODG) is a longstanding challenge for neural networks. This challenge is quite apparent in tasks with well-defined variables and rules, where explicit use of the rules could solve problems independently of the particular values of the variables, but networks tend to be tied to the range of values sampled in their training data. Large transformer-based language models have pushed the boundaries on how well neural networks can solve previously unseen problems, but their complexity and lack of clarity about the relevant content in their training data obfuscates how they achieve such robustness. As a step toward understanding how transformer-based systems generalize, we explore the question of OODG in small scale transformers trained with examples from a known distribution. Using a reasoning task based on the puzzle Sudoku, we show that OODG can occur on a complex problem if the training set includes examples sampled from the whole distribution of simpler component tasks. Successful generalization depends on carefully managing positional alignment when absolute position encoding is used, but we find that suppressing sensitivity to absolute positions overcomes this limitation. Taken together our results represent a small step toward understanding and promoting systematic generalization in transformers.}
}

@article{hppner2022advantagesdisadvantages,
  title={Advantages and disadvantages of (dedicated) model transformation languages},
  author={S. Höppner and Yves Haas and Matthias Tichy and Katharina Juhnke},
  year={2022},
  booktitle={Empirical Software Engineering},
  doi={10.1007/s10664-022-10194-7},
  url={https://www.semanticscholar.org/paper/d96fa397010fa107aadcedbff577feead334e3be},
  abstract={Model driven development envisages the use of model transformations to evolve models. Model transformation languages, developed for this task, are touted with many benefits over general purpose programming languages. However, a large number of these claims have not yet been substantiated. They are also made without the context necessary to be able to critically assess their merit or built meaningful empirical studies around them. The objective of our work is to elicit the reasoning, influences and background knowledge that lead people to assume benefits or drawbacks of model transformation languages. We conducted a large-scale interview study involving 56 participants from research and industry. Interviewees were presented with claims about model transformation languages and were asked to provide reasons for their assessment thereof. We qualitatively analysed the responses to find factors that influence the properties of model transformation languages as well as explanations as to how exactly they do so. Our interviews show, that general purpose expressiveness of GPLs, domain specific capabilities of MTLs as well as tooling all have strong influences on how people view properties of model transformation languages. Moreover, the Choice of MTL, the Use Case for which a transformation should be developed as well as the Skill s of involved stakeholders have a moderating effect on the influences, by changing the context to consider. There is a broad body of experience, that suggests positive and negative influences for properties of MTLs. Our data suggests, that much needs to be done in order to convey the viability of model transformation languages. Efforts to provide more empirical substance need to be undergone and lacklustre language capabilities and tooling need to be improved upon. We suggest several approaches for this that can be based on the results of the presented study.}
}

@article{abramson2022applicationpseudologlikelihoods,
  title={An Application of Pseudo-Log-Likelihoods to Natural Language Scoring},
  author={Darren Abramson and Ali Emami},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/16bf88a6d172699cb9a26a6936efb4941e3f3c13},
  abstract={Language models built using semi-supervised machine learning on large corpora of natural language have very quickly enveloped the fields of natural language generation and understanding. In this paper we apply a zero-shot approach independently developed by a number of researchers now gaining recognition as a significant alternative to fine-tuning for evaluation on common sense tasks. A language model with relatively few parameters and training steps compared to a more recent language model (T5) can outperform it on a recent large data set (TimeDial), while displaying robustness in its performance across a similar class of language tasks. Surprisingly, this result is achieved by using a hyperparameter-free zero-shot method with the smaller model, compared to fine-tuning to the larger model. We argue that robustness of the smaller model ought to be understood in terms of compositionality, in a sense that we draw from recent literature on a class of similar models. We identify a practical cost for our method and model: high GPU-time for natural language evaluation. The zero-shot measurement technique that produces remarkable stability, both for ALBERT and other BERT variants, is an application of pseudo-log-likelihoods to masked language models for the relative measurement of probability for substitution alternatives in forced choice language tasks such as the Winograd Schema Challenge, Winogrande, and others. One contribution of this paper is to bring together a number of similar, but independent strands of research. We produce some absolute state-of-the-art results for common sense reasoning in binary choice tasks, performing better than any published result in the literature, including fine-tuned efforts. We show a remarkable consistency of the model's performance under adversarial settings, which we argue is best explained by the model's compositionality of representations.}
}

@article{zhang2022empiricalinvestigation,
  title={An Empirical Investigation of Commonsense Self-Supervision with Knowledge Graphs},
  author={Jiarui Zhang and Filip Ilievski and Kaixin Ma and Jonathan M Francis and A. Oltramari},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2205.10661},
  url={https://www.semanticscholar.org/paper/651ae53112e73b02440773727b68cedbf8322705},
  abstract={Self-supervision based on the information extracted from large knowledge graphs has been shown to improve the generalization of language models, in zero-shot evaluation on various downstream language reasoning tasks. Since these improvements are reported in aggregate, however, little is known about (i) how to select the appropriate knowledge for solid performance across tasks, (ii) how to combine this knowledge with neural language models, and (iii) how these pairings affect granular task performance. In this paper, we study the effect of knowledge sampling strategies and sizes that can be used to generate synthetic data for adapting language models. We study the effect of different synthetic datasets on language models with various architectures and sizes. The resulting models are evaluated against four task properties: domain overlap, answer similarity, vocabulary overlap, and answer length. Our experiments show that encoder-decoder models benefit from more data to learn from, whereas sampling strategies that balance across different aspects yield best performance. Most of the improvement occurs on questions with short answers and dissimilar answer candidates, which corresponds to the characteristics of the data used for pre-training.}
}

@article{khan2022executableformal,
  title={An Executable Formal Model of the VHDL in Isabelle/HOL},
  author={Wilayat Khan and Zhé Hóu and David Sanán and J. Nebhen and Yang Liu and Alwen Tiu},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/37b0b6db785f8c37460e2bb80da138c1443af5b4},
  abstract={In the hardware design process, hardware components are usually described in a hardware description language. Most of the hardware description languages, such as Verilog and VHDL, do not have mathematical foundation and hence are not fit for formal reasoning about the design. To enable formal reasoning in one of the most commonly used description language VHDL, we define a formal model of the VHDL language in Isabelle/HOL. Our model targets the functional part of VHDL designs used in industry, specifically the design of the LEON3 processor's integer unit. We cover a wide range of features in the VHDL language that are usually not modelled in the literature and define a novel operational semantics for it. Furthermore, our model can be exported to OCaml code for execution, turning the formal model into a VHDL simulator. We have tested our simulator against simple designs used in the literature, as well as the div32 module in the LEON3 design. The Isabelle/HOL code is publicly available: https://zhehou.github.io/apps/VHDLModel.zip}
}

@article{katra2022experimentationframework,
  title={An Experimentation Framework for Specification and Verification of Web Services},
  author={Szymon Katra and Wiktor B. Daszczuk and Danny Czejdo},
  year={2022},
  booktitle={Conference on Computer Science and Information Systems},
  doi={10.15439/2022F188},
  url={https://www.semanticscholar.org/paper/9fbe3dc7a2229a5435fc7ace6978550af5ac3268},
  abstract={Designing and implementing Web Services constitutes a large and constantly growing part of the information technology market. Web Services have specific scenarios in which distributed processes and network resources are used. This aspect of services requires integration with the model checkers. This article presents the experimentation framework in which services can be specified and then formally analyzed for deadlock-freedom, achievement of process goals, and similar features. Rybu4WS language enriches the basic Rybu language with the ability to use variables in processes, service calls between servers, new structural instructions, and other constructions known to programmers while remaining in line with declarative, mathematical IMDS formalism. Additionally, the development environment allows simulation of a counterexample or a witness - obtained as a result of the model checking - in a similar way to traditional debuggers.}
}

@article{jeon2022informationtheoreticanalysis,
  title={An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws},
  author={Hong Jun Jeon and Benjamin Van Roy},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.01365},
  url={https://www.semanticscholar.org/paper/dab053b7713b77ab09f50b90b3176607912e913a},
  abstract={We study the compute-optimal trade-off between model and training data set sizes for large neural networks. Our result suggests a linear relation similar to that supported by the empirical analysis of chinchilla. While that work studies transformer-based large language models trained on the MassiveText corpus gopher, as a starting point for development of a mathematical theory, we focus on a simpler learning model and data generating process, each based on a neural network with a sigmoidal output unit and single hidden layer of ReLU activation units. We introduce general error upper bounds for a class of algorithms which incrementally update a statistic (for example gradient descent). For a particular learning model inspired by barron 1993, we establish an upper bound on the minimal information-theoretically achievable expected error as a function of model and data set sizes. We then derive allocations of computation that minimize this bound. We present empirical results which suggest that this approximation correctly identifies an asymptotic linear compute-optimal scaling. This approximation also generates new insights. Among other things, it suggests that, as the input dimension or latent space complexity grows, as might be the case for example if a longer history of tokens is taken as input to a language model, a larger fraction of the compute budget should be allocated to growing the learning model rather than training data.}
}

@article{bellomarini2022overviewvadalog,
  title={An Overview of Vadalog: a System for Reasoning over Large Knowledge Graphs},
  author={Luigi Bellomarini and Davide Benedetto and Emanuel Sallinger},
  year={2022},
  booktitle={Sistemi Evoluti per Basi di Dati},
  url={https://www.semanticscholar.org/paper/83dc0eca1a453e2970d32923bb48bb84976bd968}
}

@article{amaliyah2022analisiskesulitan,
  title={Analisis Kesulitan Belajar Matematika dalam Menyelesaikan Soal Cerita di Kelas IV Sekolah Dasar Negeri Pakujaya 02},
  author={Aam Amaliyah and Luthfia Nur Maulida and N. Safitri and Ratri Hersita Dewi and Sabgi Wulan Septiara},
  year={2022},
  booktitle={ALSYS},
  doi={10.58578/alsys.v2i3.386},
  url={https://www.semanticscholar.org/paper/5023bebd78bb5f55a0d706f94b27f718b9c83cfc},
  abstract={Students with learning difficulties in mathematics often make mistakes in solving story problems on fractional material. This research uses descriptive qualitative. The purpose of this study was to determine the types of learning difficulties in mathematics experienced by students, the factors that influence learning difficulties, and to reveal the efforts that can be made to overcome the difficulties in learning mathematics in grade IV Pakujaya 02 State Elementary School. Data collection techniques were observation and interviews. . Based on data analysis and discussion, students experienced errors, namely: 1. Understanding the problem, namely errors in interpreting language and making mathematical models. The reason is incomplete/wrong reasoning and low student ability. 2. Planning for problem solving is an error in connecting one concept with another concept. The cause of this error is the humanistic thinking of students. 3. Implement problem solving planning, namely errors in implementing incorrect formulas. Errors in this aspect are caused by incomplete or incorrect reasoning and students' humanistic thinking.}
}

@article{shidqiya2022analysisstudents,
  title={Analysis of Students’ Mathematical Thinking Ability in Terms of Self Efficacy},
  author={Adiba Idlal Shidqiya and Sukestiyarno Sukestiyarno},
  year={2022},
  journal={Unnes Journal of Mathematics Education},
  doi={10.15294/ujme.v11i3.58772},
  url={https://www.semanticscholar.org/paper/fa5b5d97f15b5244e34a49e44317a1822b3e0daa},
  abstract={Mathematical thinking ability must be owned by students to solve various problems. Students are considered capable of fulfilling the indicators of mathematical thinking ability properly if they are balanced with good self-efficacy abilities. This research method is qualitative which aims to find new indicators and describe mathematical thinking ability in terms of self-efficacy and provide recommendations for teachers. The research subjects were six students from the first year of senior high school using purposive sampling. Indicators of mathematical thinking ability, include 1) Reasoning: identifying concepts and problems; 2) Generalizing: demonstrating mathematical ideas in writing and using mathematical language to express ideas correctly; 3) Critical Thinking: using representations to create mathematical models; 4) Problem Solving: planning problem solving strategies, implementing and checking results. 5) Communicating: revealing the results of problem solving. The results: 1) low self-efficacy’s students were only able to master reasoning; 2) moderate self-efficacy’s students are able to master reasoning, generalizing, and critical thinking; 3) high self-efficacy’s students are able to master all indicators. Recommendations for teachers are by giving opportunity to low self-efficacy’s students to speak in public, give appreciation for their efforts and reprimand if it doesn’t lower their confidence when they make mistakes.}
}

@article{yu2022analysiscorrelation,
  title={Analysis of the Correlation between Academic Performance and Learning Motivation in English Course under a Corpus-Data-Driven Blended Teaching Model},
  author={Lan Yu and Jun Shen},
  year={2022},
  booktitle={Scientific Programming},
  doi={10.1155/2022/3407270},
  url={https://www.semanticscholar.org/paper/6f554d023d8e403e5ee70268e55f5b2fe1be574e},
  abstract={To explore the correlation between academic performance and learning motivation in English course under a corpus-data-driven blended teaching model, this study set research objects as 62 year-2020-enrolled undergraduate students majoring in English from a university in Jinan City, Shandong Province, eastern China. According to their previous frequencies of using information technology to learn English, these 62 students were divided into two groups: practice group with high frequency and control group with low frequency, with 31 students in each group. The two groups of students were taught 3 English lessons per week for a total of 15 weeks by the exact same teachers using a corpus-data-driven blended teaching model. The students’ English academic performances were assessed by well-organized final tests, and their English learning motivations were measured by a motivation scale and questionnaires. The results show that the correlation coefficients between the average score of motivation questionnaires, intrinsic motivation factors, extrinsic motivation factors, and the average score of academic performances in practice group were 0.894, 0.682, and 0.724, respectively, while those in control group were 0.749, 0.836, and 0.904. In all the above correlation analyses, the significance level is 0.01, and all coefficient values are higher than critical value. Hence, there is a positive correlation between learning motivation and academic performance of the two groups of subjects. It is found that the corpus-data-driven blended teaching model has a significant impact on college students’ English academic performance and learning motivation, and it has a positive effect on the improvement of their English academic performance and the cultivation of learning motivation. In general, the key to this teaching model lies in reasoning and acquisition by analyzing the language provided by the corpus, and the whole process of data-driven learning is student-centered. Students are exposed to a large number of authentic language knowledge and cultural information, which promotes the sensitivity to relevant points. The results of this paper provide a reference for further research on the analysis of the correlation between academic performance and learning motivation in English course under the corpus-data-driven blended teaching model.}
}

@article{kumar2022answerlevelcalibration,
  title={Answer-level Calibration for Free-form Multiple Choice Question Answering},
  author={Sawan Kumar},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2022.acl-long.49},
  url={https://www.semanticscholar.org/paper/a5584d2d9b0de9e1692241d46d0c70942919cd60},
  abstract={Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks. This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem. While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration. In this work, we consider the question answering format, where we need to choose from a set of (free-form) textual choices of unspecified lengths given a context. We present ALC (Answer-Level Calibration), where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context. We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks. Further, we show that popular datasets potentially favor models biased towards easy cues which are available independent of the context. We analyze such biases using an associated F1-score. Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability.}
}

@article{zhou2022applicationthreeflow,
  title={Application of Three-Flow Fusion Technology Based on Modelica in Thermal Power Digital Twin},
  author={Dongyan Zhou and Haidong Gao and Wenyu Wang and Jun Cao and Wenfei Yang and Ruirui Zeng and Yuan He},
  year={2022},
  journal={IEEE Journal of Radio Frequency Identification},
  doi={10.1109/JRFID.2022.3205855},
  url={https://www.semanticscholar.org/paper/5391cf3bf8f2cc858ee1a532be7d9e2e6b6f6983},
  abstract={Thermal power plants gather large energy infrastructure; therefore, massive historical and real-time data of equipment operation will be generated in daily operation. Digital industrialization puts forward higher requirements for the use of big data than simple tasks, such as generating reports. MWorks is a multidomain unified modeling and simulation platform based on the Modelica language. In this study, MWorks is used to realize the modeling of multidomain systems, including electrical, thermal, mechanical, fluid, and heat transfer, in power plants. The test, calibration, verification, parameter optimization, and fault diagnosis of the thermal power plant mathematical models, which are historical and real-time data-driven, are discussed. The technology of three-flow fusion, including material flow, energy flow, and information flow, and its application in thermal power digital twin are explored.}
}

@article{alghamdi2022armathdataset,
  title={ArMATH: a Dataset for Solving Arabic Math Word Problems},
  author={Reem Alghamdi and Zhenwen Liang and Xiangliang Zhang},
  year={2022},
  booktitle={International Conference on Language Resources and Evaluation},
  url={https://www.semanticscholar.org/paper/4aca69be58a271b1be45ec7ebb3586569cec50b0}
}

@article{kar2022arggenprompting,
  title={ArgGen: Prompting Text Generation Models for Document-Level Event-Argument Aggregation},
  author={Debanjana Kar and S. Sarkar and Pawan Goyal},
  year={2022},
  booktitle={AACL/IJCNLP},
  doi={10.18653/v1/2022.findings-aacl.37},
  url={https://www.semanticscholar.org/paper/61f49465c0d53663ad5264c8f683c6724d31eef1},
  abstract={Most of the existing discourse-level Information Extraction tasks have been modeled to be extractive in nature. However, we argue that extracting information from larger bodies of discourse-like documents requires more natural language understanding and reasoning capabilities. In our work, we propose the novel task of document-level event argument aggregation which generates consolidated event-arguments at a document-level with minimal loss of information. More specifically, we focus on generating precise document-level information frames in a multilingual setting using prompt-based methods. In this paper, we show the effectiveness of prompt-based text generation approach to generate document-level argument spans in a low-resource and zero-shot setting. We also release the first of its kind multilingual event argument aggregation dataset that can be lever-aged in other related multilingual text generation tasks as well: https://github.com/}
}

@article{tewes2022artificialintelligence,
  title={Artificial Intelligence in the American Healthcare Industry: Looking Forward to 2030},
  author={F. Tewes},
  year={2022},
  journal={Journal of Medical Research and Surgery},
  doi={10.52916/jmrs224089},
  url={https://www.semanticscholar.org/paper/6baa97e2ca007eb2eeb51490f604d2bfd767fa0c},
  abstract={Artificial intelligence (AI) has the potential to speed up the exponential growth of cutting-edge technology, much way the Internet did. Due to intense competition from the private sector, governments, and businesspeople around the world, the Internet has already reached its peak as an exponential technology. In contrast, artificial intelligence is still in its infancy, and people all over the world are unsure of how it will impact their lives in the future. Artificial intelligence, is a field of technology that enables robots and computer programmes to mimic human intellect by teaching a predetermined set of software rules to learn by repetitive learning from experience and slowly moving toward maximum performance. Although this intelligence is still developing, it has already demonstrated five different levels of independence. Utilized initially to resolve issues. Next, think about solutions. Third, respond to inquiries. Fourth, use data analytics to generate forecasts. Fifth, make tactical recommendations. Massive data sets and "iterative algorithms," which use lookup tables and other data structures like stacks and queues to solve issues, make all of this possible. Iteration is a strategy where software rules are regularly adjusted to patterns in the data for a certain number of iterations. The artificial intelligence continuously makes small, incremental improvements that result in exponential growth, which enables the computer to become incredibly proficient at whatever it is trained to do. For each round of data processing, the artificial intelligence tests and measures its performance to develop new expertise. In order to address complicated problems, artificial intelligence aims to create computer systems that can mimic human behavior and exhibit human-like thought processes [1]. Artificial intelligence technology is being developed to give individualized medication in the field of healthcare. By 2030, six different artificial intelligence sectors will have considerably improved healthcare delivery through the utilization of larger, more accessible data sets. The first is machine learning. This area of artificial intelligence learns automatically and produces improved results based on identifying patterns in the data, gaining new insights, and enhancing the outcomes of whatever activity the system is intended to accomplish. It does this without being trained to learn a particular topic. Here are several instances of machine learning in the healthcare industry. The first is the IBM Watson Genomics, which aids in rapid disease diagnosis and identification by fusing cognitive computing with genome-based tumour sequencing. Second, a project called Nave Bayes allows for the prediction of diabetes years before an official diagnosis, before it results in harm to the kidneys, the heart, and the nerves. Third, employing two machine learning approaches termed classification and clustering to analyse the Indian Liver Patient Data (ILPD) set in order to predict liver illness before this organ that regulates metabolism becomes susceptible to chronic hepatitis, liver cancer, and cirrhosis [2]. Second, deep learning. Deep learning employs artificial intelligence to learn from data processing, much like machine learning does. Deep learning, on the other hand, makes use of synthetic neural networks that mimic human brain function to analyse data, identify relationships between the data, and provide outputs based on positive and negative reinforcement. For instance, in the fields of Magnetic Resonance Imaging (MRI) and Computed Tomography (CT), deep learning aids in the processes of picture recognition and object detection. Deep learning algorithms for the early identification of Alzheimer's, diabetic retinopathy, and breast nodule ultrasound detection are three applications of this cutting-edge technology in the real world. Future developments in deep learning will make considerable improvements in pathology and radiology pictures [3]. Third, neural networks. The artificial intelligence system can now accept massive data sets, find patterns within the data, and respond to queries regarding the information processed because the computer learning process resembles a network of neurons in the human brain. Let's examine a few application examples that are now applicable to the healthcare sector. According to studies from John Hopkins University, surgical errors are a major contributor to medical malpractice claims since they happen more than 4,000 times a year in just the United States due to the human error of surgeons. Neural networks can be used in robot-assisted surgery to model and plan procedures, evaluate the abilities of the surgeon, and streamline surgical activities. In one study of 379 orthopaedic patients, it was discovered that robotic surgery using neural networks results in five times fewer complications than surgery performed by a single surgeon. Another application of neural networks is in visualising diagnostics, which was proven to physicians by Harvard University researchers who inserted an image of a gorilla to x-rays. Of the radiologists who saw the images, 83% did not recognise the gorilla. The Houston Medical Research Institute has created a breast cancer early detection programme that can analyse mammograms with 99 percent accuracy and offer diagnostic information 30 times faster than a human [4]. Cognitive computing is the fourth. Aims to replicate the way people and machines interact, showing how a computer may operate like the human brain when handling challenging tasks like text, speech, or image analysis. Large volumes of patient data have been analysed, with the majority of the research to date focusing on cancer, diabetes, and cardiovascular disease. Companies like Google, IBM, Facebook, and Apple have shown interest in this work. Cognitive computing made up the greatest component of the artificial market in 2020, with 39% of the total [5]. Hospitals made up 42% of the market for cognitive computing end users because of the rising demand for individualised medical data. IBM invested more than $1 billion on the development of the WATSON analytics platform ecosystem and collaboration with startups committed to creating various cloud and application-based systems for the healthcare business in 2014 because it predicted the demand for cognitive computing in this sector. Natural Language Processing (NLP) is the fifth. This area of artificial intelligence enables computers to comprehend and analyse spoken language. The initial phase of this pre-processing is to divide the data up into more manageable semantic units, which merely makes the information simpler for the NLP system to understand. Clinical trial development is experiencing exponential expansion in the healthcare sector thanks to NLP. First, the NLP uses speech-to-text dictation and structured data entry to extract clinical data at the point of care, reducing the need for manual assessment of complex clinical paperwork. Second, using NLP technology, healthcare professionals can automatically examine enormous amounts of unstructured clinical and patient data to select the most suitable patients for clinical trials, perhaps leading to an improvement in the patients' health [6]. Computer vision comes in sixth. Computer vision, an essential part of artificial intelligence, uses visual data as input to process photos and videos continuously in order to get better results faster and with higher quality than would be possible if the same job were done manually. Simply put, doctors can now diagnose their patients with diseases like cancer, diabetes, and cardiovascular disorders more quickly and at an earlier stage. Here are a few examples of real-world applications where computer vision technology is making notable strides. Mammogram images are analysed by visual systems that are intended to spot breast cancer at an early stage. Automated cell counting is another example from the real world that dramatically decreases human error and raises concerns about the accuracy of the results because they might differ greatly depending on the examiner's experience and degree of focus. A third application of computer vision in the real world is the quick and painless early-stage tumour detection enabled by artificial intelligence. Without a doubt, computer vision has the unfathomable potential to significantly enhance how healthcare is delivered. Other than for visual data analysis, clinicians can use this technology to enhance their training and skill development. Currently, Gramener is the top company offering medical facilities and research organisations computer vision solutions [7]. The usage of imperative rather than functional programming languages is one of the key difficulties in creating artificial intelligence software. As artificial intelligence starts to increase exponentially, developers employing imperative programming languages must assume that the machine is stupid and supply detailed instructions that are subject to a high level of maintenance and human error. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures of research and development. As a result, software developers have contributed to the unreasonably high cost of medical care. Functional programming languages, on the other hand, demand that the developer use their problem-solving abilities as though the computer were a mathematician. As a result, compared to the number of lines of code needed by the programme to perform the same operation, mathematical functions are orders of magnitude shorter. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures o}
}

@article{zimmerman2022assessingphysics,
  title={Assessing physics quantitative literacy in algebra-based physics: lessons learned},
  author={Charlotte Zimmerman and Andrew McCarty and Suzanne White Brahmia and Alexis Olsho and Mieke De Cock and A. Boudreaux and Trevor I. Smith and Philip Eaton},
  year={2022},
  booktitle={Physics Education Research Conference Proceedings},
  doi={10.1119/perc.2022.pr.zimmerman},
  url={https://www.semanticscholar.org/paper/8f16d42771af2c1227c7a4cf6ad219e54351c9f7},
  abstract={Physics quantitative literacy (PQL)—applying familiar mathematics in novel ways in the context of physics— is ubiquitous across physics classrooms. The Physics Inventory for Quantitative Literacy, or PIQL, is a recently published reasoning inventory that can be used to assess PQL from calculus-based introductory physics through upper division courses (White Brahmia et al. 2021). There remains a need, however, for assessment of quantitative reasoning at the algebra-based level which includes not only algebra-based college courses but also pre-college physics courses. We present recent work adapting the PIQL to an algebra-based context towards developing the GERQN—the Generalized Equation-based Reasoning inventory for Quantities and Negativity. We report lessons learned from our efforts to adapt items from the calculus-based PIQL to the algebra-based GERQN, and provide examples of how items were revised to be within students proximal zone. We also report on our experience translating the GERQN into Flemish as part of a larger, on-going research project, and what we learned about language accessibility for native and non-native English speakers alike for developing assessment items, curricular materials, and when speaking with students.}
}

@misc{kogan2022assessingacademic,
  title={Assessing the Academic Recovery of Ohio Students: An Analysis of Spring 2022 Ohio State Tests},
  author={Vladimir Kogan},
  year={2022},
  url={https://www.semanticscholar.org/paper/76ac1af061d5d2ccf19d748ca8b744a9461260f3}
}

@article{gao2022attributedtext,
  title={Attributed Text Generation via Post-hoc Research and Revision},
  author={Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Zhao and N. Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.08726},
  url={https://www.semanticscholar.org/paper/4ef5410ec4b546eda642fe786cc1bdbb5a7251e1}
}

@misc{wu2022autoformalizationneural,
  title={Autoformalization for Neural Theorem Proving},
  author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
  year={2022},
  url={https://www.semanticscholar.org/paper/15e767aa26a14455da95a2b2f11e3d59f2c250f6}
}

=== END BIBTEX ENTRIES ===

=== NEW PAPERS BEING ADDED THIS ITERATION ===
@article{katra2022experimentationframework,
  title={An Experimentation Framework for Specification and Verification of Web Services},
  author={Szymon Katra and Wiktor B. Daszczuk and Danny Czejdo},
  year={2022},
  booktitle={Conference on Computer Science and Information Systems},
  doi={10.15439/2022F188},
  url={https://www.semanticscholar.org/paper/9fbe3dc7a2229a5435fc7ace6978550af5ac3268},
  abstract={Designing and implementing Web Services constitutes a large and constantly growing part of the information technology market. Web Services have specific scenarios in which distributed processes and network resources are used. This aspect of services requires integration with the model checkers. This article presents the experimentation framework in which services can be specified and then formally analyzed for deadlock-freedom, achievement of process goals, and similar features. Rybu4WS language enriches the basic Rybu language with the ability to use variables in processes, service calls between servers, new structural instructions, and other constructions known to programmers while remaining in line with declarative, mathematical IMDS formalism. Additionally, the development environment allows simulation of a counterexample or a witness - obtained as a result of the model checking - in a similar way to traditional debuggers.}
}

@article{jeon2022informationtheoreticanalysis,
  title={An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws},
  author={Hong Jun Jeon and Benjamin Van Roy},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.01365},
  url={https://www.semanticscholar.org/paper/dab053b7713b77ab09f50b90b3176607912e913a},
  abstract={We study the compute-optimal trade-off between model and training data set sizes for large neural networks. Our result suggests a linear relation similar to that supported by the empirical analysis of chinchilla. While that work studies transformer-based large language models trained on the MassiveText corpus gopher, as a starting point for development of a mathematical theory, we focus on a simpler learning model and data generating process, each based on a neural network with a sigmoidal output unit and single hidden layer of ReLU activation units. We introduce general error upper bounds for a class of algorithms which incrementally update a statistic (for example gradient descent). For a particular learning model inspired by barron 1993, we establish an upper bound on the minimal information-theoretically achievable expected error as a function of model and data set sizes. We then derive allocations of computation that minimize this bound. We present empirical results which suggest that this approximation correctly identifies an asymptotic linear compute-optimal scaling. This approximation also generates new insights. Among other things, it suggests that, as the input dimension or latent space complexity grows, as might be the case for example if a longer history of tokens is taken as input to a language model, a larger fraction of the compute budget should be allocated to growing the learning model rather than training data.}
}

@article{bellomarini2022overviewvadalog,
  title={An Overview of Vadalog: a System for Reasoning over Large Knowledge Graphs},
  author={Luigi Bellomarini and Davide Benedetto and Emanuel Sallinger},
  year={2022},
  booktitle={Sistemi Evoluti per Basi di Dati},
  url={https://www.semanticscholar.org/paper/83dc0eca1a453e2970d32923bb48bb84976bd968}
}

@article{amaliyah2022analisiskesulitan,
  title={Analisis Kesulitan Belajar Matematika dalam Menyelesaikan Soal Cerita di Kelas IV Sekolah Dasar Negeri Pakujaya 02},
  author={Aam Amaliyah and Luthfia Nur Maulida and N. Safitri and Ratri Hersita Dewi and Sabgi Wulan Septiara},
  year={2022},
  booktitle={ALSYS},
  doi={10.58578/alsys.v2i3.386},
  url={https://www.semanticscholar.org/paper/5023bebd78bb5f55a0d706f94b27f718b9c83cfc},
  abstract={Students with learning difficulties in mathematics often make mistakes in solving story problems on fractional material. This research uses descriptive qualitative. The purpose of this study was to determine the types of learning difficulties in mathematics experienced by students, the factors that influence learning difficulties, and to reveal the efforts that can be made to overcome the difficulties in learning mathematics in grade IV Pakujaya 02 State Elementary School. Data collection techniques were observation and interviews. . Based on data analysis and discussion, students experienced errors, namely: 1. Understanding the problem, namely errors in interpreting language and making mathematical models. The reason is incomplete/wrong reasoning and low student ability. 2. Planning for problem solving is an error in connecting one concept with another concept. The cause of this error is the humanistic thinking of students. 3. Implement problem solving planning, namely errors in implementing incorrect formulas. Errors in this aspect are caused by incomplete or incorrect reasoning and students' humanistic thinking.}
}

@article{shidqiya2022analysisstudents,
  title={Analysis of Students’ Mathematical Thinking Ability in Terms of Self Efficacy},
  author={Adiba Idlal Shidqiya and Sukestiyarno Sukestiyarno},
  year={2022},
  journal={Unnes Journal of Mathematics Education},
  doi={10.15294/ujme.v11i3.58772},
  url={https://www.semanticscholar.org/paper/fa5b5d97f15b5244e34a49e44317a1822b3e0daa},
  abstract={Mathematical thinking ability must be owned by students to solve various problems. Students are considered capable of fulfilling the indicators of mathematical thinking ability properly if they are balanced with good self-efficacy abilities. This research method is qualitative which aims to find new indicators and describe mathematical thinking ability in terms of self-efficacy and provide recommendations for teachers. The research subjects were six students from the first year of senior high school using purposive sampling. Indicators of mathematical thinking ability, include 1) Reasoning: identifying concepts and problems; 2) Generalizing: demonstrating mathematical ideas in writing and using mathematical language to express ideas correctly; 3) Critical Thinking: using representations to create mathematical models; 4) Problem Solving: planning problem solving strategies, implementing and checking results. 5) Communicating: revealing the results of problem solving. The results: 1) low self-efficacy’s students were only able to master reasoning; 2) moderate self-efficacy’s students are able to master reasoning, generalizing, and critical thinking; 3) high self-efficacy’s students are able to master all indicators. Recommendations for teachers are by giving opportunity to low self-efficacy’s students to speak in public, give appreciation for their efforts and reprimand if it doesn’t lower their confidence when they make mistakes.}
}

@article{yu2022analysiscorrelation,
  title={Analysis of the Correlation between Academic Performance and Learning Motivation in English Course under a Corpus-Data-Driven Blended Teaching Model},
  author={Lan Yu and Jun Shen},
  year={2022},
  booktitle={Scientific Programming},
  doi={10.1155/2022/3407270},
  url={https://www.semanticscholar.org/paper/6f554d023d8e403e5ee70268e55f5b2fe1be574e},
  abstract={To explore the correlation between academic performance and learning motivation in English course under a corpus-data-driven blended teaching model, this study set research objects as 62 year-2020-enrolled undergraduate students majoring in English from a university in Jinan City, Shandong Province, eastern China. According to their previous frequencies of using information technology to learn English, these 62 students were divided into two groups: practice group with high frequency and control group with low frequency, with 31 students in each group. The two groups of students were taught 3 English lessons per week for a total of 15 weeks by the exact same teachers using a corpus-data-driven blended teaching model. The students’ English academic performances were assessed by well-organized final tests, and their English learning motivations were measured by a motivation scale and questionnaires. The results show that the correlation coefficients between the average score of motivation questionnaires, intrinsic motivation factors, extrinsic motivation factors, and the average score of academic performances in practice group were 0.894, 0.682, and 0.724, respectively, while those in control group were 0.749, 0.836, and 0.904. In all the above correlation analyses, the significance level is 0.01, and all coefficient values are higher than critical value. Hence, there is a positive correlation between learning motivation and academic performance of the two groups of subjects. It is found that the corpus-data-driven blended teaching model has a significant impact on college students’ English academic performance and learning motivation, and it has a positive effect on the improvement of their English academic performance and the cultivation of learning motivation. In general, the key to this teaching model lies in reasoning and acquisition by analyzing the language provided by the corpus, and the whole process of data-driven learning is student-centered. Students are exposed to a large number of authentic language knowledge and cultural information, which promotes the sensitivity to relevant points. The results of this paper provide a reference for further research on the analysis of the correlation between academic performance and learning motivation in English course under the corpus-data-driven blended teaching model.}
}

@article{kumar2022answerlevelcalibration,
  title={Answer-level Calibration for Free-form Multiple Choice Question Answering},
  author={Sawan Kumar},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2022.acl-long.49},
  url={https://www.semanticscholar.org/paper/a5584d2d9b0de9e1692241d46d0c70942919cd60},
  abstract={Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks. This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem. While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration. In this work, we consider the question answering format, where we need to choose from a set of (free-form) textual choices of unspecified lengths given a context. We present ALC (Answer-Level Calibration), where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context. We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks. Further, we show that popular datasets potentially favor models biased towards easy cues which are available independent of the context. We analyze such biases using an associated F1-score. Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability.}
}

@article{zhou2022applicationthreeflow,
  title={Application of Three-Flow Fusion Technology Based on Modelica in Thermal Power Digital Twin},
  author={Dongyan Zhou and Haidong Gao and Wenyu Wang and Jun Cao and Wenfei Yang and Ruirui Zeng and Yuan He},
  year={2022},
  journal={IEEE Journal of Radio Frequency Identification},
  doi={10.1109/JRFID.2022.3205855},
  url={https://www.semanticscholar.org/paper/5391cf3bf8f2cc858ee1a532be7d9e2e6b6f6983},
  abstract={Thermal power plants gather large energy infrastructure; therefore, massive historical and real-time data of equipment operation will be generated in daily operation. Digital industrialization puts forward higher requirements for the use of big data than simple tasks, such as generating reports. MWorks is a multidomain unified modeling and simulation platform based on the Modelica language. In this study, MWorks is used to realize the modeling of multidomain systems, including electrical, thermal, mechanical, fluid, and heat transfer, in power plants. The test, calibration, verification, parameter optimization, and fault diagnosis of the thermal power plant mathematical models, which are historical and real-time data-driven, are discussed. The technology of three-flow fusion, including material flow, energy flow, and information flow, and its application in thermal power digital twin are explored.}
}

@article{alghamdi2022armathdataset,
  title={ArMATH: a Dataset for Solving Arabic Math Word Problems},
  author={Reem Alghamdi and Zhenwen Liang and Xiangliang Zhang},
  year={2022},
  booktitle={International Conference on Language Resources and Evaluation},
  url={https://www.semanticscholar.org/paper/4aca69be58a271b1be45ec7ebb3586569cec50b0}
}

@article{kar2022arggenprompting,
  title={ArgGen: Prompting Text Generation Models for Document-Level Event-Argument Aggregation},
  author={Debanjana Kar and S. Sarkar and Pawan Goyal},
  year={2022},
  booktitle={AACL/IJCNLP},
  doi={10.18653/v1/2022.findings-aacl.37},
  url={https://www.semanticscholar.org/paper/61f49465c0d53663ad5264c8f683c6724d31eef1},
  abstract={Most of the existing discourse-level Information Extraction tasks have been modeled to be extractive in nature. However, we argue that extracting information from larger bodies of discourse-like documents requires more natural language understanding and reasoning capabilities. In our work, we propose the novel task of document-level event argument aggregation which generates consolidated event-arguments at a document-level with minimal loss of information. More specifically, we focus on generating precise document-level information frames in a multilingual setting using prompt-based methods. In this paper, we show the effectiveness of prompt-based text generation approach to generate document-level argument spans in a low-resource and zero-shot setting. We also release the first of its kind multilingual event argument aggregation dataset that can be lever-aged in other related multilingual text generation tasks as well: https://github.com/}
}

@article{tewes2022artificialintelligence,
  title={Artificial Intelligence in the American Healthcare Industry: Looking Forward to 2030},
  author={F. Tewes},
  year={2022},
  journal={Journal of Medical Research and Surgery},
  doi={10.52916/jmrs224089},
  url={https://www.semanticscholar.org/paper/6baa97e2ca007eb2eeb51490f604d2bfd767fa0c},
  abstract={Artificial intelligence (AI) has the potential to speed up the exponential growth of cutting-edge technology, much way the Internet did. Due to intense competition from the private sector, governments, and businesspeople around the world, the Internet has already reached its peak as an exponential technology. In contrast, artificial intelligence is still in its infancy, and people all over the world are unsure of how it will impact their lives in the future. Artificial intelligence, is a field of technology that enables robots and computer programmes to mimic human intellect by teaching a predetermined set of software rules to learn by repetitive learning from experience and slowly moving toward maximum performance. Although this intelligence is still developing, it has already demonstrated five different levels of independence. Utilized initially to resolve issues. Next, think about solutions. Third, respond to inquiries. Fourth, use data analytics to generate forecasts. Fifth, make tactical recommendations. Massive data sets and "iterative algorithms," which use lookup tables and other data structures like stacks and queues to solve issues, make all of this possible. Iteration is a strategy where software rules are regularly adjusted to patterns in the data for a certain number of iterations. The artificial intelligence continuously makes small, incremental improvements that result in exponential growth, which enables the computer to become incredibly proficient at whatever it is trained to do. For each round of data processing, the artificial intelligence tests and measures its performance to develop new expertise. In order to address complicated problems, artificial intelligence aims to create computer systems that can mimic human behavior and exhibit human-like thought processes [1]. Artificial intelligence technology is being developed to give individualized medication in the field of healthcare. By 2030, six different artificial intelligence sectors will have considerably improved healthcare delivery through the utilization of larger, more accessible data sets. The first is machine learning. This area of artificial intelligence learns automatically and produces improved results based on identifying patterns in the data, gaining new insights, and enhancing the outcomes of whatever activity the system is intended to accomplish. It does this without being trained to learn a particular topic. Here are several instances of machine learning in the healthcare industry. The first is the IBM Watson Genomics, which aids in rapid disease diagnosis and identification by fusing cognitive computing with genome-based tumour sequencing. Second, a project called Nave Bayes allows for the prediction of diabetes years before an official diagnosis, before it results in harm to the kidneys, the heart, and the nerves. Third, employing two machine learning approaches termed classification and clustering to analyse the Indian Liver Patient Data (ILPD) set in order to predict liver illness before this organ that regulates metabolism becomes susceptible to chronic hepatitis, liver cancer, and cirrhosis [2]. Second, deep learning. Deep learning employs artificial intelligence to learn from data processing, much like machine learning does. Deep learning, on the other hand, makes use of synthetic neural networks that mimic human brain function to analyse data, identify relationships between the data, and provide outputs based on positive and negative reinforcement. For instance, in the fields of Magnetic Resonance Imaging (MRI) and Computed Tomography (CT), deep learning aids in the processes of picture recognition and object detection. Deep learning algorithms for the early identification of Alzheimer's, diabetic retinopathy, and breast nodule ultrasound detection are three applications of this cutting-edge technology in the real world. Future developments in deep learning will make considerable improvements in pathology and radiology pictures [3]. Third, neural networks. The artificial intelligence system can now accept massive data sets, find patterns within the data, and respond to queries regarding the information processed because the computer learning process resembles a network of neurons in the human brain. Let's examine a few application examples that are now applicable to the healthcare sector. According to studies from John Hopkins University, surgical errors are a major contributor to medical malpractice claims since they happen more than 4,000 times a year in just the United States due to the human error of surgeons. Neural networks can be used in robot-assisted surgery to model and plan procedures, evaluate the abilities of the surgeon, and streamline surgical activities. In one study of 379 orthopaedic patients, it was discovered that robotic surgery using neural networks results in five times fewer complications than surgery performed by a single surgeon. Another application of neural networks is in visualising diagnostics, which was proven to physicians by Harvard University researchers who inserted an image of a gorilla to x-rays. Of the radiologists who saw the images, 83% did not recognise the gorilla. The Houston Medical Research Institute has created a breast cancer early detection programme that can analyse mammograms with 99 percent accuracy and offer diagnostic information 30 times faster than a human [4]. Cognitive computing is the fourth. Aims to replicate the way people and machines interact, showing how a computer may operate like the human brain when handling challenging tasks like text, speech, or image analysis. Large volumes of patient data have been analysed, with the majority of the research to date focusing on cancer, diabetes, and cardiovascular disease. Companies like Google, IBM, Facebook, and Apple have shown interest in this work. Cognitive computing made up the greatest component of the artificial market in 2020, with 39% of the total [5]. Hospitals made up 42% of the market for cognitive computing end users because of the rising demand for individualised medical data. IBM invested more than $1 billion on the development of the WATSON analytics platform ecosystem and collaboration with startups committed to creating various cloud and application-based systems for the healthcare business in 2014 because it predicted the demand for cognitive computing in this sector. Natural Language Processing (NLP) is the fifth. This area of artificial intelligence enables computers to comprehend and analyse spoken language. The initial phase of this pre-processing is to divide the data up into more manageable semantic units, which merely makes the information simpler for the NLP system to understand. Clinical trial development is experiencing exponential expansion in the healthcare sector thanks to NLP. First, the NLP uses speech-to-text dictation and structured data entry to extract clinical data at the point of care, reducing the need for manual assessment of complex clinical paperwork. Second, using NLP technology, healthcare professionals can automatically examine enormous amounts of unstructured clinical and patient data to select the most suitable patients for clinical trials, perhaps leading to an improvement in the patients' health [6]. Computer vision comes in sixth. Computer vision, an essential part of artificial intelligence, uses visual data as input to process photos and videos continuously in order to get better results faster and with higher quality than would be possible if the same job were done manually. Simply put, doctors can now diagnose their patients with diseases like cancer, diabetes, and cardiovascular disorders more quickly and at an earlier stage. Here are a few examples of real-world applications where computer vision technology is making notable strides. Mammogram images are analysed by visual systems that are intended to spot breast cancer at an early stage. Automated cell counting is another example from the real world that dramatically decreases human error and raises concerns about the accuracy of the results because they might differ greatly depending on the examiner's experience and degree of focus. A third application of computer vision in the real world is the quick and painless early-stage tumour detection enabled by artificial intelligence. Without a doubt, computer vision has the unfathomable potential to significantly enhance how healthcare is delivered. Other than for visual data analysis, clinicians can use this technology to enhance their training and skill development. Currently, Gramener is the top company offering medical facilities and research organisations computer vision solutions [7]. The usage of imperative rather than functional programming languages is one of the key difficulties in creating artificial intelligence software. As artificial intelligence starts to increase exponentially, developers employing imperative programming languages must assume that the machine is stupid and supply detailed instructions that are subject to a high level of maintenance and human error. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures of research and development. As a result, software developers have contributed to the unreasonably high cost of medical care. Functional programming languages, on the other hand, demand that the developer use their problem-solving abilities as though the computer were a mathematician. As a result, compared to the number of lines of code needed by the programme to perform the same operation, mathematical functions are orders of magnitude shorter. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures o}
}

@article{zimmerman2022assessingphysics,
  title={Assessing physics quantitative literacy in algebra-based physics: lessons learned},
  author={Charlotte Zimmerman and Andrew McCarty and Suzanne White Brahmia and Alexis Olsho and Mieke De Cock and A. Boudreaux and Trevor I. Smith and Philip Eaton},
  year={2022},
  booktitle={Physics Education Research Conference Proceedings},
  doi={10.1119/perc.2022.pr.zimmerman},
  url={https://www.semanticscholar.org/paper/8f16d42771af2c1227c7a4cf6ad219e54351c9f7},
  abstract={Physics quantitative literacy (PQL)—applying familiar mathematics in novel ways in the context of physics— is ubiquitous across physics classrooms. The Physics Inventory for Quantitative Literacy, or PIQL, is a recently published reasoning inventory that can be used to assess PQL from calculus-based introductory physics through upper division courses (White Brahmia et al. 2021). There remains a need, however, for assessment of quantitative reasoning at the algebra-based level which includes not only algebra-based college courses but also pre-college physics courses. We present recent work adapting the PIQL to an algebra-based context towards developing the GERQN—the Generalized Equation-based Reasoning inventory for Quantities and Negativity. We report lessons learned from our efforts to adapt items from the calculus-based PIQL to the algebra-based GERQN, and provide examples of how items were revised to be within students proximal zone. We also report on our experience translating the GERQN into Flemish as part of a larger, on-going research project, and what we learned about language accessibility for native and non-native English speakers alike for developing assessment items, curricular materials, and when speaking with students.}
}

@misc{kogan2022assessingacademic,
  title={Assessing the Academic Recovery of Ohio Students: An Analysis of Spring 2022 Ohio State Tests},
  author={Vladimir Kogan},
  year={2022},
  url={https://www.semanticscholar.org/paper/76ac1af061d5d2ccf19d748ca8b744a9461260f3}
}

@article{gao2022attributedtext,
  title={Attributed Text Generation via Post-hoc Research and Revision},
  author={Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Zhao and N. Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.08726},
  url={https://www.semanticscholar.org/paper/4ef5410ec4b546eda642fe786cc1bdbb5a7251e1}
}

@misc{wu2022autoformalizationneural,
  title={Autoformalization for Neural Theorem Proving},
  author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
  year={2022},
  url={https://www.semanticscholar.org/paper/15e767aa26a14455da95a2b2f11e3d59f2c250f6}
}

=== END NEW PAPERS ===

PREVIOUS DRAFT OF THE PAPER:
=== ABSTRACT ===
This systematic literature review examines the intersection of large language models (LLMs) and mathematical reasoning. With an expanded corpus of 30 papers, we provide a comprehensive overview of the current landscape, identifying key tasks, datasets, and methodologies. The review highlights advancements in LLMs' ability to perform mathematical operations, solve word problems, and engage in more complex forms of mathematical deduction. We explore various approaches, including fine-tuning, prompt engineering, and the integration of external knowledge sources, to enhance LLM performance in this domain. The findings underscore the significant progress made in leveraging LLMs for mathematical tasks, while also pointing to remaining challenges in areas such as robustness, interpretability, and generalization to novel mathematical concepts. This review serves as a valuable resource for researchers and practitioners interested in the evolving capabilities of LLMs in mathematical reasoning.

=== INTRODUCTION ===
\section{Introduction}

Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide spectrum of natural language processing tasks, from text generation to complex question answering \cite{brown2020language}. A particularly challenging and important domain where LLMs are increasingly being applied is mathematical reasoning. The ability to understand, process, and generate mathematical content is fundamental to scientific discovery, engineering, finance, and many aspects of daily life. Consequently, the development of artificial intelligence systems capable of robust mathematical reasoning has been a long-standing goal in the field of AI \cite{lu2022surveydeep}. The systematic analysis of reasoning processes, whether in natural language \cite{stolfo2022causalframework} or formal logic \cite{poythress2022semioticanalysis}, is crucial for advancing AI capabilities.

Recent advancements in LLMs, particularly those based on transformer architectures \cite{vaswani2017attention}, have opened new avenues for tackling complex mathematical problems. These models, trained on massive datasets, possess an emergent ability to perform arithmetic operations, solve algebraic equations, and even engage with more abstract mathematical concepts \cite{abramson2022applicationpseudologlikelihoods}. However, the robustness and reliability of LLMs in mathematical reasoning remain active areas of research. Issues such as susceptibility to superficial patterns in problem descriptions \cite{stolfo2022causalframework}, the need for explicit reasoning capabilities \cite{zhang2022multilayerattention}, and challenges in out-of-distribution generalization \cite{nam2022achievingunderstanding} highlight the ongoing challenges. The development of AI-assisted programming further underscores the integration of AI with structured problem-solving \cite{gulwani2022aiassistedprogramming}.

This systematic literature review aims to provide a comprehensive overview of the current state of research at the intersection of large language models and mathematical reasoning. We address the following research questions:

1. What are the primary tasks and datasets used to evaluate LLMs in mathematical reasoning?
2. What are the dominant methodologies and architectures employed to enhance LLM performance in mathematical reasoning?
3. What are the key findings and limitations of current research in this area?
4. What are the promising future research directions for LLMs in mathematical reasoning?

To address these questions, we conducted a systematic search of the literature. Our methodology, detailed in the following section, adheres to the principles of the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement \cite{moher2009preferred} to ensure transparency and reproducibility. We focused on original research articles that specifically investigated the application of LLMs to mathematical reasoning tasks, including those that explore the broader context of reasoning such as formal methods \cite{khan2022executableformal}, clinical reasoning \cite{ricci2022petrinetbasedapproach}, and multimodal sentiment analysis \cite{ji2022afrbertattentionbased}, as these often involve underlying logical or quantitative components. We have also considered works that analyze the foundations of reasoning, such as semiotic analysis of logic systems \cite{poythress2022semioticanalysis}.

This paper is structured as follows: Section 2 details our methodology. Section 3 presents the results of our literature search, organized thematically. Section 4 discusses the findings, identifies research gaps, and outlines implications and future directions. Finally, Section 5 concludes the review by summarizing the key contributions and insights.

=== METHODOLOGY ===
\section{Methodology}

This systematic literature review was conducted following the PRISMA guidelines to ensure a comprehensive and transparent search and selection process \cite{moher2009preferred}. The review focused on identifying research that investigates the application of large language models (LLMs) to mathematical reasoning tasks, as well as related areas of computational reasoning and analysis.

\subsection{Search Strategy}

Our search strategy was designed to capture relevant literature from major academic databases. We utilized the following search terms, combined using Boolean operators:

* (large language model OR LLM OR transformer) AND (mathematical reasoning OR math reasoning OR quantitative reasoning OR problem solving OR algebra OR calculus OR arithmetic OR logic OR formal methods OR causal reasoning OR robustness OR generalization)

The primary databases searched were IEEE Xplore, ACM Digital Library, SpringerLink, ScienceDirect, and Google Scholar. The search was restricted to publications from 2018 to the present to capture the most recent advancements, given the rapid evolution of LLMs and associated reasoning techniques. We also included recently published works in 2022 to ensure currency.

\subsection{Inclusion and Exclusion Criteria}

To ensure the relevance and quality of the included studies, we defined strict inclusion and exclusion criteria:

*   **Inclusion Criteria:**
    *   The study must explicitly involve large language models (e.g., GPT-3, BERT variants, T5, etc.) or related AI architectures with reasoning capabilities.
    *   The study must focus on mathematical reasoning tasks, which include but are not limited to arithmetic, algebra, geometry, calculus, logical reasoning in mathematical contexts, solving mathematical word problems, and formal verification of mathematical systems. This also includes studies on causal reasoning \cite{stolfo2022causalframework}, robustness of reasoning \cite{nam2022achievingunderstanding}, and AI-assisted programming \cite{gulwani2022aiassistedprogramming}.
    *   The study must present original research, including experimental results, novel methodologies, or analyses. This includes research on modular modeling \cite{kim2022novelmodular} and named entity recognition \cite{mi2022reviewdevelopment} if they contribute to structured reasoning.
    *   The study must be published in English.

*   **Exclusion Criteria:**
    *   Survey or review articles (except as background for the current review's methodology).
    *   Studies focusing solely on natural language processing tasks unrelated to mathematical or logical reasoning.
    *   Studies that do not explicitly use or analyze LLMs or advanced reasoning techniques.
    *   Workshop papers, conference abstracts without full papers, and non-peer-reviewed articles.
    *   Studies where mathematical reasoning is a minor component and not the primary focus, unless they offer a unique perspective on reasoning mechanisms, such as in personalized vision-language models \cite{cohen2022thisunicorn}.

\subsection{Study Selection}

Following the initial search, all retrieved records were imported into a reference management software. Titles and abstracts were systematically screened by two independent reviewers based on the defined inclusion and exclusion criteria. Any disagreements were resolved through discussion or consultation with a senior researcher. Full texts of potentially relevant articles were then retrieved and assessed for final inclusion. This process aimed to identify studies that met all the criteria for inclusion. A total of 462 records were initially identified. After applying the inclusion and exclusion criteria, 30 papers were included in this review. This iterative process ensured a focused selection of highly relevant research.

\subsection{Data Extraction and Synthesis}

Data extraction involved identifying key information from each included study, such as the LLM or AI model used, the specific reasoning task, the dataset employed, the proposed methodology, and the main findings. This included extracting information on how models handle data \cite{ji2022afrbertattentionbased}, adapt to tasks \cite{yu2022alertadapt}, and the formal models used \cite{khan2022executableformal}. Due to the diverse nature of the research questions and methodologies within this field, a meta-analysis was not feasible. Instead, a narrative synthesis approach was adopted to summarize and integrate the findings. The results were organized thematically to provide a structured overview of the research landscape. Special attention was paid to studies that analyzed the accuracy of self-assessment \cite{markta2022accuracypupils} or thermal error compensation models \cite{mare2022updatethermal} to understand different facets of quantitative assessment and modeling.

\subsection{Quality Assessment}

While a formal quality assessment tool was not applied to every paper, the selection process implicitly prioritized studies with clear methodologies, robust experimental designs, and statistically sound results. The focus on peer-reviewed publications and established conference proceedings also served as a proxy for quality. The PRISMA guidelines recommend assessing the risk of bias, and our meticulous screening process aimed to minimize bias in study selection and data extraction.

=== RESULTS ===
\section{Results}

Our systematic literature search identified a significant volume of research at the intersection of large language models (LLMs), formal reasoning, and quantitative analysis. A total of 462 records were initially identified, and after applying our strict inclusion and exclusion criteria, 30 papers were included in this review. This selection reflects a focused exploration of LLMs and related AI techniques applied to reasoning tasks, including mathematical, logical, and causal reasoning, as well as formal modeling. The included studies span various aspects of AI research, from core LLM development to applications in specialized domains.

\subsection{Publication Trends and Key Venues}

The research in this domain has seen a substantial increase in publications, particularly in the last two to three years (2021-2022), coinciding with the rapid advancements in LLM capabilities and the growing interest in their reasoning abilities. The majority of the research is published in top-tier artificial intelligence and natural language processing conferences and journals, including the proceedings of the Association for Computational Linguistics (ACL), Conference on Empirical Methods in Natural Language Processing (EMNLP), and NeurIPS, as well as prominent journals like the Journal of Artificial Intelligence Research and IEEE Transactions. Specialized venues focusing on formal methods, educational technology, and specific application domains also contribute significantly \cite{khan2022executableformal, ricci2022petrinetbasedapproach, li2022scenariobasedexploration, markta2022accuracypupils}.

\subsection{Core Tasks and Datasets in Mathematical Reasoning}

Several core tasks form the backbone of evaluating LLMs in mathematical reasoning. These include:

*   **Arithmetic and Algebraic Reasoning:** Tasks involving basic and complex arithmetic operations, solving equations, and manipulating algebraic expressions. Datasets like GSM8K and MATH \cite{hendrycks2021math} are frequently used benchmarks. Studies such as \cite{lu2022surveydeep} provide a broad overview of these tasks.
*   **Mathematical Word Problems:** Understanding natural language descriptions of mathematical scenarios and formulating solutions. This is a particularly challenging area, requiring both natural language understanding and mathematical computation \cite{lu2022surveydeep}. Datasets like GSM8K \cite{kobelski2022rethinking} are crucial here.
*   **Commonsense Mathematical Reasoning:** Applying everyday knowledge to solve mathematical problems that may not have explicit mathematical formulations. Papers exploring commonsense reasoning with knowledge graphs are relevant here \cite{zhang2022empiricalinvestigation}.
*   **Formal Verification and Logic:** While not strictly LLM-based, research on formal methods and executable models, such as the VHDL formal model in Isabelle/HOL \cite{khan2022executableformal}, provides a foundation for rigorous logical reasoning that can inform LLM development. Semiotic analysis of logic systems also contributes to understanding the nature of reasoning \cite{poythress2022semioticanalysis}.

Datasets such as GSM8K \cite{kobelski2022rethinking} and MATH \cite{hendrycks2021math} are frequently used benchmarks for evaluating LLMs' performance on these tasks. The complexity of these datasets ranges from elementary school math problems to advanced high school and early undergraduate level mathematics. Research on achieving and understanding out-of-distribution generalization in systematic reasoning is also critical for evaluating model robustness on unseen problems \cite{nam2022achievingunderstanding}.

\subsection{Methodologies for Enhancing Reasoning Capabilities}

Researchers have explored various methodologies to improve LLMs' and other AI systems' reasoning capabilities. These can be broadly categorized as follows:

\subsubsection{Prompt Engineering and In-Context Learning}

This approach involves carefully crafting prompts to guide the LLM towards better reasoning. Techniques like chain-of-thought (CoT) prompting have shown significant improvements by encouraging the model to generate intermediate reasoning steps, mimicking human problem-solving processes \cite{wei2022chainofthought}. For example, models prompted with CoT can decompose complex problems into smaller, manageable steps, leading to more accurate solutions. The ALERT framework specifically aims to adapt language models to reasoning tasks through such adaptations \cite{yu2022alertadapt}.

\subsubsection{Fine-tuning Pre-trained Models}

Fine-tuning large pre-trained models on specific mathematical or reasoning datasets has been a prevalent strategy. This allows the models to adapt their parameters to the nuances of mathematical language and reasoning \cite{lu2022surveydeep}. Studies have demonstrated that fine-tuning LLMs on datasets like GSM8K can significantly boost their performance on arithmetic and algebraic problems \cite{kobelski2022rethinking}. Furthermore, fine-tuning is contrasted with zero-shot approaches that use pseudo-log-likelihoods for natural language scoring, showing competitive performance \cite{abramson2022applicationpseudologlikelihoods}.

\subsubsection{Integration of External Tools and Knowledge}

Some approaches integrate LLMs with external tools such as calculators, symbolic solvers (e.g., Wolfram Alpha), or knowledge graphs. This hybrid approach leverages the LLM's natural language understanding capabilities and the precision of specialized tools \cite{lu2022surveydeep}. Research on knowledge-enhanced pre-trained language models (KE-PLMs) aims to bridge the gap between parametric knowledge learned from text and explicit external knowledge, such as knowledge graphs \cite{hu2022surveyknowledge}. Empirical investigations into commonsense self-supervision with knowledge graphs highlight the potential of these integrations \cite{zhang2022empiricalinvestigation}.

\subsubsection{Causal Reasoning and Robustness}

Ensuring the robustness of LLMs' mathematical reasoning is critical. Researchers are investigating causal frameworks to understand how different components of a problem description influence the model's output, aiming to prevent reliance on spurious correlations \cite{stolfo2022causalframework}. This is particularly important for identifying potential biases or weaknesses in the model's reasoning process. Achieving and understanding out-of-distribution generalization is another key aspect of robustness \cite{nam2022achievingunderstanding}.

\subsubsection{Personalization and Domain-Specific Adaptations}

Work in personalized vision-language models involves adapting representations for user-specific concepts using language \cite{cohen2022thisunicorn}. While distinct from core mathematical reasoning, this highlights the broader trend of adapting AI models to specific contexts. Similarly, modular modeling approaches are used to understand complex systems, such as the electromechanics of the heart \cite{kim2022novelmodular}, demonstrating the utility of breaking down complex problems into manageable components.

\subsubsection{Hybrid Algorithms and Optimization in Applied Domains}

In applied domains, hybrid algorithms combining LLM-like principles with optimization techniques are being explored. For example, in scheduling problems, hybrid genetic algorithms are used to optimize complex tasks \cite{wang2022hybridgenetic}. Clustering strategies are applied to optimize the siting of recharging stations in the electric vehicle routing problem \cite{snchez2022clusteringapproach}, showcasing computational approaches to complex quantitative problems. Updates to thermal error compensation models via on-machine measurement also highlight adaptive modeling techniques \cite{mare2022updatethermal}.

\subsubsection{Formal Methods and Structured Reasoning}

In specialized fields, formal methods like Petri nets are used to enhance reasoning capabilities, such as in clinical reasoning for medical education \cite{ricci2022petrinetbasedapproach}. The development of executable formal models, like the VHDL model in Isabelle/HOL \cite{khan2022executableformal}, enables formal reasoning about system designs. A semiotic analysis of multiple systems of logic assesses their usefulness and limitations, contributing to a deeper understanding of formal reasoning frameworks \cite{poythress2022semioticanalysis}.

\subsubsection{Analysis of Reasoning and Understanding}

Research also delves into the nature of reasoning itself. A ratiocinative study of W. V. O. Quine's criterion of ontological commitment explores the philosophical underpinnings of reasoning and existence claims \cite{ekong2022ratiocinativestudy}. Multimodal sentiment analysis models, like AFR-BERT, fuse features using attention mechanisms to analyze emotions through logical reasoning and mathematical operations \cite{ji2022afrbertattentionbased}. AI-assisted programming is another area where human and machine reasoning are combined \cite{gulwani2022aiassistedprogramming}.

\subsubsection{Educational and Assessment Contexts}

In educational contexts, the accuracy of pupils' self-assessment in subjects like mathematics and language is investigated \cite{markta2022accuracypupils}. Learning analytics adoption is explored through scenario-based studies, considering expected usefulness and privacy concerns \cite{li2022scenariobasedexploration}. The algorithm method in teaching Russian also highlights structured, step-by-step approaches to learning \cite{2022algorithmmethod}.

\subsubsection{Bias and Fairness in NLP}

Methodologies are being developed to characterize bias and harmful stereotypes in natural language processing, often involving collaboration between social scientists and ML experts \cite{alemany2022methodologycharacterize}. This is crucial for ensuring fair and equitable applications of AI, including those involving quantitative reasoning.

\subsubsection{Comparison of Approaches and Model Transformations}

Studies compare different approaches for complex problems. This includes comparisons of methods for imbalanced classification \cite{wankmller2022comparisonapproaches}, comparison of approaches to covariate effects on latent factors in statistical modeling \cite{wang2022comparisonthree}, and analysis of advantages and disadvantages of model transformation languages \cite{hppner2022advantagesdisadvantages}. Named entity recognition (NER) technology for aeronautical information intelligence is also reviewed, highlighting specific domain applications of NLP \cite{mi2022reviewdevelopment}.

=== DISCUSSION ===
\section{Discussion}

The findings from this systematic review, encompassing 30 papers, highlight the significant advancements and the evolving landscape of large language models (LLMs) and related AI techniques in mathematical and logical reasoning. The research consistently shows that LLMs, particularly when augmented with techniques like chain-of-thought prompting \cite{wei2022chainofthought} and fine-tuning on specialized datasets \cite{kobelski2022rethinking}, are making considerable progress in tasks ranging from basic arithmetic to solving complex word problems \cite{lu2022surveydeep}. The integration of external knowledge, whether through knowledge graphs \cite{zhang2022empiricalinvestigation} or symbolic solvers, further enhances these capabilities \cite{hu2022surveyknowledge}.

One of the most prominent themes is the efficacy of prompting strategies, with chain-of-thought reasoning proving effective in eliciting step-by-step problem-solving from models \cite{wei2022chainofthought}. This approach, along with frameworks like ALERT that adapt language models to reasoning tasks \cite{yu2022alertadapt}, moves beyond simple pattern recognition. The integration of LLMs with external tools and formal methods represents a pragmatic approach to leverage specialized strengths, overcoming the inherent limitations of neural networks in precise computation and formal verification \cite{lu2022surveydeep, khan2022executableformal}.

Despite these advancements, several research gaps and challenges persist. **Robustness** remains a critical concern. As demonstrated by \cite{stolfo2022causalframework}, LLMs can sometimes rely on shallow patterns rather than genuine mathematical understanding. This lack of true reasoning poses risks, especially in safety-critical applications. Research into achieving and understanding out-of-distribution generalization \cite{nam2022achievingunderstanding} and ensuring robustness against adversarial settings \cite{abramson2022applicationpseudologlikelihoods} is crucial. Ensuring that models exhibit systematic generalization is paramount for reliable mathematical reasoning.

Another significant gap lies in the **generalization capabilities** of LLMs to novel mathematical domains or more advanced theoretical concepts. While current models excel at tasks they have been trained on, their ability to perform abstract mathematical reasoning, discover new theorems, or understand complex proofs remains limited \cite{lu2022surveydeep}. The integration of knowledge, as explored in KE-PLMs \cite{hu2022surveyknowledge} and commonsense reasoning with knowledge graphs \cite{zhang2022empiricalinvestigation}, is a step towards addressing this. The challenge is to move beyond pattern recognition and develop models that can truly understand and manipulate mathematical structures, akin to how modular modeling helps understand complex physical systems \cite{kim2022novelmodular}.

**Interpretability and explainability** of LLM reasoning processes are also areas requiring further attention. While techniques like chain-of-thought provide some level of transparency \cite{wei2022chainofthought}, understanding the internal mechanisms by which LLMs arrive at a mathematical solution is still an open problem. This is vital for debugging, building trust, and advancing the theoretical understanding of AI's reasoning capabilities. Efforts to characterize bias in NLP \cite{alemany2022methodologycharacterize} also underscore the need for transparency and fairness in AI systems, including those performing quantitative analysis.

The potential for **personalization** in LLM applications, as suggested by work in vision-language models \cite{cohen2022thisunicorn}, opens up exciting avenues for personalized mathematical education or assistance. However, this also necessitates careful consideration of data privacy and ethical implications, analogous to concerns raised in learning analytics \cite{li2022scenariobasedexploration}. Studies on self-assessment accuracy \cite{markta2022accuracypupils} highlight the human element in evaluating understanding, a factor that personalized AI tutors would need to consider.

Future research should continue to focus on developing more robust, generalizable, and interpretable LLMs for mathematical reasoning. Exploring novel architectures, incorporating formal verification methods \cite{khan2022executableformal}, and developing more sophisticated benchmarks that test deeper reasoning skills are essential steps. The comparison of different algorithmic approaches, as seen in studies on imbalanced classification \cite{wankmller2022comparisonapproaches} or statistical modeling \cite{wang2022comparisonthree}, could also inform the development of more effective reasoning systems. Research into causal frameworks \cite{stolfo2022causalframework} and the foundational aspects of logic \cite{poythress2022semioticanalysis} will continue to be vital. The development of AI-assisted programming tools \cite{gulwani2022aiassistedprogramming} and adaptive modeling techniques \cite{mare2022updatethermal} further indicates the broad applicability of structured reasoning principles.

Ultimately, the goal is to create AI systems that can not only solve mathematical problems but also understand and contribute to the field of mathematics. The review of named entity recognition \cite{mi2022reviewdevelopment} and multimodal sentiment analysis \cite{ji2022afrbertattentionbased} also points to the expanding role of AI in extracting structured information and reasoning about complex data.

=== CONCLUSION ===
\section{Conclusion}

This systematic literature review, encompassing 30 papers, has provided a comprehensive overview of the current state of research concerning large language models (LLMs) and related AI techniques applied to mathematical and logical reasoning. Our findings indicate a rapidly evolving field with significant progress in enabling LLMs to perform a wide array of quantitative and logical tasks, from arithmetic and algebraic problem-solving \cite{lu2022surveydeep} to structured reasoning in specialized domains \cite{ricci2022petrinetbasedapproach}. The advancements in prompting techniques, such as chain-of-thought reasoning \cite{wei2022chainofthought}, and the application of fine-tuning strategies, alongside knowledge integration \cite{hu2022surveyknowledge}, have been instrumental in this progress.

Key contributions of this review include the identification of dominant tasks and datasets used for evaluation, a detailed categorization of methodologies employed to enhance LLM reasoning capabilities \cite{yu2022alertadapt}, and a synthesis of the current findings. We have highlighted the critical importance of robustness \cite{stolfo2022causalframework}, generalization \cite{nam2022achievingunderstanding}, and interpretability as ongoing challenges, underscoring the need for deeper, more reliable reasoning beyond pattern matching. The exploration of formal methods \cite{khan2022executableformal} and causal reasoning frameworks further enriches our understanding of rigorous AI reasoning.

The implications of this research are far-reaching, promising to revolutionize fields that rely heavily on quantitative analysis and logical deduction. Applications span scientific research, engineering, finance, and education, where LLMs could serve as powerful tools for problem-solving, discovery, and personalized learning \cite{li2022scenariobasedexploration}.

However, significant research gaps remain, particularly concerning the interpretability of LLM reasoning processes and their ability to handle highly abstract or novel mathematical concepts. Future research should continue to explore innovative architectures, refine knowledge integration strategies \cite{zhang2022empiricalinvestigation}, and develop more rigorous evaluation methodologies. The development of LLMs that can truly reason mathematically and logically, rather than merely mimic it, represents a crucial frontier in artificial intelligence \cite{poythress2022semioticanalysis}.

In conclusion, while LLMs and related AI techniques have demonstrated remarkable potential in mathematical and logical reasoning, the journey towards fully autonomous and trustworthy AI systems is ongoing. This review serves as a valuable resource for researchers and practitioners, charting the current landscape and illuminating the path forward in this exciting and critical area of AI research.

=== END PREVIOUS DRAFT ===

UPDATED STATISTICS:
- Total papers now included: 45
- Records identified: 462
- Studies included: 462

TASK: COMPLETELY REGENERATE the paper integrating the new papers.

REGENERATION INSTRUCTIONS:
1. Read the previous draft to understand existing structure and themes
2. Review the NEW papers being added (listed in "NEW PAPERS BEING ADDED" section)
3. Integrate new papers throughout ALL sections where relevant
4. In Results section:
   - Add new subsections if new themes emerge from new papers
   - Reorganize existing subsections for better coherence
   - CITE every paper discussed using \cite{citationKey}
5. Update all statistics to reflect new paper count
6. Maintain academic quality and narrative flow
7. Ensure EVERY paper (old and new) is cited using \cite{citationKey}

CITATION REQUIREMENTS:
✓ Use \cite{citationKey} format (e.g., \cite{smith2020deep})
✓ Cite papers from BOTH previous draft AND new additions
✓ Introduction: MINIMUM 5-10 citations
✓ Results: MINIMUM 15-25 citations (more with larger paper count)
✓ Discussion: MINIMUM 10-15 citations
✓ Conclusion: MINIMUM 3-5 citations
✓ Each subsection in Results MUST cite papers relevant to that theme

REGENERATE COMPLETE PAPER:
1. ABSTRACT: Update with new paper count, refined findings (NO citations)
2. INTRODUCTION: Integrate relevant new papers, update scope, CITE extensively
3. METHODOLOGY: Update statistics (cite PRISMA guidelines if needed)
4. RESULTS: **CRITICAL** - Reorganize with new papers, cite ALL papers discussed
5. DISCUSSION: Integrate new findings, synthesize across all papers, CITE extensively
6. CONCLUSION: Update with insights from complete set, CITE key papers

CRITICAL: Return your response as VALID JSON with PROPER ESCAPING:

IMPORTANT JSON ESCAPING RULES:
- Every single backslash in LaTeX commands MUST be escaped as double backslash
- \cite{} becomes \\cite{} in JSON
- \subsection{} becomes \\subsection{} in JSON
- Example: "introduction": "Recent work \\cite{smith2020} shows..."

Return ONLY valid JSON in this EXACT format:
{
  "abstract": "...",
  "introduction": "text with \\cite{} properly escaped",
  "methodology": "...",
  "results": "text with \\subsection{} and \\cite{} properly escaped",
  "discussion": "text with \\cite{} properly escaped",
  "conclusion": "text with \\cite{} properly escaped"
}

VERIFY: Check that ALL backslashes are doubled (\\) in JSON before returning!
