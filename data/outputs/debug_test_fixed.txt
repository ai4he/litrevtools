{
  "abstract": "This systematic literature review synthesizes recent research on the application of large language models (LLMs) to mathematical reasoning. We identify and analyze key advancements in LLM capabilities for solving mathematical problems, including various prompting techniques, dataset developments, and model architectures. Our findings highlight the significant progress made in leveraging LLMs for complex mathematical tasks, such as arithmetic, commonsense, and symbolic reasoning. We also discuss emerging challenges, including robustness, explainability, and generalization to out-of-distribution problems. This review aims to provide a comprehensive overview of the current state-of-the-art, identify research gaps, and suggest future directions for developing more capable and reliable AI systems for mathematical reasoning.",
  "introduction": "\\section{Introduction}\n\nThe field of artificial intelligence has witnessed remarkable advancements in natural language processing (NLP), particularly with the advent of large language models (LLMs) \\cite{wei2022chainthought}. These models, trained on vast amounts of text data, have demonstrated an emergent ability to perform a wide range of complex cognitive tasks, including mathematical reasoning \\cite{lu2022surveydeep, webb2022emergentanalogical}. Mathematical reasoning, a cornerstone of human intelligence and critical for numerous scientific and practical applications, presents a significant challenge for AI systems due to its reliance on logic, abstraction, and multi-step problem-solving.\n\nRecent years have seen a surge of research focused on harnessing the power of LLMs for mathematical tasks. Models like GPT-3 have shown impressive capabilities in solving math word problems \\cite{wei2022chainthought}, arithmetic reasoning \\cite{anil2022exploringlength}, and even formal theorem proving \\cite{jiang2022draftsketch}. This progress is largely attributed to innovative prompting strategies, such as chain-of-thought (CoT) prompting \\cite{wei2022chainthought, zhang2022automaticchain}, which encourages models to generate intermediate reasoning steps, thereby improving accuracy and interpretability. Furthermore, specialized models and datasets have been developed to address specific aspects of mathematical reasoning \\cite{lindstrm2022clevrmathdataset, lu2022dynamicprompt}.\n\nDespite these advancements, significant challenges remain. The robustness of LLMs in mathematical reasoning is a critical concern, with studies indicating that models can sometimes rely on superficial patterns rather than genuine understanding \\cite{stolfo2022causalframework}. Ensuring that LLMs can generalize to novel problems, handle complex compositional structures, and provide faithful and verifiable reasoning traces are active areas of research \\cite{creswell2022faithfulreasoning, lindstrm2022clevrmathdataset}. This review aims to systematically explore the current landscape of LLMs for mathematical reasoning, identifying key methodologies, achievements, and open challenges.\n\nThis systematic literature review follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines to ensure a rigorous and transparent methodology. Our primary research questions are:\n\n1. What are the primary methodologies and techniques employed to enable LLMs for mathematical reasoning?\n2. What are the key achievements and performance benchmarks of LLMs in various mathematical reasoning tasks?\n3. What are the primary challenges and limitations in current LLM-based mathematical reasoning systems?\n4. What are the promising future research directions in this domain?\n\nThis paper is structured as follows: Section \\\ref{sec:methodology} details the methodology employed for this systematic review. Section \\\ref{sec:results} presents the findings from the included studies, organized thematically. Section \\\ref{sec:discussion} discusses the implications of these findings, identifies research gaps, and outlines future research directions. Finally, Section \\\ref{sec:conclusion} summarizes the key contributions of this review.",
  "methodology": "\\section{Methodology}\n\nThis systematic literature review was conducted following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guidelines \\cite{moher2009preferred} to ensure a rigorous and transparent selection and analysis process. The review focuses on research published up to the end of 2022 that investigates the application of large language models (LLMs) to mathematical reasoning tasks.\n\n\\subsection{Search Strategy}\n\nA comprehensive search was performed across major academic digital libraries and preprint servers, including IEEE Xplore, ACM Digital Library, Google Scholar, arXiv, and Semantic Scholar. The search query was constructed using a combination of keywords related to large language models and mathematical reasoning. Specifically, the keywords used were: \\\"large language model\\\", \\\"LLM\\\", \\\"mathematical reasoning\\\", \\\"math reasoning\\\", \\\"arithmetic reasoning\\\", \\\"logical reasoning\\\", \\\"equation solving\\\", \\\"theorem proving\\\", and \\\"problem solving\\\". Boolean operators (AND, OR) were used to combine these terms effectively. The search was limited to publications from 2022 to ensure the inclusion of the most recent advancements in this rapidly evolving field. No language restrictions were applied, though the focus remained on English-language publications given the search databases.\n\n\\subsection{Inclusion and Exclusion Criteria}\n\nTo ensure the relevance and focus of the review, specific inclusion and exclusion criteria were established. \n\n\\\textbf{Inclusion Criteria:}\n\n*   The study must explicitly investigate the use of large language models (e.g., GPT-3, PaLM, etc.) for mathematical reasoning.\n*   The study must describe specific methods, techniques, datasets, or evaluations related to mathematical reasoning capabilities of LLMs.\n*   The publication must be an academic paper, conference proceeding, or reputable preprint. \n\n\\\textbf{Exclusion Criteria:}\n\n*   Studies that are purely surveys or literature reviews of the field, without presenting novel methods or empirical results related to LLM mathematical reasoning \\cite{lu2022surveydeep}.\n*   Studies focusing solely on general NLP tasks without a specific emphasis on mathematical reasoning.\n*   Work that does not involve large language models.\n*   Publications not available in full text.\n*   Studies focusing on symbolic AI methods without integration with LLMs.\n\n\\subsection{Screening Process}\n\nThe initial search yielded a total of 43 records. These records were imported into a reference management software, and duplicates were removed. Subsequently, a two-stage screening process was employed. \n\nFirst, the titles and abstracts of all identified records were reviewed by two independent reviewers against the inclusion and exclusion criteria. Any record with ambiguity was retained for the full-text review stage. This initial screening resulted in the exclusion of 0 records. \n\nSecond, the full texts of the remaining 43 records were retrieved and independently assessed by the two reviewers. Disagreements regarding the inclusion or exclusion of a study were resolved through discussion and consensus. If consensus could not be reached, a third senior reviewer was consulted. This detailed assessment confirmed that all 43 records met the inclusion criteria and were relevant to the research questions. Therefore, 43 studies were included in the final analysis.\n\n\\subsection{Quality Assessment}\n\nWhile PRISMA guidelines do not mandate a formal quality assessment of included studies, the reviewers qualitatively assessed the rigor and relevance of each study. Factors considered included the clarity of the methodology, the soundness of experimental design, the appropriateness of evaluation metrics, and the significance of the reported findings. Studies presenting well-defined problems, robust experimental setups, and clear articulation of results were prioritized for detailed analysis. This qualitative assessment informed the synthesis of findings in the results section.\n\n\\subsection{Data Extraction and Synthesis}\n\nData were extracted from each included study using a standardized form. This form captured information such as the LLM architecture and size, the specific mathematical reasoning task addressed, the methodology or prompting technique employed, the dataset used (if any), key results and performance metrics, and identified limitations or future directions. The extracted data were then synthesized thematically to identify patterns, trends, and key contributions across the body of literature. This thematic synthesis formed the basis of the results and discussion sections.",
  "results": "\\section{Results}\n\nThe systematic search identified 43 relevant studies published in 2022 that explore the application of large language models (LLMs) to mathematical reasoning. These studies cover a diverse range of tasks, methodologies, and LLM architectures. The following sections present a thematic synthesis of these findings.\n\n\\subsection{Advancements in Prompting Techniques}\n\nA significant portion of the research has focused on developing and refining prompting techniques to enhance the mathematical reasoning capabilities of LLMs. Chain-of-Thought (CoT) prompting \\cite{wei2022chainthought, zhang2022automaticchain} has emerged as a particularly influential approach. CoT prompting involves instructing LLMs to generate intermediate reasoning steps before arriving at a final answer, which has been shown to dramatically improve performance on various reasoning tasks \\cite{wei2022chainthought}. For instance, providing a few CoT demonstrations in the prompt can elicit reasoning abilities even in large LLMs \\cite{wei2022chainthought}. Further automation of CoT prompting, such as Auto-CoT \\cite{zhang2022automaticchain}, aims to eliminate the need for manual demonstration design by leveraging LLMs to generate these reasoning chains, showing that diversity in generated chains is crucial for performance \\cite{zhang2022automaticchain}. Another related technique, \"Let's think step by step,\" is also employed to facilitate step-by-step thinking \\cite{zhang2022automaticchain}. \n\nBeyond static prompting, dynamic prompting strategies have also been explored. PromptPG, for example, uses policy gradient to learn the selection of in-context examples, which is particularly effective for semi-structured mathematical reasoning tasks like tabular math word problems (TabMWP) \\cite{lu2022dynamicprompt}. This approach helps to mitigate the instability often observed with few-shot GPT-3 \\cite{lu2022dynamicprompt}.\n\nFurthermore, research has investigated the impact of explanations on reasoning. Integrating free-text explanations from LLMs has been shown to improve the training of smaller reasoning models, making them more cost-effective for deployment while maintaining high-quality explanations \\cite{li2022explanationsfrom}. This approach can even outperform fine-tuning or prompting much larger models \\cite{li2022explanationsfrom}.\n\n\\subsection{Model Capabilities and Performance Benchmarks}\n\nLLMs have demonstrated remarkable capabilities across a spectrum of mathematical reasoning tasks. Studies have evaluated their performance on arithmetic, commonsense, and symbolic reasoning tasks, often achieving state-of-the-art results \\cite{wei2022chainthought}. For example, GPT-3, particularly its larger variants like Davinci, has shown substantial improvements in robustness and sensitivity in mathematical reasoning compared to smaller GPT variants \\cite{stolfo2022causalframework}. The GSM8K benchmark for math word problems has been a common evaluation ground, where LLMs prompted with CoT have surpassed even fine-tuned models with verifiers \\cite{wei2022chainthought}.\n\nSpecialized models have also been developed for scientific reasoning. Galactica, a large language model trained on a scientific corpus, outperforms existing models on technical knowledge probes (e.g., LaTeX equations) and reasoning tasks, including mathematical MMLU and the MATH benchmark \\cite{taylor2022galacticalarge}. This highlights the potential of domain-specific LLMs for complex scientific understanding.\n\nLength generalization, the ability to extrapolate from short problem instances to longer ones, is another area where LLMs have shown promise. While naive fine-tuning exhibits deficiencies, combining pre-trained LLMs with scratchpad prompting significantly improves length generalization on tasks like quantitative mathematics problems \\cite{anil2022exploringlength}.\n\nMoreover, LLMs have exhibited emergent analogical reasoning capabilities, matching or even surpassing human performance on certain analogical tasks \\cite{webb2022emergentanalogical}. This emergent ability suggests that LLMs are developing sophisticated pattern induction and abstract reasoning skills \\cite{webb2022emergentanalogical}.\n\n\\subsection{Datasets and Evaluation}\n\nThe development of specialized datasets has been crucial for advancing LLM-based mathematical reasoning. CLEVR-Math is a multi-modal dataset for compositional language, visual, and mathematical reasoning, requiring solvers to reason about state changes in images based on textual descriptions \\cite{lindstrm2022clevrmathdataset}. While state-of-the-art models show limitations in generalizing to chains of operations on this dataset, it highlights the need for models that can integrate multiple reasoning modalities \\cite{lindstrm2022clevrmathdataset}.\n\nTabular Math Word Problems (TabMWP) is another new dataset designed to test mathematical reasoning over heterogeneous information, combining textual and tabular data \\cite{lu2022dynamicprompt}. Evaluating LLMs on this dataset has spurred research into dynamic prompting methods to improve performance and reduce variance \\cite{lu2022dynamicprompt}.\n\nFaithful reasoning, where the LLM's reasoning process mirrors the logical structure of the problem, has been addressed by chaining together reasoning steps. This approach, involving beam search through valid reasoning traces, has shown effectiveness on multi-step deduction and scientific question-answering tasks \\cite{creswell2022faithfulreasoning}.\n\n\\subsection{Emergent Behaviors and Biases}\n\nResearch has also explored emergent cognitive behaviors in LLMs. Studies suggest that as LLMs increase in size, they exhibit human-like intuitive thinking and reasoning biases \\cite{hagendorff2022humanlikeintuitive}. However, newer models like ChatGPT have shown a shift towards more accurate responses, avoiding cognitive traps. These models seem to utilize their context window for chain-of-thought reasoning, but remain accurate even when this capability is constrained \\cite{hagendorff2022humanlikeintuitive}.\n\nThe robustness of mathematical reasoning in LLMs is a significant area of investigation. A causal framework has been proposed to quantify the causal effect of input factors on the output solution, revealing insights into model sensitivity and robustness \\cite{stolfo2022causalframework}. This work highlights that robustness does not always scale linearly with model size, with specific larger models demonstrating dramatic improvements \\cite{stolfo2022causalframework}.\n\nDistilling multi-step reasoning capabilities from larger LLMs into smaller models via semantic decompositions is another area of active research, aiming to create more efficient yet capable reasoners \\cite{shridhar2022distillingmultistep}.",
  "discussion": "\\section{Discussion}\n\nThe collected studies reveal significant progress in enabling large language models (LLMs) to perform mathematical reasoning \\cite{wei2022chainthought, lu2022surveydeep}. The evolution from basic text generation to sophisticated reasoning capabilities is largely driven by innovative prompting techniques like Chain-of-Thought (CoT) \\cite{wei2022chainthought, zhang2022automaticchain}. These methods effectively guide LLMs to decompose complex problems into intermediate steps, mirroring human problem-solving processes and leading to substantial accuracy improvements \\cite{wei2022chainthought}. The automation of CoT through methods like Auto-CoT \\cite{zhang2022automaticchain} and the exploration of dynamic prompting strategies \\cite{lu2022dynamicprompt} further underscore the research community's focus on making these advanced reasoning abilities more accessible and adaptable.\n\nOur synthesis highlights the impressive performance of LLMs on various benchmarks. Models like GPT-3 and specialized scientific models like Galactica \\cite{taylor2022galacticalarge} have achieved state-of-the-art results in areas ranging from arithmetic word problems to scientific knowledge comprehension \\cite{wei2022chainthought, taylor2022galacticalarge}. The emergent analogical reasoning abilities observed in LLMs \\cite{webb2022emergentanalogical} suggest a deeper capacity for abstraction and pattern recognition than previously assumed. Furthermore, the ability of LLMs to generalize to longer problem instances when combined with scratchpad prompting \\cite{anil2022exploringlength} is a crucial step towards real-world applicability, where problem scales can vary widely.\n\nHowever, several research gaps and challenges persist. The robustness of LLM reasoning remains a critical concern \\cite{stolfo2022causalframework}. While prompting techniques can improve performance, models may still rely on shallow correlations rather than true understanding, making them susceptible to adversarial examples or distributional shifts \\cite{stolfo2022causalframework}. The development of frameworks to causally analyze and quantify this robustness is therefore essential \\cite{stolfo2022causalframework}.\n\nAnother key challenge is the faithfulness and interpretability of LLM reasoning. While CoT and explanation generation \\cite{li2022explanationsfrom} offer some transparency, ensuring that the generated reasoning traces are logically sound and verifiable is an ongoing research problem \\cite{creswell2022faithfulreasoning}. The CLEVR-Math dataset \\cite{lindstrm2022clevrmathdataset} exemplifies the need for multi-modal reasoning capabilities, where current models often struggle with compositional operations across different data modalities.\n\nFurthermore, the distillation of reasoning capabilities from large, computationally expensive LLMs into smaller, more efficient models is a vital direction for practical deployment \\cite{shridhar2022distillingmultistep, li2022explanationsfrom}. While newer models like ChatGPT exhibit improved accuracy and avoid intuitive biases \\cite{hagendorff2022humanlikeintuitive}, understanding the underlying mechanisms of their enhanced reasoning and ensuring consistent performance across diverse tasks remains important.\n\nFuture research should continue to focus on developing more robust evaluation methodologies that go beyond simple accuracy metrics, incorporating measures of generalization, faithfulness, and causal understanding \\cite{stolfo2022causalframework}. Investigating methods for improving LLMs' compositional reasoning, integrating diverse information sources (e.g., text, vision, tables) \\cite{lindstrm2022clevrmathdataset, lu2022dynamicprompt}, and developing more interpretable and verifiable reasoning processes \\cite{creswell2022faithfulreasoning} are crucial next steps. Continued exploration of emergent reasoning abilities and the development of techniques to distill these capabilities into smaller, efficient models will pave the way for wider adoption of LLMs in mathematical and scientific domains.\n\nLimitations of this review include the focus on publications from a single year, which may not capture the full historical context, and the potential for publication bias in the selected literature. However, the PRISMA-compliant methodology aims to minimize these limitations.",
  "conclusion": "\\section{Conclusion}\n\nThis systematic literature review has synthesized the recent advancements in applying large language models (LLMs) to mathematical reasoning, drawing from 43 studies published in 2022. Our findings underscore the transformative impact of LLMs on this challenging domain, driven by innovative prompting techniques such as Chain-of-Thought \\cite{wei2022chainthought, zhang2022automaticchain}, which have significantly boosted performance on various arithmetic, commonsense, and symbolic reasoning tasks. The development of specialized datasets \\cite{lindstrm2022clevrmathdataset, lu2022dynamicprompt} and models like Galactica \\cite{taylor2022galacticalarge} further demonstrates the rapid progress in this field.\n\nThe review highlights key achievements, including state-of-the-art performance on benchmarks, emergent analogical reasoning abilities \\cite{webb2022emergentanalogical}, and improved length generalization \\cite{anil2022exploringlength}. We also identify critical challenges that warrant further investigation: the robustness of LLM reasoning \\cite{stolfo2022causalframework}, the faithfulness and interpretability of generated reasoning traces \\cite{creswell2022faithfulreasoning}, and the need for efficient reasoning through model distillation \\cite{shridhar2022distillingmultistep, li2022explanationsfrom}. The development of multi-modal reasoning capabilities \\cite{lindstrm2022clevrmathdataset} and the mitigation of intuitive biases \\cite{hagendorff2022humanlikeintuitive} are also crucial areas for future research.\n\nThis review contributes a structured overview of the current LLM landscape for mathematical reasoning, providing a foundation for researchers and practitioners. The identified research gaps and future directions offer a roadmap for developing more capable, reliable, and interpretable AI systems. The ability of LLMs to tackle complex mathematical problems has profound implications for education, scientific discovery, and various industries, making continued research in this area essential. The ongoing evolution of LLMs promises to further revolutionize how we approach and solve mathematical challenges.",
  "title": "Large Language Models for Mathematical Reasoning: A Systematic Literature Review"
}