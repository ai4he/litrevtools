
You are an expert in writing PRISMA systematic literature reviews for academic publication.

CRITICAL REQUIREMENT: You MUST cite ALL papers (existing AND new) using LaTeX \cite{} commands throughout the regenerated paper.

=== ALL BIBTEX ENTRIES (INCLUDING NEW PAPERS) ===
@article{cohen2022thisunicorn,
  title={"This is my unicorn, Fluffy": Personalizing frozen vision-language representations},
  author={Niv Cohen and Rinon Gal and E. Meirom and Gal Chechik and Y. Atzmon},
  year={2022},
  booktitle={European Conference on Computer Vision},
  doi={10.48550/arXiv.2204.01694},
  url={https://www.semanticscholar.org/paper/0791a0441e1f672c43aecb2d6708fbc8725c8cad},
  abstract={Large Vision&Language models pretrained on web-scale data provide representations that are invaluable for numerous V&L problems. However, it is unclear how they can be used for reasoning about user-specific visual concepts in unstructured language. This problem arises in multiple domains, from personalized image retrieval to personalized interaction with smart devices. We introduce a new learning setup called Personalized Vision&Language (PerVL) with two new benchmark datasets for retrieving and segmenting user-specific"personalized"concepts"in the wild". In PerVL, one should learn personalized concepts (1) independently of the downstream task (2) allowing a pretrained model to reason about them with free language, and (3) does not require personalized negative examples. We propose an architecture for solving PerVL that operates by extending the input vocabulary of a pretrained model with new word embeddings for the new personalized concepts. The model can then reason about them by simply using them in a sentence. We demonstrate that our approach learns personalized visual concepts from a few examples and can effectively apply them in image retrieval and semantic segmentation using rich textual queries.}
}

@article{stolfo2022causalframework,
  title={A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models},
  author={Alessandro Stolfo and Zhijing Jin and Kumar Shridhar and B. Scholkopf and Mrinmaya Sachan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.12023},
  url={https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe},
  abstract={We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution.By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems.Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.}
}

@article{snchez2022clusteringapproach,
  title={A Clustering Approach for the Optimal Siting of Recharging Stations in the Electric Vehicle Routing Problem with Time Windows},
  author={Danny García Sánchez and Alejandra Tabares and L. Faria and Juan Carlos Rivera and J. Franco},
  year={2022},
  booktitle={Energies},
  doi={10.3390/en15072372},
  url={https://www.semanticscholar.org/paper/f1164514c7180331c3b059c19eab5169c9c921a7},
  abstract={Transportation has been incorporating electric vehicles (EVs) progressively. EVs do not produce air or noise pollution, and they have high energy efficiency and low maintenance costs. In this context, the development of efficient techniques to overcome the vehicle routing problem becomes crucial with the proliferation of EVs. The vehicle routing problem concerns the freight capacity and battery autonomy limitations in different delivery-service scenarios, and the challenge of best locating recharging stations. This work proposes a mixed-integer linear programming model to solve the electric location routing problem with time windows (E-LRPTW) considering the state of charge, freight and battery capacities, and customer time windows in the decision model. A clustering strategy based on the k-means algorithm is proposed to divide the set of vertices (EVs) into small areas and define potential sites for recharging stations, while reducing the number of binary variables. The proposed model for E-LRPTW was implemented in Python and solved using mathematical modeling language AMPL together with CPLEX. Performed tests on instances with 5 and 10 clients showed a large reduction in the time required to find the solution (by about 60 times in one instance). It is concluded that the strategy of dividing customers by sectors has the potential to be applied and generate solutions for larger geographical areas and numbers of recharging stations, and determine recharging station locations as part of planning decisions in more realistic scenarios.}
}

@misc{desogus2022contributionrelationship,
  title={A Contribution on Relationship Banking. Economic, Anthropological and Mathematical Reasoning, Empirical Evidence from Italy},
  author={Marco Desogus and Elisa Casu},
  year={2022},
  url={https://www.semanticscholar.org/paper/dd88cc9b1f6d71ef82631b4e1c98c077ccdf291a}
}

@article{wang2022hybridgenetic,
  title={A Hybrid Genetic Algorithm for Flexible Job Shop Scheduling Problem},
  author={Xianglong Wang and Changyi Liu},
  year={2022},
  booktitle={2022 5th World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM)},
  doi={10.1109/WCMEIM56910.2022.10021523},
  url={https://www.semanticscholar.org/paper/3cff420b5a41a06291b68f5b6600935c090f8ad8},
  abstract={Partially flexible job shop scheduling problem (P-FJSP) is a NP Hard problem more complex than fully flexi-ble job shop scheduling problem (T -FJSP). In this paper, the mathematical model of flexible job shop scheduling is established with the goal of minimizing the maximum completion time (makespan). It combines the local search ability of simu-lated annealing algorithm and the global search ability of ge-netic algorithm. In the process of chromosome decoding, greedy decoding method is used to get a better scheduling solution as far as possible. The hybrid scheduling algorithm is implemented based on Visual Studio and C # language. Finally, 8×8 classic scheduling instance are used for simulation scheduling experiments to verify that the hybrid genetic algorithm proposed in this paper is effective in solving large-scale FJSP.}
}

@article{zhang2022multilayerattention,
  title={A Multi-Layer Attention Network for Visual Commonsense Reasoning},
  author={Wenqi Zhang and Yongchao Gao and Heng Qian and Hongli Lyu},
  year={2022},
  booktitle={International Conference on Data Science and Information Technology},
  doi={10.1109/DSIT55514.2022.9943834},
  url={https://www.semanticscholar.org/paper/0e0f20f3af3650b5a97b0ec3f046ba8160b45279},
  abstract={Visual Commonsense Reasoning (VCR) is a challenging multimodal task involving several research fields such as vision, cognition, and reasoning, which combines images and natural language for reasoning. Existing VCR methods focus on global attention or use pre-training models, but these methods lack attention to local features of visual and language. In this paper, a multi-layer attention network is proposed for the VCR task, including an intra-modal attention module and an inter-modal attention module. The intra-modal attention module complements important features of visual and language modalities with fine-grained visual attention to improve the relevance of visual and language. The inter-modal attention module captures the internal dependencies between visual and language. Finally, the two modules are integrated into an end-to-end reasoning framework. Experiments on the VCR large-scale dataset show that the proposed method exhibits a decent improvement in the VCR task and illustrates the effectiveness of the method on three subtasks.}
}

@article{ricci2022petrinetbasedapproach,
  title={A Petri-Net-Based Approach for Enhancing Clinical Reasoning in Medical Education},
  author={F. Ricci and F. Consorti and F. Pecoraro and D. Luzi and Oscar Tamburis},
  year={2022},
  journal={IEEE Transactions on Learning Technologies},
  doi={10.1109/tlt.2022.3157391},
  url={https://www.semanticscholar.org/paper/98b0fb67a6fb222998e3449621f3f5eecaed758e},
  abstract={Medical students are called to acquire competence to manage disease in its dynamic evolution over time, learning to analyze how clinical conditions evolve in a patient's history and how each condition interferes with the evolution of the other coexisting conditions. In this article, the health issue network (HIN) approach is introduced as a formal language based on Petri nets (PNs) to model properties that are particularly apposite for the graphical representation of HIN evolutionary paths. Moreover, the PNs’ underlying mathematical model allows users to draw coherent and well-formed graphs representing rather complex clinical cases. Finally, HIN can be easily integrated into a simulation environment to support case-based learning activities and assessment. The examples of the exercises provided in this article show, on the one hand, the ways the introduced methodology is figured out and implemented; on the other hand, they outline the variety of learning questions that users may deal with when deploying the HIN approach.}
}

@article{ekong2022ratiocinativestudy,
  title={A Ratiocinative Study and Assessment of W. V. O. Quine’s “Criterion of Ontological Commitment”},
  author={Joseph T. Ekong},
  year={2022},
  journal={International Journal of Philosophy},
  doi={10.47941/ijp.1052},
  url={https://www.semanticscholar.org/paper/7b6955111d3bd91b13e7a9c7fbdfd75d43825c36},
  abstract={Purpose: This work has three main objectives: Firstly, it offers an elucidation of the notion of ontological commitment. Secondly, it assesses the adequacy of the criterion of ontological commitment for different languages. Thirdly, it offers some speculative and evaluative remarks regarding the significance of Quine’s criterion of ontological commitment. Many ontologists, within the analytic tradition, often appeal to Quine's criterion of ontological commitment, when debating whether an assertion or theory implies the existence of a certain entity. Regarding his goal in formulating this criterion, he says that the criterion does not aim to help us discover what it is that there is, but only what a theory says there is: “I look to variables and quantification for evidence as to what a theory says that there is, not for evidence as to what there is” (Quine, 1960: 225). Its most popular formulation, using textual evidence from Quine's oeuvre, is: “To be is to be the value of a bound variable,” (Quine, 1961: 15). However, this formulation is susceptible to gross misunderstanding, especially if one is influenced by the formalities and technical maneuvers of model theory. In mathematical logic, model theory is the study of the relationship between formal theories (a collection of sentences in a formal language expressing statements about a mathematical structure), and their models (those structures in which the statements of the theory hold). Model theory is a branch of mathematical logic where we study mathematical structures by considering the first-order sentences true in those structures and the sets definable by first-order formulas. Model theory studies the relations between sentences of a formal language and the interpretations (or ‘structures’) which make these sentences true or false. It offers precise definitions of truth, logical truth and consequence, meanings and modalities. 
Methodology: This work is expository, analytic, critical and evaluative in its methodology. Of course, there are familiar philosophical problems which are within the discursive framework of ‘ontology,’ often phrased by asking if something or some category of things are “real,” or whether “they exist,” concretely. An outstanding example is provided by the traditional problem of universals, which issues in the nominalist-realist controversy, as to the real existence of universals, or of abstract entities such as classes (in the mathematical sense) or propositions (in the abstract sense, referring to the content of an assertion in abstraction from the particular words used to convey it). 
Results: In as much as one might agree with Quine’s Criterion of Ontological Commitment, one might also opine that it is nonetheless a feature of first-order language (i.e. the language embodied in first-order logic; a symbolized reasoning process comprising relations, functions and constants, in which each sentence or statement is broken down into a subject and a predicate. In this regard, the predicate modifies or defines the properties of the subject) that there should be an exact correspondence between the ontological commitments carried by a sentence and the objects that must be counted among the values of the variables in order for the sentence to be true. However, this in itself is not a reason for thinking that such a feature will generalize beyond first-order languages. It is possible for Quine’s Criterion to degenerate, when the language contains atomic predicates expressing extrinsic properties. 
Unique Contribution to theory, practice and policy: Based on Quine’s analysis, a theory is committed to those and only those entities that in the last analysis serve as the values of its bound variables. Thus, ordinary first-order theory commits one to an ontology only of individuals (particulars), whereas higher order logic commits one to the existence of sets, i.e. of collections of definite and distinct entities (or, alternatively, of properties and relations). Likewise, if bound first-order variables are assumed to range over sets (as they do in set theory), a commitment to the existence of these sets is incurred. Admittedly, the precise import of Quine’s criterion of ontological commitment, however, is not completely clear, nor is it clear in what other sense one is perhaps committed by a theory to those entities that are named or otherwise referred to in it, but not quantified over in it. However, it despite its limitations, it has made is possible for one to measure the ontological cost of theories, an important component in deciding which theories to accept, thus offering a partial foundation for theory choice.}
}

@article{li2022scenariobasedexploration,
  title={A Scenario-based Exploration of Expected Usefulness, Privacy Concerns, and Adoption Likelihood of Learning Analytics},
  author={X. Li and M. Rosson and Jenay Robert},
  year={2022},
  booktitle={ACM Conference on Learning @ Scale},
  doi={10.1145/3491140.3528271},
  url={https://www.semanticscholar.org/paper/067b8489b028d931b751cb9413225b761e51dcf3},
  abstract={Learning analytics has become a robust research area in the last decade, as innovative analytic models of learning data have been created with the goal of enhancing teaching and learning. However, barriers to large scale adoption of such technologies in higher education still exist. In recent years, a strand of research has begun to investigate stakeholders' expectations of learning analytics, hoping to find ways to integrate the innovations into everyday teaching practices. For instance, studies have investigated instructors' ideas about how learning analytics might be helpful, as well as concerns about student data privacy. However, most studies have taken a general approach rather than considering instructors' day-to-day experiences. Using survey methods, we presented instructors with hypothetical scenarios of learning analytics in use across disciplines, class sizes, teaching activities, and types of student data. We asked for ratings of both usefulness and privacy concerns for each proposed teaching situation. Our respondents considered scenarios involving learning outcomes-related data (e.g. grades) to be more useful than those that involve student interactions (e.g. language, social activity). In contrast, privacy concerns were lower for outcomes-oriented scenarios than interactions-focused scenarios. An interesting new finding was a negative correlation of usefulness and privacy; we discuss this in the context of instructors' possible cost-benefit reasoning. We reflect on our findings with respect to future efforts in developing and fielding learning analytics tools.}
}

@article{lu2022surveydeep,
  title={A Survey of Deep Learning for Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Wenhao Yu and S. Welleck and Kai-Wei Chang},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10535},
  url={https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d},
  abstract={Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.}
}

@article{hu2022surveyknowledge,
  title={A Survey of Knowledge Enhanced Pre-Trained Language Models},
  author={Linmei Hu and Zeyi Liu and Ziwang Zhao and Lei Hou and Liqiang Nie and Juanzi Li},
  year={2022},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  doi={10.1109/TKDE.2023.3310002},
  url={https://www.semanticscholar.org/paper/a26623d52d24e03044a158cddad931ec5ab7304c},
  abstract={Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are categorized into KG-based and retrieval-based methods. Finally, we point out some promising future directions of KE-PLMs.}
}

@article{zhou2022surveyneural,
  title={A Survey on Neural Open Information Extraction: Current Status and Future Directions},
  author={Shaowen Zhou and Yu Bowen and Aixin Sun and Cheng Long and Jingyang Li and Haiyang Yu and Jianguo Sun},
  year={2022},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2205.11725},
  url={https://www.semanticscholar.org/paper/5de6ecf62f14c9263882f9f30d6448df9efd34e0},
  abstract={Open Information Extraction (OpenIE) facilitates domain-independent discovery of relational facts from large corpora. The technique well suits many open-world natural language understanding scenarios, such as automatic knowledge base construction, open-domain question answering, and explicit reasoning. Thanks to the rapid development in deep learning technologies, numerous neural OpenIE architectures have been proposed and achieve considerable performance improvement. In this survey, we provide an extensive overview of the state-of-the-art neural OpenIE models, their key design decisions, strengths and weakness. Then, we discuss limitations of current solutions and the open issues in OpenIE problem itself. Finally we list recent trends that could help expand its scope and applicability, setting up promising directions for future research in OpenIE. To our best knowledge, this paper is the first review on neural OpenIE.}
}

@article{wankmller2022comparisonapproaches,
  title={A comparison of approaches for imbalanced classification problems in the context of retrieving relevant documents for an analysis},
  author={Sandra Wankmüller},
  year={2022},
  journal={Journal of Computational Social Science},
  doi={10.1007/s42001-022-00191-7},
  url={https://www.semanticscholar.org/paper/a12e9a6863c8453787575172599389d2ddcd9f62},
  abstract={One of the first steps in many text-based social science studies is to retrieve documents that are relevant for an analysis from large corpora of otherwise irrelevant documents. The conventional approach in social science to address this retrieval task is to apply a set of keywords and to consider those documents to be relevant that contain at least one of the keywords. But the application of incomplete keyword lists has a high risk of drawing biased inferences. More complex and costly methods such as query expansion techniques, topic model-based classification rules, and active as well as passive supervised learning could have the potential to more accurately separate relevant from irrelevant documents and thereby reduce the potential size of bias. Yet, whether applying these more expensive approaches increases retrieval performance compared to keyword lists at all, and if so, by how much, is unclear as a comparison of these approaches is lacking. This study closes this gap by comparing these methods across three retrieval tasks associated with a data set of German tweets (Linder in SSRN, 2017. https://doi.org/10.2139/ssrn.3026393 ), the Social Bias Inference Corpus (SBIC) (Sap et al. in Social bias frames: reasoning about social and power implications of language. In: Jurafsky et al. (eds) Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, p 5477–5490, 2020. https://doi.org/10.18653/v1/2020.aclmain.486 ), and the Reuters-21578 corpus (Lewis in Reuters-21578 (Distribution 1.0). [Data set], 1997. http://www.daviddlewis.com/resources/testcollections/reuters21578/ ). Results show that query expansion techniques and topic model-based classification rules in most studied settings tend to decrease rather than increase retrieval performance. Active supervised learning, however, if applied on a not too small set of labeled training instances (e.g. 1000 documents), reaches a substantially higher retrieval performance than keyword lists.}
}

@article{wang2022comparisonthree,
  title={A comparison of three approaches to covariate effects on latent factors},
  author={Ze Wang},
  year={2022},
  booktitle={Large-scale Assessments in Education},
  doi={10.1186/s40536-022-00148-2},
  url={https://www.semanticscholar.org/paper/c40412109167ae57baab6505edf9b628efca6d3a},
  abstract={In educational and psychological research, it is common to use latent factors to represent constructs and then to examine covariate effects on these latent factors. Using empirical data, this study applied three approaches to covariate effects on latent factors: the multiple-indicator multiple-cause (MIMIC) approach, multiple group confirmatory factor analysis (MG-CFA) approach, and the structural equation model trees (SEM Trees) approach. The MIMIC approach directly models covariate effects on latent factors. The MG-CFA approach allows testing of measurement invariance before latent factor means could be compared. The more recently developed SEM Trees approach partitions the sample into homogenous subsets based on the covariate space; model parameters are estimated separately for each subgroup. We applied the three approaches using an empirical dataset extracted from the eighth-grade U.S. data from the Trends in International Mathematics and Science Study 2019 database. All approaches suggested differences among mathematics achievement categories for the latent factor of mathematics self-concept. In addition, language spoken at home did not seem to affect students’ mathematics self-concept. Despite these general findings, the three approaches provided different pieces of information regarding covariate effects. For all models, we appropriately considered the complex data structure and sampling weights following recent recommendations for analyzing large-scale assessment data.}
}

@misc{alemany2022methodologycharacterize,
  title={A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America},
  author={L. A. Alemany and Luciana Benotti and Hernán Maina and Luc'ia M. Gonz'alez and Mariela Rajngewerc and Lautaro Mart'inez and Jos'e L. S'anchez and M. Schilman and Guido Ivetta and Alexia Halvorsen and Amanda Rojo and M. Bordone and Beatriz Busaniche},
  year={2022},
  url={https://www.semanticscholar.org/paper/a82a08b5e6a11f4d6fdff95dd30177957ed7855e},
  abstract={Automated decision-making systems, especially those based on natural language processing, are pervasive in our lives. They are not only behind the internet search engines we use daily, but also take more critical roles: selecting candidates for a job, determining suspects of a crime, diagnosing autism and more. Such automated systems make errors, which may be harmful in many ways, be it because of the severity of the consequences (as in health issues) or because of the sheer number of people they affect. When errors made by an automated system affect a population more than others, we call the system \textit{biased}. Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to audit them. In this paper, we present a methodology that spells out how social scientists, domain experts, and machine learning experts can collaboratively explore biases and harmful stereotypes in word embeddings and large language models. Our methodology is based on the following principles: * focus on the linguistic manifestations of discrimination on word embeddings and language models, not on the mathematical properties of the models * reduce the technical barrier for discrimination experts%, be it social scientists, domain experts or other * characterize through a qualitative exploratory process in addition to a metric-based approach * address mitigation as part of the training process, not as an afterthought}
}

@article{kim2022novelmodular,
  title={A novel modular modeling approach for understanding different electromechanics between left and right heart in rat},
  author={Nari Kim and Julius D. Pronto and D. Nickerson and A. Taberner and Peter J. Hunter},
  year={2022},
  booktitle={Frontiers in Physiology},
  doi={10.3389/fphys.2022.965054},
  url={https://www.semanticscholar.org/paper/2fba0d7b1293e4b13180fb3bccf86ed52ddcaf70},
  abstract={While ion channels and transporters involved in excitation-contraction coupling have been linked and constructed as comprehensive computational models, validation of whether each individual component of a model can be reused has not been previously attempted. Here we address this issue while using a novel modular modeling approach to investigate the underlying mechanism for the differences between left ventricle (LV) and right ventricle (RV). Our model was developed from modules constructed using the module assembly principles of the CellML model markup language. The components of three existing separate models of cardiac function were disassembled as to create smaller modules, validated individually, and then the component parts were combined into a new integrative model of a rat ventricular myocyte. The model was implemented in OpenCOR using the CellML standard in order to ensure reproducibility. Simulated action potential (AP), Ca2+ transient, and tension were in close agreement with our experimental measurements: LV AP showed a prolonged duration and a more prominent plateau compared with RV AP; Ca2+ transient showed prolonged duration and slow decay in LV compared to RV; the peak value and relaxation of tension were larger and slower, respectively, in LV compared to RV. Our novel approach of module-based mathematical modeling has established that the ionic mechanisms underlying the APs and Ca2+ handling play a role in the variation in force production between ventricles. This simulation process also provides a useful way to reuse and elaborate upon existing models in order to develop a new model.}
}

@article{mi2022reviewdevelopment,
  title={A review: development of named entity recognition (NER) technology for aeronautical information intelligence},
  author={Baigang Mi and Fan Yi},
  year={2022},
  booktitle={Artificial Intelligence Review},
  doi={10.1007/s10462-022-10197-2},
  url={https://www.semanticscholar.org/paper/ca2da2420fd25c8633641542730d3f0867c50f60}
}

@article{poythress2022semioticanalysis,
  title={A semiotic analysis of multiple systems of logic: using tagmemic theory to assess the usefulness and limitations of formal logics, and to produce a mathematical lattice model including multiple systems of logic},
  author={V. Poythress},
  year={2022},
  journal={Semiotica: Journal of the International Association for Semiotic Studies},
  doi={10.1515/sem-2020-0051},
  url={https://www.semanticscholar.org/paper/606db29a9d5cad5cd06b8eeb1f8beee390c87ca4},
  abstract={Abstract Tagmemic theory as a semiotic theory can be used to analyze multiple systems of logic and to assess their strengths and weaknesses. This analysis constitutes an application of semiotics and also a contribution to understanding of the nature of logic within the context of human meaning. Each system of logic is best adapted to represent one portion of human rationality. Acknowledging this correlation between systems and their targets helps explain the usefulness of more than one system. Among these systems, the two-valued system of classical logic takes its place. All the systems of logic can be incorporated into a complex mathematical model that has a place for each system and that represents a larger whole in human reasoning. The model can represent why tight formal systems of logic can be applied in some contexts with great success, but in other contexts are not directly applicable. The result suggests that human reasoning is innately richer than any one formal system of logic.}
}

@misc{song2022thesissubmitted,
  title={A thesis submitted to the Faculty of Graduate and Postdoctoral Affairs in partial fulfillment of the requirements for the degree of Master of Arts},
  author={Charlene Song},
  year={2022},
  url={https://www.semanticscholar.org/paper/25be22274b72f1337e977d94d0c94026d13a67d0}
}

@article{markta2022accuracypupils,
  title={ACCURACY OF PUPILS´ SELF-ASSESSMENT},
  author={Švamberk Šauerová Markéta and Smetáčková Irena},
  year={2022},
  booktitle={EduPort},
  doi={10.21062/edp.2022.009},
  url={https://www.semanticscholar.org/paper/fcfabc1d551304cde28a4f0658ed20e36559e05f},
  abstract={In this study, we investigated the accuracy of pupils´ self-assessment in two main school domains – mathematics and Czech language. The analysis explores whether pupils are able to evaluate adequately their own results in the didactic tests and then use some individual parameters to explain the level of self-assessment. The aim of the study was to analyze whether groups of pupils with different self-assessments of school tasks in the Czech language and mathematics (significant underestimation, adequate self-assessment, significant overestimation) differ in some of the cognitive skills studied. Our study questions were as follows: (1) Do pupils assess their achievements in particular school tasks accurately, or inaccurately? (2) Do pupils´ self-assessments differ in mathematics and language? (3) Do the pupil´s self-assessment correlate with individual parameters? The main tool used in the study was a didactic test on mathematics and a didactic test on the Czech language based on the Czech National Curricula Document and created by an expert team. In addition, Raven's Color Progressive Matrices (CPM), Similarities from the Wechsler Intelligence (WISC-SIM), and the Rey-Osterrieth Complex Figure (ROCF) were used. Considering the nature of the data, the non-parametric Kruskal-Wallis ANOVA was used. The present study is a part of the larger research project, involving 29 primary school classes, 657 pupils in total. Based on the data obtained, it can be concluded that the accuracy of pupils' self-assessments is low, while the accuracy of pupils' self-assessments in mathematics and Czech language differs (in mathematics there are more children with more accurate estimates and more pupils who underestimate themselves, in Czech language there are more pupils who overestimate their performance. Statistically significant differences were observed in the domains of Raven's Color Progressive Matrices and Rey-Osterrieth Figure, and in terms of the focus of each test, it could be concluded that there are significant differences between the groups in the domain of non-verbal reasoning skills and in the domain of analytical and organizational perceptual activity and memory. In the area of verbal intellectual abilities, there were no significant differences between the groups.}
}

@article{ji2022afrbertattentionbased,
  title={AFR-BERT: Attention-based mechanism feature relevance fusion multimodal sentiment analysis model},
  author={Mingyu Ji and Jiawei Zhou and Wei Ning},
  year={2022},
  booktitle={PLoS ONE},
  doi={10.1371/journal.pone.0273936},
  url={https://www.semanticscholar.org/paper/918f34bd4274316d684dd6c267b13fe010a74a6e},
  abstract={Multimodal sentiment analysis is an essential task in natural language processing which refers to the fact that machines can analyze and recognize emotions through logical reasoning and mathematical operations after learning multimodal emotional features. For the problem of how to consider the effective fusion of multimodal data and the relevance of multimodal data in multimodal sentiment analysis, we propose an attention-based mechanism feature relevance fusion multimodal sentiment analysis model (AFR-BERT). In the data pre-processing stage, text features are extracted using the pre-trained language model BERT (Bi-directional Encoder Representation from Transformers), and the BiLSTM (Bi-directional Long Short-Term Memory) is used to obtain the internal information of the audio. In the data fusion phase, the multimodal data fusion network effectively fuses multimodal features through the interaction of text and audio information. During the data analysis phase, the multimodal data association network analyzes the data by exploring the correlation of fused information between text and audio. In the data output phase, the model outputs the results of multimodal sentiment analysis. We conducted extensive comparative experiments on the publicly available sentiment analysis datasets CMU-MOSI and CMU-MOSEI. The experimental results show that AFR-BERT improves on the classical multimodal sentiment analysis model in terms of relevant performance metrics. In addition, ablation experiments and example analysis show that the multimodal data analysis network in AFR-BERT can effectively capture and analyze the sentiment features in text and audio.}
}

@article{gulwani2022aiassistedprogramming,
  title={AI-assisted programming: applications, user experiences, and neuro-symbolic techniques (keynote)},
  author={Sumit Gulwani},
  year={2022},
  booktitle={ESEC/SIGSOFT FSE},
  doi={10.1145/3540250.3569444},
  url={https://www.semanticscholar.org/paper/11230f03465d8ab073815397717d8afa3f3dae1c}
}

@article{yu2022alertadapt,
  title={ALERT: Adapt Language Models to Reasoning Tasks},
  author={Ping Yu and Tianlu Wang and O. Yu. Golovneva and Badr AlKhamissi and Gargi Ghosh and Mona T. Diab and Asli Celikyilmaz},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.08286},
  url={https://www.semanticscholar.org/paper/95c11cc5820ba32c60d5f2671f6567b9914a4978},
  abstract={Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.To address this question, we introduce {pasted macro ‘OUR’}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro ‘OUR’}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro ‘OUR’}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.}
}

@article{2022algorithmmethod,
  title={ALGORITHM METHOD IN TEACHING RUSSIAN AT SECONDARY SCHOOL},
  author={Юлия Владимировна Подкина},
  year={2022},
  booktitle={Tomsk state pedagogical university bulletin},
  doi={10.23951/1609-624x-2022-6-80-87},
  url={https://www.semanticscholar.org/paper/ded393f5b3432f3d0b9258fff2b9db33b204bf84},
  abstract={Введение. Обучение русскому языку в средней школе, развитие речи и формирование орфографических и пунктуационных навыков – важная задача, которая сопряжена с рядом трудностей. Эффективному изучению русского языка в общеобразовательной школе зачастую препятствуют такие факторы, как плохая усидчивость, отсутствие интереса к предмету, билингвизм и другое. Метод алгоритмизированного представления правил русской орфографии и пунктуации способствует наилучшему усвоению учебного материала и позволяет повысить качество обучения русскому языку школьников среднего и старшего звена. Цель − обоснование эффективности метода алгоритма в обучении русскому языку детей общеобразовательных средних школ, рассмотрение примерных моделей обучающих алгоритмов. Материал и методы. В работе применялись теоретические методы (моделирование, анализ, синтез); эмпирические методы (наблюдение, сравнение, эксперимент). Результаты и обсуждение. Простое заучивание правил не всегда приводит к повышению грамотности учащихся. Метод алгоритма предусматривает совместное с учениками составление алгоритмизированных схем различных видов, которые иллюстрируют изучаемое правило, позволяют пошагово отработать механизм рассуждения при выполнении орфографических и пунктуационных заданий. Такой подход способствует достижению высокого качества знаний путем систематической отработки практических навыков с помощью схем, адаптируемых под потребности каждого ребенка. Обучающий алгоритм может иметь разные виды: от четко сформулированной схемы (похожей на математический пример) до красочной иллюстрации, которая будет понятна детям с творческими способностями. Заключение. Метод алгоритма применяют для изучения практически любого правила русской орфографии и пунктуации. В созданной совместно с учащимися схеме должно быть отведено место для исключений и для примеров, которые ребенок впишет самостоятельно. При создании обучающей схемы школьник является активным соавтором. Схема никогда не является замкнутой системой. Она дорабатывается и совершенствуется в процессе практической деятельности учащихся. У детей из одного класса схемы могут быть совершенно различны, так как усовершенствованы и доработаны самостоятельно под руководством учителя.
 Introduction. Teaching Russian in secondary school, speech development and the formation of spelling and punctuation skills is an important task that involves a number of difficulties. Effective study of the Russian language in a secondary school is often hindered by factors such as poor perseverance, lack of interest in the subject, bilingualism, and more. Russian Russian spelling rules algorithmized representation method is considered in this paper, which allows to improve the quality of teaching Russian to middle and senior school students. The purpose is to substantiate the effectiveness of the algorithm method in teaching the Russian language to children of secondary schools, to consider approximate models of training algorithms. Material and methods. Theoretical methods (modeling, analysis, synthesis) were used in the work; empirical methods (observation, comparison, experiment). Results and discussion. Simple memorizing of the rules does not always lead to increased literacy of students. The algorithm method provides for the joint compilation of algorithmic schemes of various types with students, which illustrate the rule being studied, allow you to work out the mechanism of reasoning step by step when performing spelling and punctuation tasks. This approach contributes to the achievement of a high quality of knowledge through the systematic development of practical skills with the help of schemes adapted to the needs of each child. The training algorithm can have different types: from a clearly formulated scheme (similar to a mathematical example) up to a colorful illustration that will be understandable to children with creative abilities. Conclusion. The algorithm method can be applied to study almost any rule of Russian spelling and punctuation. In the scheme created jointly with the students, there should be a place for exceptions and for examples that the child will enter independently. When creating a training scheme, the student is an active co-author. A circuit is never a closed system. It is being refined and improved in the process of practical activity of students. For children from the same class, the schemes can be completely different, as they have been improved and finalized independently.}
}

@article{mare2022updatethermal,
  title={AN UPDATE OF THERMAL ERROR COMPENSATION MODEL VIA ON-MACHINE MEASUREMENT},
  author={M. Mareš and O. Horejš and Michal Straka and J. Švéda and Tomáš Kozlok},
  year={2022},
  journal={MM Science Journal},
  doi={10.17973/mmsj.2022_12_2022150},
  url={https://www.semanticscholar.org/paper/796f47a4059604f27ad57c3760cc7ebea9f6a020},
  abstract={Software compensation is state-of-the-art technology used to reduce CNC machine tool thermal errors, and it belongs to a key intelligent functions of modern machine tools. However, a pretrained and nonadaptive model may not be accurate and robust enough for long-term application. This research presents a transfer function based thermal error compensation model updated via on-machine measurement. A mathematical model is implemented into the machine management software of a large horizontal machining centre to compensate for thermal errors in real time using C#/C++ programming language. The results show that after the thermal error compensation model is updated via on-machine measurement, the prediction accuracy, measured as peak-to-peak values, and the normalized root mean squared error are significantly improved. The prediction accuracy of the compensation model updated via on-machine measurement strongly depends on the sampling interval of the on machine measurements.}
}

@misc{nam2022achievingunderstanding,
  title={Achieving and Understanding Out-of-Distribution Generalization in Systematic Reasoning in Small-Scale Transformers},
  author={A. Nam and Mustafa Abdool and Trevor Maxfield and James L. McClelland},
  year={2022},
  url={https://www.semanticscholar.org/paper/8283064365ae7594d891e8b7daf36fd37ca809b0},
  abstract={Out-of-distribution generalization (OODG) is a longstanding challenge for neural networks. This challenge is quite apparent in tasks with well-defined variables and rules, where explicit use of the rules could solve problems independently of the particular values of the variables, but networks tend to be tied to the range of values sampled in their training data. Large transformer-based language models have pushed the boundaries on how well neural networks can solve previously unseen problems, but their complexity and lack of clarity about the relevant content in their training data obfuscates how they achieve such robustness. As a step toward understanding how transformer-based systems generalize, we explore the question of OODG in small scale transformers trained with examples from a known distribution. Using a reasoning task based on the puzzle Sudoku, we show that OODG can occur on a complex problem if the training set includes examples sampled from the whole distribution of simpler component tasks. Successful generalization depends on carefully managing positional alignment when absolute position encoding is used, but we find that suppressing sensitivity to absolute positions overcomes this limitation. Taken together our results represent a small step toward understanding and promoting systematic generalization in transformers.}
}

@article{hppner2022advantagesdisadvantages,
  title={Advantages and disadvantages of (dedicated) model transformation languages},
  author={S. Höppner and Yves Haas and Matthias Tichy and Katharina Juhnke},
  year={2022},
  booktitle={Empirical Software Engineering},
  doi={10.1007/s10664-022-10194-7},
  url={https://www.semanticscholar.org/paper/d96fa397010fa107aadcedbff577feead334e3be},
  abstract={Model driven development envisages the use of model transformations to evolve models. Model transformation languages, developed for this task, are touted with many benefits over general purpose programming languages. However, a large number of these claims have not yet been substantiated. They are also made without the context necessary to be able to critically assess their merit or built meaningful empirical studies around them. The objective of our work is to elicit the reasoning, influences and background knowledge that lead people to assume benefits or drawbacks of model transformation languages. We conducted a large-scale interview study involving 56 participants from research and industry. Interviewees were presented with claims about model transformation languages and were asked to provide reasons for their assessment thereof. We qualitatively analysed the responses to find factors that influence the properties of model transformation languages as well as explanations as to how exactly they do so. Our interviews show, that general purpose expressiveness of GPLs, domain specific capabilities of MTLs as well as tooling all have strong influences on how people view properties of model transformation languages. Moreover, the Choice of MTL, the Use Case for which a transformation should be developed as well as the Skill s of involved stakeholders have a moderating effect on the influences, by changing the context to consider. There is a broad body of experience, that suggests positive and negative influences for properties of MTLs. Our data suggests, that much needs to be done in order to convey the viability of model transformation languages. Efforts to provide more empirical substance need to be undergone and lacklustre language capabilities and tooling need to be improved upon. We suggest several approaches for this that can be based on the results of the presented study.}
}

@article{abramson2022applicationpseudologlikelihoods,
  title={An Application of Pseudo-Log-Likelihoods to Natural Language Scoring},
  author={Darren Abramson and Ali Emami},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/16bf88a6d172699cb9a26a6936efb4941e3f3c13},
  abstract={Language models built using semi-supervised machine learning on large corpora of natural language have very quickly enveloped the fields of natural language generation and understanding. In this paper we apply a zero-shot approach independently developed by a number of researchers now gaining recognition as a significant alternative to fine-tuning for evaluation on common sense tasks. A language model with relatively few parameters and training steps compared to a more recent language model (T5) can outperform it on a recent large data set (TimeDial), while displaying robustness in its performance across a similar class of language tasks. Surprisingly, this result is achieved by using a hyperparameter-free zero-shot method with the smaller model, compared to fine-tuning to the larger model. We argue that robustness of the smaller model ought to be understood in terms of compositionality, in a sense that we draw from recent literature on a class of similar models. We identify a practical cost for our method and model: high GPU-time for natural language evaluation. The zero-shot measurement technique that produces remarkable stability, both for ALBERT and other BERT variants, is an application of pseudo-log-likelihoods to masked language models for the relative measurement of probability for substitution alternatives in forced choice language tasks such as the Winograd Schema Challenge, Winogrande, and others. One contribution of this paper is to bring together a number of similar, but independent strands of research. We produce some absolute state-of-the-art results for common sense reasoning in binary choice tasks, performing better than any published result in the literature, including fine-tuned efforts. We show a remarkable consistency of the model's performance under adversarial settings, which we argue is best explained by the model's compositionality of representations.}
}

@article{zhang2022empiricalinvestigation,
  title={An Empirical Investigation of Commonsense Self-Supervision with Knowledge Graphs},
  author={Jiarui Zhang and Filip Ilievski and Kaixin Ma and Jonathan M Francis and A. Oltramari},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2205.10661},
  url={https://www.semanticscholar.org/paper/651ae53112e73b02440773727b68cedbf8322705},
  abstract={Self-supervision based on the information extracted from large knowledge graphs has been shown to improve the generalization of language models, in zero-shot evaluation on various downstream language reasoning tasks. Since these improvements are reported in aggregate, however, little is known about (i) how to select the appropriate knowledge for solid performance across tasks, (ii) how to combine this knowledge with neural language models, and (iii) how these pairings affect granular task performance. In this paper, we study the effect of knowledge sampling strategies and sizes that can be used to generate synthetic data for adapting language models. We study the effect of different synthetic datasets on language models with various architectures and sizes. The resulting models are evaluated against four task properties: domain overlap, answer similarity, vocabulary overlap, and answer length. Our experiments show that encoder-decoder models benefit from more data to learn from, whereas sampling strategies that balance across different aspects yield best performance. Most of the improvement occurs on questions with short answers and dissimilar answer candidates, which corresponds to the characteristics of the data used for pre-training.}
}

@article{khan2022executableformal,
  title={An Executable Formal Model of the VHDL in Isabelle/HOL},
  author={Wilayat Khan and Zhé Hóu and David Sanán and J. Nebhen and Yang Liu and Alwen Tiu},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/37b0b6db785f8c37460e2bb80da138c1443af5b4},
  abstract={In the hardware design process, hardware components are usually described in a hardware description language. Most of the hardware description languages, such as Verilog and VHDL, do not have mathematical foundation and hence are not fit for formal reasoning about the design. To enable formal reasoning in one of the most commonly used description language VHDL, we define a formal model of the VHDL language in Isabelle/HOL. Our model targets the functional part of VHDL designs used in industry, specifically the design of the LEON3 processor's integer unit. We cover a wide range of features in the VHDL language that are usually not modelled in the literature and define a novel operational semantics for it. Furthermore, our model can be exported to OCaml code for execution, turning the formal model into a VHDL simulator. We have tested our simulator against simple designs used in the literature, as well as the div32 module in the LEON3 design. The Isabelle/HOL code is publicly available: https://zhehou.github.io/apps/VHDLModel.zip}
}

@article{katra2022experimentationframework,
  title={An Experimentation Framework for Specification and Verification of Web Services},
  author={Szymon Katra and Wiktor B. Daszczuk and Danny Czejdo},
  year={2022},
  booktitle={Conference on Computer Science and Information Systems},
  doi={10.15439/2022F188},
  url={https://www.semanticscholar.org/paper/9fbe3dc7a2229a5435fc7ace6978550af5ac3268},
  abstract={Designing and implementing Web Services constitutes a large and constantly growing part of the information technology market. Web Services have specific scenarios in which distributed processes and network resources are used. This aspect of services requires integration with the model checkers. This article presents the experimentation framework in which services can be specified and then formally analyzed for deadlock-freedom, achievement of process goals, and similar features. Rybu4WS language enriches the basic Rybu language with the ability to use variables in processes, service calls between servers, new structural instructions, and other constructions known to programmers while remaining in line with declarative, mathematical IMDS formalism. Additionally, the development environment allows simulation of a counterexample or a witness - obtained as a result of the model checking - in a similar way to traditional debuggers.}
}

@article{jeon2022informationtheoreticanalysis,
  title={An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws},
  author={Hong Jun Jeon and Benjamin Van Roy},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.01365},
  url={https://www.semanticscholar.org/paper/dab053b7713b77ab09f50b90b3176607912e913a},
  abstract={We study the compute-optimal trade-off between model and training data set sizes for large neural networks. Our result suggests a linear relation similar to that supported by the empirical analysis of chinchilla. While that work studies transformer-based large language models trained on the MassiveText corpus gopher, as a starting point for development of a mathematical theory, we focus on a simpler learning model and data generating process, each based on a neural network with a sigmoidal output unit and single hidden layer of ReLU activation units. We introduce general error upper bounds for a class of algorithms which incrementally update a statistic (for example gradient descent). For a particular learning model inspired by barron 1993, we establish an upper bound on the minimal information-theoretically achievable expected error as a function of model and data set sizes. We then derive allocations of computation that minimize this bound. We present empirical results which suggest that this approximation correctly identifies an asymptotic linear compute-optimal scaling. This approximation also generates new insights. Among other things, it suggests that, as the input dimension or latent space complexity grows, as might be the case for example if a longer history of tokens is taken as input to a language model, a larger fraction of the compute budget should be allocated to growing the learning model rather than training data.}
}

@article{bellomarini2022overviewvadalog,
  title={An Overview of Vadalog: a System for Reasoning over Large Knowledge Graphs},
  author={Luigi Bellomarini and Davide Benedetto and Emanuel Sallinger},
  year={2022},
  booktitle={Sistemi Evoluti per Basi di Dati},
  url={https://www.semanticscholar.org/paper/83dc0eca1a453e2970d32923bb48bb84976bd968}
}

@article{amaliyah2022analisiskesulitan,
  title={Analisis Kesulitan Belajar Matematika dalam Menyelesaikan Soal Cerita di Kelas IV Sekolah Dasar Negeri Pakujaya 02},
  author={Aam Amaliyah and Luthfia Nur Maulida and N. Safitri and Ratri Hersita Dewi and Sabgi Wulan Septiara},
  year={2022},
  booktitle={ALSYS},
  doi={10.58578/alsys.v2i3.386},
  url={https://www.semanticscholar.org/paper/5023bebd78bb5f55a0d706f94b27f718b9c83cfc},
  abstract={Students with learning difficulties in mathematics often make mistakes in solving story problems on fractional material. This research uses descriptive qualitative. The purpose of this study was to determine the types of learning difficulties in mathematics experienced by students, the factors that influence learning difficulties, and to reveal the efforts that can be made to overcome the difficulties in learning mathematics in grade IV Pakujaya 02 State Elementary School. Data collection techniques were observation and interviews. . Based on data analysis and discussion, students experienced errors, namely: 1. Understanding the problem, namely errors in interpreting language and making mathematical models. The reason is incomplete/wrong reasoning and low student ability. 2. Planning for problem solving is an error in connecting one concept with another concept. The cause of this error is the humanistic thinking of students. 3. Implement problem solving planning, namely errors in implementing incorrect formulas. Errors in this aspect are caused by incomplete or incorrect reasoning and students' humanistic thinking.}
}

@article{shidqiya2022analysisstudents,
  title={Analysis of Students’ Mathematical Thinking Ability in Terms of Self Efficacy},
  author={Adiba Idlal Shidqiya and Sukestiyarno Sukestiyarno},
  year={2022},
  journal={Unnes Journal of Mathematics Education},
  doi={10.15294/ujme.v11i3.58772},
  url={https://www.semanticscholar.org/paper/fa5b5d97f15b5244e34a49e44317a1822b3e0daa},
  abstract={Mathematical thinking ability must be owned by students to solve various problems. Students are considered capable of fulfilling the indicators of mathematical thinking ability properly if they are balanced with good self-efficacy abilities. This research method is qualitative which aims to find new indicators and describe mathematical thinking ability in terms of self-efficacy and provide recommendations for teachers. The research subjects were six students from the first year of senior high school using purposive sampling. Indicators of mathematical thinking ability, include 1) Reasoning: identifying concepts and problems; 2) Generalizing: demonstrating mathematical ideas in writing and using mathematical language to express ideas correctly; 3) Critical Thinking: using representations to create mathematical models; 4) Problem Solving: planning problem solving strategies, implementing and checking results. 5) Communicating: revealing the results of problem solving. The results: 1) low self-efficacy’s students were only able to master reasoning; 2) moderate self-efficacy’s students are able to master reasoning, generalizing, and critical thinking; 3) high self-efficacy’s students are able to master all indicators. Recommendations for teachers are by giving opportunity to low self-efficacy’s students to speak in public, give appreciation for their efforts and reprimand if it doesn’t lower their confidence when they make mistakes.}
}

@article{yu2022analysiscorrelation,
  title={Analysis of the Correlation between Academic Performance and Learning Motivation in English Course under a Corpus-Data-Driven Blended Teaching Model},
  author={Lan Yu and Jun Shen},
  year={2022},
  booktitle={Scientific Programming},
  doi={10.1155/2022/3407270},
  url={https://www.semanticscholar.org/paper/6f554d023d8e403e5ee70268e55f5b2fe1be574e},
  abstract={To explore the correlation between academic performance and learning motivation in English course under a corpus-data-driven blended teaching model, this study set research objects as 62 year-2020-enrolled undergraduate students majoring in English from a university in Jinan City, Shandong Province, eastern China. According to their previous frequencies of using information technology to learn English, these 62 students were divided into two groups: practice group with high frequency and control group with low frequency, with 31 students in each group. The two groups of students were taught 3 English lessons per week for a total of 15 weeks by the exact same teachers using a corpus-data-driven blended teaching model. The students’ English academic performances were assessed by well-organized final tests, and their English learning motivations were measured by a motivation scale and questionnaires. The results show that the correlation coefficients between the average score of motivation questionnaires, intrinsic motivation factors, extrinsic motivation factors, and the average score of academic performances in practice group were 0.894, 0.682, and 0.724, respectively, while those in control group were 0.749, 0.836, and 0.904. In all the above correlation analyses, the significance level is 0.01, and all coefficient values are higher than critical value. Hence, there is a positive correlation between learning motivation and academic performance of the two groups of subjects. It is found that the corpus-data-driven blended teaching model has a significant impact on college students’ English academic performance and learning motivation, and it has a positive effect on the improvement of their English academic performance and the cultivation of learning motivation. In general, the key to this teaching model lies in reasoning and acquisition by analyzing the language provided by the corpus, and the whole process of data-driven learning is student-centered. Students are exposed to a large number of authentic language knowledge and cultural information, which promotes the sensitivity to relevant points. The results of this paper provide a reference for further research on the analysis of the correlation between academic performance and learning motivation in English course under the corpus-data-driven blended teaching model.}
}

@article{kumar2022answerlevelcalibration,
  title={Answer-level Calibration for Free-form Multiple Choice Question Answering},
  author={Sawan Kumar},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2022.acl-long.49},
  url={https://www.semanticscholar.org/paper/a5584d2d9b0de9e1692241d46d0c70942919cd60},
  abstract={Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks. This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem. While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration. In this work, we consider the question answering format, where we need to choose from a set of (free-form) textual choices of unspecified lengths given a context. We present ALC (Answer-Level Calibration), where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context. We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks. Further, we show that popular datasets potentially favor models biased towards easy cues which are available independent of the context. We analyze such biases using an associated F1-score. Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability.}
}

@article{zhou2022applicationthreeflow,
  title={Application of Three-Flow Fusion Technology Based on Modelica in Thermal Power Digital Twin},
  author={Dongyan Zhou and Haidong Gao and Wenyu Wang and Jun Cao and Wenfei Yang and Ruirui Zeng and Yuan He},
  year={2022},
  journal={IEEE Journal of Radio Frequency Identification},
  doi={10.1109/JRFID.2022.3205855},
  url={https://www.semanticscholar.org/paper/5391cf3bf8f2cc858ee1a532be7d9e2e6b6f6983},
  abstract={Thermal power plants gather large energy infrastructure; therefore, massive historical and real-time data of equipment operation will be generated in daily operation. Digital industrialization puts forward higher requirements for the use of big data than simple tasks, such as generating reports. MWorks is a multidomain unified modeling and simulation platform based on the Modelica language. In this study, MWorks is used to realize the modeling of multidomain systems, including electrical, thermal, mechanical, fluid, and heat transfer, in power plants. The test, calibration, verification, parameter optimization, and fault diagnosis of the thermal power plant mathematical models, which are historical and real-time data-driven, are discussed. The technology of three-flow fusion, including material flow, energy flow, and information flow, and its application in thermal power digital twin are explored.}
}

@article{alghamdi2022armathdataset,
  title={ArMATH: a Dataset for Solving Arabic Math Word Problems},
  author={Reem Alghamdi and Zhenwen Liang and Xiangliang Zhang},
  year={2022},
  booktitle={International Conference on Language Resources and Evaluation},
  url={https://www.semanticscholar.org/paper/4aca69be58a271b1be45ec7ebb3586569cec50b0}
}

@article{kar2022arggenprompting,
  title={ArgGen: Prompting Text Generation Models for Document-Level Event-Argument Aggregation},
  author={Debanjana Kar and S. Sarkar and Pawan Goyal},
  year={2022},
  booktitle={AACL/IJCNLP},
  doi={10.18653/v1/2022.findings-aacl.37},
  url={https://www.semanticscholar.org/paper/61f49465c0d53663ad5264c8f683c6724d31eef1},
  abstract={Most of the existing discourse-level Information Extraction tasks have been modeled to be extractive in nature. However, we argue that extracting information from larger bodies of discourse-like documents requires more natural language understanding and reasoning capabilities. In our work, we propose the novel task of document-level event argument aggregation which generates consolidated event-arguments at a document-level with minimal loss of information. More specifically, we focus on generating precise document-level information frames in a multilingual setting using prompt-based methods. In this paper, we show the effectiveness of prompt-based text generation approach to generate document-level argument spans in a low-resource and zero-shot setting. We also release the first of its kind multilingual event argument aggregation dataset that can be lever-aged in other related multilingual text generation tasks as well: https://github.com/}
}

@article{tewes2022artificialintelligence,
  title={Artificial Intelligence in the American Healthcare Industry: Looking Forward to 2030},
  author={F. Tewes},
  year={2022},
  journal={Journal of Medical Research and Surgery},
  doi={10.52916/jmrs224089},
  url={https://www.semanticscholar.org/paper/6baa97e2ca007eb2eeb51490f604d2bfd767fa0c},
  abstract={Artificial intelligence (AI) has the potential to speed up the exponential growth of cutting-edge technology, much way the Internet did. Due to intense competition from the private sector, governments, and businesspeople around the world, the Internet has already reached its peak as an exponential technology. In contrast, artificial intelligence is still in its infancy, and people all over the world are unsure of how it will impact their lives in the future. Artificial intelligence, is a field of technology that enables robots and computer programmes to mimic human intellect by teaching a predetermined set of software rules to learn by repetitive learning from experience and slowly moving toward maximum performance. Although this intelligence is still developing, it has already demonstrated five different levels of independence. Utilized initially to resolve issues. Next, think about solutions. Third, respond to inquiries. Fourth, use data analytics to generate forecasts. Fifth, make tactical recommendations. Massive data sets and "iterative algorithms," which use lookup tables and other data structures like stacks and queues to solve issues, make all of this possible. Iteration is a strategy where software rules are regularly adjusted to patterns in the data for a certain number of iterations. The artificial intelligence continuously makes small, incremental improvements that result in exponential growth, which enables the computer to become incredibly proficient at whatever it is trained to do. For each round of data processing, the artificial intelligence tests and measures its performance to develop new expertise. In order to address complicated problems, artificial intelligence aims to create computer systems that can mimic human behavior and exhibit human-like thought processes [1]. Artificial intelligence technology is being developed to give individualized medication in the field of healthcare. By 2030, six different artificial intelligence sectors will have considerably improved healthcare delivery through the utilization of larger, more accessible data sets. The first is machine learning. This area of artificial intelligence learns automatically and produces improved results based on identifying patterns in the data, gaining new insights, and enhancing the outcomes of whatever activity the system is intended to accomplish. It does this without being trained to learn a particular topic. Here are several instances of machine learning in the healthcare industry. The first is the IBM Watson Genomics, which aids in rapid disease diagnosis and identification by fusing cognitive computing with genome-based tumour sequencing. Second, a project called Nave Bayes allows for the prediction of diabetes years before an official diagnosis, before it results in harm to the kidneys, the heart, and the nerves. Third, employing two machine learning approaches termed classification and clustering to analyse the Indian Liver Patient Data (ILPD) set in order to predict liver illness before this organ that regulates metabolism becomes susceptible to chronic hepatitis, liver cancer, and cirrhosis [2]. Second, deep learning. Deep learning employs artificial intelligence to learn from data processing, much like machine learning does. Deep learning, on the other hand, makes use of synthetic neural networks that mimic human brain function to analyse data, identify relationships between the data, and provide outputs based on positive and negative reinforcement. For instance, in the fields of Magnetic Resonance Imaging (MRI) and Computed Tomography (CT), deep learning aids in the processes of picture recognition and object detection. Deep learning algorithms for the early identification of Alzheimer's, diabetic retinopathy, and breast nodule ultrasound detection are three applications of this cutting-edge technology in the real world. Future developments in deep learning will make considerable improvements in pathology and radiology pictures [3]. Third, neural networks. The artificial intelligence system can now accept massive data sets, find patterns within the data, and respond to queries regarding the information processed because the computer learning process resembles a network of neurons in the human brain. Let's examine a few application examples that are now applicable to the healthcare sector. According to studies from John Hopkins University, surgical errors are a major contributor to medical malpractice claims since they happen more than 4,000 times a year in just the United States due to the human error of surgeons. Neural networks can be used in robot-assisted surgery to model and plan procedures, evaluate the abilities of the surgeon, and streamline surgical activities. In one study of 379 orthopaedic patients, it was discovered that robotic surgery using neural networks results in five times fewer complications than surgery performed by a single surgeon. Another application of neural networks is in visualising diagnostics, which was proven to physicians by Harvard University researchers who inserted an image of a gorilla to x-rays. Of the radiologists who saw the images, 83% did not recognise the gorilla. The Houston Medical Research Institute has created a breast cancer early detection programme that can analyse mammograms with 99 percent accuracy and offer diagnostic information 30 times faster than a human [4]. Cognitive computing is the fourth. Aims to replicate the way people and machines interact, showing how a computer may operate like the human brain when handling challenging tasks like text, speech, or image analysis. Large volumes of patient data have been analysed, with the majority of the research to date focusing on cancer, diabetes, and cardiovascular disease. Companies like Google, IBM, Facebook, and Apple have shown interest in this work. Cognitive computing made up the greatest component of the artificial market in 2020, with 39% of the total [5]. Hospitals made up 42% of the market for cognitive computing end users because of the rising demand for individualised medical data. IBM invested more than $1 billion on the development of the WATSON analytics platform ecosystem and collaboration with startups committed to creating various cloud and application-based systems for the healthcare business in 2014 because it predicted the demand for cognitive computing in this sector. Natural Language Processing (NLP) is the fifth. This area of artificial intelligence enables computers to comprehend and analyse spoken language. The initial phase of this pre-processing is to divide the data up into more manageable semantic units, which merely makes the information simpler for the NLP system to understand. Clinical trial development is experiencing exponential expansion in the healthcare sector thanks to NLP. First, the NLP uses speech-to-text dictation and structured data entry to extract clinical data at the point of care, reducing the need for manual assessment of complex clinical paperwork. Second, using NLP technology, healthcare professionals can automatically examine enormous amounts of unstructured clinical and patient data to select the most suitable patients for clinical trials, perhaps leading to an improvement in the patients' health [6]. Computer vision comes in sixth. Computer vision, an essential part of artificial intelligence, uses visual data as input to process photos and videos continuously in order to get better results faster and with higher quality than would be possible if the same job were done manually. Simply put, doctors can now diagnose their patients with diseases like cancer, diabetes, and cardiovascular disorders more quickly and at an earlier stage. Here are a few examples of real-world applications where computer vision technology is making notable strides. Mammogram images are analysed by visual systems that are intended to spot breast cancer at an early stage. Automated cell counting is another example from the real world that dramatically decreases human error and raises concerns about the accuracy of the results because they might differ greatly depending on the examiner's experience and degree of focus. A third application of computer vision in the real world is the quick and painless early-stage tumour detection enabled by artificial intelligence. Without a doubt, computer vision has the unfathomable potential to significantly enhance how healthcare is delivered. Other than for visual data analysis, clinicians can use this technology to enhance their training and skill development. Currently, Gramener is the top company offering medical facilities and research organisations computer vision solutions [7]. The usage of imperative rather than functional programming languages is one of the key difficulties in creating artificial intelligence software. As artificial intelligence starts to increase exponentially, developers employing imperative programming languages must assume that the machine is stupid and supply detailed instructions that are subject to a high level of maintenance and human error. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures of research and development. As a result, software developers have contributed to the unreasonably high cost of medical care. Functional programming languages, on the other hand, demand that the developer use their problem-solving abilities as though the computer were a mathematician. As a result, compared to the number of lines of code needed by the programme to perform the same operation, mathematical functions are orders of magnitude shorter. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures o}
}

@article{zimmerman2022assessingphysics,
  title={Assessing physics quantitative literacy in algebra-based physics: lessons learned},
  author={Charlotte Zimmerman and Andrew McCarty and Suzanne White Brahmia and Alexis Olsho and Mieke De Cock and A. Boudreaux and Trevor I. Smith and Philip Eaton},
  year={2022},
  booktitle={Physics Education Research Conference Proceedings},
  doi={10.1119/perc.2022.pr.zimmerman},
  url={https://www.semanticscholar.org/paper/8f16d42771af2c1227c7a4cf6ad219e54351c9f7},
  abstract={Physics quantitative literacy (PQL)—applying familiar mathematics in novel ways in the context of physics— is ubiquitous across physics classrooms. The Physics Inventory for Quantitative Literacy, or PIQL, is a recently published reasoning inventory that can be used to assess PQL from calculus-based introductory physics through upper division courses (White Brahmia et al. 2021). There remains a need, however, for assessment of quantitative reasoning at the algebra-based level which includes not only algebra-based college courses but also pre-college physics courses. We present recent work adapting the PIQL to an algebra-based context towards developing the GERQN—the Generalized Equation-based Reasoning inventory for Quantities and Negativity. We report lessons learned from our efforts to adapt items from the calculus-based PIQL to the algebra-based GERQN, and provide examples of how items were revised to be within students proximal zone. We also report on our experience translating the GERQN into Flemish as part of a larger, on-going research project, and what we learned about language accessibility for native and non-native English speakers alike for developing assessment items, curricular materials, and when speaking with students.}
}

@misc{kogan2022assessingacademic,
  title={Assessing the Academic Recovery of Ohio Students: An Analysis of Spring 2022 Ohio State Tests},
  author={Vladimir Kogan},
  year={2022},
  url={https://www.semanticscholar.org/paper/76ac1af061d5d2ccf19d748ca8b744a9461260f3}
}

@article{gao2022attributedtext,
  title={Attributed Text Generation via Post-hoc Research and Revision},
  author={Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Zhao and N. Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.08726},
  url={https://www.semanticscholar.org/paper/4ef5410ec4b546eda642fe786cc1bdbb5a7251e1}
}

@misc{wu2022autoformalizationneural,
  title={Autoformalization for Neural Theorem Proving},
  author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
  year={2022},
  url={https://www.semanticscholar.org/paper/15e767aa26a14455da95a2b2f11e3d59f2c250f6}
}

@article{wu2022autoformalizationwith,
  title={Autoformalization with Large Language Models},
  author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2205.12615},
  url={https://www.semanticscholar.org/paper/c28e95a06dfcf13fc65a1cac83722f53e34f12a5},
  abstract={Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from $29.6\%$ to $35.2\%$.}
}

@article{zhang2022automaticchain,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alexander J. Smola},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/90350aa626bed47b02d0c162462e5b0ca82be6b2},
  abstract={Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like"Let's think step by step"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the"Let's think step by step"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot}
}

@article{shridhar2022automaticgeneration,
  title={Automatic Generation of Socratic Subquestions for Teaching Math Word Problems},
  author={Kumar Shridhar and Jakub Macina and Mennatallah El-Assady and Tanmay Sinha and Manu Kapur and Mrinmaya Sachan},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2211.12835},
  url={https://www.semanticscholar.org/paper/e6745fb621481ccb0ed53c267a37292e499c1b42},
  abstract={Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers.In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning.On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.}
}

@article{xiao2022auxiliaryteaching,
  title={Auxiliary Teaching System of Higher Mathematics Based on Random Matrix Model},
  author={Yabin Xiao and Bing Zhou and Dan-ni He and Jingzhong Liu},
  year={2022},
  booktitle={Mathematical Problems in Engineering},
  doi={10.1155/2022/7983989},
  url={https://www.semanticscholar.org/paper/869011d58c272450dc8cf95bd4f81601e17b8511},
  abstract={With the development of computer technology, computers have become a part of people’s lives and the Internet has connected the world’s networks as a whole. Computer technology is changing people’s study, life, and work. People’s traditional education mode, thinking, content, method, and talent training program have a significant impact. The development from traditional to computer technology-based teaching methods has brought new developments and leaps in educational technology. This paper analyzes the research background, significance, and research status of the advanced mathematics auxiliary teaching system, introduces the related technologies and development modes used in the development of the system, and especially discusses the access database technology by ADO and the mathematical expression based on MathML language. Secondly, starting from the actual teaching, we analyze the functional requirements and performance requirements of the system in detail and make detailed planning and design for the system architecture, database selection, functional modules, etc. The design and implementation process of this teaching system are summarized. The teaching strategy inference engine is the key to the personalization and intelligence of the ICAI system. According to the learning models provided by different students, the system designs a corresponding teaching sequence for the learners by controlling the meta-knowledge of the domain knowledge base. The teaching strategy inference engine cuts the domain knowledge tree, selects the knowledge points suitable for the student, and sorts the selected knowledge points reasonably to generate an optimal teaching sequence. According to the students’ learning situation, combined with the teaching rules in the teaching rule library, the students’ grades are dynamically adjusted, so as to select new learning content for students and provide teaching suggestions in time. The student model is the premise of the ICAI system to achieve individualization and intelligence. The system makes a comprehensive evaluation and diagnosis of students through fuzzy comprehensive evaluation and fuzzy reasoning. On this basis, a cognitive student model is established, which is the teaching strategy that provided the basis for the formulation.}
}

@misc{an2022bevbertmultimodal,
  title={BEVBert: Multimodal Map Pre-training for Language-guided Navigation},
  author={Dongyan An and Yuankai Qi and Yangguang Li and Yan Huang and Liangsheng Wang and T. Tan and Jing Shao},
  year={2022},
  url={https://www.semanticscholar.org/paper/d7abc3bcf368c7c0e3487da7cecae1ac209a7284},
  abstract={Large-scale pre-training has shown promising results on the vision-and-language navigation (VLN) task. However, most existing pre-training methods employ discrete panoramas to learn visual-textual associations. This requires the model to implicitly correlate incomplete, duplicate observations within the panoramas, which may impair an agent's spatial understanding. Thus, we propose a new map-based pre-training paradigm that is spatial-aware for use in VLN. Concretely, we build a local metric map to explicitly aggregate incomplete observations and remove duplicates, while modeling navigation dependency in a global topological map. This hybrid design can balance the demand of VLN for both short-term reasoning and long-term planning. Then, based on the hybrid map, we devise a pre-training framework to learn a multimodal map representation, which enhances spatial-aware cross-modal reasoning thereby facilitating the language-guided navigation goal. Extensive experiments demonstrate the effectiveness of the map-based pre-training route for VLN, and the proposed method achieves state-of-the-art on four VLN benchmarks.}
}

@article{chen2022btpkbasedlearning,
  title={BTPK-based learning: An Interpretable Method for Named Entity Recognition},
  author={Yulin Chen and Zelai Yao and Haixiao Chi and D. Gabbay and Bo Yuan and Bruno Bentzen and Beishui Liao},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/811151315ac5fefb1629a4d02c0274370db468a7}
}

@article{hua2022bayesvarbrulunified,
  title={BayesVarbrul: a unified multidimensional analysis of language change in a speaker community},
  author={Xia Hua},
  year={2022},
  journal={Journal of Language Evolution},
  doi={10.1093/jole/lzac004},
  url={https://www.semanticscholar.org/paper/c294f2479c50d70b8e6b32b630e197aea1d9309d},
  abstract={
 Exchange in ideas between language evolution and biological evolution has a long history, due to a shared theoretical foundation between language and biology as two evolving systems. Both systems evolve in terms of the frequency of a variant in a population for each of a large number of variables, that is how often a particular variant of a language variable is used in a speaker community and how many individuals in a biological population carry a particular variant of a gene. The way these frequencies change has been modelled under a similar mathematical framework. Here, I show how we can use concepts from genome wide association studies that identify the source of natural selection and the genes under selection in a biological population to study how social factors affect the usage of language variables in a speaker community or how some social groups use some language variables differently from other groups. Using the Gurindji Kriol language as a case study, I show how this approach unifies existing mathematical and statistical tools in studying language evolution over a large number of speakers and a large number of language variables, which provides a promising link between micro- and macro-evolution in language. The approach is named BayesVarbrul and is ready to apply to datasets other than the Gurindji Kriol dataset, including existing corpus data. The code and the instructions are available at https://github.com/huaxia1985/BayesVarbrul.}
}

@misc{si2022benchmarkinggpt3,
  title={Benchmarking GPT-3 For Closed-Book QA: Strengths and Weaknesses},
  author={Chenglei Si and Naman Molri and Gurmehar Cheema and Elliot Huang and Arjun Akkiraju},
  year={2022},
  url={https://www.semanticscholar.org/paper/b9a0bc80aa136027327697fe40189792a32c8b0c}
}

@article{gopinath2022benchmarkinglargescale,
  title={Benchmarking Large-Scale ACOPF Solutions and Optimality Bounds},
  author={S. Gopinath and H. Hijazi},
  year={2022},
  booktitle={IEEE Power & Energy Society General Meeting},
  doi={10.1109/PESGM48719.2022.9916662},
  url={https://www.semanticscholar.org/paper/dea559bde46a1b9efc64c4418eebb3e9f3b775b7},
  abstract={We present the results of a comprehensive bench-marking effort aimed at evaluating and comparing state-of-the-art open-source tools for solving the Alternating-Current Optimal Power Flow (ACOPF) problem. Our numerical experiments include all instances found in the public library PGLIB with network sizes up to 30,000 nodes. The benchmarked tools span a number of programming languages (Python, Julia, Matlab/Octave, and C++), nonlinear optimization solvers (Ipopt, MIPS, and INLP) as well as different mathematical modeling tools (JuMP and Gravity). We also present state-of-the-art optimality bounds obtained using sparsity-exploiting semidefinite programming approaches and corresponding computational times.}
}

@article{gokhale2022benchmarkingspatial,
  title={Benchmarking Spatial Relationships in Text-to-Image Generation},
  author={Tejas Gokhale and Hamid Palangi and Besmira Nushi and Vibhav Vineet and E. Horvitz and Ece Kamar and Chitta Baral and Yezhou Yang},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.10015},
  url={https://www.semanticscholar.org/paper/4bf77d64b860ed0cd84a63aecd92a3cb295b88ee},
  abstract={Spatial understanding is a fundamental aspect of computer vision and integral for human-level reasoning about images, making it an important component for grounded language understanding. While recent text-to-image synthesis (T2I) models have shown unprecedented improvements in photorealism, it is unclear whether they have reliable spatial understanding capabilities. We investigate the ability of T2I models to generate correct spatial relationships among objects and present VISOR, an evaluation metric that captures how accurately the spatial relationship described in text is generated in the image. To benchmark existing models, we introduce a dataset, $\mathrm{SR}_{2D}$, that contains sentences describing two or more objects and the spatial relationships between them. We construct an automated evaluation pipeline to recognize objects and their spatial relationships, and employ it in a large-scale evaluation of T2I models. Our experiments reveal a surprising finding that, although state-of-the-art T2I models exhibit high image quality, they are severely limited in their ability to generate multiple objects or the specified spatial relations between them. Our analyses demonstrate several biases and artifacts of T2I models such as the difficulty with generating multiple objects, a bias towards generating the first object mentioned, spatially inconsistent outputs for equivalent relationships, and a correlation between object co-occurrence and spatial understanding capabilities. We conduct a human study that shows the alignment between VISOR and human judgement about spatial understanding. We offer the $\mathrm{SR}_{2D}$ dataset and the VISOR metric to the community in support of T2I reasoning research.}
}

@article{srivastava2022beyondimitation,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and A. Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmuller and Andrew M. Dai and A. La and Andrew Kyle Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and A. Tabassum and Arul Menezes and Arun Kirubarajan and A. Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakacs and B. R. Roberts and B. S. Loe and Barret Zoph and Bartlomiej Bojanowski and Batuhan Ozyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and B. Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and C'esar Ferri Ram'irez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and D. Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and D. Gonz'alez and Danielle R. Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and D. Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and E. D. Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodolà and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and E. Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Mart'inez-Plumed and Francesca Happ'e and François Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Xinyue Wang and Gonzalo Jaimovitch-L'opez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and H. Bogar and Henry Shevlin and Hinrich Schutze and H. Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and John Kernion and Jacob Hilton and Jaehoon Lee and J. Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Koco'n and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Narain Sohl-Dickstein and Jason Phang and Jason Wei and J. Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Oluwadara Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Jane W Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jorg Frohberg and Jos Rozen and J. Hernández-Orallo and Joseph Boudeman and J. Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and K. Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and K. Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and K. Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-philippe Morency and Luca Moschella and Luca Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Col'on and Luke Metz and Lutfi Kerem cSenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ram’irez Quintana and M. Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and M. Schubert and Medina Baitemirova and Melody Arnaud and M. McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and M. Strube and Michal Swkedrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Monica Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and T. MukundVarma and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and N. Keskar and Niveditha Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and P. Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and P. Eckersley and Phu Mon Htut and P. Hwang and P. Milkowski and P. Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphael Milliere and Rhythm Garg and Richard Barnes and R. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan Le Bras and Rosanne Liu and Rowan Jacobs and Rui Zhang and R. Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and R. Teehan and Rylan Yang and Sahib Singh and Saif Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi S. Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and S. Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima Debnath and Siamak Shakeri and Simon Thormeyer and S. Melzi and Siva Reddy and S. Makini and Soo-Hwan Lee and Spencer Bradley Torene and Sriharsha Hatwar and S. Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T Piantadosi and Stuart M. Shieber and Summer Misherghi and S. Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsunori Hashimoto and Te-Lin Wu and T. Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and T. Kornev and T. Tunduny and Tobias Gerstenberg and T. Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and V. Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and W. Fedus and W. Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yu Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881},
  abstract={Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit"breakthrough"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.}
}

@article{jung2022blankcollapse,
  title={Blank Collapse: Compressing CTC emission for the faster decoding},
  author={Minkyu Jung and Ohhyeok Kwon and S. Seo and Soonshin Seo},
  year={2022},
  booktitle={Interspeech},
  doi={10.48550/arXiv.2210.17017},
  url={https://www.semanticscholar.org/paper/6498d95d3f988e684bc6a70004decbefec655222},
  abstract={Connectionist Temporal Classification (CTC) model is a very efficient method for modeling sequences, especially for speech data. In order to use CTC model as an Automatic Speech Recognition (ASR) task, the beam search decoding with an external language model like n-gram LM is necessary to obtain reasonable results. In this paper we analyze the blank label in CTC beam search deeply and propose a very simple method to reduce the amount of calculation resulting in faster beam search decoding speed. With this method, we can get up to 78% faster decoding speed than ordinary beam search decoding with a very small loss of accuracy in LibriSpeech datasets. We prove this method is effective not only practically by experiments but also theoretically by mathematical reasoning. We also observe that this reduction is more obvious if the accuracy of the model is higher.}
}

@article{wan2022bridgingbetween,
  title={Bridging the Gap between Recognition-level Pre-training and Commonsensical Vision-language Tasks},
  author={Yue Wan and Yueen Ma and Haoxuan You and Zhecan Wang and Shih-Fu Chang},
  year={2022},
  booktitle={CSRR},
  doi={10.18653/v1/2022.csrr-1.4},
  url={https://www.semanticscholar.org/paper/ac13afe6c5f456a1f250e03e3258f5cfecc99373},
  abstract={Large-scale visual-linguistic pre-training aims to capture the generic representations from multimodal features, which are essential for downstream vision-language tasks. Existing methods mostly focus on learning the semantic connections between visual objects and linguistic content, which tend to be recognitionlevel information and may not be sufficient for commonsensical reasoning tasks like VCR. In this paper, we propose a novel commonsensical vision-language pre-training framework to bridge the gap. We first augment the conventional image-caption pre-training datasets with commonsense inferences from a visuallinguistic GPT-2. To pre-train models on image, caption and commonsense inferences together, we propose two new tasks: masked commonsense modeling (MCM) and commonsense type prediction (CTP). To reduce the shortcut effect between captions and commonsense inferences, we further introduce the domain-wise adaptive masking that dynamically adjusts the masking ratio. Experimental results on downstream tasks, VCR and VQA, show the improvement of our pre-training strategy over previous methods. Human evaluation also validates the relevance, informativeness, and diversity of the generated commonsense inferences. Overall, we demonstrate the potential of incorporating commonsense knowledge into the conventional recognition-level visual-linguistic pre-training.}
}

@misc{leemann2022oherencevaluation,
  title={C OHERENCE E VALUATION OF V ISUAL C ONCEPTS WITH O BJECTS AND L ANGUAGE},
  author={Tobias Leemann and Yao Rong and Stefan Kraft and Enkelejda Kasneci and Gjergji Kasneci},
  year={2022},
  url={https://www.semanticscholar.org/paper/75e3f61b69dc6bc8bfb7fd28aa1001edbbc8eab4}
}

@article{raman2022capecorrective,
  title={CAPE: Corrective Actions from Precondition Errors using Large Language Models},
  author={S. S. Raman and Vanya Cohen and Eric Rosen and Ifrah Idrees and D. Paulius and Stefanie Tellex},
  year={2022},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA57147.2024.10611376},
  url={https://www.semanticscholar.org/paper/c82d8d80ea68400adb7faebb2f1cff38dd83093a},
  abstract={Extracting knowledge and reasoning from large language models (LLMs) offers a path to designing intelligent robots. Common approaches that leverage LLMs for planning are unable to recover when actions fail and resort to retrying failed actions without resolving the underlying cause. We propose a novel approach (CAPE) that generates corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans through few-shot reasoning on action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while maintaining semantic correctness and minimizing re-prompting. In VirtualHome, CAPE improves a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan, whilst achieving competitive executability. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves correctness by 76.49% with higher executability compared to SayCan. Our approach enables embodied agents to follow natural language commands and robustly recover from failures.}
}

=== END BIBTEX ENTRIES ===

=== NEW PAPERS BEING ADDED THIS ITERATION ===
@article{wu2022autoformalizationwith,
  title={Autoformalization with Large Language Models},
  author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2205.12615},
  url={https://www.semanticscholar.org/paper/c28e95a06dfcf13fc65a1cac83722f53e34f12a5},
  abstract={Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from $29.6\%$ to $35.2\%$.}
}

@article{zhang2022automaticchain,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alexander J. Smola},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/90350aa626bed47b02d0c162462e5b0ca82be6b2},
  abstract={Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like"Let's think step by step"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the"Let's think step by step"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot}
}

@article{shridhar2022automaticgeneration,
  title={Automatic Generation of Socratic Subquestions for Teaching Math Word Problems},
  author={Kumar Shridhar and Jakub Macina and Mennatallah El-Assady and Tanmay Sinha and Manu Kapur and Mrinmaya Sachan},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2211.12835},
  url={https://www.semanticscholar.org/paper/e6745fb621481ccb0ed53c267a37292e499c1b42},
  abstract={Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers.In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning.On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.}
}

@article{xiao2022auxiliaryteaching,
  title={Auxiliary Teaching System of Higher Mathematics Based on Random Matrix Model},
  author={Yabin Xiao and Bing Zhou and Dan-ni He and Jingzhong Liu},
  year={2022},
  booktitle={Mathematical Problems in Engineering},
  doi={10.1155/2022/7983989},
  url={https://www.semanticscholar.org/paper/869011d58c272450dc8cf95bd4f81601e17b8511},
  abstract={With the development of computer technology, computers have become a part of people’s lives and the Internet has connected the world’s networks as a whole. Computer technology is changing people’s study, life, and work. People’s traditional education mode, thinking, content, method, and talent training program have a significant impact. The development from traditional to computer technology-based teaching methods has brought new developments and leaps in educational technology. This paper analyzes the research background, significance, and research status of the advanced mathematics auxiliary teaching system, introduces the related technologies and development modes used in the development of the system, and especially discusses the access database technology by ADO and the mathematical expression based on MathML language. Secondly, starting from the actual teaching, we analyze the functional requirements and performance requirements of the system in detail and make detailed planning and design for the system architecture, database selection, functional modules, etc. The design and implementation process of this teaching system are summarized. The teaching strategy inference engine is the key to the personalization and intelligence of the ICAI system. According to the learning models provided by different students, the system designs a corresponding teaching sequence for the learners by controlling the meta-knowledge of the domain knowledge base. The teaching strategy inference engine cuts the domain knowledge tree, selects the knowledge points suitable for the student, and sorts the selected knowledge points reasonably to generate an optimal teaching sequence. According to the students’ learning situation, combined with the teaching rules in the teaching rule library, the students’ grades are dynamically adjusted, so as to select new learning content for students and provide teaching suggestions in time. The student model is the premise of the ICAI system to achieve individualization and intelligence. The system makes a comprehensive evaluation and diagnosis of students through fuzzy comprehensive evaluation and fuzzy reasoning. On this basis, a cognitive student model is established, which is the teaching strategy that provided the basis for the formulation.}
}

@misc{an2022bevbertmultimodal,
  title={BEVBert: Multimodal Map Pre-training for Language-guided Navigation},
  author={Dongyan An and Yuankai Qi and Yangguang Li and Yan Huang and Liangsheng Wang and T. Tan and Jing Shao},
  year={2022},
  url={https://www.semanticscholar.org/paper/d7abc3bcf368c7c0e3487da7cecae1ac209a7284},
  abstract={Large-scale pre-training has shown promising results on the vision-and-language navigation (VLN) task. However, most existing pre-training methods employ discrete panoramas to learn visual-textual associations. This requires the model to implicitly correlate incomplete, duplicate observations within the panoramas, which may impair an agent's spatial understanding. Thus, we propose a new map-based pre-training paradigm that is spatial-aware for use in VLN. Concretely, we build a local metric map to explicitly aggregate incomplete observations and remove duplicates, while modeling navigation dependency in a global topological map. This hybrid design can balance the demand of VLN for both short-term reasoning and long-term planning. Then, based on the hybrid map, we devise a pre-training framework to learn a multimodal map representation, which enhances spatial-aware cross-modal reasoning thereby facilitating the language-guided navigation goal. Extensive experiments demonstrate the effectiveness of the map-based pre-training route for VLN, and the proposed method achieves state-of-the-art on four VLN benchmarks.}
}

@article{chen2022btpkbasedlearning,
  title={BTPK-based learning: An Interpretable Method for Named Entity Recognition},
  author={Yulin Chen and Zelai Yao and Haixiao Chi and D. Gabbay and Bo Yuan and Bruno Bentzen and Beishui Liao},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/811151315ac5fefb1629a4d02c0274370db468a7}
}

@article{hua2022bayesvarbrulunified,
  title={BayesVarbrul: a unified multidimensional analysis of language change in a speaker community},
  author={Xia Hua},
  year={2022},
  journal={Journal of Language Evolution},
  doi={10.1093/jole/lzac004},
  url={https://www.semanticscholar.org/paper/c294f2479c50d70b8e6b32b630e197aea1d9309d},
  abstract={
 Exchange in ideas between language evolution and biological evolution has a long history, due to a shared theoretical foundation between language and biology as two evolving systems. Both systems evolve in terms of the frequency of a variant in a population for each of a large number of variables, that is how often a particular variant of a language variable is used in a speaker community and how many individuals in a biological population carry a particular variant of a gene. The way these frequencies change has been modelled under a similar mathematical framework. Here, I show how we can use concepts from genome wide association studies that identify the source of natural selection and the genes under selection in a biological population to study how social factors affect the usage of language variables in a speaker community or how some social groups use some language variables differently from other groups. Using the Gurindji Kriol language as a case study, I show how this approach unifies existing mathematical and statistical tools in studying language evolution over a large number of speakers and a large number of language variables, which provides a promising link between micro- and macro-evolution in language. The approach is named BayesVarbrul and is ready to apply to datasets other than the Gurindji Kriol dataset, including existing corpus data. The code and the instructions are available at https://github.com/huaxia1985/BayesVarbrul.}
}

@misc{si2022benchmarkinggpt3,
  title={Benchmarking GPT-3 For Closed-Book QA: Strengths and Weaknesses},
  author={Chenglei Si and Naman Molri and Gurmehar Cheema and Elliot Huang and Arjun Akkiraju},
  year={2022},
  url={https://www.semanticscholar.org/paper/b9a0bc80aa136027327697fe40189792a32c8b0c}
}

@article{gopinath2022benchmarkinglargescale,
  title={Benchmarking Large-Scale ACOPF Solutions and Optimality Bounds},
  author={S. Gopinath and H. Hijazi},
  year={2022},
  booktitle={IEEE Power & Energy Society General Meeting},
  doi={10.1109/PESGM48719.2022.9916662},
  url={https://www.semanticscholar.org/paper/dea559bde46a1b9efc64c4418eebb3e9f3b775b7},
  abstract={We present the results of a comprehensive bench-marking effort aimed at evaluating and comparing state-of-the-art open-source tools for solving the Alternating-Current Optimal Power Flow (ACOPF) problem. Our numerical experiments include all instances found in the public library PGLIB with network sizes up to 30,000 nodes. The benchmarked tools span a number of programming languages (Python, Julia, Matlab/Octave, and C++), nonlinear optimization solvers (Ipopt, MIPS, and INLP) as well as different mathematical modeling tools (JuMP and Gravity). We also present state-of-the-art optimality bounds obtained using sparsity-exploiting semidefinite programming approaches and corresponding computational times.}
}

@article{gokhale2022benchmarkingspatial,
  title={Benchmarking Spatial Relationships in Text-to-Image Generation},
  author={Tejas Gokhale and Hamid Palangi and Besmira Nushi and Vibhav Vineet and E. Horvitz and Ece Kamar and Chitta Baral and Yezhou Yang},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.10015},
  url={https://www.semanticscholar.org/paper/4bf77d64b860ed0cd84a63aecd92a3cb295b88ee},
  abstract={Spatial understanding is a fundamental aspect of computer vision and integral for human-level reasoning about images, making it an important component for grounded language understanding. While recent text-to-image synthesis (T2I) models have shown unprecedented improvements in photorealism, it is unclear whether they have reliable spatial understanding capabilities. We investigate the ability of T2I models to generate correct spatial relationships among objects and present VISOR, an evaluation metric that captures how accurately the spatial relationship described in text is generated in the image. To benchmark existing models, we introduce a dataset, $\mathrm{SR}_{2D}$, that contains sentences describing two or more objects and the spatial relationships between them. We construct an automated evaluation pipeline to recognize objects and their spatial relationships, and employ it in a large-scale evaluation of T2I models. Our experiments reveal a surprising finding that, although state-of-the-art T2I models exhibit high image quality, they are severely limited in their ability to generate multiple objects or the specified spatial relations between them. Our analyses demonstrate several biases and artifacts of T2I models such as the difficulty with generating multiple objects, a bias towards generating the first object mentioned, spatially inconsistent outputs for equivalent relationships, and a correlation between object co-occurrence and spatial understanding capabilities. We conduct a human study that shows the alignment between VISOR and human judgement about spatial understanding. We offer the $\mathrm{SR}_{2D}$ dataset and the VISOR metric to the community in support of T2I reasoning research.}
}

@article{srivastava2022beyondimitation,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and A. Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmuller and Andrew M. Dai and A. La and Andrew Kyle Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and A. Tabassum and Arul Menezes and Arun Kirubarajan and A. Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakacs and B. R. Roberts and B. S. Loe and Barret Zoph and Bartlomiej Bojanowski and Batuhan Ozyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and B. Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and C'esar Ferri Ram'irez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and D. Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and D. Gonz'alez and Danielle R. Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and D. Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and E. D. Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodolà and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and E. Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Mart'inez-Plumed and Francesca Happ'e and François Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Xinyue Wang and Gonzalo Jaimovitch-L'opez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and H. Bogar and Henry Shevlin and Hinrich Schutze and H. Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and John Kernion and Jacob Hilton and Jaehoon Lee and J. Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Koco'n and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Narain Sohl-Dickstein and Jason Phang and Jason Wei and J. Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Oluwadara Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Jane W Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jorg Frohberg and Jos Rozen and J. Hernández-Orallo and Joseph Boudeman and J. Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and K. Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and K. Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and K. Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-philippe Morency and Luca Moschella and Luca Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Col'on and Luke Metz and Lutfi Kerem cSenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ram’irez Quintana and M. Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and M. Schubert and Medina Baitemirova and Melody Arnaud and M. McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and M. Strube and Michal Swkedrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Monica Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and T. MukundVarma and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and N. Keskar and Niveditha Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and P. Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and P. Eckersley and Phu Mon Htut and P. Hwang and P. Milkowski and P. Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphael Milliere and Rhythm Garg and Richard Barnes and R. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan Le Bras and Rosanne Liu and Rowan Jacobs and Rui Zhang and R. Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and R. Teehan and Rylan Yang and Sahib Singh and Saif Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi S. Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and S. Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima Debnath and Siamak Shakeri and Simon Thormeyer and S. Melzi and Siva Reddy and S. Makini and Soo-Hwan Lee and Spencer Bradley Torene and Sriharsha Hatwar and S. Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T Piantadosi and Stuart M. Shieber and Summer Misherghi and S. Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsunori Hashimoto and Te-Lin Wu and T. Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and T. Kornev and T. Tunduny and Tobias Gerstenberg and T. Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and V. Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and W. Fedus and W. Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yu Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881},
  abstract={Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit"breakthrough"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.}
}

@article{jung2022blankcollapse,
  title={Blank Collapse: Compressing CTC emission for the faster decoding},
  author={Minkyu Jung and Ohhyeok Kwon and S. Seo and Soonshin Seo},
  year={2022},
  booktitle={Interspeech},
  doi={10.48550/arXiv.2210.17017},
  url={https://www.semanticscholar.org/paper/6498d95d3f988e684bc6a70004decbefec655222},
  abstract={Connectionist Temporal Classification (CTC) model is a very efficient method for modeling sequences, especially for speech data. In order to use CTC model as an Automatic Speech Recognition (ASR) task, the beam search decoding with an external language model like n-gram LM is necessary to obtain reasonable results. In this paper we analyze the blank label in CTC beam search deeply and propose a very simple method to reduce the amount of calculation resulting in faster beam search decoding speed. With this method, we can get up to 78% faster decoding speed than ordinary beam search decoding with a very small loss of accuracy in LibriSpeech datasets. We prove this method is effective not only practically by experiments but also theoretically by mathematical reasoning. We also observe that this reduction is more obvious if the accuracy of the model is higher.}
}

@article{wan2022bridgingbetween,
  title={Bridging the Gap between Recognition-level Pre-training and Commonsensical Vision-language Tasks},
  author={Yue Wan and Yueen Ma and Haoxuan You and Zhecan Wang and Shih-Fu Chang},
  year={2022},
  booktitle={CSRR},
  doi={10.18653/v1/2022.csrr-1.4},
  url={https://www.semanticscholar.org/paper/ac13afe6c5f456a1f250e03e3258f5cfecc99373},
  abstract={Large-scale visual-linguistic pre-training aims to capture the generic representations from multimodal features, which are essential for downstream vision-language tasks. Existing methods mostly focus on learning the semantic connections between visual objects and linguistic content, which tend to be recognitionlevel information and may not be sufficient for commonsensical reasoning tasks like VCR. In this paper, we propose a novel commonsensical vision-language pre-training framework to bridge the gap. We first augment the conventional image-caption pre-training datasets with commonsense inferences from a visuallinguistic GPT-2. To pre-train models on image, caption and commonsense inferences together, we propose two new tasks: masked commonsense modeling (MCM) and commonsense type prediction (CTP). To reduce the shortcut effect between captions and commonsense inferences, we further introduce the domain-wise adaptive masking that dynamically adjusts the masking ratio. Experimental results on downstream tasks, VCR and VQA, show the improvement of our pre-training strategy over previous methods. Human evaluation also validates the relevance, informativeness, and diversity of the generated commonsense inferences. Overall, we demonstrate the potential of incorporating commonsense knowledge into the conventional recognition-level visual-linguistic pre-training.}
}

@misc{leemann2022oherencevaluation,
  title={C OHERENCE E VALUATION OF V ISUAL C ONCEPTS WITH O BJECTS AND L ANGUAGE},
  author={Tobias Leemann and Yao Rong and Stefan Kraft and Enkelejda Kasneci and Gjergji Kasneci},
  year={2022},
  url={https://www.semanticscholar.org/paper/75e3f61b69dc6bc8bfb7fd28aa1001edbbc8eab4}
}

@article{raman2022capecorrective,
  title={CAPE: Corrective Actions from Precondition Errors using Large Language Models},
  author={S. S. Raman and Vanya Cohen and Eric Rosen and Ifrah Idrees and D. Paulius and Stefanie Tellex},
  year={2022},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA57147.2024.10611376},
  url={https://www.semanticscholar.org/paper/c82d8d80ea68400adb7faebb2f1cff38dd83093a},
  abstract={Extracting knowledge and reasoning from large language models (LLMs) offers a path to designing intelligent robots. Common approaches that leverage LLMs for planning are unable to recover when actions fail and resort to retrying failed actions without resolving the underlying cause. We propose a novel approach (CAPE) that generates corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans through few-shot reasoning on action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while maintaining semantic correctness and minimizing re-prompting. In VirtualHome, CAPE improves a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan, whilst achieving competitive executability. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves correctness by 76.49% with higher executability compared to SayCan. Our approach enables embodied agents to follow natural language commands and robustly recover from failures.}
}

=== END NEW PAPERS ===

PREVIOUS DRAFT OF THE PAPER:
=== ABSTRACT ===
This systematic literature review provides a comprehensive overview of the intersection of large language models (LLMs), mathematical reasoning, and related computational reasoning techniques. With an expanded corpus of 45 papers, we detail key tasks, datasets, and methodologies, highlighting advancements in LLMs' ability to perform mathematical operations, solve word problems, engage in logical deduction, and undergo formal verification. The review explores diverse approaches, including prompt engineering, fine-tuning, integration of external knowledge, and formal methods, to enhance AI reasoning performance. Findings underscore significant progress in leveraging AI for quantitative and logical tasks, while also identifying persistent challenges in robustness, generalization, interpretability, and bias. This review serves as a valuable resource for researchers and practitioners in the evolving fields of AI reasoning and mathematical computation.

=== INTRODUCTION ===
\section{Introduction}

Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide spectrum of natural language processing tasks, from text generation to complex question answering \cite{brown2020language}. A particularly challenging and important domain where LLMs are increasingly being applied is mathematical reasoning. The ability to understand, process, and generate mathematical content is fundamental to scientific discovery, engineering, finance, and many aspects of daily life. Consequently, the development of artificial intelligence systems capable of robust mathematical reasoning has been a long-standing goal in the field of AI \cite{lu2022surveydeep}. The systematic analysis of reasoning processes, whether in natural language \cite{stolfo2022causalframework, yu2022analysiscorrelation}, formal logic \cite{poythress2022semioticanalysis}, or applied domains like clinical reasoning \cite{ricci2022petrinetbasedapproach}, is crucial for advancing AI capabilities.

Recent advancements in LLMs, particularly those based on transformer architectures \cite{vaswani2017attention}, have opened new avenues for tackling complex mathematical problems. These models, trained on massive datasets, possess an emergent ability to perform arithmetic operations, solve algebraic equations, and even engage with more abstract mathematical concepts \cite{abramson2022applicationpseudologlikelihoods}. However, the robustness and reliability of LLMs in mathematical reasoning remain active areas of research. Issues such as susceptibility to superficial patterns in problem descriptions \cite{stolfo2022causalframework}, the need for explicit reasoning capabilities \cite{zhang2022multilayerattention}, and challenges in out-of-distribution generalization \cite{nam2022achievingunderstanding} highlight the ongoing challenges. The development of AI-assisted programming further underscores the integration of AI with structured problem-solving \cite{gulwani2022aiassistedprogramming}. Furthermore, understanding the information-theoretic aspects of neural scaling laws \cite{jeon2022informationtheoreticanalysis} and developing frameworks for formal methods \cite{khan2022executableformal, katra2022experimentationframework} are critical for advancing rigorous reasoning systems.

This systematic literature review aims to provide a comprehensive overview of the current state of research at the intersection of large language models, mathematical reasoning, and related AI reasoning techniques. We address the following research questions:

1. What are the primary tasks and datasets used to evaluate AI models in mathematical and logical reasoning?
2. What are the dominant methodologies and architectures employed to enhance AI reasoning performance?
3. What are the key findings and limitations of current research in this area, including aspects of robustness, generalization, and interpretability?
4. What are the promising future research directions for AI in mathematical and logical reasoning?

To address these questions, we conducted a systematic search of the literature. Our methodology, detailed in the following section, adheres to the principles of the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement \cite{moher2009preferred} to ensure transparency and reproducibility. We focused on original research articles that specifically investigated the application of LLMs and related AI techniques to mathematical and logical reasoning tasks. This includes studies on multimodal sentiment analysis \cite{ji2022afrbertattentionbased}, educational assessment \cite{markta2022accuracypupils, zimmerman2022assessingphysics}, and the development of specific reasoning frameworks \cite{yu2022alertadapt, stolfo2022causalframework}.

This paper is structured as follows: Section 2 details our methodology. Section 3 presents the results of our literature search, organized thematically. Section 4 discusses the findings, identifies research gaps, and outlines implications and future directions. Finally, Section 5 concludes the review by summarizing the key contributions and insights.

=== METHODOLOGY ===
\section{Methodology}

This systematic literature review was conducted following the PRISMA guidelines to ensure a comprehensive and transparent search and selection process \cite{moher2009preferred}. The review focused on identifying research that investigates the application of large language models (LLMs), neural networks, and other advanced AI techniques to mathematical reasoning, logical deduction, and related quantitative and formal analysis tasks.

\subsection{Search Strategy}

Our search strategy was designed to capture relevant literature from major academic databases. We utilized the following search terms, combined using Boolean operators:

* (large language model OR LLM OR transformer OR neural network OR AI) AND (mathematical reasoning OR math reasoning OR quantitative reasoning OR logic OR logical reasoning OR problem solving OR algebra OR calculus OR arithmetic OR formal methods OR causal reasoning OR robustness OR generalization OR theorem proving OR verification OR computational reasoning)

The primary databases searched were IEEE Xplore, ACM Digital Library, SpringerLink, ScienceDirect, arXiv, and Google Scholar. The search was restricted to publications from 2018 to the present to capture the most recent advancements, given the rapid evolution of LLMs and associated reasoning techniques. All 45 identified papers were published in 2022, ensuring the recency of the corpus.

\subsection{Inclusion and Exclusion Criteria}

To ensure the relevance and quality of the included studies, we defined strict inclusion and exclusion criteria:

*   **Inclusion Criteria:**
    *   The study must explicitly involve large language models (e.g., GPT-3, BERT variants, T5, etc.), neural networks, or other advanced AI architectures with reasoning capabilities.
    *   The study must focus on mathematical reasoning tasks (arithmetic, algebra, word problems, quantitative reasoning), logical reasoning, formal methods, theorem proving, verification, or computational analysis involving structured reasoning.
    *   The study must present original research, including experimental results, novel methodologies, or analyses. This includes research on multimodal reasoning \cite{ji2022afrbertattentionbased}, educational assessment \cite{markta2022accuracypupils, zimmerman2022assessingphysics}, and specialized reasoning frameworks \cite{yu2022alertadapt, stolfo2022causalframework}.
    *   The study must be published in English.

*   **Exclusion Criteria:**
    *   Survey or review articles (unless serving as background for the current review's methodology).
    *   Studies focusing solely on natural language processing tasks unrelated to mathematical or logical reasoning.
    *   Studies that do not explicitly use or analyze LLMs or advanced reasoning techniques.
    *   Workshop papers, conference abstracts without full papers, and non-peer-reviewed articles.
    *   Studies where mathematical or logical reasoning is a minor component and not the primary focus, unless they offer a unique perspective on reasoning mechanisms \cite{cohen2022thisunicorn}.

\subsection{Study Selection}

Following the initial search, all retrieved records (462 identified) were imported into a reference management software. Titles and abstracts were systematically screened based on the defined inclusion and exclusion criteria. Full texts of potentially relevant articles were then retrieved and assessed for final inclusion. This process resulted in the inclusion of 45 papers in this review.

\subsection{Data Extraction and Synthesis}

Data extraction involved identifying key information from each included study: the AI model or system used, the specific reasoning task, the dataset employed, the proposed methodology, and the main findings. This included extracting information on how models handle data \cite{ji2022afrbertattentionbased}, adapt to tasks \cite{yu2022alertadapt}, and the formal models used \cite{khan2022executableformal}. Due to the diverse nature of the research, a narrative synthesis approach was adopted to summarize and integrate the findings, organized thematically to provide a structured overview. Special attention was paid to studies that analyzed quantitative assessment \cite{markta2022accuracypupils, zimmerman2022assessingphysics} and adaptive modeling techniques \cite{mare2022updatethermal}.

=== RESULTS ===
\section{Results}

Our systematic literature search identified a substantial body of research, totaling 45 papers, at the intersection of large language models (LLMs), neural networks, and various forms of reasoning, including mathematical, logical, and causal reasoning. These papers, all published in 2022, reflect the rapid advancements and growing interest in AI's ability to perform structured and quantitative tasks.

\subsection{Publication Trends and Key Venues}

The research landscape shows a concentration of publications in top-tier artificial intelligence and natural language processing conferences and journals. Prominent venues include the Association for Computational Linguistics (ACL) proceedings \cite{kumar2022answerlevelcalibration, yu2022alertadapt}, the Conference on Empirical Methods in Natural Language Processing (EMNLP) \cite{kar2022arggenprompting}, and the International Conference on Machine Learning (ICML), alongside journals like IEEE Transactions and arXiv preprints \cite{stolfo2022causalframework, lu2022surveydeep, nam2022achievingunderstanding, zhang2022empiricalinvestigation, wu2022autoformalizationneural, gao2022attributedtext, jeon2022informationtheoreticanalysis, abramson2022applicationpseudologlikelihoods, khan2022executableformal}. Specialized venues focusing on formal methods \cite{khan2022executableformal, hppner2022advantagesdisadvantages}, educational technology \cite{ricci2022petrinetbasedapproach, markta2022accuracypupils, zimmerman2022assessingphysics, amaliyah2022analisiskesulitan, shidqiya2022analysisstudents}, and specific application domains such as healthcare \cite{tewes2022artificialintelligence} and engineering \cite{snchez2022clusteringapproach, wang2022hybridgenetic, mare2022updatethermal, zhou2022applicationthreeflow} also contribute significantly.

\subsection{Core Tasks and Datasets in Mathematical and Logical Reasoning}

The evaluation of AI models in reasoning tasks relies on a variety of benchmarks and problem types. These can be broadly categorized as follows:

*   **Arithmetic, Algebraic, and Mathematical Word Problems:** These tasks form the core of quantitative reasoning evaluations. Datasets like GSM8K and MATH \cite{kobelski2022rethinking, hendrycks2021math} are frequently used to assess LLMs' ability to perform calculations, solve equations, and interpret natural language descriptions of mathematical scenarios. Surveys such as \cite{lu2022surveydeep} provide a comprehensive overview of these tasks and associated datasets. Datasets like ArMATH are specifically designed for Arabic math word problems \cite{alghamdi2022armathdataset}.
*   **Commonsense and Multimodal Reasoning:** Reasoning extends beyond pure mathematics to include everyday knowledge. Commonsense reasoning tasks are often evaluated using datasets that require understanding implicit information \cite{zhang2022multilayerattention, zhang2022empiricalinvestigation}. Multimodal sentiment analysis, which involves logical reasoning and mathematical operations on different data types (text, audio), is another area of focus \cite{ji2022afrbertattentionbased}.
*   **Formal Methods, Logic, and Verification:** While not always directly involving LLMs, research in formal logic \cite{poythress2022semioticanalysis}, executable formal models \cite{khan2022executableformal}, and experimentation frameworks for specification and verification \cite{katra2022experimentationframework} provide foundational principles for rigorous reasoning. These areas explore how to ensure correctness and analyze system properties, which can inform the development of more reliable AI reasoning systems.
*   **Causal Reasoning and Robustness:** Understanding the causal relationships within problem statements is crucial for robust reasoning. Frameworks for quantifying the robustness of mathematical reasoning \cite{stolfo2022causalframework} and achieving out-of-distribution generalization \cite{nam2022achievingunderstanding} are key areas of investigation.
*   **Educational Assessment and Learning Analytics:** Studies assess quantitative literacy \cite{zimmerman2022assessingphysics}, mathematical thinking abilities \cite{shidqiya2022analysisstudents}, and learning difficulties in mathematics \cite{amaliyah2022analisiskesulitan}. The accuracy of self-assessment in subjects like mathematics is also explored \cite{markta2022accuracypupils}. Research on learning analytics considers expected usefulness and privacy concerns \cite{li2022scenariobasedexploration}.

\subsection{Methodologies for Enhancing Reasoning Capabilities}

Researchers employ a diverse set of methodologies to enhance the reasoning capabilities of AI systems:

\subsubsection{Prompt Engineering and In-Context Learning}

Techniques like chain-of-thought (CoT) prompting have been highly effective in enabling LLMs to generate intermediate reasoning steps, thereby improving performance on complex tasks \cite{wei2022chainofthought}. The ALERT framework specifically aims to adapt language models to reasoning tasks through prompt-based methods \cite{yu2022alertadapt}. Answer-level calibration (ALC) is proposed for free-form question answering to model and remove context-independent biases \cite{kumar2022answerlevelcalibration}. Prompt-based text generation is used for document-level event-argument aggregation \cite{kar2022arggenprompting}.

\subsubsection{Fine-tuning and Model Adaptation}

Fine-tuning pre-trained models on specific reasoning datasets, such as mathematical problem-solving corpora, is a common strategy to adapt models to specialized tasks \cite{lu2022surveydeep, kobelski2022rethinking}. This contrasts with zero-shot approaches that use pseudo-log-likelihoods for natural language scoring, demonstrating competitive performance \cite{abramson2022applicationpseudologlikelihoods}. Personalizing frozen vision-language representations addresses user-specific concepts through language \cite{cohen2022thisunicorn}.

\subsubsection{Integration of External Tools and Knowledge}

Hybrid approaches combine LLMs with external tools like calculators, symbolic solvers (e.g., Wolfram Alpha), or knowledge graphs. This leverages the LLM's natural language understanding with the precision of specialized systems \cite{lu2022surveydeep}. Knowledge-enhanced pre-trained language models (KE-PLMs) integrate parametric knowledge with external sources like knowledge graphs \cite{hu2022surveyknowledge}. Empirical investigations into commonsense self-supervision with knowledge graphs explore these integrations \cite{zhang2022empiricalinvestigation}. Vadalog is a system designed for reasoning over large knowledge graphs \cite{bellomarini2022overviewvadalog}.

\subsubsection{Formal Methods and Structured Reasoning}

Formal methods are employed to ensure rigor and correctness. Petri nets enhance clinical reasoning \cite{ricci2022petrinetbasedapproach}. Executable formal models, such as for VHDL \cite{khan2022executableformal}, enable formal reasoning about designs. Experimentation frameworks for web service verification use declarative, mathematical formalisms \cite{katra2022experimentationframework}. A semiotic analysis assesses the usefulness and limitations of multiple systems of logic \cite{poythress2022semioticanalysis}.

\subsubsection{Causal Reasoning, Robustness, and Generalization}

Ensuring model robustness is a key concern. Causal frameworks help understand the influence of different input factors on model output, preventing reliance on spurious correlations \cite{stolfo2022causalframework}. Achieving and understanding out-of-distribution (OOD) generalization in systematic reasoning is investigated using small-scale transformers \cite{nam2022achievingunderstanding}. Autoformalization for neural theorem proving aims to bridge the gap between neural and formal methods \cite{wu2022autoformalizationneural}.

\subsubsection{Hybrid Algorithms and Optimization in Applied Domains}

Hybrid algorithms are used for optimization in complex domains. A hybrid genetic algorithm addresses the flexible job shop scheduling problem \cite{wang2022hybridgenetic}. Clustering strategies optimize the siting of recharging stations for electric vehicles \cite{snchez2022clusteringapproach}. Updates to thermal error compensation models via on-machine measurement demonstrate adaptive modeling \cite{mare2022updatethermal}. Three-flow fusion technology is applied in thermal power digital twins \cite{zhou2022applicationthreeflow}.

\subsubsection{Analysis of Reasoning and Understanding}

Research delves into the nature of reasoning itself. A ratiocinative study examines W. V. O. Quine's criterion of ontological commitment \cite{ekong2022ratiocinativestudy}. AFR-BERT is a multimodal sentiment analysis model that fuses features using attention mechanisms for emotional analysis through logical reasoning \cite{ji2022afrbertattentionbased}. AI-assisted programming integrates AI with structured problem-solving \cite{gulwani2022aiassistedprogramming}.

\subsubsection{Educational and Assessment Contexts}

Studies investigate the accuracy of pupils' self-assessment in mathematics and language \cite{markta2022accuracypupils}. Learning analytics adoption is explored through scenario-based studies \cite{li2022scenariobasedexploration}. Mathematical thinking abilities in students are analyzed in terms of self-efficacy \cite{shidqiya2022analysisstudents}. The algorithm method is examined for teaching Russian \cite{2022algorithmmethod}. Assessing physics quantitative literacy involves adapting reasoning inventories \cite{zimmerman2022assessingphysics}. Academic recovery of students is assessed using test data \cite{kogan2022assessingacademic}.

\subsubsection{Bias and Fairness in NLP}

Methodologies are developed to characterize bias and harmful stereotypes in NLP, emphasizing collaborative approaches \cite{alemany2022methodologycharacterize}. This is crucial for ensuring equitable AI applications, including those involving quantitative reasoning.

\subsubsection{Comparison of Approaches and Model Transformations}

Studies compare different methods for classification \cite{wankmller2022comparisonapproaches}, covariate effects in latent factors \cite{wang2022comparisonthree}, and model transformation languages \cite{hppner2022advantagesdisadvantages}. A review covers named entity recognition technology for aeronautical information intelligence \cite{mi2022reviewdevelopment}. Information-theoretic analysis of compute-optimal neural scaling laws provides insights into model and data size trade-offs \cite{jeon2022informationtheoreticanalysis}.

\subsubsection{Attributed Text Generation}

Attributed text generation via post-hoc research and revision is explored, focusing on generating text that is grounded in external information \cite{gao2022attributedtext}.

=== DISCUSSION ===
\section{Discussion}

The collected 45 papers reveal a dynamic and rapidly evolving research landscape at the nexus of AI, particularly LLMs, and structured reasoning, including mathematical and logical domains. The findings underscore significant progress in AI's ability to perform quantitative tasks, ranging from basic arithmetic and algebraic problem-solving \cite{lu2022surveydeep, kobelski2022rethinking} to more complex mathematical word problems \cite{alghamdi2022armathdataset}. Methodologies such as chain-of-thought prompting \cite{wei2022chainofthought}, fine-tuning on specialized datasets \cite{abramson2022applicationpseudologlikelihoods}, and the integration of external knowledge sources \cite{hu2022surveyknowledge, zhang2022empiricalinvestigation} have been pivotal in achieving these advancements.

A recurring theme is the effectiveness of prompt engineering techniques like CoT for eliciting step-by-step reasoning \cite{yu2022alertadapt}. This moves beyond mere pattern matching towards more transparent problem-solving processes. The combination of LLMs with formal methods, symbolic solvers, and knowledge graphs represents a powerful paradigm for leveraging the strengths of both neural and symbolic AI \cite{lu2022surveydeep, khan2022executableformal, katra2022experimentationframework}. The development of systems like Vadalog for knowledge graph reasoning \cite{bellomarini2022overviewvadalog} further exemplifies this trend.

Despite these successes, several challenges and research gaps remain prominent. **Robustness** is a critical concern, as highlighted by studies demonstrating that models can still rely on superficial cues rather than deep understanding \cite{stolfo2022causalframework}. Ensuring true mathematical reasoning, rather than mimicry, requires models that are resilient to variations in problem presentation and robust against adversarial manipulations \cite{abramson2022applicationpseudologlikelihoods}. Research on achieving out-of-distribution generalization \cite{nam2022achievingunderstanding} and understanding the causal underpinnings of reasoning \cite{stolfo2022causalframework} is essential to address this.

**Generalization to novel mathematical concepts and abstract reasoning** remains a significant hurdle \cite{lu2022surveydeep}. While current models excel in well-defined problem spaces, their ability to perform advanced theoretical reasoning, discover new mathematical principles, or adapt to entirely new domains is limited. Bridging this gap might involve deeper integration of structured knowledge \cite{hu2022surveyknowledge} and potentially new architectural innovations inspired by formal systems \cite{khan2022executableformal}.

**Interpretability and explainability** of AI reasoning processes are also areas demanding further investigation. While methods like CoT offer some transparency \cite{wei2022chainofthought}, understanding the internal decision-making mechanisms of LLMs in complex reasoning tasks is still an open problem. This lack of transparency can hinder trust and debugging efforts, particularly in critical applications. Characterizing biases in NLP systems \cite{alemany2022methodologycharacterize} is intrinsically linked to interpretability, ensuring fairness and accountability.

The potential for **personalization** in AI-driven reasoning tools, inspired by work in vision-language models \cite{cohen2022thisunicorn} and applied to areas like education \cite{li2022scenariobasedexploration, markta2022accuracypupils, zimmerman2022assessingphysics}, offers exciting possibilities. However, ethical considerations, including data privacy and the accuracy of AI-driven assessments \cite{kogan2022assessingacademic}, must be carefully addressed.

Furthermore, the study of **foundational aspects of logic and reasoning** continues to inform AI development. Semiotic analyses of logic systems \cite{poythress2022semioticanalysis} and explorations of ontological commitment \cite{ekong2022ratiocinativestudy} provide philosophical grounding. Research into information-theoretic scaling laws \cite{jeon2022informationtheoreticanalysis} and comparisons of different algorithmic approaches \cite{wankmller2022comparisonapproaches, wang2022comparisonthree, hppner2022advantagesdisadvantages} offer insights into optimizing AI systems.

Future research should focus on developing AI systems that exhibit deeper understanding, greater robustness, and more profound generalization capabilities in mathematical and logical reasoning. This includes exploring novel neuro-symbolic architectures \cite{wu2022autoformalizationneural}, refining hybrid reasoning approaches \cite{gulwani2022aiassistedprogramming}, and developing more sophisticated evaluation metrics that capture genuine reasoning skills rather than surface-level correlations \cite{stolfo2022causalframework}. The advancement of executable formal models \cite{khan2022executableformal} and experimentation frameworks \cite{katra2022experimentationframework} will be crucial for verifying the correctness of AI-generated reasoning.

In summary, while AI, particularly LLMs, has made remarkable strides in quantitative and logical reasoning tasks, the path toward truly intelligent and trustworthy systems requires continued innovation in robustness, generalization, interpretability, and the integration of diverse AI paradigms. The insights gleaned from multimodal analysis \cite{ji2022afrbertattentionbased} and applied domains like healthcare \cite{tewes2022artificialintelligence} and engineering \cite{snchez2022clusteringapproach, mare2022updatethermal, zhou2022applicationthreeflow} further emphasize the broad impact and ongoing development in this critical research area.

=== CONCLUSION ===
\section{Conclusion}

This systematic literature review, encompassing 45 papers, provides a comprehensive overview of the current state of research in large language models (LLMs), neural networks, and related AI techniques applied to mathematical, logical, and computational reasoning. Our findings highlight significant advancements in AI's capacity to perform quantitative tasks, from arithmetic and algebraic problem-solving \cite{lu2022surveydeep, kobelski2022rethinking} to sophisticated logical deduction and formal verification \cite{khan2022executableformal, katra2022experimentationframework}. Methodologies such as chain-of-thought prompting \cite{wei2022chainofthought}, fine-tuning \cite{abramson2022applicationpseudologlikelihoods}, and the integration of external knowledge and tools \cite{hu2022surveyknowledge, zhang2022empiricalinvestigation} have been instrumental in these developments.

Key contributions of this review include the identification of dominant tasks, datasets, and evaluation methodologies \cite{yu2022alertadapt}. We have synthesized findings regarding the effectiveness of various approaches, while also critically examining persistent challenges. The crucial areas of **robustness** \cite{stolfo2022causalframework}, **generalization** \cite{nam2022achievingunderstanding}, and **interpretability** remain significant research frontiers, underscoring the need for AI systems that exhibit genuine understanding rather than mere pattern recognition.

The implications of this research are far-reaching, promising to transform fields reliant on quantitative analysis and logical reasoning, including science, engineering, finance, and education \cite{li2022scenariobasedexploration, tewes2022artificialintelligence}. The development of AI-assisted programming tools \cite{gulwani2022aiassistedprogramming} and adaptive modeling techniques \cite{mare2022updatethermal} further demonstrates the growing integration of structured reasoning principles into practical applications.

While remarkable progress has been made, the quest for AI systems capable of truly independent and profound mathematical and logical reasoning continues. Future research should focus on novel architectures \cite{wu2022autoformalizationneural}, more rigorous evaluation of generalization capabilities \cite{nam2022achievingunderstanding}, and enhancing the transparency of AI reasoning processes \cite{alemany2022methodologycharacterize}. The insights from foundational studies on logic \cite{poythress2022semioticanalysis} and information theory \cite{jeon2022informationtheoreticanalysis} will undoubtedly continue to guide these efforts.

In conclusion, the field of AI-driven reasoning is advancing rapidly, with LLMs and related techniques showing immense potential. This review provides a comprehensive snapshot of the current landscape, highlighting both achievements and the critical avenues for future research, ultimately contributing to the development of more capable and trustworthy AI systems in quantitative and logical domains.

=== END PREVIOUS DRAFT ===

UPDATED STATISTICS:
- Total papers now included: 60
- Records identified: 462
- Studies included: 462

TASK: COMPLETELY REGENERATE the paper integrating the new papers.

REGENERATION INSTRUCTIONS:
1. Read the previous draft to understand existing structure and themes
2. Review the NEW papers being added (listed in "NEW PAPERS BEING ADDED" section)
3. Integrate new papers throughout ALL sections where relevant
4. In Results section:
   - Add new subsections if new themes emerge from new papers
   - Reorganize existing subsections for better coherence
   - CITE every paper discussed using \cite{citationKey}
5. Update all statistics to reflect new paper count
6. Maintain academic quality and narrative flow
7. Ensure EVERY paper (old and new) is cited using \cite{citationKey}

CITATION REQUIREMENTS:
✓ Use \cite{citationKey} format (e.g., \cite{smith2020deep})
✓ Cite papers from BOTH previous draft AND new additions
✓ Introduction: MINIMUM 5-10 citations
✓ Results: MINIMUM 15-25 citations (more with larger paper count)
✓ Discussion: MINIMUM 10-15 citations
✓ Conclusion: MINIMUM 3-5 citations
✓ Each subsection in Results MUST cite papers relevant to that theme

REGENERATE COMPLETE PAPER:
1. ABSTRACT: Update with new paper count, refined findings (NO citations)
2. INTRODUCTION: Integrate relevant new papers, update scope, CITE extensively
3. METHODOLOGY: Update statistics (cite PRISMA guidelines if needed)
4. RESULTS: **CRITICAL** - Reorganize with new papers, cite ALL papers discussed
5. DISCUSSION: Integrate new findings, synthesize across all papers, CITE extensively
6. CONCLUSION: Update with insights from complete set, CITE key papers

CRITICAL: Return your response as VALID JSON with PROPER ESCAPING:

IMPORTANT JSON ESCAPING RULES:
- Every single backslash in LaTeX commands MUST be escaped as double backslash
- \cite{} becomes \\cite{} in JSON
- \subsection{} becomes \\subsection{} in JSON
- Example: "introduction": "Recent work \\cite{smith2020} shows..."

Return ONLY valid JSON in this EXACT format:
{
  "abstract": "...",
  "introduction": "text with \\cite{} properly escaped",
  "methodology": "...",
  "results": "text with \\subsection{} and \\cite{} properly escaped",
  "discussion": "text with \\cite{} properly escaped",
  "conclusion": "text with \\cite{} properly escaped"
}

VERIFY: Check that ALL backslashes are doubled (\\) in JSON before returning!
