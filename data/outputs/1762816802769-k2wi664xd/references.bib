@article{wynter2023likehave,
  title={"I'd Like to Have an Argument, Please": Argumentative Reasoning in Large Language Models},
  author={Adrian de Wynter and Tommy Yuan},
  year={2023},
  booktitle={Comma},
  doi={10.3233/FAIA240311},
  url={https://www.semanticscholar.org/paper/673ff042b66940d600ab864e5680b0355aaeb147},
  abstract={We evaluate two large language models (LLMs) ability to perform argumentative reasoning. We experiment with argument mining (AM) and argument pair extraction (APE), and evaluate the LLMs' ability to recognize arguments under progressively more abstract input and output (I/O) representations (e.g., arbitrary label sets, graphs, etc.). Unlike the well-known evaluation of prompt phrasings, abstraction evaluation retains the prompt's phrasing but tests reasoning capabilities. We find that scoring-wise the LLMs match or surpass the SOTA in AM and APE, and under certain I/O abstractions LLMs perform well, even beating chain-of-thought--we call this symbolic prompting. However, statistical analysis on the LLMs outputs when subject to small, yet still human-readable, alterations in the I/O representations (e.g., asking for BIO tags as opposed to line numbers) showed that the models are not performing reasoning. This suggests that LLM applications to some tasks, such as data labelling and paper reviewing, must be done with care.},
  keywords={arxiv:2309.16938}
}

@article{hong20233dllminjecting,
  title={3D-LLM: Injecting the 3D World into Large Language Models},
  author={Yining Hong and Haoyu Zhen and Peihao Chen and Shuhong Zheng and Yilun Du and Zhenfang Chen and Chuang Gan},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2307.12981},
  url={https://www.semanticscholar.org/paper/7637ed79d30d0139901175ae4abedd822c217ab4}
}

@article{hong2023closerlook,
  title={A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning},
  author={Ruixin Hong and Hongming Zhang and Xinyu Pang and Dong Yu and Changshui Zhang},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2311.07954},
  url={https://www.semanticscholar.org/paper/6233b5863f9a0e8bacce47ce21bc3e81c09497bd},
  abstract={Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a series of models on their verification abilities. Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification methods. Drawing from these observations, we offer suggestions for future research and practical applications of self-verification methods.},
  keywords={arxiv:2311.07954}
}

@article{shui2023comprehensiveevaluation,
  title={A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction},
  author={Ruihao Shui and Yixin Cao and Xiang Wang and Tat-Seng Chua},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.11761},
  url={https://www.semanticscholar.org/paper/1c8840fc74ed2fa44b5995d6879ef10cf62101c4},
  abstract={Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4's law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an information retrieval (IR) system to learn from similar cases or solve simplified multi-choice questions. We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning. We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes redundant. Our evaluation pipeline can be easily extended into other tasks to facilitate evaluations in other domains. Code is available at https://github.com/srhthu/LM-CompEval-Legal},
  keywords={arxiv:2310.11761}
}

@article{parker2023largelanguage,
  title={A Large Language Model Approach to Educational Survey Feedback Analysis},
  author={Michael J. Parker and Caitlin Anderson and Claire Stone and YeaRim Oh},
  year={2023},
  journal={International Journal of Artificial Intelligence in Education},
  doi={10.1007/s40593-024-00414-0},
  url={https://www.semanticscholar.org/paper/5fb8dd5e31bf423d456ae580e494b406b4403bbf},
  abstract={This paper assesses the potential for the large language models (LLMs) GPT-4 and GPT-3.5 to aid in deriving insight from education feedback surveys. Exploration of LLM use cases in education has focused on teaching and learning, with less exploration of capabilities in education feedback analysis. Survey analysis in education involves goals such as finding gaps in curricula or evaluating teachers, often requiring time-consuming manual processing of textual responses. LLMs have the potential to provide a flexible means of achieving these goals without specialized machine learning models or fine-tuning. We demonstrate a versatile approach to such goals by treating them as sequences of natural language processing (NLP) tasks including classification (multi-label, multi-class, and binary), extraction, thematic analysis, and sentiment analysis, each performed by LLM. We apply these workflows to a real-world dataset of 2500 end-of-course survey comments from biomedical science courses, and evaluate a zero-shot approach (i.e., requiring no examples or labeled training data) across all tasks, reflecting education settings, where labeled data is often scarce. By applying effective prompting practices, we achieve human-level performance on multiple tasks with GPT-4, enabling workflows necessary to achieve typical goals. We also show the potential of inspecting LLMs’ chain-of-thought (CoT) reasoning for providing insight that may foster confidence in practice. Moreover, this study features development of a versatile set of classification categories, suitable for various course types (online, hybrid, or in-person) and amenable to customization. Our results suggest that LLMs can be used to derive a range of insights from survey text.},
  keywords={arxiv:2309.17447}
}

@article{feng2023largelanguage,
  title={A Large Language Model Enhanced Conversational Recommender System},
  author={Yue Feng and Shuchang Liu and Zhenghai Xue and Qingpeng Cai and Lantao Hu and Peng Jiang and Kun Gai and Fei Sun},
  year={2023},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/19f57c712d0a5dd47fb499aef9f76374dd5acea9},
  abstract={Conversational recommender systems (CRSs) aim to recommend high-quality items to users through a dialogue interface. It usually contains multiple sub-tasks, such as user preference elicitation, recommendation, explanation, and item information search. To develop effective CRSs, there are some challenges: 1) how to properly manage sub-tasks; 2) how to effectively solve different sub-tasks; and 3) how to correctly generate responses that interact with users. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to reason and generate, presenting a new opportunity to develop more powerful CRSs. In this work, we propose a new LLM-based CRS, referred to as LLMCRS, to address the above challenges. For sub-task management, we leverage the reasoning ability of LLM to effectively manage sub-task. For sub-task solving, we collaborate LLM with expert models of different sub-tasks to achieve the enhanced performance. For response generation, we utilize the generation ability of LLM as a language interface to better interact with users. Specifically, LLMCRS divides the workflow into four stages: sub-task detection, model matching, sub-task execution, and response generation. LLMCRS also designs schema-based instruction, demonstration-based instruction, dynamic sub-task and model matching, and summary-based generation to instruct LLM to generate desired results in the workflow. Finally, to adapt LLM to conversational recommendations, we also propose to fine-tune LLM with reinforcement learning from CRSs performance feedback, referred to as RLPF. Experimental results on benchmark datasets show that LLMCRS with RLPF outperforms the existing methods.},
  keywords={arxiv:2308.06212}
}

@article{stolfo2023mechanisticinterpretation,
  title={A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis},
  author={Alessandro Stolfo and Yonatan Belinkov and Mrinmaya Sachan},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.emnlp-main.435},
  url={https://www.semanticscholar.org/paper/5dc15ac1c92ab7492f121471823fb13a95d273ba},
  abstract={Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.},
  keywords={arxiv:2305.15054}
}

@article{charalambous2023softwaresecurity,
  title={A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification},
  author={Yiannis Charalambous and Norbert Tihanyi and Ridhi Jain and Youcheng Sun and M. Ferrag and L. Cordeiro},
  year={2023},
  booktitle={International Conference/Workshop on Automation of Software Test},
  doi={10.1109/AST66626.2025.00020},
  url={https://www.semanticscholar.org/paper/f542c184eec4c3252d678118a7f32cf327b6f23a},
  abstract={This paper presents a novel approach integrating Large Language Models (LLMs) with Formal Verification for automatic software vulnerability repair. Initially, we employ Bounded Model Checking (BMC) to identify vulnerabilities and extract counterexamples. Mathematical proofs and the stack trace of the vulnerabilities support these counterexamples. Using a specially designed prompt, we combine the source code with the identified vulnerability, including its stack trace and counterexample that specifies the line number and error type. This combined information is then fed into an LLM, which is instructed to attempt to fix the code. The new code is subsequently verified again using BMC to ensure the fix succeeded. We present the ESBMC-AI framework as a proof of concept, leveraging the well-recognized and industry-adopted Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained transformer model to detect and fix errors in C programs, particularly in critical software components. We evaluated our approach on 50, 000 C programs randomly selected from the FormAI dataset with their respective vulnerability classifications. Our results demonstrate ESBMC-AI’s capability to automate the detection and repair of issues such as buffer overflow, arithmetic overflow, and pointer dereference failures with high accuracy. ESBMC-AI is a pioneering initiative, integrating LLMs with BMC techniques, offering potential integration into the continuous integration and deployment (CI/CD) process within the software development lifecycle.},
  keywords={arxiv:2305.14752}
}

@article{canfora2023novelclassification,
  title={A Novel Classification Technique based on Formal Methods},
  author={G. Canfora and F. Mercaldo and A. Santone},
  year={2023},
  journal={ACM Transactions on Knowledge Discovery from Data},
  doi={10.1145/3592796},
  url={https://www.semanticscholar.org/paper/0e6f4f74c4d83836c3cefe1d754b6bd65e2b16f9},
  abstract={In last years, we are witnessing a growing interest in the application of supervised machine learning techniques in the most disparate fields. One winning factor of machine learning is represented by its ability to easily create models, as it does not require prior knowledge about the application domain. Complementary to machine learning are formal methods, that intrinsically offer safeness check and mechanism for reasoning on failures. Considering the weaknesses of machine learning, a new challenge could be represented by the use of formal methods. However, formal methods require the expertise of the domain, knowledge about modeling language with its semantic and mathematical rigour to specify properties. In this article, we propose a novel learning technique based on the adoption of formal methods for classification thanks to the automatic generation both of the formula and of the model. In this way the proposed method does not require any human intervention and thus it can be applied also to complex/large datasets. This leads to less effort both in using formal methods and in a better explainability and reasoning about the obtained results. Through a set of case studies from different real-world domains (i.e., driver detection, scada attack identification, arrhythmia characterization, mobile malware detection, and radiomics for lung cancer analysis), we demonstrate the usefulness of the proposed method, by showing that we are able to overcome the performances obtained from widespread classification algorithms.}
}

@article{chang2023surveyevaluation,
  title={A Survey on Evaluation of Large Language Models},
  author={Yu-Chu Chang and Xu Wang and Jindong Wang and Yuan Wu and Kaijie Zhu and Hao Chen and Linyi Yang and Xiaoyuan Yi and Cunxiang Wang and Yidong Wang and Weirong Ye and Yue Zhang and Yi Chang and Philip S. Yu and Qian Yang and Xingxu Xie},
  year={2023},
  journal={ACM Transactions on Intelligent Systems and Technology},
  doi={10.1145/3641289},
  url={https://www.semanticscholar.org/paper/888728745dbb769e29ed475d4f7661eebe1a71cf},
  abstract={Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey},
  keywords={arxiv:2307.03109}
}

@article{bao2023systematicevaluation,
  title={A Systematic Evaluation of Large Language Models on Out-of-Distribution Logical Reasoning Tasks},
  author={Qiming Bao and Gaël Gendron and A. Peng and Wanjun Zhong and N. Tan and Yang Chen and Michael Witbrock and Jiamou Liu},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.09430},
  url={https://www.semanticscholar.org/paper/c703b0f38a99eb7e6e2bd404e02f228235b5f94d}
}

@article{laskar2023systematicstudy,
  title={A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets},
  author={Md Tahmid Rahman Laskar and M Saiful Bari and Mizanur Rahman and Md Amran Hossen Bhuiyan and Shafiq R. Joty and J. Huang},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2305.18486},
  url={https://www.semanticscholar.org/paper/d3060876d9ad4e4e50e1c88a8c04186df00f24e2},
  abstract={The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instructions that we mostly found in ChatGPT and other instruction-tuned models. Our extensive evaluation shows that even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of ChatGPT's performance across diverse NLP tasks, this paper sets the stage for a targeted deployment of ChatGPT-like LLMs in real-world applications.},
  keywords={arxiv:2305.18486}
}

@article{yin2023surveymultimodal,
  title={A survey on multimodal large language models},
  author={Shukang Yin and Chaoyou Fu and Sirui Zhao and Ke Li and Xing Sun and Tong Xu and Enhong Chen},
  year={2023},
  booktitle={National Science Review},
  doi={10.1093/nsr/nwae403},
  url={https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df},
  abstract={ABSTRACT Recently, the multimodal large language model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful large language models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of the MLLM, such as writing stories based on images and optical character recognition–free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even outperform GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First, we present the basic formulation of the MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages and scenarios. We continue with multimodal hallucination and extended techniques, including multimodal in-context learning, multimodal chain of thought and LLM-aided visual reasoning. To conclude the paper, we discuss existing challenges and point out promising research directions.},
  keywords={arxiv:2306.13549}
}

@article{yin2023alcunalarge,
  title={ALCUNA: Large Language Models Meet New Knowledge},
  author={Xunjian Yin and Baizhou Huang and Xiaojun Wan},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.14820},
  url={https://www.semanticscholar.org/paper/896845aa992d50182227e4eb2883f784e16fe60b},
  abstract={With the rapid development of NLP, large-scale language models (LLMs) excel in various tasks across multiple domains now. However, existing benchmarks may not adequately measure these models' capabilities, especially when faced with new knowledge. In this paper, we address the lack of benchmarks to evaluate LLMs' ability to handle new knowledge, an important and challenging aspect in the rapidly evolving world. We propose an approach called KnowGen that generates new knowledge by altering existing entity attributes and relationships, resulting in artificial entities that are distinct from real-world entities. With KnowGen, we introduce a benchmark named ALCUNA to assess LLMs' abilities in knowledge understanding, differentiation, and association. We benchmark several LLMs, reveals that their performance in face of new knowledge is not satisfactory, particularly in reasoning between new and internal knowledge. We also explore the impact of entity similarity on the model's understanding of entity knowledge and the influence of contextual entities. We appeal to the need for caution when using LLMs in new scenarios or with new knowledge, and hope that our benchmarks can help drive the development of LLMs in face of new knowledge.},
  keywords={arxiv:2310.14820}
}

@article{sawada2023advancedreasoning,
  title={ARB: Advanced Reasoning Benchmark for Large Language Models},
  author={Tomohiro Sawada and Daniel Paleka and Alexander Havrilla and Pranav Tadepalli and Paula Vidas and Alexander Kranias and John J. Nay and Kshitij Gupta and Aran Komatsuzaki},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2307.13692},
  url={https://www.semanticscholar.org/paper/c29dbfbc17fa190b787a2662d49f08a38c8bd166},
  abstract={Large Language Models (LLMs) have demonstrated remarkable performance on various quantitative reasoning and knowledge benchmarks. However, many of these benchmarks are losing utility as LLMs get increasingly high scores, despite not yet reaching expert performance in these domains. We introduce ARB, a novel benchmark composed of advanced reasoning problems in multiple fields. ARB presents a more challenging test than prior benchmarks, featuring problems in mathematics, physics, biology, chemistry, and law. As a subset of ARB, we introduce a challenging set of math and physics problems which require advanced symbolic reasoning and domain knowledge. We evaluate recent models such as GPT-4 and Claude on ARB and demonstrate that current models score well below 50\% on more demanding tasks. In order to improve both automatic and assisted evaluation capabilities, we introduce a rubric-based evaluation approach, allowing GPT-4 to score its own intermediate reasoning steps. Further, we conduct a human evaluation of the symbolic subset of ARB, finding promising agreement between annotators and GPT-4 rubric evaluation scores.},
  keywords={arxiv:2307.13692}
}

@article{paranjape2023automaticmultistep,
  title={ART: Automatic multi-step reasoning and tool-use for large language models},
  author={Bhargavi Paranjape and Scott M. Lundberg and Sameer Singh and Hannaneh Hajishirzi and Luke Zettlemoyer and Marco Tulio Ribeiro},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2303.09014},
  url={https://www.semanticscholar.org/paper/0d42221038c05cee8443c5b5af838505ee137dc3},
  abstract={Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.},
  keywords={arxiv:2303.09014}
}

@article{diao2023activeprompting,
  title={Active Prompting with Chain-of-Thought for Large Language Models},
  author={Shizhe Diao and Pengcheng Wang and Yong Lin and Xiang Liu and Tong Zhang},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2302.12246},
  url={https://www.semanticscholar.org/paper/3fc3460c4554a28e489a0ea6ef067b79b7d301d9},
  abstract={The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.},
  keywords={arxiv:2302.12246}
}

@article{zhou2023adaptivesolverframework,
  title={Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning},
  author={Jianpeng Zhou and Wanjun Zhong and Yanlin Wang and Jiahai Wang},
  year={2023},
  booktitle={Information Processing \& Management},
  doi={10.48550/arXiv.2310.01446},
  url={https://www.semanticscholar.org/paper/5076bbbf831a92174c9cc1b347bd0584560435fc},
  abstract={Large Language Models (LLMs) demonstrate impressive ability in handling reasoning tasks. However, unlike humans who can instinctively adapt their problem-solving strategies to the complexity of task, most LLM-based methods adopt a one-size-fits-all approach. These methods employ consistent models, sample sizes, prompting methods and levels of problem decomposition, regardless of the problem complexity. The inflexibility of these methods can bring unnecessary computational overhead or sub-optimal performance. To address this limitation, we introduce an Adaptive-Solver (AS) framework tha dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources. The framework functions with two primary modules. The initial evaluation module assesses the reliability of the current solution using answer consistency. If the solution is deemed unreliable, the subsequent adaptation module comes into play. Within this module, various types of adaptation strategies are employed collaboratively. Through such dynamic and multi-faceted adaptations, our framework can help reduce computational consumption and improve performance. Experimental results from complex reasoning benchmarks reveal that our method can significantly reduce API costs (up to 85\%) while maintaining original performance. Alternatively, it achieves up to 4.5\% higher accuracy compared to the baselines at the same cost. The code and dataset are available at https://github.com/john1226966735/Adaptive-Solver.},
  keywords={arxiv:2310.01446}
}

@article{ojo2023afrobenchgood,
  title={AfroBench: How Good are Large Language Models on African Languages?},
  author={Jessica Ojo and Kelechi Ogueji and Pontus Stenetorp and David Ifeoluwa Adelani},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2025.findings-acl.976},
  url={https://www.semanticscholar.org/paper/c7972d2993d9de92062a55fa844d2b13b981a037},
  abstract={Large-scale multilingual evaluations, such as MEGA, often include only a handful of African languages due to the scarcity of high-quality evaluation data and the limited discoverability of existing African datasets. This lack of representation hinders comprehensive LLM evaluation across a diverse range of languages and tasks. To address these challenges, we introduce AfroBench -- a multi-task benchmark for evaluating the performance of LLMs across 64 African languages, 15 tasks and 22 datasets. AfroBench consists of nine natural language understanding datasets, six text generation datasets, six knowledge and question answering tasks, and one mathematical reasoning task. We present results comparing the performance of prompting LLMs to fine-tuned baselines based on BERT and T5-style models. Our results suggest large gaps in performance between high-resource languages, such as English, and African languages across most tasks; but performance also varies based on the availability of monolingual data resources. Our findings confirm that performance on African languages continues to remain a hurdle for current LLMs, underscoring the need for additional efforts to close this gap. https://mcgill-nlp.github.io/AfroBench/},
  keywords={arxiv:2311.07978}
}

@article{crispino2023agentinstructs,
  title={Agent Instructs Large Language Models to be General Zero-Shot Reasoners},
  author={Nicholas Crispino and Kyle Montgomery and Fankun Zeng and D. Song and Chenguang Wang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.03710},
  url={https://www.semanticscholar.org/paper/6d003fce1bfa44f99afc752ec173c9004fe3d59e},
  abstract={We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3\%), Llama-2-70b-chat (23.2\%), and GPT-3.5 Turbo (17.0\%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5\%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2\%.},
  keywords={arxiv:2310.03710}
}

@article{sel2023algorithmthoughts,
  title={Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models},
  author={Bilgehan Sel and Ahmad S. Al-Tawaha and Vanshaj Khattar and Lucy Wang and R. Jia and Ming Jin},
  year={2023},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2308.10379},
  url={https://www.semanticscholar.org/paper/fca92fe287c44c9ec79ca1f2762b0bf2e5e8df2b},
  abstract={Current literature, aiming to surpass the"Chain-of-Thought"approach, often resorts to external modi operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. Due to their myopic perspective, they escalate the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways. By employing algorithmic examples fully in-context, this overarching view of the whole process exploits the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and even more recent multi-query strategies that employ an extensive tree search algorithms while using significantly fewer tokens. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application. The code and related content can be found in: https://algorithm-of-thoughts.github.io.},
  keywords={arxiv:2308.10379}
}

@article{yang2023alignedcotprompting,
  title={AlignedCoT: Prompting Large Language Models via Native-Speaking Demonstrations},
  author={Zhicheng YANG and Yiwei Wang and Yinya Huang and Jing Xiong and Xiaodan Liang and Jing Tang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2024.findings-emnlp.163},
  url={https://www.semanticscholar.org/paper/9788e5a2857769dd39bb98f570f9e2a7d482f066},
  abstract={Large Language Models prompting, such as using in-context demonstrations, is a mainstream technique for invoking LLMs to perform high-performance and solid complex reasoning (e.g., mathematical reasoning, commonsense reasoning), and has the potential for further human-machine collaborative scientific findings. However, current LLMs are delicate and elusive in prompt words and styles. And there is an unseen gap between LLM understanding and human-written prompts. This paper introduces Alignedcot, an LLM-acquainted prompting technique that includes proficient ``native-speaking'' in in-context learning for the LLMs. Specifically, it achieves consistent and correct step-wise prompts in zero-shot scenarios by progressively probing, refining, and formatting the LLM chain of thoughts so that free from handcrafted few-shot demonstrations while maintaining the prompt quality. We conduct experiments on mathematical reasoning and commonsense reasoning. We find that LLMs with Alignedcot perform significantly superior to them with human-crafted demonstrations. We further apply Alignedcot for rewriting the GSM8K training set, resulting in a GSM8K-Align dataset. We observe its benefits for retrieval augmented generation. The code and data can be found at https://github.com/yangzhch6/AlignedCoT.},
  keywords={arxiv:2311.13538}
}

@article{sun2023alliesprompting,
  title={Allies: Prompting Large Language Model with Beam Search},
  author={Hao-Lun Sun and Xiao Liu and Yeyun Gong and Yan Zhang and Nan Duan},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.findings-emnlp.247},
  url={https://www.semanticscholar.org/paper/c6cb8b76d9308138bfa58d1fb764b62d49f22a57},
  abstract={With the advance of large language models (LLMs), the research field of LLM applications becomes more and more popular and the idea of constructing pipelines to accomplish complex tasks by stacking LLM API calls come true. However, this kind of methods face two limitations: narrow information coverage and low fault tolerance. In this work, we propose a novel method called ALLIES. Given an input query, ALLIES leverages LLMs to iteratively generate new queries related to the original query, enabling an iterative reasoning process. By iteratively refining and expanding the scope of the original query, ALLIES captures and utilizes hidden knowledge that may not be directly obtainable through retrieval. We take zero-shot open-domain question answering (ODQA) as an application scene and evaluate ALLIES on the widely-used benchmarks, such as NQ, WebQ and TriviaQA. The experimental results demonstrate that ALLIES significantly outperforms other zero-shot baselines, indicating its effectiveness in tackling those challenges. Our code is available in https://github.com/microsoft/SimXNS/tree/main/ALLIES.},
  keywords={arxiv:2305.14766}
}

@article{feng2023alphazeroliketreesearch,
  title={Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training},
  author={Xidong Feng and Ziyu Wan and Muning Wen and Ying Wen and Weinan Zhang and Jun Wang},
  year={2023},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2309.17179},
  url={https://www.semanticscholar.org/paper/e8df1cf6742b50a15500b8dd3dde3942e9c91418},
  abstract={Recent works like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by using tree-search algorithms to guide multi-step reasoning. These methods rely on prompting a pre-trained model to serve as a value function and focus on problems with low search depth. As a result, these methods will not work in domains where the pre-trained LLM does not have enough knowledge to serve as an effective value function or in domains that require long-horizon planning. To address these limitations, we present an AlphaZero-like tree-search learning framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLM decoding. TS-LLM distinguishes itself in two key ways. (1) Leveraging a learned value function and AlphaZero-like algorithms, our approach can be generally adaptable to a wide range of tasks, language models of any size, and tasks of varying search depths. (2) Our approach can guide LLMs during both inference and training, iteratively improving the LLM. Empirical results across reasoning, planning, alignment, and decision-making tasks show that TS-LLM outperforms existing approaches and can handle trees with a depth of 64.},
  keywords={arxiv:2309.17179}
}

@article{hu2023amortizingintractable,
  title={Amortizing intractable inference in large language models},
  author={Edward J. Hu and Moksh Jain and Eric Elmoznino and Younesse Kaddar and Guillaume Lajoie and Y. Bengio and Nikolay Malkin},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.04363},
  url={https://www.semanticscholar.org/paper/1effda8ce21573ed864eadfdc7b233aac39b8fe6},
  abstract={Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate that our approach enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use.},
  keywords={arxiv:2310.04363}
}

@article{luo2023empiricalstudy,
  title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-Tuning},
  author={Yun Luo and Zhen Yang and Fandong Meng and Yafu Li and Jie Zhou and Yue Zhang},
  year={2023},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  doi={10.1109/TASLPRO.2025.3606231},
  url={https://www.semanticscholar.org/paper/838cd69a0b6c9c244a6eebb0f4742c0625132de6},
  abstract={Catastrophic forgetting (CF) is a phenomenon that occurs in machine learning when a model forgets previously learned information while acquiring new knowledge for achieving satisfactory performance in downstream tasks. As large language models (LLMs) have demonstrated remarkable performance, it is intriguing to investigate whether CF exists during the continual instruction tuning of LLMs. This study empirically evaluates the forgetting phenomenon in LLMs’ knowledge during continual instruction tuning from the perspectives of domain knowledge, reasoning, and reading comprehension. The experiments reveal that catastrophic forgetting is generally observed in LLMs ranging from 1 b to 7 b parameters. Surprisingly, as the model scale increases, the severity of forgetting intensifies in such a model scale range, which may result from the much more significant initial performance in the larger LLM. The finding is also observed by the experiment of Qwen-2.5-Inst from 3 B to 14 B. Comparing the decoder-only model BLOOMZ with the encoder-decoder model mT0, BLOOMZ exhibits less forgetting and retains more knowledge. Interestingly, we also observe that LLMs can mitigate language biases, such as gender bias, during continual fine-tuning. Furthermore, our findings indicate that general instruction tuning can help alleviate the forgetting phenomenon in LLMs during subsequent fine-tuning.},
  keywords={arxiv:2308.08747}
}

@article{yang2023improvedbaseline,
  title={An Improved Baseline for Reasoning Segmentation with Large Language Model},
  author={Senqiao Yang and Tianyuan Qu and Xin Lai and Zhuotao Tian and Bohao Peng and Shu Liu and Jiaya Jia},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.17240},
  url={https://www.semanticscholar.org/paper/46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9},
  abstract={While LISA effectively bridges the gap between segmentation and large language models to enable reasoning segmentation, it poses certain limitations: unable to distinguish different instances of the target region, and constrained by the pre-defined textual response formats. In this work, we introduce LISA++, an update to the existing LISA model, focusing on improving core functionalities while keeping the base architecture intact. The main enhancements in LISA++ include: \textbackslash\{\}textbf\{1) Enhanced Segmentation\}: The instance segmentation ability has been added, providing a more detailed scene analysis along with the existing multi-region semantic segmentation. \textbackslash\{\}textbf\{2) More Natural Conversation\}: Improved capability for multi-turn dialogue, with the ability to incorporate segmentation results directly into text responses, i.e., Segmentation in Dialogue (SiD). These improvements are achieved by curating the existing samples of generic segmentation datasets, aimed specifically at enhancing the segmentation and conversational skills without structural change and additional data sources. Comparative analysis with the original LISA model shows significant advancements in these areas, positioning LISA++ as a notable upgrade in visual understanding and interaction. LISA++'s adaptability and improved features highlight the versatility of the mask-as-embedding paradigm proposed by LISA, and the potential as a foundational model for diverse applications.},
  keywords={arxiv:2312.17240}
}

@article{zhao2023indepthsurvey,
  title={An In-depth Survey of Large Language Model-based Artificial Intelligence Agents},
  author={Pengyu Zhao and Zijian Jin and Ning Cheng},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2309.14365},
  url={https://www.semanticscholar.org/paper/b3d0871705f246045d58b77bdb9574d5bc3d949f},
  abstract={Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance. In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents. Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities. Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use. Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent's memory system. We firmly believe that in-depth research and understanding of these core components will lay a solid foundation for the future advancement of AI agent technology. At the end of the paper, we provide directional suggestions for further research in this field, with the hope of offering valuable insights to scholars and researchers in the field.},
  keywords={arxiv:2309.14365}
}

@article{petridis2023anglekindlingsupporting,
  title={AngleKindling: Supporting Journalistic Angle Ideation with Large Language Models},
  author={S. Petridis and N. Diakopoulos and Kevin Crowston and Mark Hansen and K. Henderson and Stanislaw Jastrzebski and J. Nickerson and Lydia B. Chilton},
  year={2023},
  booktitle={International Conference on Human Factors in Computing Systems},
  doi={10.1145/3544548.3580907},
  url={https://www.semanticscholar.org/paper/3d7a364e83161f9a186837915a9c45083f6e1621},
  abstract={News media often leverage documents to find ideas for stories, while being critical of the frames and narratives present. Developing angles from a document such as a press release is a cognitively taxing process, in which journalists critically examine the implicit meaning of its claims. Informed by interviews with journalists, we developed AngleKindling, an interactive tool which employs the common sense reasoning of large language models to help journalists explore angles for reporting on a press release. In a study with 12 professional journalists, we show that participants found AngleKindling significantly more helpful and less mentally demanding to use for brainstorming ideas, compared to a prior journalistic angle ideation tool. AngleKindling helped journalists deeply engage with the press release and recognize angles that were useful for multiple types of stories. From our findings, we discuss how to help journalists customize and identify promising angles, and extending AngleKindling to other knowledge-work domains.}
}

@article{schaeffer2023emergentabilities,
  title={Are Emergent Abilities of Large Language Models a Mirage?},
  author={Rylan Schaeffer and B. Miranda and Oluwasanmi Koyejo},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2304.15004},
  url={https://www.semanticscholar.org/paper/29c7f009df21d0112c48dec254ff80cc45fac3af},
  abstract={Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
  keywords={arxiv:2304.15004}
}

@article{bhandari2023largelanguage,
  title={Are Large Language Models Geospatially Knowledgeable?},
  author={Prabin Bhandari and Antonios Anastasopoulos and D. Pfoser},
  year={2023},
  booktitle={SIGSPATIAL/GIS},
  doi={10.1145/3589132.3625625},
  url={https://www.semanticscholar.org/paper/137094dc64e1bad43f68333dc1f82d56168a3de7},
  abstract={Despite the impressive performance of Large Language Models (LLM) for various natural language processing tasks, little is known about their comprehension of geographic data and related ability to facilitate informed geospatial decision-making. This paper investigates the extent of geospatial knowledge, awareness, and reasoning abilities encoded within such pretrained LLMs. With a focus on autoregressive language models, we devise experimental approaches related to (i) probing LLMs for geo-coordinates to assess geospatial knowledge, (ii) using geospatial and non-geospatial prepositions to gauge their geospatial awareness, and (iii) utilizing a multidimensional scaling (MDS) experiment to assess the models' geospatial reasoning capabilities and to determine locations of cities based on prompting. Our results confirm that it does not only take larger but also more sophisticated LLMs to synthesize geospatial knowledge from textual information. As such, this research contributes to understanding the potential and limitations of LLMs in dealing with geospatial information.},
  keywords={arxiv:2310.13002}
}

@article{singh2023assessinggpt4v,
  title={Assessing GPT4-V on Structured Reasoning Tasks},
  author={Mukul Singh and J. Cambronero and Sumit Gulwani and Vu Le and Gust Verbruggen},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.11524},
  url={https://www.semanticscholar.org/paper/559ec2f23e7b65825b614346bdbabdfd8b56a667},
  abstract={Multi-modality promises to unlock further uses for large language models. Recently, the state-of-the-art language model GPT-4 was enhanced with vision capabilities. We carry out a prompting evaluation of GPT-4V and five other baselines on structured reasoning tasks, such as mathematical reasoning, visual data analysis, and code generation. We show that visual Chain-of-Thought, an extension of Chain-of-Thought to multi-modal LLMs, yields significant improvements over the vanilla model. We also present a categorized analysis of scenarios where these models perform well and where they struggle, highlighting challenges associated with coherent multimodal reasoning.},
  keywords={arxiv:2312.11524}
}

@article{bao2023assessingenhancing,
  title={Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning},
  author={Qiming Bao and Gaël Gendron and A. Peng and Wanjun Zhong and N. Tan and Yang Chen and Michael Witbrock and Jiamou Liu},
  year={2023},
  booktitle={International Conference on Neural Information Processing},
  doi={10.1007/978-981-96-6603-4_22},
  url={https://www.semanticscholar.org/paper/6e5daccbab84481909578ae070507f4887b9808e},
  abstract={Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and GPT-4, have advanced the performance of AI systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness when performing logical reasoning has not been sufficiently assessed. To comprehensively evaluate this ability, we develop three new logical reasoning datasets named"ReClor-plus","LogiQA-plus"and"LogiQAv2-plus"that extend standard logical reasoning datasets to evaluate the robustness of the LLM's reasoning. For each, we create three subsets: the first with randomly shuffled options, the second with the correct choices replaced by"none of the other options is correct", and the third with a combination of shuffling and substitution. Experiments on these datasets show that these simple augmentations greatly hinder the models' performance. Despite their high performance on the original publicly available datasets, we find that all models perform poorly on these newly constructed datasets. We also demonstrate that introducing task variations into the training set can markedly improve the model's performance on both the original and our developed datasets. Finally, we show that applying logic-driven data augmentation for fine-tuning and prompting can enhance generalisation in both discriminative and generative models, offering a path to improving their robustness for tasks involving logical reasoning. Source code and data are made publicly available at https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning.},
  keywords={arxiv:2310.09430}
}

@misc{chen2023assessingimpact,
  title={Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities},
  author={Yuhao Chen and Chloe Wong and Hanwen Yang and Juan Aguenza and Sai Bhujangari and Benthan Vu and Xun Lei and Amisha Prasad and Manny Fluss and Eric Phuong and Minghao Liu and Raja Kumar and Vanshika Vats and James Davis},
  year={2023},
  url={https://www.semanticscholar.org/paper/e02a4282c10c8e7329d4513bd4635635b9d3a50a},
  abstract={This study critically evaluates the efficacy of prompting methods in enhancing the mathematical reasoning capability of large language models (LLMs). The investigation uses three prescriptive prompting methods - simple, persona, and conversational prompting - known for their effectiveness in enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and MMLU datasets, encompassing a broad spectrum of mathematical challenges. A grading script adapted to each dataset is used to determine the effectiveness of these prompting interventions in enhancing the model's mathematical analysis power. Contrary to expectations, our empirical analysis reveals that none of the investigated methods consistently improves over ChatGPT-3.5's baseline performance, with some causing significant degradation. Our findings suggest that prompting strategies do not necessarily generalize to new domains, in this study failing to enhance mathematical performance.},
  keywords={arxiv:2312.15006}
}

@article{chen2023automateddomain,
  title={Automated Domain Modeling with Large Language Models: A Comparative Study},
  author={Kua Chen and Yujing Yang and Boqi Chen and José Antonio Hernández López and Gunter Mussbacher and Dániel Varró},
  year={2023},
  booktitle={ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
  doi={10.1109/MODELS58315.2023.00037},
  url={https://www.semanticscholar.org/paper/b6b49a667dd268782150d2af60c73324a7d78aac},
  abstract={Domain modeling is an essential part of software engineering and serves as a way to represent and understand the concepts and relationships in a problem domain. Typically, software engineers interpret the problem description written in natural language and manually translate it into a domain model. Domain modeling can be time-consuming and highly depends on the expertise of software engineers. Recently, Large Language Models (LLMs) have exhibited remarkable ability in language understanding, generation, and reasoning. In this paper, we conduct a comprehensive, comparative study of using LLMs for fully automated domain modeling. We assess two powerful LLMs, GPT3.5 and GPT4, employing various prompt engineering techniques on a data set containing ten diverse domain modeling examples with reference solutions created by modeling experts. Our findings reveal that while LLMs demonstrate impressive domain understanding capabilities, they are still impractical for full automation, with the top-performing LLM achieving F1 scores of 0.76 for class generation, 0.61 for attribute generation, and 0.34 for relationship generation. Moreover, the F1 score is characterized by higher precision and lower recall; thus, domain elements retrieved by LLMs are often reliable, but there are many missing elements. Furthermore, modeling best practices are rarely followed in auto-generated domain models. Our data set and evaluation provide a valuable baseline for future research in automated LLM-based domain modeling.}
}

@article{zhao2023automaticmodel,
  title={Automatic Model Selection with Large Language Models for Reasoning},
  author={Xu Zhao and Yuxi Xie and Kenji Kawaguchi and Junxian He and Qizhe Xie},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.14333},
  url={https://www.semanticscholar.org/paper/6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3},
  abstract={Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We introduce a model selection method to combine the best of both worlds by employing a large language model (LLM) to dynamically select between them. Our theoretical analysis underscores the feasibility of this method, which is further corroborated by empirical results. Our proposed method demonstrates significant performance improvements across eight reasoning datasets with Codex, ChatGPT, and GPT-4. Additionally, our method is complementary to self-consistency; when integrated, it can further enhance performance while significantly reducing computation costs. Moreover, we achieve new state-of-the-art results on GSM8K and SVAMP, with respective accuracies of 96.8\% and 93.7\%. Our code, data and prompts are available at https://github.com/XuZhao0/Model-Selection-Reasoning},
  keywords={arxiv:2305.14333}
}

@article{pan2023automaticallycorrecting,
  title={Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies},
  author={Liangming Pan and Michael Stephen Saxon and Wenda Xu and Deepak Nathani and Xinyi Wang and William Yang Wang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.03188},
  url={https://www.semanticscholar.org/paper/ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04},
  abstract={Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.},
  keywords={arxiv:2308.03188}
}

@article{jiang2023brainteaserlateral,
  title={BRAINTEASER: Lateral Thinking Puzzles for Large Language Models},
  author={Yifan Jiang and Filip Ilievski and Kaixin Ma},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.05057},
  url={https://www.semanticscholar.org/paper/62ee2d23968b8ee97c958d75ae2b6ae13c84da1a},
  abstract={The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.},
  keywords={arxiv:2310.05057}
}

@article{zhang2023babyscothought,
  title={Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models},
  author={Zheyu Zhang and Han Yang and Bolei Ma and David Rugamer and Ercong Nie},
  year={2023},
  booktitle={Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning},
  doi={10.48550/arXiv.2308.01684},
  url={https://www.semanticscholar.org/paper/7691311f15c9ddcea8eb81e1ad592447fd2fa4ab},
  abstract={Large Language Models (LLMs) demonstrate remarkable performance on a variety of natural language understanding (NLU) tasks, primarily due to their in-context learning ability. This ability could be applied to building babylike models, i.e. models at small scales, improving training efficiency. In this paper, we propose a"CoThought"pipeline, which efficiently trains smaller"baby"language models (BabyLMs) by leveraging the Chain of Thought prompting of LLMs. Our pipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo, transforming it into task-oriented, human-readable texts that are comparable to the school texts for language learners. The BabyLM is then pretrained on this restructured dataset in a RoBERTa fashion. In evaluations across 4 benchmarks, our BabyLM outperforms the vanilla RoBERTa in 10 linguistic, NLU, and question-answering tasks by more than 3 points, showing a superior ability to extract contextual information. These results suggest that compact LMs pretrained on small, LLM-restructured data can better understand tasks and achieve improved performance.},
  keywords={arxiv:2308.01684}
}

@article{cheng2023batchprompting,
  title={Batch Prompting: Efficient Inference with Large Language Model APIs},
  author={Zhoujun Cheng and Jungo Kasai and Tao Yu},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2301.08721},
  url={https://www.semanticscholar.org/paper/27f0ce04403158b61328716ae4aaab5840c0d123},
  abstract={Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly\~{}(up to 5x with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Further analysis shows that the number of samples in each batch and the complexity of tasks affect its performance. Moreover, batch prompting can be applied across different reasoning methods using LLMs. Our code can be found at the site https://github.com/xlang-ai/batch-prompting.},
  keywords={arxiv:2301.08721}
}

@article{chen2023benchmarkinglarge,
  title={Benchmarking large language models for biomedical natural language processing applications and recommendations},
  author={Qingyu Chen and Jingcheng Du and Yan Hu and V. Keloth and Xueqing Peng and Kalpana Raja and Rui Zhang and Zhiyong Lu and Huan Xu},
  year={2023},
  booktitle={Nature Communications},
  doi={10.1038/s41467-025-56989-2},
  url={https://www.semanticscholar.org/paper/ef9b84a57a654888729ce73a3f2400d56e12a300},
  abstract={The rapid growth of biomedical literature poses challenges for manual knowledge curation and synthesis. Biomedical Natural Language Processing (BioNLP) automates the process. While Large Language Models (LLMs) have shown promise in general domains, their effectiveness in BioNLP tasks remains unclear due to limited benchmarks and practical guidelines. We perform a systematic evaluation of four LLMs—GPT and LLaMA representatives—on 12 BioNLP benchmarks across six applications. We compare their zero-shot, few-shot, and fine-tuning performance with the traditional fine-tuning of BERT or BART models. We examine inconsistencies, missing information, hallucinations, and perform cost analysis. Here, we show that traditional fine-tuning outperforms zero- or few-shot LLMs in most tasks. However, closed-source LLMs like GPT-4 excel in reasoning-related tasks such as medical question answering. Open-source LLMs still require fine-tuning to close performance gaps. We find issues like missing information and hallucinations in LLM outputs. These results offer practical insights for applying LLMs in BioNLP. Baseline performance, benchmarks, and guidance for LLMs in biomedicine are limited. The authors assess four LLMs on 12 tasks, establish baselines, examine hallucinations, and provide recommendations for optimal LLM use.},
  keywords={arxiv:2305.16326}
}

@article{lin2023beneathsurface,
  title={Beneath the Surface: Unveiling Harmful Memes with Multimodal Reasoning Distilled from Large Language Models},
  author={Hongzhan Lin and Ziyang Luo and Jing Ma and Long Chen},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.findings-emnlp.611},
  url={https://www.semanticscholar.org/paper/63b1b40781faf00a0d948bba653e0be02895e587},
  abstract={The age of social media is rife with memes. Understanding and detecting harmful memes pose a significant challenge due to their implicit meaning that is not explicitly conveyed through the surface text and image. However, existing harmful meme detection approaches only recognize superficial harm-indicative signals in an end-to-end classification manner but ignore in-depth cognition of the meme text and image. In this paper, we attempt to detect harmful memes based on advanced reasoning over the interplay of multimodal information in memes. Inspired by the success of Large Language Models (LLMs) on complex reasoning, we first conduct abductive reasoning with LLMs. Then we propose a novel generative framework to learn reasonable thoughts from LLMs for better multimodal fusion and lightweight fine-tuning, which consists of two training stages: 1) Distill multimodal reasoning knowledge from LLMs; and 2) Fine-tune the generative framework to infer harmfulness. Extensive experiments conducted on three meme datasets demonstrate that our proposed approach achieves superior performance than state-of-the-art methods on the harmful meme detection task.},
  keywords={arxiv:2312.05434}
}

@article{yao2023beyondchainofthought,
  title={Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models},
  author={Yao Yao and Z. Li and Hai Zhao},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2305.16582},
  url={https://www.semanticscholar.org/paper/adb9acaf9184bdbd23105f1a383848eed9bc82fc}
}

@article{gonzlez2023beyondwords,
  title={Beyond Words: A Mathematical Framework for Interpreting Large Language Models},
  author={Javier González and Aditya V. Nori},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.03033},
  url={https://www.semanticscholar.org/paper/17322c997e993bee1ec05309e5c8d45799a0e4b8},
  abstract={Large language models (LLMs) are powerful AI tools that can generate and comprehend natural language text and other complex information. However, the field lacks a mathematical framework to systematically describe, compare and improve LLMs. We propose Hex a framework that clarifies key terms and concepts in LLM research, such as hallucinations, alignment, self-verification and chain-of-thought reasoning. The Hex framework offers a precise and consistent way to characterize LLMs, identify their strengths and weaknesses, and integrate new findings. Using Hex, we differentiate chain-of-thought reasoning from chain-of-thought prompting and establish the conditions under which they are equivalent. This distinction clarifies the basic assumptions behind chain-of-thought prompting and its implications for methods that use it, such as self-verification and prompt programming. Our goal is to provide a formal framework for LLMs that can help both researchers and practitioners explore new possibilities for generative AI. We do not claim to have a definitive solution, but rather a tool for opening up new research avenues. We argue that our formal definitions and results are crucial for advancing the discussion on how to build generative AI systems that are safe, reliable, fair and robust, especially in domains like healthcare and software engineering.},
  keywords={arxiv:2311.03033}
}

@article{luu2023bioinspiredllmconversational,
  title={BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio‐Inspired Materials},
  author={Rachel K. Luu and M. Buehler},
  year={2023},
  booktitle={Advancement of science},
  doi={10.1002/advs.202306724},
  url={https://www.semanticscholar.org/paper/8db921900955a447d389582143912eee3046fd3e},
  abstract={The study of biological materials and bio‐inspired materials science is well established; however, surprisingly little knowledge is systematically translated to engineering solutions. To accelerate discovery and guide insights, an open‐source autoregressive transformer large language model (LLM), BioinspiredLLM, is reported. The model is finetuned with a corpus of over a thousand peer‐reviewed articles in the field of structural biological and bio‐inspired materials and can be prompted to recall information, assist with research tasks, and function as an engine for creativity. The model has proven that it is able to accurately recall information about biological materials and is further strengthened with enhanced reasoning ability, as well as with Retrieval‐Augmented Generation (RAG) to incorporate new data during generation that can also help to traceback sources, update the knowledge base, and connect knowledge domains. BioinspiredLLM also has shown to develop sound hypotheses regarding biological materials design and remarkably so for materials that have never been explicitly studied before. Lastly, the model shows impressive promise in collaborating with other generative artificial intelligence models in a workflow that can reshape the traditional materials design process. This collaborative generative artificial intelligence method can stimulate and enhance bio‐inspired materials design workflows. Biological materials are at a critical intersection of multiple scientific fields and models like BioinspiredLLM help to connect knowledge domains.},
  keywords={arxiv:2309.08788}
}

@article{pitis2023boostedprompt,
  title={Boosted Prompt Ensembles for Large Language Models},
  author={Silviu Pitis and Michael Ruogu Zhang and Andrew Wang and Jimmy Ba},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2304.05970},
  url={https://www.semanticscholar.org/paper/dca6c3927ade6481a1ae080f5c24decbfeced1be},
  abstract={Methods such as chain-of-thought prompting and self-consistency have pushed the frontier of language model reasoning performance with no additional training. To further improve performance, we propose a prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble''. The few shot examples for each prompt are chosen in a stepwise fashion to be ``hard'' examples on which the previous step's ensemble is uncertain. We show that this outperforms single-prompt output-space ensembles and bagged prompt-space ensembles on the GSM8k and AQuA datasets, among others. We propose both train-time and test-time versions of boosted prompting that use different levels of available annotation and conduct a detailed empirical study of our algorithm.},
  keywords={arxiv:2304.05970}
}

@article{lei2023boostinglogical,
  title={Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought},
  author={Bin Lei and Pei-Hung Lin and C. Liao and Caiwen Ding},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.08614},
  url={https://www.semanticscholar.org/paper/ba4aa83248a1d08b521392eb971e47d10b7c74e1},
  abstract={Recent advancements in large-scale models, such as GPT-4, have showcased remarkable capabilities in addressing standard queries. However, when facing complex problems that require multi-step logical reasoning, their accuracy dramatically decreases. Current research has explored the realm of \textbackslash\{\}textit\{prompting engineering\} to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique, dubbed \textbackslash\{\}textit\{Graph of Thoughts (GoT)\}. Through testing on a trio of escalating challenges: the 24-point game, resolution of high-degree polynomial equations, and derivation of formulas for recursive sequences, our method outperformed GPT-4, achieving accuracy improvements of \$89.7\textbackslash\{\}\%\$, \$86\textbackslash\{\}\%\$, and \$56\textbackslash\{\}\%\$ for each respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA) prompting method, \textbackslash\{\}textit\{Tree of Thought (ToT)\}, our approach registered an average accuracy boost of \$23\textbackslash\{\}\%\$, \$24\textbackslash\{\}\%\$, and \$15\textbackslash\{\}\%\$.},
  keywords={arxiv:2308.08614}
}

@article{moghaddam2023boostingtheoryofmind,
  title={Boosting Theory-of-Mind Performance in Large Language Models via Prompting},
  author={Shima Rahimi Moghaddam and C. Honey},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2304.11490},
  url={https://www.semanticscholar.org/paper/96d6bb5d6abdeda9b2db9af6296527200ba7aa32},
  abstract={Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80\% ToM accuracy, but still fell short of the 87\% human accuracy on the test set. However, when supplied with prompts for in-context learning, all RLHF-trained LLMs exceeded 80\% ToM accuracy, with GPT-4 reaching 100\%. These results demonstrate that appropriate prompting enhances LLM ToM reasoning, and they underscore the context-dependent nature of LLM cognitive capacities.},
  keywords={arxiv:2304.11490}
}

@article{chen2023breakinglanguage,
  title={Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations},
  author={Nuo Chen and Zinan Zheng and Ning Wu and Linjun Shou and Ming Gong and Yangqiu Song and Dongmei Zhang and Jia Li},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.20246},
  url={https://www.semanticscholar.org/paper/07cdf957a11506f87fbc030dcfaaa6399847648c},
  abstract={Existing research predominantly focuses on developing powerful language learning models (LLMs) for mathematical reasoning within monolingual languages, with few explorations in preserving efficacy in a multilingual context. To bridge this gap, this paper pioneers exploring and training powerful Multilingual Math Reasoning (xMR) LLMs. Firstly, by utilizing translation, we construct the first multilingual math reasoning instruction dataset, MGSM8KInstruct, encompassing ten distinct languages, thus addressing the issue of training data scarcity in xMR tasks. Based on the collected dataset, we propose different training strategies to build powerful xMR LLMs, named MathOctopus, notably outperform conventional open-source LLMs and exhibit superiority over ChatGPT in few-shot scenarios. Notably, MathOctopus-13B reaches 47.6\% accuracy which exceeds ChatGPT 46.3\% on MGSM testset. Beyond remarkable results, we unearth several pivotal observations and insights from extensive experiments: (1) When extending the rejection sampling strategy to the multilingual context, it proves effective for model performances, albeit limited. (2) Employing parallel corpora for math Supervised Fine-Tuning (SFT) across multiple languages not only significantly enhances model performance multilingually but also elevates their monolingual performance. This indicates that crafting multilingual corpora can be regarded as a vital strategy for enhancing model performance in a specific language, especially in mathematical reasoning tasks. For instance, MathOctopus-7B improves its counterparts that trained on English from 42.2\% to 50.8\% on GSM8K testset.},
  keywords={arxiv:2310.20246}
}

@article{holmstrm2023bridgingresource,
  title={Bridging the Resource Gap: Exploring the Efficacy of English and Multilingual LLMs for Swedish},
  author={Oskar Holmström and Jenny Kunz and Marco Kuhlmann},
  year={2023},
  booktitle={RESOURCEFUL},
  url={https://www.semanticscholar.org/paper/322c9e4ef9dc7724935ba818c0ff38d2c3d11483}
}

@article{zhang2023buildingcooperative,
  title={Building Cooperative Embodied Agents Modularly with Large Language Models},
  author={Hongxin Zhang and Weihua Du and Jiaming Shan and Qinhong Zhou and Yilun Du and J. Tenenbaum and Tianmin Shu and Chuang Gan},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2307.02485},
  url={https://www.semanticscholar.org/paper/587352c3b95c90de6d37f061c8e117f42be0b575},
  abstract={In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.},
  keywords={arxiv:2307.02485}
}

@article{chan2023clairevaluating,
  title={CLAIR: Evaluating Image Captions with Large Language Models},
  author={David Chan and Suzanne Petryk and Joseph Gonzalez and Trevor Darrell and John F. Canny},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.12971},
  url={https://www.semanticscholar.org/paper/da4deaf81232d94e2f38a9d23c6b04ae1d79fbfc},
  abstract={The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6\% and over image-augmented methods such as RefCLIP-S of 18.3\%. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the underlying reasoning behind its assigned score. Code is available at https://davidmchan.github.io/clair/},
  keywords={arxiv:2310.12971}
}

@article{huang2023clomocounterfactual,
  title={CLOMO: Counterfactual Logical Modification with Large Language Models},
  author={Yinya Huang and Ruixin Hong and Hongming Zhang and Wei Shao and Zhicheng YANG and Dong Yu and Changshui Zhang and Xiaodan Liang and Linqi Song},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2311.17438},
  url={https://www.semanticscholar.org/paper/dab4f70d75a04e62553e583f2450d9bb1f0ead46},
  abstract={In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the decomposed Self-Evaluation Score (SES) to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance. Code and data are available at https://github.com/Eleanor-H/CLOMO.},
  keywords={arxiv:2311.17438}
}

@article{jin2023cladderbenchmark,
  title={CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models},
  author={Zhijing Jin and Yuen Chen and Felix Leeb and Luigi Gresele and Ojasv Kamal and Zhiheng Lyu and Kevin Blin and Fernando Gonzalez Adauto and Max Kleiman-Weiner and Mrinmaya Sachan and Bernhard Schölkopf},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2312.04350},
  url={https://www.semanticscholar.org/paper/f30b720e34d405f200270a6ef2d09e98585fb4d1}
}

@misc{jin2023cladderassessing,
  title={CLadder: Assessing Causal Reasoning in Language Models},
  author={Zhijing Jin and Yuen Chen and Felix Leeb and Luigi Gresele and Ojasv Kamal and Zhiheng Lyu and Kevin Blin and Fernando Gonzalez Adauto and Max Kleiman-Weiner and Mrinmaya Sachan and Bernhard Scholkopf},
  year={2023},
  url={https://www.semanticscholar.org/paper/6ac627f57b26354ab537734d820da4a6a7dde2c6},
  abstract={The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the"causal inference engine"postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insights into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.},
  keywords={arxiv:2312.04350}
}

@article{wei2023cmathyour,
  title={CMATH: Can Your Language Model Pass Chinese Elementary School Math Test?},
  author={Tianwen Wei and Jian Luan and W. Liu and Shuang Dong and B. Wang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2306.16636},
  url={https://www.semanticscholar.org/paper/5efec343015f9329c5cd56e2259f68f03c2ef8b5},
  abstract={We present the Chinese Elementary School Math Word Problems (CMATH) dataset, comprising 1.7k elementary school-level math word problems with detailed annotations, source from actual Chinese workbooks and exams. This dataset aims to provide a benchmark tool for assessing the following question: to what grade level of elementary school math do the abilities of popular large language models (LLMs) correspond? We evaluate a variety of popular LLMs, including both commercial and open-source options, and discover that only GPT-4 achieves success (accuracy \$\textbackslash\{\}geq\$ 60\textbackslash\{\}\%) across all six elementary school grades, while other models falter at different grade levels. Furthermore, we assess the robustness of several top-performing LLMs by augmenting the original problems in the CMATH dataset with distracting information. Our findings reveal that GPT-4 is able to maintains robustness, while other model fail. We anticipate that our study will expose limitations in LLMs' arithmetic and reasoning capabilities, and promote their ongoing development and advancement.},
  keywords={arxiv:2306.16636}
}

@article{qian2023creatordisentangling,
  title={CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation},
  author={Cheng Qian and Chi Han and Y. Fung and Yujia Qin and Zhiyuan Liu and Heng Ji},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2305.14318},
  url={https://www.semanticscholar.org/paper/ba704774f194938b04b1e2be40b1d111a4ca08e1}
}

@article{qian2023creatortool,
  title={CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models},
  author={Cheng Qian and Chi Han and Y. Fung and Yujia Qin and Zhiyuan Liu and Heng Ji},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.findings-emnlp.462},
  url={https://www.semanticscholar.org/paper/8da9b1436212b233fc49c7daf1ba15c22874ff5a},
  abstract={Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance. We evaluate CREATOR on MATH and TabMWP benchmarks, respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably, CREATOR outperforms existing chain-of-thought, program-of-thought, and tool-using baselines. Additionally, we introduce the Creation Challenge dataset, featuring 2K diverse questions, to emphasize the necessity and benefits of LLMs' tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to adapt to diverse situations. The tool creation ability revolutionizes the LLM's problem-solving paradigm, driving us closer to the next frontier of artificial intelligence. All the codes and data are released.},
  keywords={arxiv:2305.14318}
}

@article{gou2023criticlarge,
  title={CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing},
  author={Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Nan Duan and Weizhu Chen},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2305.11738},
  url={https://www.semanticscholar.org/paper/bcdaf6c98ddbd6809cf6241aa77200d7394db163},
  abstract={Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially"black boxes"to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.},
  keywords={arxiv:2305.11738}
}

@article{xu2023cvaluesmeasuring,
  title={CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility},
  author={Guohai Xu and Jiayi Liu and Mingshi Yan and Haotian Xu and Jinghui Si and Zhuoran Zhou and Peng Yi and Xing Gao and Jitao Sang and Rong Zhang and Ji Zhang and Chao Peng and Feiyan Huang and Jingren Zhou},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2307.09705},
  url={https://www.semanticscholar.org/paper/bc0549a5f07474c18987c219ecf367fb73a1b79c},
  abstract={With the rapid evolution of large language models (LLMs), there is a growing concern that they may pose risks or have negative social impacts. Therefore, evaluation of human values alignment is becoming increasingly important. Previous work mainly focuses on assessing the performance of LLMs on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a Chinese context. In this paper, we present CValues, the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria. As a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts. To provide a comprehensive values evaluation of Chinese LLMs, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation. Our findings suggest that while most Chinese LLMs perform well in terms of safety, there is considerable room for improvement in terms of responsibility. Moreover, both the automatic and human evaluation are important for assessing the human values alignment in different aspects. The benchmark and code is available on ModelScope and Github.},
  keywords={arxiv:2307.09705}
}

@article{lopezlira2023chatgptforecast,
  title={Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models},
  author={Alejandro Lopez-Lira and Yuehua Tang},
  year={2023},
  booktitle={Social Science Research Network},
  doi={10.2139/ssrn.4412788},
  url={https://www.semanticscholar.org/paper/d26c55bee1ac6856a20862b0f7b4ff38fa39af50},
  abstract={We document the capability of large language models (LLMs) like ChatGPT to predict stock market reactions from news headlines without direct financial training. Using post-knowledge-cutoff headlines, GPT-4 captures initial market responses, achieving approximately 90\% portfolio-day hit rates for the non-tradable initial reaction. GPT-4 scores also significantly predict the subsequent drift, especially for small stocks and negative news. Forecasting ability generally increases with model size, suggesting that financial reasoning is an emerging capacity of complex LLMs. Strategy returns decline as LLM adoption rises, consistent with improved price efficiency. To rationalize these findings, we develop a theoretical model that incorporates LLM technology, information-processing capacity constraints, underreaction, and limits to arbitrage.},
  keywords={arxiv:2304.07619}
}

@article{valmeekam2023largelanguage,
  title={Can Large Language Models Really Improve by Self-critiquing Their Own Plans?},
  author={Karthik Valmeekam and Matthew Marquez and Subbarao Kambhampati},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.08118},
  url={https://www.semanticscholar.org/paper/e879f54b2b5760bbb6d010977ddcedfb62452b38},
  abstract={There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.},
  keywords={arxiv:2310.08118}
}

@article{kcman2023causalreasoning,
  title={Causal Reasoning and Large Language Models: Opening a New Frontier for Causality},
  author={Emre Kıcıman and R. Ness and Amit Sharma and Chenhao Tan},
  year={2023},
  booktitle={Trans. Mach. Learn. Res.},
  doi={10.48550/arXiv.2305.00050},
  url={https://www.semanticscholar.org/paper/10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3},
  abstract={The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a"behavorial"study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97\%, 13 points gain), counterfactual reasoning task (92\%, 20 points gain) and event causality (86\% accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date. That said, LLMs exhibit unpredictable failure modes, and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM-based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py-why/pywhy-llm.},
  keywords={arxiv:2305.00050}
}

@article{ban2023causalstructure,
  title={Causal Structure Learning Supervised by Large Language Model},
  author={Taiyu Ban and Lyuzhou Chen and Derui Lyu and Xiangyu Wang and Huanhuan Chen},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.11689},
  url={https://www.semanticscholar.org/paper/9e557a199972b963d8ac064ac6e625c115c03cde},
  abstract={Causal discovery from observational data is pivotal for deciphering complex relationships. Causal Structure Learning (CSL), which focuses on deriving causal Directed Acyclic Graphs (DAGs) from data, faces challenges due to vast DAG spaces and data sparsity. The integration of Large Language Models (LLMs), recognized for their causal reasoning capabilities, offers a promising direction to enhance CSL by infusing it with knowledge-based causal inferences. However, existing approaches utilizing LLMs for CSL have encountered issues, including unreliable constraints from imperfect LLM inferences and the computational intensity of full pairwise variable analyses. In response, we introduce the Iterative LLM Supervised CSL (ILS-CSL) framework. ILS-CSL innovatively integrates LLM-based causal inference with CSL in an iterative process, refining the causal DAG using feedback from LLMs. This method not only utilizes LLM resources more efficiently but also generates more robust and high-quality structural constraints compared to previous methodologies. Our comprehensive evaluation across eight real-world datasets demonstrates ILS-CSL's superior performance, setting a new standard in CSL efficacy and showcasing its potential to significantly advance the field of causal discovery. The codes are available at \textbackslash\{\}url\{https://github.com/tyMadara/ILS-CSL\}.},
  keywords={arxiv:2311.11689}
}

@article{lee2023chainempathy,
  title={Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models},
  author={Y. Lee and Inju Lee and Minjung Shin and Seoyeon Bae and Sowon Hahn},
  year={2023},
  booktitle={arXiv.org},
  doi={10.19066/cogsci.2024.35.1.002},
  url={https://www.semanticscholar.org/paper/3f49bd6b95e254d82ce1ed1bd556a7a8f81a47db},
  abstract={We present a novel method, the Chain of Empathy (CoE) prompting, that utilizes insights from psychotherapy to induce Large Language Models (LLMs) to reason about human emotional states. This method is inspired by various psychotherapy approaches including Cognitive Behavioral Therapy (CBT), Dialectical Behavior Therapy (DBT), Person Centered Therapy (PCT), and Reality Therapy (RT), each leading to different patterns of interpreting clients' mental states. LLMs without reasoning generated predominantly exploratory responses. However, when LLMs used CoE reasoning, we found a more comprehensive range of empathetic responses aligned with the different reasoning patterns of each psychotherapy model. The CBT based CoE resulted in the most balanced generation of empathetic responses. The findings underscore the importance of understanding the emotional context and how it affects human and AI communication. Our research contributes to understanding how psychotherapeutic models can be incorporated into LLMs, facilitating the development of context-specific, safer, and empathetic AI.},
  keywords={arxiv:2311.04915}
}

@article{li2023chainofknowledgegrounding,
  title={Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources},
  author={Xingxuan Li and Ruochen Zhao and Yew Ken Chia and Bosheng Ding and Shafiq R. Joty and Soujanya Poria and Lidong Bing},
  year={2023},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/e468ed6b824e60f45ba9a20b034e4090c6630751},
  abstract={We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.},
  keywords={arxiv:2305.13269}
}

@article{fu2023chainofthoughtcontinuous,
  title={Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance},
  author={Yao Fu and Litu Ou and Mingyu Chen and Yuhao Wan and Hao-Chun Peng and Tushar Khot},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2305.17306},
  url={https://www.semanticscholar.org/paper/ea75117f34b168a20f2a4309ac2eb685ca6b1436},
  abstract={As large language models (LLMs) are continuously being developed, their evaluation becomes increasingly important yet challenging. This work proposes Chain-of-Thought Hub, an open-source evaluation suite on the multi-step reasoning capabilities of large language models. We are interested in this setting for two reasons: (1) from the behavior of GPT and PaLM model family, we observe that complex reasoning is likely to be a key differentiator between weaker and stronger LLMs; (2) we envisage large language models to become the next-generation computational platform and foster an ecosystem of LLM-based new applications, this naturally requires the foundation models to perform complex tasks that often involve the composition of linguistic and logical operations. Our approach is to compile a suite of challenging reasoning benchmarks to track the progress of LLMs. Our current results show that: (1) model scale clearly correlates with reasoning capabilities; (2) As of May 2023, Claude-v1.3 and PaLM-2 are the only two models that are comparable with GPT-4, while open-sourced models still lag behind; (3) LLaMA-65B performs closely to code-davinci-002, indicating that with successful further development such as reinforcement learning from human feedback (RLHF), it has great potential to be close to GPT-3.5-Turbo. Our results also suggest that for the open-source efforts to catch up, the community may focus more on building better base models and exploring RLHF.},
  keywords={arxiv:2305.17306}
}

@article{lu2023chameleonplugandplay,
  title={Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models},
  author={Pan Lu and Baolin Peng and Hao Cheng and Michel Galley and Kai-Wei Chang and Y. Wu and Song-Chun Zhu and Jianfeng Gao},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2304.09842},
  url={https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e},
  abstract={Large language models (LLMs) have achieved remarkable progress in solving various natural language processing tasks due to emergent reasoning abilities. However, LLMs have inherent limitations as they are incapable of accessing up-to-date information (stored on the Web or in task-specific knowledge bases), using external tools, and performing precise mathematical and logical reasoning. In this paper, we present Chameleon, an AI system that mitigates these limitations by augmenting LLMs with plug-and-play modules for compositional reasoning. Chameleon synthesizes programs by composing various tools (e.g., LLMs, off-the-shelf vision models, web search engines, Python functions, and heuristic-based modules) for accomplishing complex reasoning tasks. At the heart of Chameleon is an LLM-based planner that assembles a sequence of tools to execute to generate the final response. We showcase the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54\% overall accuracy on ScienceQA, improving the best published few-shot result by 11.37\%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0\%, lifting the state of the art to 98.78\%. Our analysis also shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT-powered planner. The project is available at https://chameleon-llm.github.io.},
  keywords={arxiv:2304.09842}
}

@article{zhao2023chatwith,
  title={Chat with the Environment: Interactive Multimodal Perception Using Large Language Models},
  author={Xufeng Zhao and Mengdi Li and C. Weber and Muhammad Burhan Hafez and Stefan Wermter},
  year={2023},
  booktitle={IEEE/RJS International Conference on Intelligent RObots and Systems},
  doi={10.1109/IROS55552.2023.10342363},
  url={https://www.semanticscholar.org/paper/00a48c76e123ab77f301bf4dfd88b9b376b234c6},
  abstract={Programming robot behavior in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning. Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold. We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly. An interactive perception framework is therefore proposed with an LLM as its backbone, whose ability is exploited to instruct epistemic actions and to reason over the resulting multimodal sensations (vision, sound, haptics, proprioception), as well as to plan an entire task execution based on the interactively acquired information. Our study demonstrates that LLMs can provide high-level planning and reasoning skills and control interactive robot behavior in a multimodal environment, while multimodal modules with the context of the environmental state help ground the LLMs and extend their processing ability. The project website can be found at https://matcha-model.github.io/.},
  keywords={arxiv:2303.08268}
}

@article{wang2023chat3ddataefficiently,
  title={Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes},
  author={Zehan Wang and Haifeng Huang and Yang Zhao and Ziang Zhang and Zhou Zhao},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.08769},
  url={https://www.semanticscholar.org/paper/30cfc4e7174211aa48c965826d51db773f0d37c7},
  abstract={3D scene understanding has gained significant attention due to its wide range of applications. However, existing methods for 3D scene understanding are limited to specific downstream tasks, which hinders their practicality in real-world applications. This paper presents Chat-3D, which combines the 3D visual perceptual ability of pre-trained 3D representations and the impressive reasoning and conversation capabilities of advanced LLMs to achieve the first universal dialogue systems for 3D scenes. Specifically, we align 3D representations into the feature space of LLMs, thus enabling LLMs to perceive the 3D world. Given the scarcity of 3D scene-text data, we propose a three-stage training strategy to efficiently utilize the available data for better alignment. To enhance the reasoning ability and develop a user-friendly interaction scheme, we further construct a high-quality object-centric 3D instruction dataset and design an associated object-centric prompt. Our experiments show that Chat-3D achieves an impressive ability to comprehend diverse instructions for 3D scenes, engage in intricate spatial reasoning, and incorporate external knowledge into its responses. Chat-3D achieves a 75.6\% relative score compared with GPT-4 on the constructed instruction dataset.},
  keywords={arxiv:2308.08769}
}

@article{chen2023chatcottoolaugmented,
  title={ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models},
  author={Z. Chen and Kun Zhou and Beichen Zhang and Zheng Gong and Wayne Xin Zhao and Ji-rong Wen},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.14323},
  url={https://www.semanticscholar.org/paper/95ca67ba607d7859ee8eec457f4b59b115d69bf5},
  abstract={Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose \textbackslash\{\}textbf\{ChatCoT\}, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model the chain-of-thought\~{}(CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the tools, tasks and reasoning format, and propose an iterative \textbackslash\{\}emph\{tool-augmented reasoning\} step to perform step-by-step tool-augmented reasoning. The experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 6.8\textbackslash\{\}\% relative improvement over the state-of-the-art baseline. Our code and data are available at: \textbackslash\{\}url\{https://github.com/RUCAIBOX/ChatCoT\}.},
  keywords={arxiv:2305.14323}
}

@article{huang2023chatgptshaping,
  title={ChatGPT for shaping the future of dentistry: the potential of multi-modal large language model},
  author={Hanyao Huang and Ou Zheng and Dongdong Wang and Jiayi Yin and Zijin Wang and Shengxuan Ding and H. Yin and Chuan Xu and Renjie Yang and Q. Zheng and B. Shi},
  year={2023},
  journal={International Journal of Oral Science},
  doi={10.1038/s41368-023-00239-y},
  url={https://www.semanticscholar.org/paper/256979852e0e0a5fe5cc8ddbf54fa1af2a843722},
  abstract={The ChatGPT, a lite and conversational variant of Generative Pretrained Transformer 4 (GPT-4) developed by OpenAI, is one of the milestone Large Language Models (LLMs) with billions of parameters. LLMs have stirred up much interest among researchers and practitioners in their impressive skills in natural language processing tasks, which profoundly impact various fields. This paper mainly discusses the future applications of LLMs in dentistry. We introduce two primary LLM deployment methods in dentistry, including automated dental diagnosis and cross-modal dental diagnosis, and examine their potential applications. Especially, equipped with a cross-modal encoder, a single LLM can manage multi-source data and conduct advanced natural language reasoning to perform complex clinical operations. We also present cases to demonstrate the potential of a fully automatic Multi-Modal LLM AI system for dentistry clinical application. While LLMs offer significant potential benefits, the challenges, such as data privacy, data quality, and model bias, need further study. Overall, LLMs have the potential to revolutionize dental diagnosis and treatment, which indicates a promising avenue for clinical application and research in dentistry.},
  keywords={arxiv:2304.03086}
}

@article{luo2023chatrulemining,
  title={ChatRule: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning},
  author={Linhao Luo and Jiaxin Ju and Bo Xiong and Yuan-Fang Li and Gholamreza Haffari and Shirui Pan},
  year={2023},
  booktitle={Pacific-Asia Conference on Knowledge Discovery and Data Mining},
  doi={10.48550/arXiv.2309.01538},
  url={https://www.semanticscholar.org/paper/18664b47516ba5424ba5efa79d3f816224245325},
  abstract={Logical rules are essential for uncovering the logical connections between relations, which could improve reasoning performance and provide interpretable results on knowledge graphs (KGs). Although there have been many efforts to mine meaningful logical rules over KGs, existing methods suffer from computationally intensive searches over the rule space and a lack of scalability for large-scale KGs. Besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models (LLMs) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, ChatRule, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an LLM-based rule generator, leveraging both the semantic and structural information of KGs to prompt LLMs to generate logical rules. To refine the generated rules, a rule ranking module estimates the rule quality by incorporating facts from existing KGs. Last, the ranked rules can be used to conduct reasoning over KGs. ChatRule is evaluated on four large-scale KGs, w.r.t. different rule quality metrics and downstream tasks, showing the effectiveness and scalability of our method.},
  keywords={arxiv:2309.01538}
}

@article{shapira2023cleverhans,
  title={Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models},
  author={Natalie Shapira and Mosh Levy and S. Alavi and Xuhui Zhou and Yejin Choi and Yoav Goldberg and Maarten Sap and Vered Shwartz},
  year={2023},
  booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2305.14763},
  url={https://www.semanticscholar.org/paper/ddcd2bcc809bd0c2755a4a9487473d61ac327c50},
  abstract={The escalating debate on AI’s capabilities warrants developing reliable metrics to assess machine “intelligence.” Recently, many anecdotal examples were used to suggest that newer Large Language Models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs’ N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.},
  keywords={arxiv:2305.14763}
}

@article{ngel2023clinicalknowledge,
  title={Clinical Knowledge and Reasoning Abilities of AI Large Language Models in Anesthesiology: A Comparative Study on the ABA Exam},
  author={M. Ángel and J. Rinehart and M. Canneson and P. Baldi},
  year={2023},
  booktitle={medRxiv},
  doi={10.1101/2023.05.10.23289805},
  url={https://www.semanticscholar.org/paper/2891066160465c9b1c792a8294911f6facf9842c}
}

@article{wang2023clinicalgptlarge,
  title={ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation},
  author={Guangyu Wang and Guoxing Yang and Zongxin Du and Longjun Fan and Xiaohu Li},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2306.09968},
  url={https://www.semanticscholar.org/paper/ebc502a4d173f6550a8cd6384cb06f2c43c7c1a3},
  abstract={Large language models have exhibited exceptional performance on various Natural Language Processing (NLP) tasks, leveraging techniques such as the pre-training, and instruction fine-tuning. Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience. In this study, we present ClinicalGPT, a language model explicitly designed and optimized for clinical scenarios. By incorporating extensive and diverse real-world data, such as medical records, domain-specific knowledge, and multi-round dialogue consultations in the training process, ClinicalGPT is better prepared to handle multiple clinical task. Furthermore, we introduce a comprehensive evaluation framework that includes medical knowledge question-answering, medical exams, patient consultations, and diagnostic analysis of medical records. Our results demonstrate that ClinicalGPT significantly outperforms other models in these tasks, highlighting the effectiveness of our approach in adapting large language models to the critical domain of healthcare.},
  keywords={arxiv:2306.09968}
}

@article{wang2023clouddevicecollaborative,
  title={Cloud-Device Collaborative Learning for Multimodal Large Language Models},
  author={Guanqun Wang and Jiaming Liu and Chenxuan Li and Junpeng Ma and Yuan Zhang and Xinyu Wei and Kevin Zhang and Maurice Chong and Ray Zhang and Yijiang Liu and Shanghang Zhang},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.01202},
  url={https://www.semanticscholar.org/paper/5acbf917da5be89e4eebd7e98c81d87069450a5d},
  abstract={The burgeoning field of Multimodal Large Language Models (MLLMs) has exhibited remarkable performance in diverse tasks such as captioning, commonsense reasoning, and visual scene understanding. However, the deployment of these large-scale MLLMs on client devices is hindered by their extensive model parameters, leading to a notable de-cline in generalization capabilities when these models are compressed for device deployment. Addressing this chal-lenge, we introduce a Cloud-Device Collaborative Contin-ual Adaptation framework, designed to enhance the performance of compressed, device-deployed MLLMs by lever-aging the robust capabilities of cloud-based, larger-scale MLLMs. Our framework is structured into three key components: a device-to-cloud uplink for efficient data transmission, cloud-based knowledge adaptation, and an optimized cloud-to-device downlink for model deployment. In the up-link phase, we employ an Uncertainty-guided Token Sam-pling (UTS) strategy to effectively filter out-of-distribution tokens, thereby reducing transmission costs and improving training efficiency. On the cloud side, we propose Adapter-based Knowledge Distillation (AKD) method to transfer refined knowledge from large-scale to compressed, pocket-size MLLMs. Furthermore, we propose a Dynamic Weight update Compression (DWC) strategy for the down-link, which adaptively selects and quantizes updated weight parameters, enhancing transmission efficiency and reducing the representational disparity between cloud and de-vice models. Extensive experiments on several multimodal benchmarks demonstrate the superiority of our proposed framework over prior Knowledge Distillation and device-cloud collaboration methods. Notably, we also validate the feasibility of our approach to real-world experiments.},
  keywords={arxiv:2312.16279}
}

@article{joublin2023copalcorrective,
  title={CoPAL: Corrective Planning of Robot Actions with Large Language Models},
  author={F. Joublin and Antonello Ceravola and Pavel Smirnov and Felix Ocker and Joerg Deigmoeller and Anna Belardinelli and Chao Wang and Stephan Hasler and Daniel Tanneberg and M. Gienger},
  year={2023},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA57147.2024.10610434},
  url={https://www.semanticscholar.org/paper/40df8b43583aceaa48e7e4bd789071e2c7cc0b1b},
  abstract={In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation.},
  keywords={arxiv:2310.07263}
}

@article{hu2023codeprompting,
  title={Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models},
  author={Y. Hu and Haotong Yang and Zhouchen Lin and Muhan Zhang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2305.18507},
  url={https://www.semanticscholar.org/paper/0875651b68e6602d45ae08bee67cf63c02faa512},
  abstract={Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show through experiments how code annotations and their locations affect code prompting.},
  keywords={arxiv:2305.18507}
}

@article{sonkar2023codesoliloquies,
  title={Code Soliloquies for Accurate Calculations in Large Language Models},
  author={Shashank Sonkar and Myco Le and Xinghe Chen and Naiming Liu and D. B. Mallick and Richard Baraniuk},
  year={2023},
  booktitle={International Conference on Learning Analytics and Knowledge},
  doi={10.1145/3636555.3636889},
  url={https://www.semanticscholar.org/paper/5558226fa14f7467d25690b97febdf0e8299e432},
  abstract={High-quality conversational datasets are crucial for the successful development of Intelligent Tutoring Systems (ITS) that utilize a Large Language Model (LLM) backend. Synthetic student-teacher dialogues, generated using advanced GPT-4 models, are a common strategy for creating these datasets. However, subjects like physics that entail complex calculations pose a challenge. While GPT-4 presents impressive language processing capabilities, its limitations in fundamental mathematical reasoning curtail its efficacy for such subjects. To tackle this limitation, we introduce in this paper an innovative stateful prompt design. Our design orchestrates a mock conversation where both student and tutorbot roles are simulated by GPT-4. Each student response triggers an internal monologue, or ‘code soliloquy’ in the GPT-tutorbot, which assesses whether its subsequent response would necessitate calculations. If a calculation is deemed necessary, it scripts the relevant Python code and uses the Python output to construct a response to the student. Our approach notably enhances the quality of synthetic conversation datasets, especially for subjects that are calculation-intensive. The preliminary Subject Matter Expert evaluations reveal that our Higgs model, a fine-tuned LLaMA model, effectively uses Python for computations, which significantly enhances the accuracy and computational reliability of Higgs’ responses.},
  keywords={arxiv:2309.12161}
}

@article{bi2023codekgccode,
  title={CodeKGC: Code Language Model for Generative Knowledge Graph Construction},
  author={Zhen Bi and Jing Chen and Yinuo Jiang and Feiyu Xiong and Wei Guo and Huajun Chen and Ningyu Zhang},
  year={2023},
  booktitle={ACM Trans. Asian Low Resour. Lang. Inf. Process.},
  doi={10.1145/3641850},
  url={https://www.semanticscholar.org/paper/c4e4b72da211dbf2ab2fd5263d453cf22ee0cf44},
  abstract={Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provide intermediate steps, thereby improving knowledge extraction abilities. Experimental results indicate that the proposed approach can obtain better performance on benchmark datasets compared with baselines.1},
  keywords={arxiv:2304.09048}
}

@article{xu2023cognitiveoverload,
  title={Cognitive Overload: Jailbreaking Large Language Models with Overloaded Logical Thinking},
  author={Nan Xu and Fei Wang and Ben Zhou and Bangzheng Li and Chaowei Xiao and Muhao Chen},
  year={2023},
  booktitle={NAACL-HLT},
  doi={10.48550/arXiv.2311.09827},
  url={https://www.semanticscholar.org/paper/54c9a97637822c9e1956b1ec70b0c9a0f2338d2c},
  abstract={While large language models (LLMs) have demonstrated increasing power, they have also given rise to a wide range of harmful behaviors. As representatives, jailbreak attacks can provoke harmful or unethical responses from LLMs, even after safety alignment. In this paper, we investigate a novel category of jailbreak attacks specifically designed to target the cognitive structure and processes of LLMs. Specifically, we analyze the safety vulnerability of LLMs in the face of (1) multilingual cognitive overload, (2) veiled expression, and (3) effect-to-cause reasoning. Different from previous jailbreak attacks, our proposed cognitive overload is a black-box attack with no need for knowledge of model architecture or access to model weights. Experiments conducted on AdvBench and MasterKey reveal that various LLMs, including both popular open-source model Llama 2 and the proprietary model ChatGPT, can be compromised through cognitive overload. Motivated by cognitive psychology work on managing cognitive load, we further investigate defending cognitive overload attack from two perspectives. Empirical studies show that our cognitive overload from three perspectives can jailbreak all studied LLMs successfully, while existing defense strategies can hardly mitigate the caused malicious uses effectively.},
  keywords={arxiv:2311.09827}
}

@article{dasgupta2023collaboratingwith,
  title={Collaborating with language models for embodied reasoning},
  author={Ishita Dasgupta and Christine Kaeser-Chen and Kenneth Marino and Arun Ahuja and Sheila Babayan and Felix Hill and R. Fergus},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2302.00763},
  url={https://www.semanticscholar.org/paper/102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f},
  abstract={Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.},
  keywords={arxiv:2302.00763}
}

@article{leinonen2023comparingcode,
  title={Comparing Code Explanations Created by Students and Large Language Models},
  author={Juho Leinonen and Paul Denny and Stephen MacNeil and Sami Sarsa and Seth Bernstein and Joanne Kim and Andrew Tran and Arto Hellas},
  year={2023},
  booktitle={Annual Conference on Innovation and Technology in Computer Science Education},
  doi={10.1145/3587102.3588785},
  url={https://www.semanticscholar.org/paper/326ff0157f655598ca9cb21d1e703654ccbde033},
  abstract={Reasoning about code and explaining its purpose are fundamental skills for computer scientists. There has been extensive research in the field of computing education on the relationship between a student's ability to explain code and other skills such as writing and tracing code. In particular, the ability to describe at a high-level of abstraction how code will behave over all possible inputs correlates strongly with code writing skills. However, developing the expertise to comprehend and explain code accurately and succinctly is a challenge for many students. Existing pedagogical approaches that scaffold the ability to explain code, such as producing exemplar code explanations on demand, do not currently scale well to large classrooms. The recent emergence of powerful large language models (LLMs) may offer a solution. In this paper, we explore the potential of LLMs in generating explanations that can serve as examples to scaffold students' ability to understand and explain code. To evaluate LLM-created explanations, we compare them with explanations created by students in a large course (n ≈ 1000) with respect to accuracy, understandability and length. We find that LLM-created explanations, which can be produced automatically on demand, are rated as being significantly easier to understand and more accurate summaries of code than student-created explanations. We discuss the significance of this finding, and suggest how such models can be incorporated into introductory programming education.},
  keywords={arxiv:2304.03938}
}

@article{choudhary2023complexlogical,
  title={Complex Logical Reasoning over Knowledge Graphs using Large Language Models},
  author={Nurendra Choudhary and Chandan K. Reddy},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2305.01157},
  url={https://www.semanticscholar.org/paper/5f9f6b462759b56c242459b7e976b8858b141eeb},
  abstract={Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show that the performance of our approach improves proportionally to the increase in size of the underlying LLM, enabling the integration of the latest advancements in LLMs for logical reasoning over KGs. Our work presents a new direction for addressing the challenges of complex KG reasoning and paves the way for future research in this area.},
  keywords={arxiv:2305.01157}
}

@article{liu2023conciseorganized,
  title={Concise and Organized Perception Facilitates Reasoning in Large Language Models},
  author={Junjie Liu and Shaotian Yan and Chen Shen and Zhengdong Xiao and Liang Xie and Wenxiao Wang and Jieping Ye},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.18653/v1/2025.findings-naacl.193},
  url={https://www.semanticscholar.org/paper/c200be7be643444f3c7692e03755ee83dbe948c0},
  abstract={Exploiting large language models (LLMs) to tackle reasoning has garnered growing attention. It still remains highly challenging to achieve satisfactory results in complex logical problems, characterized by plenty of premises within the context and requiring multi-hop reasoning. In particular, the reasoning capabilities of LLMs are brittle to disorder and distractibility. In this work, we first examine the mechanism from the perspective of information flow and reveal that LLMs confront difficulties akin to human-like cognitive biases when dealing with disordered and irrelevant content in reasoning tasks. However, in contrast to LLMs, disordered and irrelevant content does not significantly decrease human performance, as humans have a propensity to distill the most relevant information and systematically organize their thoughts, aiding them in responding to questions.Stem from that, we further propose a novel reasoning approach named Concise and Organized Perception (COP). COP carefully analyzes the given statements to identify the most pertinent information while eliminating redundancy efficiently. It then prompts the LLMs in a more organized form that adapts to the model's inference process. By perceiving concise and organized context, the reasoning abilities of LLMs can be better elicited. Extensive experimental results on several popular logical benchmarks (ProofWriter, PrOntoQA, PrOntoQA-OOD, and FOLIO) and mathematical benchmark (DI-GSM) show that COP significantly outperforms previous state-of-the-art methods.},
  keywords={arxiv:2310.03309}
}

@article{wu2023conic10kchallenging,
  title={Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset},
  author={Haoyi Wu and Wenyang Hui and Yezeng Chen and Weiqi Wu and Kewei Tu and Yi Zhou},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2311.05113},
  url={https://www.semanticscholar.org/paper/b6667ba4f586489f12587446c6daaa3f09cfc539},
  abstract={Mathematical understanding and reasoning are crucial tasks for assessing the capabilities of artificial intelligence (AI). However, existing benchmarks either require just a few steps of reasoning, or only contain a small amount of data in one specific topic, making it hard to analyse AI's behaviour with reference to different problems within a specific topic in detail. In this work, we propose Conic10K, a challenging math problem dataset on conic sections in Chinese senior high school education. Our dataset contains various problems with different reasoning depths, while only the knowledge from conic sections is required. Since the dataset only involves a narrow range of knowledge, it is easy to separately analyse the knowledge a model possesses and the reasoning ability it has. For each problem, we provide a high-quality formal representation, the reasoning steps, and the final solution. Experiments show that existing large language models, including GPT-4, exhibit weak performance on complex reasoning. We hope that our findings could inspire more advanced techniques for precise natural language understanding and reasoning. Our dataset and codes are available at https://github.com/whyNLP/Conic10K.},
  keywords={arxiv:2311.05113}
}

@article{obrien2023contrastivedecoding,
  title={Contrastive Decoding Improves Reasoning in Large Language Models},
  author={Sean O'Brien and Mike Lewis},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2309.09117},
  url={https://www.semanticscholar.org/paper/e716e6e0b3dd5124268780dc9bed521a07f371b8},
  abstract={We demonstrate that Contrastive Decoding -- a simple, computationally light, and training-free text generation method proposed by Li et al 2022 -- achieves large out-of-the-box improvements over greedy decoding on a variety of reasoning tasks. Originally shown to improve the perceived quality of long-form text generation, Contrastive Decoding searches for strings that maximize a weighted difference in likelihood between strong and weak models. We show that Contrastive Decoding leads LLaMA-65B to outperform LLaMA 2, GPT-3.5 and PaLM 2-L on the HellaSwag commonsense reasoning benchmark, and to outperform LLaMA 2, GPT-3.5 and PaLM-540B on the GSM8K math word reasoning benchmark, in addition to improvements on a collection of other tasks. Analysis suggests that Contrastive Decoding improves over existing methods by preventing some abstract reasoning errors, as well as by avoiding simpler modes such as copying sections of the input during chain-of-thought. Overall, Contrastive Decoding outperforms nucleus sampling for long-form generation and greedy decoding for reasoning tasks, making it a powerful general purpose method for generating text from language models.},
  keywords={arxiv:2309.09117}
}

@misc{meadows2023controllingequational,
  title={Controlling Equational Reasoning in Large Language Models with Prompt Interventions},
  author={Jordan Meadows and Marco Valentino and Andre Freitas},
  year={2023},
  url={https://www.semanticscholar.org/paper/4200b8253f56de8948d20fd69e7731b92c3ac3a4},
  abstract={This paper investigates how hallucination rates in Large Language Models (LLMs) may be controlled via a symbolic data generation framework, exploring a fundamental relationship between the rate of certain mathematical errors and types of input intervention. Specifically, we systematically generate data for a derivation generation task using a symbolic engine, applying targeted interventions to prompts to perturb features of mathematical derivations such as the surface forms of symbols, equational tree structures, and mathematical context. We then evaluate the effect of prompt interventions across a range of LLMs including fine-tuned T5 models, GPT, and LLaMa-based models. Our experiments suggest that T5-Large can outperform the few-shot performance of GPT-4 on various evaluation sets generated via the framework. However, an extensive evaluation based on human analysis, template-based error detection, and text generation metrics reveals model weaknesses beyond what the reference-based metrics singularly describe. We use these results to tie characteristic distributional footprints of interventions to the human evaluation of LLM derivation quality, potentially leading to significant control over fine-grained mathematical capabilities of language models with respect to specific types of errors.},
  keywords={arxiv:2307.09998}
}

@article{zhang2023controllinglarge,
  title={Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach},
  author={Bin Zhang and Hangyu Mao and Jingqing Ruan and Ying Wen and Yang Li and Shao Zhang and Zhiwei Xu and Dapeng Li and Ziyue Li and Rui Zhao and Lijuan Li and Guoliang Fan},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.13884},
  url={https://www.semanticscholar.org/paper/bc8d248fb86a3b6a285e8b9a6fe2c09e7f0b19c9},
  abstract={The remarkable progress in Large Language Models (LLMs) opens up new avenues for addressing planning and decision-making problems in Multi-Agent Systems (MAS). However, as the number of agents increases, the issues of hallucination in LLMs and coordination in MAS have become increasingly prominent. Additionally, the efficient utilization of tokens emerges as a critical consideration when employing LLMs to facilitate the interactions among a substantial number of agents. In this paper, we develop a modular framework called LLaMAC to mitigate these challenges. LLaMAC implements a value distribution encoding similar to that found in the human brain, utilizing internal and external feedback mechanisms to facilitate collaboration and iterative reasoning among its modules. Through evaluations involving system resource allocation and robot grid transportation, we demonstrate the considerable advantages afforded by our proposed approach.},
  keywords={arxiv:2311.13884}
}

@article{yang2023couplinglarge,
  title={Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text},
  author={Zhun Yang and Adam Ishay and Joohyung Lee},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2307.07696},
  url={https://www.semanticscholar.org/paper/162e2e9ac70702c146c0aa8432e4a6806bb8c42e},
  abstract={While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot planning tasks that an LLM alone fails to solve.},
  keywords={arxiv:2307.07696}
}

@article{kim2023creamvisuallysituated,
  title={Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models},
  author={Geewook Kim and Hodong Lee and D. Kim and Haeji Jung and S. Park and Yoon Kim and Sangdoo Yun and T. Kil and Bado Lee and Seunghyun Park},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.15080},
  url={https://www.semanticscholar.org/paper/08b562aa8066c2342f0d03824221dea18f0a18d2},
  abstract={Recent advances in Large Language Models (LLMs) have stimulated a surge of research aimed at extending their applications to the visual domain. While these models exhibit promise in generating abstract image captions and facilitating natural conversations, their performance on text-rich images still requires improvement. In this paper, we introduce Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods. Cream combines vision and auxiliary encoders, fortified by a contrastive feature alignment technique, to achieve a more effective comprehension of language information in visually situated contexts within the images. Our approach bridges the gap between vision and language understanding, paving the way for the development of more sophisticated Document Intelligence Assistants. Through rigorous evaluations across diverse visually-situated language understanding tasks that demand reasoning capabilities, we demonstrate the compelling performance of Cream, positioning it as a prominent model in the field of visual document understanding. We provide our codebase and newly-generated datasets at https://github.com/naver-ai/cream .},
  keywords={arxiv:2305.15080}
}

@article{tao2023culturalbias,
  title={Cultural bias and cultural alignment of large language models},
  author={Yan Tao and Olga Viberg and Ryan S. Baker and René F. Kizilcec},
  year={2023},
  booktitle={PNAS Nexus},
  doi={10.1093/pnasnexus/pgae346},
  url={https://www.semanticscholar.org/paper/5f8bf881c80125452e4a73ad51fdb2c72c65c551},
  abstract={Abstract Culture fundamentally shapes people’s reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people’s authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI’s GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models’ responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For later models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models’ output for 71–81\% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI.},
  keywords={arxiv:2311.14096}
}

@article{zhang2023cumulativereasoning,
  title={Cumulative Reasoning with Large Language Models},
  author={Yifan Zhang and Jingqin Yang and Yang Yuan and A. Yao},
  year={2023},
  booktitle={Trans. Mach. Learn. Res.},
  doi={10.48550/arXiv.2308.04371},
  url={https://www.semanticscholar.org/paper/507acddb0b7f36b83fd7c8bff2f121eb506ac8fb},
  abstract={Recent advancements in large language models (LLMs) have shown remarkable progress, yet their ability to solve complex problems remains limited. In this work, we introduce Cumulative Reasoning (CR), a structured framework that enhances LLM problem-solving by emulating human-like iterative and cumulative thought processes. CR orchestrates LLMs in three distinct roles--Proposer, Verifier(s), and Reporter--to systematically decompose tasks, generate and validate intermediate reasoning steps, and compose them into a solution by building a dynamic Directed Acyclic Graph (DAG) of verified propositions. This approach substantially enhances problem-solving capabilities. We demonstrate CR's advantage through several complex reasoning tasks: it outperforms existing methods in logical inference tasks with up to a 9.3\% improvement, achieving 98.04\% accuracy on the curated FOLIO wiki dataset. In the Game of 24, it achieves 98\% accuracy, marking a 24\% improvement over previous methods. In solving MATH problems, CR achieves a 4.2\% increase from previous methods and a 43\% relative improvement in the most challenging level 5 problems. When incorporating a code environment with CR, we further harness LLMs'reasoning capabilities and outperform the Program of Thought (PoT) method by 38.8\%. The code is available at https://github.com/iiis-ai/cumulative-reasoning.},
  keywords={arxiv:2308.04371}
}

@misc{jiang2023raftketch,
  title={D RAFT , S KETCH , AND P ROVE : G UIDING F ORMAL T HEOREM P ROVERS WITH I NFORMAL P ROOFS},
  author={Albert Qiaochu Jiang and S. Welleck and J. Zhou and Wenda Li and Jiacheng Liu and M. Jamnik and Timothée Lacroix and Guillaume Lample and Yuhuai Wu},
  year={2023},
  url={https://www.semanticscholar.org/paper/46b294941c397699fde0ee7e7fc441f6a755f671}
}

@article{chen2023discfinllmchinese,
  title={DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning},
  author={Wei Chen and Qiushi Wang and Zefei Long and Xianyin Zhang and Zhongtian Lu and Bingxuan Li and Siyuan Wang and Jiarong Xu and Xiang Bai and Xuanjing Huang and Zhongyu Wei},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.15205},
  url={https://www.semanticscholar.org/paper/814f0b1658c49c79bc32f3d2b89045de007871c6},
  abstract={We propose Multiple Experts Fine-tuning Framework to build a financial large language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by endowing them with multi-turn question answering abilities, domain text processing capabilities, mathematical computation skills, and retrieval-enhanced generation capabilities. We build a financial instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of four categories (consulting, NLP tasks, computing and retrieval-augmented generation). Evaluations conducted on multiple benchmarks demonstrate that our model performs better than baseline models in various financial scenarios. Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.},
  keywords={arxiv:2310.15205}
}

@article{yue2023disclawllmfinetuning,
  title={DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services},
  author={Shengbin Yue and Wei Chen and Siyuan Wang and Bingxuan Li and Chenchen Shen and Shujun Liu and Yuxuan Zhou and Yao Xiao and Song Yun and Wei Lin and Xuanjing Huang and Zhongyu Wei},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2309.11325},
  url={https://www.semanticscholar.org/paper/6806ecad90a778aaa7f6a3cd3a539582d823066c},
  abstract={We propose DISC-LawLLM, an intelligent legal system utilizing large language models (LLMs) to provide a wide range of legal services. We adopt legal syllogism prompting strategies to construct supervised fine-tuning datasets in the Chinese Judicial domain and fine-tune LLMs with legal reasoning capability. We augment LLMs with a retrieval module to enhance models' ability to access and utilize external legal knowledge. A comprehensive legal benchmark, DISC-Law-Eval, is presented to evaluate intelligent legal systems from both objective and subjective dimensions. Quantitative and qualitative results on DISC-Law-Eval demonstrate the effectiveness of our system in serving various users across diverse legal scenarios. The detailed resources are available at https://github.com/FudanDISC/DISC-LawLLM.},
  keywords={arxiv:2309.11325}
}

@article{zhou2023davirdata,
  title={DavIR: Data Selection via Implicit Reward for Large Language Models},
  author={Haotian Zhou and Tingkai Liu and Qianli Ma and Yufeng Zhang and Jianbo Yuan and Pengfei Liu and Yang You and Hongxia Yang},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2025.acl-long.452},
  url={https://www.semanticscholar.org/paper/e9f8d5c51e1d889ff3783e9afe7cb475c10e1af1},
  abstract={We introduce DavIR, a model-based data selection method for post-training Large Language Models. DavIR generalizes Reducible Holdout Loss to core-set selection problem of causal language modeling, and quantifies the learnability of a given datum with respect to a pre-trained LLM based on relative reduction in loss during fine-tuning, a metric we show to be closely related to the implicit reward model described in Direct Preference Optimization (DPO). We show that 6\% of Alpaca dataset selected with DavIR can steer both the LLaMA and Gemma model family to produce superior performance compared to the same models trained on the full 52K dataset. We also show that Alpaca dataset compressed with DavIR can be combined with GSM8K dataset to effectively balance open-domain freeform QA and mathematical reasoning capabilities. Finally, we apply the DavIR objective to DPO and develop a normalized DavIR-DPO objective which improves alignment performance of Zephyr-7B-SFT model by 8\% (relative) on AlpacaEval, compared against training on vanilla DPO objective.},
  keywords={arxiv:2310.13008}
}

@article{wang2023democratizingreasoning,
  title={Democratizing Reasoning Ability: Tailored Learning from Large Language Model},
  author={Zhaoyang Wang and Shaohan Huang and Yuxuan Liu and Jiahai Wang and Minghui Song and Zihan Zhang and Haizhen Huang and Furu Wei and Weiwei Deng and Feng Sun and Qi Zhang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.13332},
  url={https://www.semanticscholar.org/paper/be7dbac2bcaed4cd034a7371004a011933e1bdca},
  abstract={Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instruction-following ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a tailored learning approach to distill such reasoning ability to smaller LMs to facilitate the democratization of the exclusive reasoning ability. In contrast to merely employing LLM as a data annotator, we exploit the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm. This paradigm enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return. Further, to exploit the reasoning potential of the smaller LM, we propose self-reflection learning to motivate the student to learn from self-made mistakes. The learning from self-reflection and LLM are all tailored to the student's learning status, thanks to the seamless integration with the multi-round learning paradigm. Comprehensive experiments and analysis on mathematical and commonsense reasoning tasks demonstrate the effectiveness of our method. The code will be available at https://github.com/Raibows/Learn-to-Reason.},
  keywords={arxiv:2310.13332}
}

@article{wang2023describeexplain,
  title={Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents},
  author={Zihao Wang and Shaofei Cai and Anji Liu and Xiaojian Ma and Yitao Liang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2302.01560},
  url={https://www.semanticscholar.org/paper/ccb1ccc4deacc4fb18000f0e1ce24329548963ae},
  abstract={We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose"\$\textbackslash\{\}underline\{D\}\$escribe, \$\textbackslash\{\}underline\{E\}\$xplain, \$\textbackslash\{\}underline\{P\}\$lan and \$\textbackslash\{\}underline\{S\}\$elect"(\$\textbackslash\{\}textbf\{DEPS\}\$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated \$\textbackslash\{\}textit\{plan\}\$ by integrating \$\textbackslash\{\}textit\{description\}\$ of the plan execution process and providing self-\$\textbackslash\{\}textit\{explanation\}\$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal \$\textbackslash\{\}textit\{selector\}\$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the \$\textbackslash\{\}texttt\{ObtainDiamond\}\$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.},
  keywords={arxiv:2302.01560}
}

@article{wen2023diluknowledgedriven,
  title={DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models},
  author={Licheng Wen and Daocheng Fu and Xin Li and Xinyu Cai and Tengyu Ma and Pinlong Cai and Min Dou and Botian Shi and Liang He and Y. Qiao},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2309.16292},
  url={https://www.semanticscholar.org/paper/3cbfe152220de84ecf8059fa50c47587a3134c86},
  abstract={Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/},
  keywords={arxiv:2309.16292}
}

@article{savage2023diagnosticreasoning,
  title={Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine},
  author={Thomas Savage and Ashwin Nayak and Roberta Gallo and E. Rangan and Jonathan H. Chen},
  year={2023},
  booktitle={npj Digit. Medicine},
  doi={10.1038/s41746-024-01010-1},
  url={https://www.semanticscholar.org/paper/ee692adce2e5cbb42eae2d516641ae907b0483ce},
  abstract={One of the major barriers to using large language models (LLMs) in medicine is the perception they use uninterpretable methods to make clinical decisions that are inherently different from the cognitive processes of clinicians. In this manuscript we develop diagnostic reasoning prompts to study whether LLMs can imitate clinical reasoning while accurately forming a diagnosis. We find that GPT-4 can be prompted to mimic the common clinical reasoning processes of clinicians without sacrificing diagnostic accuracy. This is significant because an LLM that can imitate clinical reasoning to provide an interpretable rationale offers physicians a means to evaluate whether an LLMs response is likely correct and can be trusted for patient care. Prompting methods that use diagnostic reasoning have the potential to mitigate the “black box” limitations of LLMs, bringing them one step closer to safe and effective use in medicine.},
  keywords={arxiv:2308.06834}
}

@article{rafailov2023directpreference,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafael Rafailov and Archit Sharma and E. Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
  year={2023},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/0d1c76d45afa012ded7ab741194baf142117c495},
  abstract={While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  keywords={arxiv:2305.18290}
}

@article{qin2023disentangledrepresentation,
  title={Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs},
  author={Yi Qin and Xin Wang and Ziwei Zhang and Wenwu Zhu},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.18152},
  url={https://www.semanticscholar.org/paper/7a922abcda328f9333c5a3819ade8917b98f08c9},
  abstract={Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing computational costs and allowing much more flexibility in combining with different LLM models. Experimental evaluations demonstrate the effectiveness of the proposed DGTL model on achieving superior or comparable performance over state-of-the-art baselines. Additionally, we also demonstrate that our DGTL model can offer natural language explanations for predictions, thereby significantly enhancing model interpretability.},
  keywords={arxiv:2310.18152}
}

@article{mahowald2023dissociatinglanguage,
  title={Dissociating language and thought in large language models: a cognitive perspective},
  author={Kyle Mahowald and Anna A. Ivanova and I. Blank and N. Kanwisher and J. Tenenbaum and Evelina Fedorenko},
  year={2023},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/9a9e68d400069f023f7dc9b982226c95159a509d},
  abstract={Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become --"thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
  keywords={arxiv:2301.06627}
}

@article{naik2023diversitythought,
  title={Diversity of Thought Improves Reasoning Abilities of Large Language Models},
  author={Ranjita Naik and Varun Chandrasekaran and Mert Yuksekgonul and Hamid Palangi and Besmira Nushi},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.07088},
  url={https://www.semanticscholar.org/paper/f6007270b8aba3a69eac7c98375c99b7ca26c9b0}
}

@article{liu2023emergentabilities,
  title={Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study},
  author={Peiyu Liu and Zikang Liu and Ze-Feng Gao and Dawei Gao and Wayne Xin Zhao and Yaliang Li and Bolin Ding and Ji-rong Wen},
  year={2023},
  booktitle={International Conference on Language Resources and Evaluation},
  doi={10.48550/arXiv.2307.08072},
  url={https://www.semanticscholar.org/paper/c48fc69b62c749e78928d6a3bae98ffe278f761a},
  abstract={Despite the superior performance, Large Language Models (LLMs) require significant computational resources for deployment and use. To overcome this issue, quantization methods have been widely applied to reduce the memory footprint of LLMs as well as increase the inference rate. However, a major challenge is that low-bit quantization methods often lead to performance degradation. It is important to understand how quantization impacts the capacity of LLMs. Different from previous studies focused on overall performance, this work aims to investigate the impact of quantization on emergent abilities, which are important characteristics that distinguish LLMs from small language models. Specifically, we examine the abilities of in-context learning, chain-of-thought reasoning, and instruction-following in quantized LLMs. Our empirical experiments show that these emergent abilities still exist in 4-bit quantization models, while 2-bit models encounter severe performance degradation on the test of these abilities. To improve the performance of low-bit models, we conduct two special experiments: (1) fine-gained impact analysis that studies which components (or substructures) are more sensitive to quantization, and (2) performance compensation through model fine-tuning. Our work derives a series of important findings to understand the impact of quantization on emergent abilities and sheds light on the possibilities of extremely low-bit quantization for LLMs.},
  keywords={arxiv:2307.08072}
}

@article{choi2023llmsunderstand,
  title={Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark},
  author={Minje Choi and Jiaxin Pei and Sagar Kumar and Chang Shu and David Jurgens},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.emnlp-main.699},
  url={https://www.semanticscholar.org/paper/c1592c211f8b7791a55afd7162249c723b87c237},
  abstract={Large language models (LLMs) have been shown to perform well at a variety of syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed in many forms including conversational agents that interact with humans, we lack a grounded benchmark to measure how well LLMs understand \textbackslash\{\}textit\{social\} language. Here, we introduce a new theory-driven benchmark, SocKET, that contains 58 NLP tasks testing social knowledge which we group into five categories: humor\&sarcasm, offensiveness, sentiment\&emotion, and trustworthiness. In tests on the benchmark, we demonstrate that current models attain only moderate performance but reveal significant potential for task transfer among different types and categories of tasks, which were predicted from theory. Through zero-shot evaluations, we show that pretrained models already possess some innate but limited capabilities of social language understanding and training on one category of tasks can improve zero-shot testing on others. Our benchmark provides a systematic way to analyze model performance on an important dimension of language and points to clear room for improvement to build more socially-aware LLMs. The associated resources are released at https://github.com/minjechoi/SOCKET.},
  keywords={arxiv:2305.14938}
}

@article{jain2023languagemodels,
  title={Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models},
  author={Raghav Jain and Daivik Sojitra and Arkadeep Acharya and Sriparna Saha and Adam Jatowt and Sandipan Dandapat},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.emnlp-main.418},
  url={https://www.semanticscholar.org/paper/46137166084af64a7d115c7a731a1ce0da4c066e}
}

@article{guo2023doremigrounding,
  title={DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment},
  author={Yanjiang Guo and Yen-Jen Wang and Lihan Zha and Zheyuan Jiang and Jianyu Chen},
  year={2023},
  booktitle={IEEE/RJS International Conference on Intelligent RObots and Systems},
  doi={10.1109/IROS58592.2024.10802284},
  url={https://www.semanticscholar.org/paper/42b920abd44e76d73708859bfe13034555f1f8cb},
  abstract={Large language models (LLMs) encode a vast amount of semantic knowledge and possess remarkable understanding and reasoning capabilities. Previous work has explored how to ground LLMs in robotic tasks to generate feasible and executable textual plans. However, low-level execution in the physical world may deviate from the high-level textual plan due to environmental perturbations or imperfect controller design. In this paper, we propose DoReMi, a novel language model grounding framework that enables immediate Detection and Recovery from Misalignments between plan and execution. Specifically, we leverage LLMs to play a dual role, aiding not only in high-level planning but also generating constraints that can indicate misalignment during execution. Then vision language models (VLMs) are utilized to detect constraint violations continuously. Our pipeline can monitor the low-level execution and enable timely recovery if certain plan-execution misalignment occurs. Experiments on various complex tasks including robot arms and humanoid robots demonstrate that our method can lead to higher task success rates and shorter task completion times.},
  keywords={arxiv:2307.00329}
}

@article{wang2023docllmlayoutaware,
  title={DocLLM: A layout-aware generative language model for multimodal document understanding},
  author={Dongsheng Wang and Natraj Raman and Mathieu Sibue and Zhiqiang Ma and Petr Babkin and Simerjot Kaur and Yulong Pei and Armineh Nourbakhsh and Xiaomo Liu},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2401.00908},
  url={https://www.semanticscholar.org/paper/575f403261d5f99526f0b4dfc8644352d6c4467a},
  abstract={Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.},
  keywords={arxiv:2401.00908}
}

@article{fu2023drivelike,
  title={Drive Like a Human: Rethinking Autonomous Driving with Large Language Models},
  author={Daocheng Fu and Xin Li and Licheng Wen and Min Dou and Pinlong Cai and Botian Shi and Y. Qiao},
  year={2023},
  booktitle={2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)},
  doi={10.1109/WACVW60836.2024.00102},
  url={https://www.semanticscholar.org/paper/11bca2cafe89e14dc733504f97e2489de697ceab},
  abstract={In this paper, we explore the potential of using a large language model (LLM) to understand the driving environment in a human-like manner and analyze its ability to reason, interpret, and memorize when facing complex scenarios. We argue that traditional optimization-based and modular autonomous driving (AD) systems face inherent performance limitations when dealing with long-tail corner cases. To address this problem, we propose that an ideal AD system should drive like a human, accumulating experience through continuous driving and using common sense to solve problems. To achieve this goal, we identify three key abilities necessary for an AD system: reasoning, interpretation, and memorization. We demonstrate the feasibility of employing an LLM in driving scenarios by building a closed-loop system to showcase its comprehension and environment-interaction abilities. Our extensive experiments show that the LLM exhibits the impressive ability to reason and solve long-tailed cases, providing valuable insights for the development of human-like autonomous driving. The related code are available at https:/ithub.om/PJLab-ADG/DriveLikeAHuman.},
  keywords={arxiv:2307.07162}
}

@article{cui2023drivespeak,
  title={Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles},
  author={Can Cui and Yunsheng Ma and Xu Cao and Wenqian Ye and Ziran Wang},
  year={2023},
  booktitle={2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)},
  doi={10.1109/WACVW60836.2024.00101},
  url={https://www.semanticscholar.org/paper/482665786ce1956fb9ea4b694d2d8e8cf92276fa},
  abstract={The future of autonomous vehicles lies in the convergence of human-centric design and advanced AI capabilities. Autonomous vehicles of the future will not only transport passengers but also interact and adapt to their desires, making the journey comfortable, efficient, and pleasant. In this paper, we present a novel framework that leverages Large Language Models (LLMs) to enhance autonomous vehicles' decision-making processes. By integrating LLMs' natural language capabilities and contextual understanding, specialized tools usage, synergizing reasoning, and acting with various modules on autonomous vehicles, this framework aims to seamlessly integrate the advanced language and reasoning capabilities of LLMs into autonomous vehicles. The proposed framework holds the potential to revolutionize the way autonomous vehicles operate, offering personalized assistance, continuous learning, and transparent decision-making, ultimately contributing to safer and more efficient autonomous driving technologies.},
  keywords={arxiv:2309.10228}
}

@article{xu2023drivegpt4interpretable,
  title={DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model},
  author={Zhenhua Xu and Yujia Zhang and Enze Xie and Zhen Zhao and Yong Guo and K. K. Wong and Zhenguo Li and Hengshuang Zhao},
  year={2023},
  booktitle={IEEE Robotics and Automation Letters},
  doi={10.1109/LRA.2024.3440097},
  url={https://www.semanticscholar.org/paper/ccd6f8b6544f112de632e49bfbe592a0a654537d},
  abstract={Multimodallarge language models (MLLMs) have emerged as a prominent area of interest within the research community, given their proficiency in handling and reasoning with non-textual data, including images and videos. This study seeks to extend the application of MLLMs to the realm of autonomous driving by introducing DriveGPT4, a novel interpretable end-to-end autonomous driving system based on LLMs. Capable of processing multi-frame video inputs and textual queries, DriveGPT4 facilitates the interpretation of vehicle actions, offers pertinent reasoning, and effectively addresses a diverse range of questions posed by users. Furthermore, DriveGPT4 predicts low-level vehicle control signals in an end-to-end fashion. These advanced capabilities are achieved through the utilization of a bespoke visual instruction tuning dataset, specifically tailored for autonomous driving applications, in conjunction with a mix-finetuning training strategy. DriveGPT4 represents the pioneering effort to leverage LLMs for the development of an interpretable end-to-end autonomous driving solution. Evaluations conducted on the BDD-X dataset showcase the superior qualitative and quantitative performance of DriveGPT4. Additionally, the fine-tuning of domain-specific data enables DriveGPT4 to yield close or even improved results in terms of autonomous driving grounding when contrasted with GPT4-V.},
  keywords={arxiv:2310.01412}
}

@article{zhu2023dyvaldynamic,
  title={DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks},
  author={Kaijie Zhu and Jiaao Chen and Jindong Wang and Neil Zhenqiang Gong and Diyi Yang and Xing Xie},
  year={2023},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/1d7f414983eb847c4618489baa44e99b01162f98},
  abstract={Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns are raised about potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a general and flexible protocol for dynamic evaluation of LLMs. Based on our framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to GPT-3.5-Turbo and GPT-4. Experiments show that LLMs perform worse in DyVal-generated evaluation samples with different complexities, highlighting the significance of dynamic evaluation. We also analyze the failure cases and results of different prompting methods. Moreover, DyVal-generated samples are not only evaluation sets, but also helpful data for fine-tuning to improve the performance of LLMs on existing benchmarks. We hope that DyVal can shed light on future evaluation research of LLMs. Code is available at: https://github.com/microsoft/promptbench.},
  keywords={arxiv:2309.17167}
}

@article{xue2023dynamicvoting,
  title={Dynamic Voting for Efficient Reasoning in Large Language Models},
  author={Mingfeng Xue and Dayiheng Liu and Wenqiang Lei and Xingzhang Ren and Baosong Yang and Jun Xie and Yidan Zhang and Dezhong Peng and Jiancheng Lv},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.findings-emnlp.203},
  url={https://www.semanticscholar.org/paper/056b111a787210fcd75be1d0f278bf8d8aa32490},
  abstract={,}
}

@article{song2023dynamicsinstruction,
  title={Dynamics of Instruction Fine-Tuning for Chinese Large Language Models},
  author={Chiyu Song and Zhanchao Zhou and Jianhao Yan and Yuejiao Fei and Zhenzhong Lan and Yue Zhang},
  year={2023},
  booktitle={International Conference on Computational Linguistics},
  url={https://www.semanticscholar.org/paper/90614ab3c5fc2ff3da6f31cbb2f51f5568774fa9},
  abstract={Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). While numerous studies have examined the impact of factors such as data volume and model size on English models, the scaling properties of instruction tuning in other languages remain largely unexplored. In this work, we systematically investigate the effects of data quantity, model size, and data construction methods on instruction tuning for Chinese LLMs. We utilize a newly curated dataset, DoIT, which includes over 40,000 high-quality instruction instances covering ten underlying abilities, such as creative writing, code generation, and logical reasoning. Our experiments, conducted on models ranging from 7b to 33b parameters, yield three key findings: (i) While these factors directly affect overall model performance, some abilities are more responsive to scaling, whereas others demonstrate significant resistance. (ii) The scaling sensitivity of different abilities to these factors can be explained by two features: Complexity and Transference. (iii) By tailoring training strategies to their varying sensitivities, specific abilities can be efficiently learned, enhancing performance on two public benchmarks.},
  keywords={arxiv:2310.19651}
}

@misc{chen2023egoplanbenchbenchmarking,
  title={EgoPlan-Bench: Benchmarking Multimodal Large Language Models for Human-Level Planning},
  author={Yi Chen and Yuying Ge and Yixiao Ge and Mingyu Ding and Bohao Li and Rui Wang and Rui-Lan Xu and Ying Shan and Xihui Liu},
  year={2023},
  url={https://www.semanticscholar.org/paper/c60305f2a719c0ab5427a1f55304293ce18cd2e1},
  abstract={The pursuit of artificial general intelligence (AGI) has been accelerated by Multimodal Large Language Models (MLLMs), which exhibit superior reasoning, generalization capabilities, and proficiency in processing multimodal inputs. A crucial milestone in the evolution of AGI is the attainment of human-level planning, a fundamental ability for making informed decisions in complex environments, and solving a wide range of real-world problems. Despite the impressive advancements in MLLMs, a question remains: How far are current MLLMs from achieving human-level planning? To shed light on this question, we introduce EgoPlan-Bench, a comprehensive benchmark to evaluate the planning abilities of MLLMs in real-world scenarios from an egocentric perspective, mirroring human perception. EgoPlan-Bench emphasizes the evaluation of planning capabilities of MLLMs, featuring realistic tasks, diverse action plans, and intricate visual observations. Our rigorous evaluation of a wide range of MLLMs reveals that EgoPlan-Bench poses significant challenges, highlighting a substantial scope for improvement in MLLMs to achieve human-level task planning. To facilitate this advancement, we further present EgoPlan-IT, a specialized instruction-tuning dataset that effectively enhances model performance on EgoPlan-Bench. We have made all codes, data, and a maintained benchmark leaderboard available to advance future research.},
  keywords={arxiv:2312.06722}
}

@article{li2023emotionpromptleveraging,
  title={EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus},
  author={Cheng Li and Jindong Wang and Kaijie Zhu and Yixuan Zhang and Wenxin Hou and Jianxun Lian and Xing Xie},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2307.11760},
  url={https://www.semanticscholar.org/paper/b8395045e0129b4152fdbd547467fe76be19471e}
}

@article{wang2023emotionalintelligence,
  title={Emotional intelligence of Large Language Models},
  author={Xuena Wang and Xueting Li and Zi Yin and Yue Wu and Liu Jia Department of PsychologyTsinghua Laboratory of Brain and Intelligence and Tsinghua University and Departmentof Psychology and Renmin University},
  year={2023},
  journal={Journal of Pacific Rim Psychology},
  doi={10.1177/18344909231213958},
  url={https://www.semanticscholar.org/paper/bff499d51b002fd0b1aa05ba151a4a515e5bf36f},
  abstract={Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI. This test is an objective, performance-driven, and text-based evaluation, which requires evaluating complex emotions in realistic scenarios, providing a consistent assessment for both human and LLM capabilities. With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achieved above-average Emotional Quotient (EQ) scores, with GPT-4 exceeding 89\% of human participants with an EQ of 117. Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not rely on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ. In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence. Project website: https://emotional-intelligence.github.io/},
  keywords={arxiv:2307.09042}
}

@article{wang2023empoweringautonomous,
  title={Empowering Autonomous Driving with Large Language Models: A Safety Perspective},
  author={Yixuan Wang and Ruochen Jiao and Chengtian Lang and Sinong Zhan and Chao Huang and Zhaoran Wang and Zhuoran Yang and Qi Zhu},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.00812},
  url={https://www.semanticscholar.org/paper/c579ab910bd0ef8d6e06fc1b3557c16068af4fe5},
  abstract={Autonomous Driving (AD) encounters significant safety hurdles in long-tail unforeseen driving scenarios, largely stemming from the non-interpretability and poor generalization of the deep neural networks within the AD system, particularly in out-of-distribution and uncertain data. To this end, this paper explores the integration of Large Language Models (LLMs) into AD systems, leveraging their robust common-sense knowledge and reasoning abilities. The proposed methodologies employ LLMs as intelligent decision-makers in behavioral planning, augmented with a safety verifier shield for contextual safety learning, for enhancing driving performance and safety. We present two key studies in a simulated environment: an adaptive LLM-conditioned Model Predictive Control (MPC) and an LLM-enabled interactive behavior planning scheme with a state machine. Demonstrating superior performance and safety metrics compared to state-of-the-art approaches, our approach shows the promising potential for using LLMs for autonomous vehicles.},
  keywords={arxiv:2312.00812}
}

@article{chen2023empoweringpsychotherapy,
  title={Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting},
  author={Z. Chen and Yujie Lu and W. Wang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.07146},
  url={https://www.semanticscholar.org/paper/4f8ca3ccb77ff3c60ce90910854175eb8ab21a57},
  abstract={Mental illness remains one of the most critical public health issues of our time, due to the severe scarcity and accessibility limit of professionals. Psychotherapy requires high-level expertise to conduct deep, complex reasoning and analysis on the cognition modeling of the patients. In the era of Large Language Models, we believe it is the right time to develop AI assistance for computational psychotherapy. We study the task of cognitive distortion detection and propose the Diagnosis of Thought (DoT) prompting. DoT performs diagnosis on the patient's speech via three stages: subjectivity assessment to separate the facts and the thoughts; contrastive reasoning to elicit the reasoning processes supporting and contradicting the thoughts; and schema analysis to summarize the cognition schemas. The generated diagnosis rationales through the three stages are essential for assisting the professionals. Experiments demonstrate that DoT obtains significant improvements over ChatGPT for cognitive distortion detection, while generating high-quality rationales approved by human experts.},
  keywords={arxiv:2310.07146}
}

@article{guo2023empoweringworking,
  title={Empowering Working Memory for Large Language Model Agents},
  author={Jing Guo and Nan Li and J. Qi and Hang Yang and Ruiqiao Li and Yuzhen Feng and Si Zhang and Ming Xu},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.17259},
  url={https://www.semanticscholar.org/paper/f25624bdf639a34c8e9fd3c55911e0a82d15a20d},
  abstract={Large language models (LLMs) have achieved impressive linguistic capabilities. However, a key limitation persists in their lack of human-like memory faculties. LLMs exhibit constrained memory retention across sequential interactions, hindering complex reasoning. This paper explores the potential of applying cognitive psychology's working memory frameworks, to enhance LLM architecture. The limitations of traditional LLM memory designs are analyzed, including their isolation of distinct dialog episodes and lack of persistent memory links. To address this, an innovative model is proposed incorporating a centralized Working Memory Hub and Episodic Buffer access to retain memories across episodes. This architecture aims to provide greater continuity for nuanced contextual reasoning during intricate tasks and collaborative scenarios. While promising, further research is required into optimizing episodic memory encoding, storage, prioritization, retrieval, and security. Overall, this paper provides a strategic blueprint for developing LLM agents with more sophisticated, human-like memory capabilities, highlighting memory mechanisms as a vital frontier in artificial general intelligence.},
  keywords={arxiv:2312.17259}
}

@article{liang2023encouragingdivergent,
  title={Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate},
  author={Tian Liang and Zhiwei He and Wenxiang Jiao and Xing Wang and Yan Wang and Rui Wang and Yujiu Yang and Zhaopeng Tu and Shuming Shi},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.19118},
  url={https://www.semanticscholar.org/paper/385c74957858e7d6856d48e72b5a902b4c1aa28c},
  abstract={Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of “tit for tat” and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of “tit for tat” state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents.},
  keywords={arxiv:2305.19118}
}

@article{sun2023enhancingchainofthoughts,
  title={Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models},
  author={Jiashuo Sun and Yi Luo and Yeyun Gong and Chen Lin and Yelong Shen and Jian Guo and Nan Duan},
  year={2023},
  booktitle={NAACL-HLT},
  doi={10.48550/arXiv.2304.11657},
  url={https://www.semanticscholar.org/paper/ac37accd7aedf1c25c3d54c7982579b297b3ff2b},
  abstract={Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability across varying levels of difficulty. Experimental results indicate that Iter-CoT exhibits superiority, achieving competitive performance across three distinct reasoning tasks on ten datasets.},
  keywords={arxiv:2304.11657}
}

@article{chen2023enhancingemergency,
  title={Enhancing Emergency Decision-making with Knowledge Graphs and Large Language Models},
  author={Minze Chen and Zhenxiang Tao and Weitong Tang and Tingxin Qin and Rui Yang and Chunli Zhu},
  year={2023},
  journal={International Journal of Disaster Risk Reduction},
  doi={10.48550/arXiv.2311.08732},
  url={https://www.semanticscholar.org/paper/751c3659116b9c13bbe05a309778cdcef3ad86ee},
  abstract={Emergency management urgently requires comprehensive knowledge while having a high possibility to go beyond individuals' cognitive scope. Therefore, artificial intelligence(AI) supported decision-making under that circumstance is of vital importance. Recent emerging large language models (LLM) provide a new direction for enhancing targeted machine intelligence. However, the utilization of LLM directly would inevitably introduce unreliable output for its inherent issue of hallucination and poor reasoning skills. In this work, we develop a system called Enhancing Emergency decision-making with Knowledge Graph and LLM (E-KELL), which provides evidence-based decision-making in various emergency stages. The study constructs a structured emergency knowledge graph and guides LLMs to reason over it via a prompt chain. In real-world evaluations, E-KELL receives scores of 9.06, 9.09, 9.03, and 9.09 in comprehensibility, accuracy, conciseness, and instructiveness from a group of emergency commanders and firefighters, demonstrating a significant improvement across various situations compared to baseline models. This work introduces a novel approach to providing reliable emergency decision support.},
  keywords={arxiv:2311.08732}
}

@article{trajanoska2023enhancingknowledge,
  title={Enhancing Knowledge Graph Construction Using Large Language Models},
  author={Milena Trajanoska and Riste Stojanov and D. Trajanov},
  year={2023},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/694d9b45adcffa4bbc130e4ccaa681e275640128},
  abstract={The growing trend of Large Language Models (LLM) development has attracted significant attention, with models for various applications emerging consistently. However, the combined application of Large Language Models with semantic technologies for reasoning and inference is still a challenging task. This paper analyzes how the current advances in foundational LLM, like ChatGPT, can be compared with the specialized pretrained models, like REBEL, for joint entity and relation extraction. To evaluate this approach, we conducted several experiments using sustainability-related text as our use case. We created pipelines for the automatic creation of Knowledge Graphs from raw texts, and our findings indicate that using advanced LLM models can improve the accuracy of the process of creating these graphs from unstructured text. Furthermore, we explored the potential of automatic ontology creation using foundation LLM models, which resulted in even more relevant and accurate knowledge graphs.},
  keywords={arxiv:2305.04676}
}

@article{huang2023enhancinglarge,
  title={Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency},
  author={Baizhou Huang and Shuai Lu and Weizhu Chen and Xiaojun Wan and Nan Duan},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2309.17272},
  url={https://www.semanticscholar.org/paper/0d22f06a1f5ad9f62b2f35c126b514f927586c85},
  abstract={Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs' reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph. MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91\%), MBPP (+6.43\%) and CodeContests (+9.37\%), even surpassing GPT-4.},
  keywords={arxiv:2309.17272}
}

@article{nguyen2023enhancinglogical,
  title={Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications},
  author={Ha-Thanh Nguyen and Wachara Fungwacharakorn and Ken Satoh},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.13095},
  url={https://www.semanticscholar.org/paper/cac951523d6b7d1bbd014bd8b5e87afd9ecdd318},
  abstract={Language serves as a vehicle for conveying thought, enabling communication among individuals. The ability to distinguish between diverse concepts, identify fairness and injustice, and comprehend a range of legal notions fundamentally relies on logical reasoning. Large Language Models (LLMs) attempt to emulate human language understanding and generation, but their competency in logical reasoning remains limited. This paper seeks to address the philosophical question: How can we effectively teach logical reasoning to LLMs while maintaining a deep understanding of the intricate relationship between language and logic? By focusing on bolstering LLMs' capabilities in logical reasoning, we aim to expand their applicability in law and other logic-intensive disciplines. To this end, we propose a Reinforcement Learning from Logical Feedback (RLLF) approach, which serves as a potential framework for refining LLMs' reasoning capacities. Through RLLF and a revised evaluation methodology, we explore new avenues for research in this domain and contribute to the development of LLMs capable of handling complex legal reasoning tasks while acknowledging the fundamental connection between language and logic.},
  keywords={arxiv:2311.13095}
}

@article{wang2023enhancingrecommender,
  title={Enhancing Recommender Systems with Large Language Model Reasoning Graphs},
  author={Yan Wang and Zhixuan Chu and Ouyang Xin and Simeng Wang and Hongyan Hao and Yue Shen and Jinjie Gu and Siqiao Xue and James Y. Zhang and Qing Cui and Longfei Li and Jun Zhou and Shenghe Li},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.10835},
  url={https://www.semanticscholar.org/paper/f8a2813614f4e9c8adab3da2cbb667ad7e4d6bcf},
  abstract={Recommendation systems aim to provide users with relevant suggestions, but often lack interpretability and fail to capture higher-level semantic relationships between user behaviors and profiles. In this paper, we propose a novel approach that leverages large language models (LLMs) to construct personalized reasoning graphs. These graphs link a user's profile and behavioral sequences through causal and logical inferences, representing the user's interests in an interpretable way. Our approach, LLM reasoning graphs (LLMRG), has four components: chained graph reasoning, divergent extension, self-verification and scoring, and knowledge base self-improvement. The resulting reasoning graph is encoded using graph neural networks, which serves as additional input to improve conventional recommender systems, without requiring extra user or item information. Our approach demonstrates how LLMs can enable more logical and interpretable recommender systems through personalized reasoning graphs. LLMRG allows recommendations to benefit from both engineered recommendation systems and LLM-derived reasoning graphs. We demonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios in enhancing base recommendation models.},
  keywords={arxiv:2308.10835}
}

@article{shao2023enhancingretrievalaugmented,
  title={Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy},
  author={Zhihong Shao and Yeyun Gong and Yelong Shen and Minlie Huang and Nan Duan and Weizhu Chen},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.15294},
  url={https://www.semanticscholar.org/paper/a1675f47125aa409525c5f759b5e6bcc1c8831aa},
  abstract={Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.},
  keywords={arxiv:2305.15294}
}

@article{zhao2023enhancingzeroshot,
  title={Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic},
  author={Xufeng Zhao and Mengdi Li and Wenhao Lu and C. Weber and Jae Hee Lee and Kun-Mo Chu and Stefan Wermter},
  year={2023},
  booktitle={International Conference on Language Resources and Evaluation},
  doi={10.48550/arXiv.2309.13339},
  url={https://www.semanticscholar.org/paper/eed2a631d672a4130407f8d69a0ad9118a1e6e7d},
  abstract={Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts), a self-improvement prompting framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of enhanced reasoning by logic. The implementation code for LoT can be accessed at: https://github.com/xf-zhao/LoT.},
  keywords={arxiv:2309.13339}
}

@misc{patten2023evaluatingdomain,
  title={Evaluating Domain Specific LLM Performance Within Economics Evaluating Domain Specific LLM Performance Within Economics Using the Novel EconQA Dataset Using the Novel EconQA Dataset},
  author={Tate Van Patten and Van Patten},
  year={2023},
  url={https://www.semanticscholar.org/paper/f73871b36eb728270b5b369d25308f7c63673077}
}

@article{liu2023evaluatinglarge,
  title={Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis},
  author={Chang Liu and Bo Wu},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.11224},
  url={https://www.semanticscholar.org/paper/927fc7652e033c9eb17296df087e3e6491112bb0},
  abstract={Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has demonstrated the capacity to rectify responses from GPT-3.5-turbo and its own previous iterations. The code is available at: https://github.com/Ayame1006/LLMtoGraph.},
  keywords={arxiv:2308.11224}
}

@article{seals2023evaluatingdeductive,
  title={Evaluating the Deductive Competence of Large Language Models},
  author={S. M. Seals and V. Shalin},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2309.05452},
  url={https://www.semanticscholar.org/paper/e5d0a261a8e224ab4ed9fada3e6cbb88429a0a9e},
  abstract={The development of highly fluent large language models (LLMs) has prompted increased interest in assessing their reasoning and problem-solving capabilities. We investigate whether several LLMs can solve a classic type of deductive reasoning problem from the cognitive science literature. The tested LLMs have limited abilities to solve these problems in their conventional form. We performed follow up experiments to investigate if changes to the presentation format and content improve model performance. We do find performance differences between conditions; however, they do not improve overall performance. Moreover, we find that performance interacts with presentation format and content in unexpected ways that differ from human performance. Overall, our results suggest that LLMs have unique reasoning biases that are only partially predicted from human reasoning performance and the human-generated language corpora that informs them.},
  keywords={arxiv:2309.05452}
}

@article{munikoti2023evaluatingeffectiveness,
  title={Evaluating the Effectiveness of Retrieval-Augmented Large Language Models in Scientific Document Reasoning},
  author={Sai Munikoti and Anurag Acharya and S. Wagle and Sameera Horawalavithana},
  year={2023},
  booktitle={SDP},
  doi={10.48550/arXiv.2311.04348},
  url={https://www.semanticscholar.org/paper/a1caeac57526b5d1ea90ee87fceb8ec44a56183c},
  abstract={Despite the dramatic progress in Large Language Model (LLM) development, LLMs often provide seemingly plausible but not factual information, often referred to as hallucinations. Retrieval-augmented LLMs provide a non-parametric approach to solve these issues by retrieving relevant information from external data sources and augment the training process. These models help to trace evidence from an externally provided knowledge base allowing the model predictions to be better interpreted and verified. In this work, we critically evaluate these models in their ability to perform in scientific document reasoning tasks. To this end, we tuned multiple such model variants with science-focused instructions and evaluated them on a scientific document reasoning benchmark for the usefulness of the retrieved document passages. Our findings suggest that models justify predictions in science tasks with fabricated evidence and leveraging scientific corpus as pretraining data does not alleviate the risk of evidence fabrication.},
  keywords={arxiv:2311.04348}
}

@article{gong2023evaluatingpotential,
  title={Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions},
  author={Xinyu Gong and J. Holmes and Yiwei Li and Zheng Liu and Qi Gan and Zihao Wu and Jianli Zhang and Yusong Zou and Yuxi Teng and Tian Jiang and Hongtu Zhu and Wei Liu and Tianming Liu and Yajun Yan},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.07582},
  url={https://www.semanticscholar.org/paper/74bbaacd31283c2ae5aeaef378133acefad229b1},
  abstract={Recent advances in Large Language Models (LLMs) have presented new opportunities for integrating Artificial General Intelligence (AGI) into biological research and education. This study evaluated the capabilities of leading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in answering conceptual biology questions. The models were tested on a 108-question multiple-choice exam covering biology topics in molecular biology, biological techniques, metabolic engineering, and synthetic biology. Among the models, GPT-4 achieved the highest average score of 90 and demonstrated the greatest consistency across trials with different prompts. The results indicated GPT-4's proficiency in logical reasoning and its potential to aid biology research through capabilities like data analysis, hypothesis generation, and knowledge integration. However, further development and validation are still required before the promise of LLMs in accelerating biological discovery can be realized.},
  keywords={arxiv:2311.07582}
}

@article{kang2023evermitigating,
  title={Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification},
  author={Haoqiang Kang and Juntong Ni and Huaxiu Yao},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.09114},
  url={https://www.semanticscholar.org/paper/b10482ab3dd1d340c3c926d92c3e617c24ee3949},
  abstract={Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the"snowballing"issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based baselines, Ever demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including short-form QA, biography generation, and multi-hop reasoning.},
  keywords={arxiv:2311.09114}
}

@article{xiong2023examininginterconsistency,
  title={Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate},
  author={Kai Xiong and Xiao Ding and Yixin Cao and Ting Liu and Bing Qin},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.findings-emnlp.508},
  url={https://www.semanticscholar.org/paper/f8cbcb106a48524edc39df23e2a95f1e6d4c739a},
  abstract={Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues. Existing works primarily focus on the inconsistency issues within a single LLM, while we complementarily explore the inter-consistency among multiple LLMs for collaboration. To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs. Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost collaboration performance. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for developing future collaboration methods. Codes and data are available at https://github.com/Waste-Wood/FORD},
  keywords={arxiv:2305.11595}
}

@article{yin2023exchangeofthoughtenhancing,
  title={Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication},
  author={Zhangyue Yin and Qiushi Sun and Cheng Chang and Qipeng Guo and Junqi Dai and Xuanjing Huang and Xipeng Qiu},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2312.01823},
  url={https://www.semanticscholar.org/paper/a9e78765a4d49a50d67d0dacb033fb47f8d9f8c9},
  abstract={Large Language Models (LLMs) have recently made significant strides in complex reasoning tasks through the Chain-of-Thought technique. Despite this progress, their reasoning is often constrained by their intrinsic understanding, lacking external insights. To address this, we propose Exchange-of-Thought (EoT), a novel framework that enables cross-model communication during problem-solving. Drawing inspiration from network topology, EoT integrates four unique communication paradigms: Memory, Report, Relay, and Debate. This paper delves into the communication dynamics and volume associated with each paradigm. To counterbalance the risks of incorrect reasoning chains, we implement a robust confidence evaluation mechanism within these communications. Our experiments across diverse complex reasoning tasks demonstrate that EoT significantly surpasses established baselines, underscoring the value of external insights in enhancing LLM performance. Furthermore, we show that EoT achieves these superior results in a cost-effective manner, marking a promising advancement for efficient and collaborative AI problem-solving.},
  keywords={arxiv:2312.01823}
}

@article{wang2023explainableclaim,
  title={Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models},
  author={Haoran Wang and Kai Shu},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.05253},
  url={https://www.semanticscholar.org/paper/d75387fcf6a839f2aa8af5778aa6931eea5ec969},
  abstract={Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. This process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. Our experiment results indicate that FOLK outperforms strong baselines on three datasets encompassing various claim verification challenges. Our code and data are available.},
  keywords={arxiv:2310.05253}
}

@article{johansson2023exploringmathematical,
  title={Exploring Mathematical Conjecturing with Large Language Models},
  author={Moa Johansson and Nicholas Smallbone},
  year={2023},
  booktitle={International Workshop on Neural-Symbolic Learning and Reasoning},
  url={https://www.semanticscholar.org/paper/c575262ed1fb7f7eaa3825ee26f74f2e9b06747a}
}

@article{englhardt2023exploringcharacterizing,
  title={Exploring and Characterizing Large Language Models for Embedded System Development and Debugging},
  author={Zachary Englhardt and R. Li and Dilini Nissanka and Zhihan Zhang and Girish Narayanswamy and Joseph Breda and Xin Liu and Shwetak N. Patel and Vikram Iyer},
  year={2023},
  booktitle={CHI Extended Abstracts},
  doi={10.1145/3613905.3650764},
  url={https://www.semanticscholar.org/paper/4709921aca4dddbdeb06be3f49dc2be93a39a0d1},
  abstract={Large language models (LLMs) have shown remarkable abilities to generate code. However, their ability to develop software for physical computing and embedded systems, which requires cross-domain hardware and software knowledge, has not been thoroughly studied. We observe through our experiments and a 15-user pilot study that even when LLMs fail to produce working code, they can generate helpful reasoning about embedded design tasks, as well as specific debugging suggestions for both novice and expert developers. These results highlight the potential to develop AI assistants to dramatically lower the barrier to entry for working with hardware. To evaluate the capabilities and limitations of LLMs, we develop an automated testbench to quantify LLM performance on embedded programming tasks and perform 450 trials. We leverage these findings to analyze how programmers interact with these tools including their productivity and sense of fulfillment and outline a human-AI collaborative workflow for developing and debugging embedded systems.},
  keywords={arxiv:2307.03817}
}

@article{sharma2023exploringimproving,
  title={Exploring and Improving the Spatial Reasoning Abilities of Large Language Models},
  author={Manasi Sharma},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.01054},
  url={https://www.semanticscholar.org/paper/152e0185bbae8a58e7e03aa4a630c0e7e6e19513},
  abstract={Large Language Models (LLMs) represent formidable tools for sequence modeling, boasting an innate capacity for general pattern recognition. Nevertheless, their broader spatial reasoning capabilities, especially applied to numerical trajectory data, remain insufficiently explored. In this paper, we investigate the out-of-the-box performance of ChatGPT-3.5, ChatGPT-4 and Llama 2 7B models when confronted with 3D robotic trajectory data from the CALVIN baseline and associated tasks, including 2D directional and shape labeling. Additionally, we introduce a novel prefix-based prompting mechanism, which yields a 33\% improvement on the 3D trajectory data and an increase of up to 10\% on SpartQA tasks over zero-shot prompting (with gains for other prompting types as well). The experimentation with 3D trajectory data offers an intriguing glimpse into the manner in which LLMs engage with numerical and spatial information, thus laying a solid foundation for the identification of target areas for future enhancements.},
  keywords={arxiv:2312.01054}
}

@article{wu2023exploringtradeoffs,
  title={Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task},
  author={Zihao Wu and Lu Zhang and Chao-Yang Cao and Xiao-Xing Yu and Haixing Dai and Chong-Yi Ma and Zheng Liu and Lin Zhao and Gang Li and Wei Liu and Quanzheng Li and Dinggang Shen and Xiang Li and Dajiang Zhu and Tianming Liu},
  year={2023},
  journal={IEEE Transactions on Big Data},
  doi={10.1109/TBDATA.2025.3536928},
  url={https://www.semanticscholar.org/paper/258605dc5b00fe66b72091f947642a554e472aee},
  abstract={Recently, ChatGPT and GPT-4 have emerged and gained immense global attention due to their unparalleled performance in language processing. Despite demonstrating impressive capability in various open-domain tasks, their adequacy in highly specific fields like radiology remains untested. Radiology presents unique linguistic phenomena distinct from open-domain data due to its specificity and complexity. Assessing the performance of large language models (LLMs) in such specific domains is crucial not only for a thorough evaluation of their overall performance but also for providing valuable insights into future model design directions: whether model design should be generic or domain-specific. To this end, in this study, we evaluate the performance of ChatGPT/GPT-4 on a radiology natural language inference (NLI) task and compare it to other models fine-tuned specifically on task-related data samples. We also conduct a comprehensive investigation on ChatGPT/GPT-4’s reasoning ability by introducing varying levels of inference difficulty. Our results show that 1) ChatGPT and GPT-4 outperform other LLMs in the radiology NLI task and 2) other specifically fine-tuned Bert-based models require significant amounts of data samples to achieve comparable performance to ChatGPT/GPT-4. These findings not only demonstrate the feasibility and promise of constructing a generic model capable of addressing various tasks across different domains, but also highlight several key factors crucial for developing a unified model, particularly in a medical context, paving the way for future artificial general intelligence (AGI) systems. We release our code and data to the research community.},
  keywords={arxiv:2304.09138}
}

@article{balas2023exploringpotential,
  title={Exploring the potential utility of AI large language models for medical ethics: an expert panel evaluation of GPT-4},
  author={M. Balas and Jordan Joseph Wadden and Philip C Hébert and Eric Mathison and Marika D Warren and Victoria Seavilleklein and Daniel Wyzynski and Alison Callahan and Sean A Crawford and Parnian Arjmand and Edsel B. Ing},
  year={2023},
  journal={Journal of Medical Ethics},
  doi={10.1136/jme-2023-109549},
  url={https://www.semanticscholar.org/paper/b8d0ded3a9c76ccafa60f20f4ed4de51f2430fbc},
  abstract={Integrating large language models (LLMs) like GPT-4 into medical ethics is a novel concept, and understanding the effectiveness of these models in aiding ethicists with decision-making can have significant implications for the healthcare sector. Thus, the objective of this study was to evaluate the performance of GPT-4 in responding to complex medical ethical vignettes and to gauge its utility and limitations for aiding medical ethicists. Using a mixed-methods, cross-sectional survey approach, a panel of six ethicists assessed LLM-generated responses to eight ethical vignettes. The main outcomes measured were relevance, reasoning, depth, technical and non-technical clarity, as well as acceptability of GPT-4’s responses. The readability of the responses was also assessed. Of the six metrics evaluating the effectiveness of GPT-4’s responses, the overall mean score was 4.1/5. GPT-4 was rated highest in providing technical (4.7/5) and non-technical clarity (4.4/5), whereas the lowest rated metrics were depth (3.8/5) and acceptability (3.8/5). There was poor-to-moderate inter-rater reliability characterised by an intraclass coefficient of 0.54 (95\% CI: 0.30 to 0.71). Based on panellist feedback, GPT-4 was able to identify and articulate key ethical issues but struggled to appreciate the nuanced aspects of ethical dilemmas and misapplied certain moral principles. This study reveals limitations in the ability of GPT-4 to appreciate the depth and nuanced acceptability of real-world ethical dilemmas, particularly those that require a thorough understanding of relational complexities and context-specific values. Ongoing evaluation of LLM capabilities within medical ethics remains paramount, and further refinement is needed before it can be used effectively in clinical settings.}
}

@article{chen2023felmbenchmarking,
  title={FELM: Benchmarking Factuality Evaluation of Large Language Models},
  author={Shiqi Chen and Yiran Zhao and Jinghan Zhang and Ethan Chern and Siyang Gao and Pengfei Liu and Junxian He},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2310.00741},
  url={https://www.semanticscholar.org/paper/837a3c0417fb677d4f22c346b345a450ec417f2c},
  abstract={Assessing factuality of text generated by large language models (LLMs) is an emerging yet crucial research area, aimed at alerting users to potential errors and guiding the development of more reliable LLMs. Nonetheless, the evaluators assessing factuality necessitate suitable evaluation themselves to gauge progress and foster advancements. This direction remains under-explored, resulting in substantial impediments to the progress of factuality evaluators. To mitigate this issue, we introduce a benchmark for Factuality Evaluation of large Language Models, referred to as felm. In this benchmark, we collect responses generated from LLMs and annotate factuality labels in a fine-grained manner. Contrary to previous studies that primarily concentrate on the factuality of world knowledge (e.g.\~{}information from Wikipedia), felm focuses on factuality across diverse domains, spanning from world knowledge to math and reasoning. Our annotation is based on text segments, which can help pinpoint specific factual errors. The factuality annotations are further supplemented by predefined error types and reference links that either support or contradict the statement. In our experiments, we investigate the performance of several LLM-based factuality evaluators on felm, including both vanilla LLMs and those augmented with retrieval mechanisms and chain-of-thought processes. Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.},
  keywords={arxiv:2310.00741}
}

@article{liu2023federatedprompting,
  title={Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering},
  author={Xiangyang Liu and Tianqi Pang and Chenyou Fan},
  year={2023},
  booktitle={Knowledge Science, Engineering and Management},
  doi={10.48550/arXiv.2304.13911},
  url={https://www.semanticscholar.org/paper/a7c0d9bf44045c9d4c41e329e2a87df0ae7e0af6},
  abstract={We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that our proposed methods can significantly enhance question accuracy by fully exploring the synonymous nature of the questions and the consistency of the answers.},
  keywords={arxiv:2304.13911}
}

@article{zhang2023finevalchinese,
  title={FinEval: A Chinese Financial Domain Knowledge Evaluation Benchmark for Large Language Models},
  author={Liwen Zhang and Wei Cai and Zhaowei Liu and Zhi Yang and Wei Dai and Yujie Liao and Qi Qin and Yifei Li and Xingxian Liu and Zhiqiang Liu and Zhoufan Zhu and Anbo Wu and Xin Guo and Yun Chen},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2308.09975},
  url={https://www.semanticscholar.org/paper/3b88526a0f0337e3a6b632b4af8fd0882eb4b470},
  abstract={Large language models have demonstrated outstanding performance in various natural language processing tasks, but their security capabilities in the financial domain have not been explored, and their performance on complex tasks like financial agent remains unknown. This paper presents FinEval, a benchmark designed to evaluate LLMs' financial domain knowledge and practical abilities. The dataset contains 8,351 questions categorized into four different key areas: Financial Academic Knowledge, Financial Industry Knowledge, Financial Security Knowledge, and Financial Agent. Financial Academic Knowledge comprises 4,661 multiple-choice questions spanning 34 subjects such as finance and economics. Financial Industry Knowledge contains 1,434 questions covering practical scenarios like investment research. Financial Security Knowledge assesses models through 1,640 questions on topics like application security and cryptography. Financial Agent evaluates tool usage and complex reasoning with 616 questions. FinEval has multiple evaluation settings, including zero-shot, five-shot with chain-of-thought, and assesses model performance using objective and subjective criteria. Our results show that Claude 3.5-Sonnet achieves the highest weighted average score of 72.9 across all financial domain categories under zero-shot setting. Our work provides a comprehensive benchmark closely aligned with Chinese financial domain.},
  keywords={arxiv:2308.09975}
}

@article{kamath2023findinginductive,
  title={Finding Inductive Loop Invariants using Large Language Models},
  author={Adharsh Kamath and Aditya Senthilnathan and Saikat Chakraborty and Pantazis Deligiannis and Shuvendu K. Lahiri and Akash Lal and Aseem Rastogi and Subhajit Roy and Rahul Sharma},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.07948},
  url={https://www.semanticscholar.org/paper/5824788bc6b355e1a655add875b100541aef4b59},
  abstract={Loop invariants are fundamental to reasoning about programs with loops. They establish properties about a given loop's behavior. When they additionally are inductive, they become useful for the task of formal verification that seeks to establish strong mathematical guarantees about program's runtime behavior. The inductiveness ensures that the invariants can be checked locally without consulting the entire program, thus are indispensable artifacts in a formal proof of correctness. Finding inductive loop invariants is an undecidable problem, and despite a long history of research towards practical solutions, it remains far from a solved problem. This paper investigates the capabilities of the Large Language Models (LLMs) in offering a new solution towards this old, yet important problem. To that end, we first curate a dataset of verification problems on programs with loops. Next, we design a prompt for exploiting LLMs, obtaining inductive loop invariants, that are checked for correctness using sound symbolic tools. Finally, we explore the effectiveness of using an efficient combination of a symbolic tool and an LLM on our dataset and compare it against a purely symbolic baseline. Our results demonstrate that LLMs can help improve the state-of-the-art in automated program verification.},
  keywords={arxiv:2311.07948}
}

@article{baldazzi2023finetuninglarge,
  title={Fine-tuning Large Enterprise Language Models via Ontological Reasoning},
  author={Teodoro Baldazzi and Luigi Bellomarini and S. Ceri and Andrea Colombo and Andrea Gentili and Emanuel Sallinger},
  year={2023},
  booktitle={RuleML+RR},
  doi={10.48550/arXiv.2306.10723},
  url={https://www.semanticscholar.org/paper/1e7301ca09f604f56ee268c5ffaddcd7e537d46f},
  abstract={Large Language Models (LLMs) exploit fine-tuning as a technique to adapt to diverse goals, thanks to task-specific training data. Task specificity should go hand in hand with domain orientation, that is, the specialization of an LLM to accurately address the tasks of a given realm of interest. However, models are usually fine-tuned over publicly available data or, at most, over ground data from databases, ignoring business-level definitions and domain experience. On the other hand, Enterprise Knowledge Graphs (EKGs) are able to capture and augment such domain knowledge via ontological reasoning. With the goal of combining LLM flexibility with the domain orientation of EKGs, we propose a novel neurosymbolic architecture that leverages the power of ontological reasoning to build task- and domain-specific corpora for LLM fine-tuning.},
  keywords={arxiv:2306.10723}
}

@article{ding2023fluidtransformers,
  title={Fluid Transformers and Creative Analogies: Exploring Large Language Models’ Capacity for Augmenting Cross-Domain Analogical Creativity},
  author={Zijian Ding and Arvind Srinivasan and Stephen MacNeil and Joel Chan},
  year={2023},
  booktitle={Creativity \& Cognition},
  doi={10.1145/3591196.3593516},
  url={https://www.semanticscholar.org/paper/d0cbc7cdf5c9a2fd9426f255acb5ee29f3351640},
  abstract={Cross-domain analogical reasoning is a core creative ability that can be challenging for humans. Recent work has shown some proofs-of-concept of Large language Models’ (LLMs) ability to generate cross-domain analogies. However, the reliability and potential usefulness of this capacity for augmenting human creative work has received little systematic exploration. In this paper, we systematically explore LLMs capacity to augment cross-domain analogical reasoning. Across three studies, we found: 1) LLM-generated cross-domain analogies were frequently judged as helpful in the context of a problem reformulation task (median 4 out of 5 helpfulness rating), and frequently (∼ 80\% of cases) led to observable changes in problem formulations, and 2) there was an upper bound of ∼ 25\% of outputs being rated as potentially harmful, with a majority due to potentially upsetting content, rather than biased or toxic content. These results demonstrate the potential utility — and risks — of LLMs for augmenting cross-domain analogical creativity.},
  keywords={arxiv:2302.12832}
}

@article{jiang2023forwardbackwardreasoning,
  title={Forward-Backward Reasoning in Large Language Models for Verification},
  author={Weisen Jiang and Han Shi and L. Yu and Zheng Liu and Yu Zhang and Zheng Li and J. Kwok},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.07758},
  url={https://www.semanticscholar.org/paper/cb4f6823e25896d5919046696066d0d498cc4397}
}

@article{myers2023foundationlarge,
  title={Foundation and large language models: fundamentals, challenges, opportunities, and social impacts},
  author={Devon Myers and Rami Mohawesh and Venkata Ishwarya Chellaboina and Anantha Lakshmi Sathvik and Praveen Venkatesh and Yi-Hui Ho and Hanna Henshaw and Muna Alhawawreh and David Berdik and Yaser Jararweh},
  year={2023},
  booktitle={Cluster Computing},
  doi={10.1007/s10586-023-04203-7},
  url={https://www.semanticscholar.org/paper/7f976f21bb571941042906d964c6cb676c29decb}
}

@article{gao2023gllavasolving,
  title={G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model},
  author={Jiahui Gao and Renjie Pi and Jipeng Zhang and Jiacheng Ye and Wanjun Zhong and Yufei Wang and Lanqing Hong and Jianhua Han and Hang Xu and Zhenguo Li and Lingpeng Kong},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2312.11370},
  url={https://www.semanticscholar.org/paper/3713112311efbcf785de17fa86e5bf42e4360f77},
  abstract={Large language models (LLMs) have shown remarkable proficiency in human-level reasoning and generation capabilities, which encourages extensive research on their application in mathematical problem solving. However, current work has been largely focused on text-based mathematical problems, with limited investigation in problems involving geometric information. Addressing this gap, we aim to enable LLMs to solve geometric problems by understanding image input. We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area: they struggle to accurately comprehending basic geometric elements and their relationships. To overcome these challenges, we take advantage of the unique characteristics of geometric problems (such as unique geometric logical form, and geometric scalability) and the capacity of the textual LLMs to build an enriched multimodal geometry dataset based on existing data. The augmented dataset, Geo170K, contains more than 170K geometric image-caption and question-answer pairs. Utilizing our constructed Geo170K dataset, we develop G-LLaVA, which demonstrates exceptional performance in solving geometric problems, significantly outperforming GPT-4-V on the MathVista benchmark with only 7B parameters.},
  keywords={arxiv:2312.11370}
}

@article{liu2023gloreevaluating,
  title={GLoRE: Evaluating Logical Reasoning of Large Language Models},
  author={Hanmeng Liu and Zhiyang Teng and Ruoxi Ning and Jian Liu and Qiji Zhou and Yuexin Zhang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.09107},
  url={https://www.semanticscholar.org/paper/806b5882c983bd156a8c10bcd34fe285d8a0593b},
  abstract={Large language models (LLMs) have shown significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a General Logical Reasoning Evaluation platform that not only consolidates diverse datasets but also standardizes them into a unified format suitable for evaluating large language models across zero-shot and few-shot scenarios. Our experimental results show that compared to the performance of humans and supervised fine-tuning models, the logical reasoning capabilities of large reasoning models, such as OpenAI's o1 mini, DeepSeek R1 and QwQ-32B, have seen remarkable improvements, with QwQ-32B achieving the highest benchmark performance to date. GLoRE is designed as a living project that continuously integrates new datasets and models, facilitating robust and comparative assessments of model performance in both commercial and Huggingface communities.},
  keywords={arxiv:2310.09107}
}

@article{zheng2023gptfathombenchmarking,
  title={GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond},
  author={Shen Zheng and Yuyu Zhang and Yijie Zhu and Chenguang Xi and Pengyang Gao and Xun Zhou and Kevin Chen-Chuan Chang},
  year={2023},
  booktitle={NAACL-HLT},
  doi={10.48550/arXiv.2309.16583},
  url={https://www.semanticscholar.org/paper/532430bfcedf0ca4d5ca695967b52fc21cb5b778},
  abstract={With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capability can be improved by SFT and RLHF, how much is the alignment tax, etc. Our analysis sheds light on many of these questions, aiming to improve the transparency of advanced LLMs.},
  keywords={arxiv:2309.16583}
}

@article{wan2023gptreincontext,
  title={GPT-RE: In-context Learning for Relation Extraction using Large Language Models},
  author={Zhen Wan and Fei Cheng and Zhuoyuan Mao and Qianying Liu and Haiyue Song and Jiwei Li and S. Kurohashi},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.02105},
  url={https://www.semanticscholar.org/paper/f2cd02c03d0169374442d9bc227c9aed178f4b20},
  abstract={In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE: (1) low relevance regarding entity and relation in retrieved demonstrations for in-context learning; and (2) the strong inclination to wrongly classify NULL examples into other pre-defined labels. In this paper, we propose GPT-RE to bridge the gap between LLMs and fully-supervised baselines. GPT-RE successfully addresses the aforementioned issues by (1) incorporating task-specific entity representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE datasets, and observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE achieves SOTA performances on the Semeval and SciERC datasets, and competitive performances on the TACRED and ACE05 datasets.},
  keywords={arxiv:2305.02105}
}

@article{roberts2023gpt4geolanguage,
  title={GPT4GEO: How a Language Model Sees the World's Geography},
  author={Jonathan Roberts and T. Luddecke and Sowmen Das and Kai Han and Samuel Albanie},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2306.00020},
  url={https://www.semanticscholar.org/paper/714740e38dbb0642475ff3eae5681ae0a4103670},
  abstract={Large language models (LLMs) have shown remarkable capabilities across a broad range of tasks involving question answering and the generation of coherent text and code. Comprehensively understanding the strengths and weaknesses of LLMs is beneficial for safety, downstream applications and improving performance. In this work, we investigate the degree to which GPT-4 has acquired factual geographic knowledge and is capable of using this knowledge for interpretative reasoning, which is especially important for applications that involve geographic data, such as geospatial analysis, supply chain management, and disaster response. To this end, we design and conduct a series of diverse experiments, starting from factual tasks such as location, distance and elevation estimation to more complex questions such as generating country outlines and travel networks, route finding under constraints and supply chain analysis. We provide a broad characterisation of what GPT-4 (without plugins or Internet access) knows about the world, highlighting both potentially surprising capabilities but also limitations.},
  keywords={arxiv:2306.00020}
}

@article{zhang2023gpt4roiinstruction,
  title={GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest},
  author={Shilong Zhang and Pei Sun and Shoufa Chen and Min Xiao and Wenqi Shao and Wenwei Zhang and Kai Chen and Ping Luo},
  year={2023},
  booktitle={ECCV Workshops},
  doi={10.48550/arXiv.2307.03601},
  url={https://www.semanticscholar.org/paper/094883e42bb9a41f602c0715c1059bc431e33fb2},
  abstract={Visual instruction tuning large language model(LLM) on image-text pairs has achieved general-purpose vision-language abilities. However, the lack of region-text pairs limits their advancements to fine-grained multimodal understanding. In this paper, we propose spatial instruction tuning, which introduces the reference to the region-of-interest(RoI) in the instruction. Before sending to LLM, the reference is replaced by RoI features and interleaved with language embeddings as a sequence. Our model GPT4RoI, trained on 7 region-text pair datasets, brings an unprecedented interactive and conversational experience compared to previous image-level models. (1) Interaction beyond language: Users can interact with our model by both language and drawing bounding boxes to flexibly adjust the referring granularity. (2) Versatile multimodal abilities: A variety of attribute information within each RoI can be mined by GPT4RoI, e.g., color, shape, material, action, etc. Furthermore, it can reason about multiple RoIs based on common sense. On the Visual Commonsense Reasoning(VCR) dataset, GPT4RoI achieves a remarkable accuracy of 81.6\%, surpassing all existing models by a significant margin (the second place is 75.6\%) and almost reaching human-level performance of 85.0\%. The code and model can be found at https://github.com/jshilong/GPT4RoI.},
  keywords={arxiv:2307.03601}
}

@article{wang2023geminireasoning,
  title={Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language Models},
  author={Yuqing Wang and Yun Zhao},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.17661},
  url={https://www.semanticscholar.org/paper/557182159154a0478b50f19838767ebb1749db0d},
  abstract={The burgeoning interest in Multimodal Large Language Models (MLLMs), such as OpenAI's GPT-4V(ision), has significantly impacted both academic and industrial realms. These models enhance Large Language Models (LLMs) with advanced visual understanding capabilities, facilitating their application in a variety of multimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM designed specifically for multimodal integration. Despite its advancements, preliminary benchmarks indicate that Gemini lags behind GPT models in commonsense reasoning tasks. However, this assessment, based on a limited dataset (i.e., HellaSWAG), does not fully capture Gemini's authentic commonsense reasoning potential. To address this gap, our study undertakes a thorough evaluation of Gemini's performance in complex reasoning tasks that necessitate the integration of commonsense knowledge across modalities. We carry out a comprehensive analysis of 12 commonsense reasoning datasets, ranging from general to domain-specific tasks. This includes 11 datasets focused solely on language, as well as one that incorporates multimodal elements. Our experiments across four LLMs and two MLLMs demonstrate Gemini's competitive commonsense reasoning capabilities. Additionally, we identify common challenges faced by current LLMs and MLLMs in addressing commonsense problems, underscoring the need for further advancements in enhancing the commonsense reasoning abilities of these models.},
  keywords={arxiv:2312.17661}
}

@article{kuckreja2023geochatgroundedlarge,
  title={GeoChat:Grounded Large Vision-Language Model for Remote Sensing},
  author={Kartik Kuckreja and M. S. Danish and Muzammal Naseer and Abhijit Das and Salman H. Khan and F. Khan},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.02629},
  url={https://www.semanticscholar.org/paper/d0a5a6a5b5540967d6f12651aebd446e4c0dc807},
  abstract={Recent advancements in Large Vision-Language Models (VLMs) have shown great promise in natural image domains, allowing users to hold a dialogue about given visual content. However, such general-domain VLMs perform poorly for Remote Sensing (RS) scenarios, leading to inaccurate or fabricated information when presented with RS domain-specific queries. Such a behavior emerges due to the unique challenges introduced by RS imagery. For example, to handle high-resolution RS imagery with diverse scale changes across categories and many small objects, region-level reasoning is necessary alongside holistic scene inter-pretation. Furthermore, the lack of domain-specific multimodal instruction following data as well as strong back-bone models for RS make it hard for the models to align their behavior with user queries. To address these limitations, we propose GeoChat - the first versatile remote sensing VLM that offers multitask conversational capabilities with high-resolution RS images. Specifically, GeoChat can not only answer image-level queries but also accepts region inputs to hold region-specific dialogue. Further-more, it can visually ground objects in its responses by referring to their spatial coordinates. To address the lack of domain-specific datasets, we generate a novel RS multimodal instruction-following dataset by extending image-text pairs from existing diverse RS datasets. We establish a comprehensive benchmarkfor RS multitask conversations and compare with a number of baseline methods. GeoChat demonstrates robust zero-shot performance on various RS tasks, e.g., image and region captioning, visual question answering, scene classification, visually grounded conversations and referring detection. Our code is available here.},
  keywords={arxiv:2311.15826}
}

@article{kazemi2023geomversesystematic,
  title={GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning},
  author={Mehran Kazemi and Hamidreza Alvari and Ankit Anand and Jialin Wu and Xi Chen and Radu Soricut},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.12241},
  url={https://www.semanticscholar.org/paper/608a2b333fd8262e8c918f36c5700bafd3ea3cdd},
  abstract={Large language models have shown impressive results for multi-hop mathematical reasoning when the input question is only textual. Many mathematical reasoning problems, however, contain both text and image. With the ever-increasing adoption of vision language models (VLMs), understanding their reasoning abilities for such problems is crucial. In this paper, we evaluate the reasoning capabilities of VLMs along various axes through the lens of geometry problems. We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation. The empirical results obtained using our benchmark for state-of-the-art VLMs indicate that these models are not as capable in subjects like geometry (and, by generalization, other topics requiring similar reasoning) as suggested by previous benchmarks. This is made especially clear by the construction of our benchmark at various depth levels, since solving higher-depth problems requires long chains of reasoning rather than additional memorized knowledge. We release the dataset for further research in this area.},
  keywords={arxiv:2312.12241}
}

@article{si2023gettingmore,
  title={Getting MoRE out of Mixture of Language Model Reasoning Experts},
  author={Chenglei Si and Weijia Shi and Chen Zhao and Luke Zettlemoyer and Jordan L. Boyd-Graber},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.findings-emnlp.552},
  url={https://www.semanticscholar.org/paper/7283d616e40d7ab7422e3697218f3fc42f292bf2},
  abstract={While recent large language models (LLMs) improve on various question answering (QA) datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt. To remedy this, we propose a Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diverse specialized language models. We specialize the backbone language model with prompts optimized for different reasoning categories, including factual, multihop, mathematical, and commonsense reasoning. Our key insight is to leverage agreement among the specialized experts to select the best answer for each question, or to abstain from answering. This gives MoRE higher accuracy than any single specialized model on a collection of 12 QA datasets from four reasoning types. Beyond generalizability, the interpretable design of MoRE improves selective question answering results compared to baselines without incorporating inter-expert agreement. This framework is also more interpretable and useful to human consumers of QA outputs. Our human study confirms that presenting expert predictions and the answer selection process helps annotators more accurately calibrate when to trust the system's output. We release all code and data to facilitate future work.},
  keywords={arxiv:2305.14628}
}

@article{yang2023givefacts,
  title={Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling},
  author={Lin F. Yang and Hongyang Chen and Zhao Li and Xiao Ding and Xindong Wu},
  year={2023},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  doi={10.1109/TKDE.2024.3360454},
  url={https://www.semanticscholar.org/paper/02033e83ff310f35e4623bd339982c52d926f2d5},
  abstract={Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention. Due to their powerful emergent abilities, recent LLMs are considered as a possible alternative to structured knowledge bases like knowledge graphs (KGs). However, while LLMs are proficient at learning probabilistic language patterns and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance in generating texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes enhancing LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs’ factual reasoning ability, opening up new avenues for LLM research.},
  keywords={arxiv:2306.11489}
}

@article{patil2023gorillalarge,
  title={Gorilla: Large Language Model Connected with Massive APIs},
  author={Shishir G. Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.52202/079017-4020},
  url={https://www.semanticscholar.org/paper/7d8905a1fd288068f12c8347caeabefd36d0dd6c},
  abstract={Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu},
  keywords={arxiv:2305.15334}
}

@article{tian2023graphneural,
  title={Graph Neural Prompting with Large Language Models},
  author={Yijun Tian and Huan Song and Zichen Wang and Haozhu Wang and Ziqing Hu and Fang Wang and N. Chawla and Panpan Xu},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2309.15427},
  url={https://www.semanticscholar.org/paper/9a4e4ab77c3d836bab35e0578de68e8ce79af1e8},
  abstract={Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available at https://github.com/meettyj/GNP.},
  keywords={arxiv:2309.15427}
}

@article{besta2023graphthoughts,
  title={Graph of Thoughts: Solving Elaborate Problems with Large Language Models},
  author={Maciej Besta and Nils Blach and Aleš Kubíček and Robert Gerstenberger and Lukas Gianinazzi and Joanna Gajda and Tomasz Lehmann and Michal Podstawski and H. Niewiadomski and P. Nyczyk and Torsten Hoefler},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.1609/aaai.v38i16.29720},
  url={https://www.semanticscholar.org/paper/aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3},
  abstract={We introduce Graph of Thoughts (GoT): a framework that
advances prompting capabilities in large language models
(LLMs) beyond those offered by paradigms such as 
Chain-of-Thought or Tree of Thoughts (ToT). The key idea and 
primary advantage of GoT is the ability to model the information 
generated by an LLM as an arbitrary graph, where units of 
information ("LLM thoughts") are vertices, and edges correspond
to dependencies between these vertices. This approach enables 
combining arbitrary LLM thoughts into synergistic outcomes, 
distilling the essence of whole networks of thoughts,
or enhancing thoughts using feedback loops. We illustrate
that GoT offers advantages over state of the art on different
tasks, for example increasing the quality of sorting by 62\%
over ToT, while simultaneously reducing costs by >31\%.
We ensure that GoT is extensible with new thought 
transformations and thus can be used to spearhead new prompting
schemes. This work brings the LLM reasoning closer to human 
thinking or brain mechanisms such as recurrence, both
of which form complex networks},
  keywords={arxiv:2308.09687}
}

@article{zhang2023graphtoolformerempower,
  title={Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT},
  author={Jiawei Zhang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2304.11116},
  url={https://www.semanticscholar.org/paper/0d502a1e300336ae628f5c8b99ee4d3766c8f60b},
  abstract={In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing \{multi-step logic reasoning\}, \{precise mathematical calculation\} and \{perception about the spatial and temporal factors\}. To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools. Specifically, we will investigate to teach Graph-ToolFormer to handle various graph data reasoning tasks in this paper, including both (1) very basic graph data loading and graph property reasoning tasks, ranging from simple graph order and size to the graph diameter and periphery, and (2) more advanced reasoning tasks on real-world graph data, such as bibliographic networks, protein molecules, sequential recommender systems, social networks and knowledge graphs.},
  keywords={arxiv:2304.11116}
}

@article{chai2023graphllmboosting,
  title={GraphLLM: Boosting Graph Reasoning Ability of Large Language Model},
  author={Ziwei Chai and Tianjie Zhang and Liang Wu and Kaiqiao Han and Xiaohai Hu and Xuanwen Huang and Yang Yang},
  year={2023},
  journal={IEEE Transactions on Big Data},
  doi={10.48550/arXiv.2310.05845},
  url={https://www.semanticscholar.org/paper/062fab31d30478b57457c8b7a94d7467f5bd770c},
  abstract={The advancement of Large Language Models (LLMs) has remarkably pushed the boundaries towards artificial general intelligence (AGI), with their exceptional ability on understanding diverse types of information, including but not limited to images and audio. Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks. In this paper, we endeavor to unearth the obstacles that impede LLMs in graph reasoning, pinpointing the common practice of converting graphs into natural language descriptions (Graph2Text) as a fundamental bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs. This synergy equips LLMs with the ability to proficiently interpret and reason on graph data, harnessing the superior expressive power of graph learning models. Our empirical evaluations across four fundamental graph reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a substantial average accuracy enhancement of 54.44\%, alongside a noteworthy context reduction of 96.45\% across various graph reasoning tasks.},
  keywords={arxiv:2310.05845}
}

@article{cao2023graphreasonenhancing,
  title={GraphReason: Enhancing Reasoning Capabilities of Large Language Models through A Graph-Based Verification Approach},
  author={Lang Cao},
  year={2023},
  booktitle={NLRSE},
  doi={10.18653/v1/2024.nlrse-1.1},
  url={https://www.semanticscholar.org/paper/1a60d3f243c785dc7f4a2b14b8b14b2cbca6a8a6},
  abstract={Large Language Models (LLMs) have showcased impressive reasoning capabilities, particularly when guided by specifically designed prompts in complex reasoning tasks such as math word problems. These models typically solve tasks using a chain-of-thought approach, which not only bolsters their reasoning abilities but also provides valuable insights into their problem-solving process. However, there is still significant room for enhancing the reasoning abilities of LLMs. Some studies suggest that the integration of an LLM output verifier can boost reasoning accuracy without necessitating additional model training. In this paper, we follow these studies and introduce a novel graph-based method to further augment the reasoning capabilities of LLMs. We posit that multiple solutions to a reasoning task, generated by an LLM, can be represented as a reasoning graph due to the logical connections between intermediate steps from different reasoning paths. Therefore, we propose the Reasoning Graph Verifier (GraphReason) to analyze and verify the solutions generated by LLMs. By evaluating these graphs, models can yield more accurate and reliable results.Our experimental results show that our graph-based verification method not only significantly enhances the reasoning abilities of LLMs but also outperforms existing verifier methods in terms of improving these models’ reasoning performance.},
  keywords={arxiv:2308.09267}
}

@article{wang2023guidinglanguage,
  title={Guiding Language Model Reasoning with Planning Tokens},
  author={Xinyi Wang and Lucas Caccia and O. Ostapenko and Xingdi Yuan and Alessandro Sordoni},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.05707},
  url={https://www.semanticscholar.org/paper/b29134737a0c81c13d31fc0263b3c4d4f05ccb78},
  abstract={Large language models (LLMs) have recently attracted considerable interest for their ability to perform complex reasoning tasks, such as chain-of-thought (CoT) reasoning. However, most of the existing approaches to enhance this ability rely heavily on data-driven methods, while neglecting the structural aspects of the model's reasoning capacity. To encourage a more structural generation of CoT steps, we propose a hierarchical generation scheme: we let the LM generate a planning token at the start of each reasoning step, intuitively serving as a high-level plan of the current step, and add their embeddings to the model parameters. Our approach requires a negligible increase in trainable parameters (0.001\%) and can be applied through either full fine-tuning or a more parameter-efficient scheme. We demonstrate our method's effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets and one multihop QA dataset with respect to standard fine-tuning baselines.},
  keywords={arxiv:2310.05707}
}

@article{li2023guidinglarge,
  title={Guiding Large Language Models via Directional Stimulus Prompting},
  author={Zekun Li and Baolin Peng and Pengcheng He and Michel Galley and Jianfeng Gao and Xi Yan},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2302.11520},
  url={https://www.semanticscholar.org/paper/b0435af3063195e8ae880489e64ccde64e6d7563},
  abstract={We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4\%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available at \textbackslash\{\}url\{https://github.com/Leezekun/Directional-Stimulus-Prompting\}.},
  keywords={arxiv:2302.11520}
}

@article{son2023haeraebench,
  title={HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models},
  author={Guijin Son and Hanwool Albert Lee and Suwan Kim and Jaecheol Lee and Je Won Yeom and Jihyu Jung and Jung Woo Kim and Songseong Kim},
  year={2023},
  booktitle={International Conference on Language Resources and Evaluation},
  doi={10.48550/arXiv.2309.02706},
  url={https://www.semanticscholar.org/paper/b118deb1e0715a5aeec11e399321501ba09ce2c4},
  abstract={Large language models (LLMs) trained on massive corpora demonstrate impressive capabilities in a wide range of tasks. While there are ongoing efforts to adapt these models to languages beyond English, the attention given to their evaluation methodologies remains limited. Current multilingual benchmarks often rely on back translations or re-implementations of English tests, limiting their capacity to capture unique cultural and linguistic nuances. To bridge this gap for the Korean language, we introduce the HAE-RAE Bench, a dataset curated to challenge models lacking Korean cultural and contextual depth. The dataset encompasses six downstream tasks across four domains: vocabulary, history, general knowledge, and reading comprehension. Unlike traditional evaluation suites focused on token and sequence classification or mathematical and logical reasoning, the HAE-RAE Bench emphasizes a model’s aptitude for recalling Korean-specific knowledge and cultural contexts. Comparative analysis with prior Korean benchmarks indicates that the HAE-RAE Bench presents a greater challenge to non-Korean models by disturbing abilities and knowledge learned from English being transferred.},
  keywords={arxiv:2309.02706}
}

@article{he2023hitombenchmark,
  title={HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models},
  author={Yinghui He and Yufan Wu and Yilin Jia and Rada Mihalcea and Yulong Chen and Naihao Deng},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.16755},
  url={https://www.semanticscholar.org/paper/2361bae8f0ff3627a91408c172e6612b4d554cf2},
  abstract={Theory of Mind (ToM) is the ability to reason about one's own and others' mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others' beliefs. We introduce HI-TOM, a Higher Order Theory of Mind benchmark. Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.},
  keywords={arxiv:2310.16755}
}

@article{loh2023harnessinglarge,
  title={Harnessing Large Language Models' Empathetic Response Generation Capabilities for Online Mental Health Counselling Support},
  author={Siyuan Brandon Loh and Aravind Sesagiri Raamkumar},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.08017},
  url={https://www.semanticscholar.org/paper/88a3abf671d922ebd61a34007908a5f6b6978bd4},
  abstract={Large Language Models (LLMs) have demonstrated remarkable performance across various information-seeking and reasoning tasks. These computational systems drive state-of-the-art dialogue systems, such as ChatGPT and Bard. They also carry substantial promise in meeting the growing demands of mental health care, albeit relatively unexplored. As such, this study sought to examine LLMs' capability to generate empathetic responses in conversations that emulate those in a mental health counselling setting. We selected five LLMs: version 3.5 and version 4 of the Generative Pre-training (GPT), Vicuna FastChat-T5, Pathways Language Model (PaLM) version 2, and Falcon-7B-Instruct. Based on a simple instructional prompt, these models responded to utterances derived from the EmpatheticDialogues (ED) dataset. Using three empathy-related metrics, we compared their responses to those from traditional response generation dialogue systems, which were fine-tuned on the ED dataset, along with human-generated responses. Notably, we discovered that responses from the LLMs were remarkably more empathetic in most scenarios. We position our findings in light of catapulting advancements in creating empathetic conversational systems.},
  keywords={arxiv:2310.08017}
}

@article{yang2023harnessingpower,
  title={Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation},
  author={Yuan Yang and Siheng Xiong and Ali Payani and Ehsan Shareghi and F. Fekri},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2305.15541},
  url={https://www.semanticscholar.org/paper/e4e625f8e8ae5ee82e75de5ad6e07af57cca7f53},
  abstract={Translating natural language sentences to first-order logic (NL-FOL translation) is a longstanding challenge in the NLP and formal logic literature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for NL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of directly translating natural language into FOL rules, which outperforms GPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5, and can achieve similar performance as GPT-4 with a fraction of the cost. This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model. To train LogicLLaMA, we present MALLS (large language \$\textbackslash\{\}textbf\{M\}\$odel gener\$\textbackslash\{\}textbf\{A\}\$ted N\$\textbackslash\{\}textbf\{L\}\$-FO\$\textbackslash\{\}textbf\{L\}\$ pair\$\textbackslash\{\}textbf\{S\}\$), a dataset of 34K high-quality and diverse sentence-level NL-FOL pairs collected from GPT-4. The dataset was created by implementing a pipeline that prompts GPT-4 for pairs, and dynamically adjusts the prompts to ensure the collection of pairs with rich and diverse contexts at different levels of complexity, and verifies the validity of the generated FOL rules. Codes, weights, and data are available at \$\textbackslash\{\}href\{https://github.com/gblackout/LogicLLaMA\}\{\{\textbackslash\{\}small \textbackslash\{\}text\{https://github.com/gblackout/LogicLLaMA\}\}\}\$.},
  keywords={arxiv:2305.15541}
}

@article{arora2023havellms,
  title={Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models},
  author={Daman Arora and H. Singh and Mausam},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.15074},
  url={https://www.semanticscholar.org/paper/2cf1f6c723006f258599fd9f000bb616ae83387a},
  abstract={The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40\%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.},
  keywords={arxiv:2305.15074}
}

@article{dong2023abilitieslarge,
  title={How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition},
  author={Guanting Dong and Hongyi Yuan and Keming Lu and Chengpeng Li and Mingfeng Xue and Dayiheng Liu and Wei Wang and Zheng Yuan and Chang Zhou and Jingren Zhou},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2310.05492},
  url={https://www.semanticscholar.org/paper/5088a04d1a9f42b967f3dcf791145e8aa367fc54},
  abstract={Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specifically focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.},
  keywords={arxiv:2310.05492}
}

@article{lin2023recommendersystems,
  title={How Can Recommender Systems Benefit from Large Language Models: A Survey},
  author={Jianghao Lin and Xinyi Dai and Yunjia Xi and Weiwen Liu and Bo Chen and Xiangyang Li and Chenxu Zhu and Huifeng Guo and Yong Yu and Ruiming Tang and Weinan Zhang},
  year={2023},
  booktitle={ACM Trans. Inf. Syst.},
  doi={10.1145/3678004},
  url={https://www.semanticscholar.org/paper/bac54736112098616f0e1c90435888ef3e119d32},
  abstract={With the rapid development of online services and web applications, recommender systems (RS) have become increasingly indispensable for mitigating information overload and matching users’ information needs by providing personalized suggestions over items. Although the RS research community has made remarkable progress over the past decades, conventional recommendation models (CRM) still have some limitations, e.g., lacking open-domain world knowledge, and difficulties in comprehending users’ underlying preferences and motivations. Meanwhile, large language models (LLM) have shown impressive general intelligence and human-like capabilities for various natural language processing (NLP) tasks, which mainly stem from their extensive open-world knowledge, logical and commonsense reasoning abilities, as well as their comprehension of human culture and society. Consequently, the emergence of LLM is inspiring the design of RS and pointing out a promising research direction, i.e., whether we can incorporate LLM and benefit from their common knowledge and capabilities to compensate for the limitations of CRM. In this article, we conduct a comprehensive survey on this research direction, and draw a bird’s-eye view from the perspective of the whole pipeline in real-world RS. Specifically, we summarize existing research works from two orthogonal aspects: where and how to adapt LLM to RS. For the “WHERE” question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, user interaction, and pipeline controller. For the “HOW” question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLM or not during training, and whether to involve CRM for inference. Detailed analysis and general development paths are provided for both “WHERE” and “HOW” questions, respectively. Then, we highlight the key challenges in adapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and ethics. Finally, we summarize the survey and discuss the future prospects.},
  keywords={arxiv:2306.05817}
}

@article{chu2023protectcopyright,
  title={How to Protect Copyright Data in Optimization of Large Language Models?},
  author={T. Chu and Zhao Song and Chiwun Yang},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2308.12247},
  url={https://www.semanticscholar.org/paper/761af20e7966e8a7d899975a332ac7eee3f92116},
  abstract={Large language models (LLMs) and generative AI have played a transformative role in computer research and applications. Controversy has arisen as to whether these models output copyrighted data, which can occur if the data the models are trained on is copyrighted. LLMs are built on the transformer neural network architecture, which in turn relies on a mathematical computation called Attention that uses the softmax function.

In this paper, we observe that large language model training and optimization can be seen as a softmax regression problem. We then establish a method of efficiently performing softmax regression, in a way that prevents the regression function from generating copyright data. This establishes a theoretical method of training large language models in a way that avoids generating copyright data.},
  keywords={arxiv:2308.12247}
}

@article{zhou2023informinformation,
  title={INFORM : Information eNtropy based multi-step reasoning FOR large language Models},
  author={Chuyue Zhou and Wangjie You and Juntao Li and Jing Ye and Kehai Chen and Min Zhang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.emnlp-main.216},
  url={https://www.semanticscholar.org/paper/d42e04e2650b85495aa695a90aaf437b5ad90516},
  abstract={,}
}

@article{you2023idealgptiteratively,
  title={IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models},
  author={Haoxuan You and Rui Sun and Zhecan Wang and Long Chen and Gengyu Wang and Hammad A. Ayyubi and Kai-Wei Chang and Shih-Fu Chang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.14985},
  url={https://www.semanticscholar.org/paper/7cf64070fd3d7e53d80f260c10e6bd7018d580e1},
  abstract={The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10\% on VCR and 15\% on SNLI-VE. Code is available at https://github.com/Hxyou/IdealGPT},
  keywords={arxiv:2305.14985}
}

@article{lahoti2023improvingdiversity,
  title={Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting},
  author={Preethi Lahoti and Nicholas Blumm and Xiao Ma and Raghavendra Kotikalapudi and Sahitya Potluri and Qijun Tan and Hansa Srinivasan and Ben Packer and Ahmad Beirami and Alex Beutel and Jilin Chen},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.16523},
  url={https://www.semanticscholar.org/paper/9c893f54d86a362b8e62e5883bb38c14240441f5},
  abstract={A crucial challenge for generative large language models (LLMs) is diversity: when a user's prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being under-represented or even erased from the generated responses. In this paper, we formalize diversity of representation in generative LLMs. We present evaluation datasets and propose metrics to measure diversity in generated responses along people and culture axes. We find that LLMs understand the notion of diversity, and that they can reason and critique their own responses for that goal. This finding motivated a new prompting technique called collective-critique and self-voting (CCSV) to self-improve people diversity of LLMs by tapping into its diversity reasoning capabilities, without relying on handcrafted examples or prompt tuning. Extensive empirical experiments with both human and automated evaluations show that our proposed approach is effective at improving people and culture diversity, and outperforms all baseline methods by a large margin.},
  keywords={arxiv:2310.16523}
}

@article{du2023improvingfactuality,
  title={Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author={Yilun Du and Shuang Li and A. Torralba and J. Tenenbaum and Igor Mordatch},
  year={2023},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2305.14325},
  url={https://www.semanticscholar.org/paper/4780d0a027c5c5a8e01d7cf697f6296880ffc945},
  abstract={Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such"society of minds"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.},
  keywords={arxiv:2305.14325}
}

@article{lan2023improvingzeroshot,
  title={Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts},
  author={Yunshi Lan and Xiang Li and Xin Liu and Yang Li and Wei Qin and Weining Qian},
  year={2023},
  booktitle={ACM Multimedia},
  doi={10.1145/3581783.3612389},
  url={https://www.semanticscholar.org/paper/af44c205c648c21d06064b23613dd60ecbd4adf8},
  abstract={Zero-shot Visual Question Answering (VQA) is a prominent vision-language task that examines both the visual and textual understanding capability of systems in the absence of training data. Recently, by converting the images into captions, information across multi-modalities is bridged and Large Language Models (LLMs) can apply their strong zero-shot generalization capability to unseen questions. To design ideal prompts for solving VQA via LLMs, several studies have explored different strategies to select or generate question-answer pairs as the exemplar prompts, which guide LLMs to answer the current questions effectively. However, they totally ignore the role of question prompts. The original questions in VQA tasks usually encounter ellipses and ambiguity which require intermediate reasoning. To this end, we present Reasoning Question Prompts for VQA tasks, which can further activate the potential of LLMs in zero-shot scenarios. Specifically, for each question, we first generate self-contained questions as reasoning question prompts via an unsupervised question edition module considering sentence fluency, semantic integrity and syntactic invariance. Each reasoning question prompt clearly indicates the intent of the original question. This results in a set of candidate answers. Then, the candidate answers associated with their confidence scores acting as answer heuristics are fed into LLMs and produce the final answer. We evaluate reasoning question prompts on three VQA challenges, experimental results demonstrate that they can significantly improve the results of LLMs on zero-shot setting and outperform existing state-of-the-art zero-shot methods on three out of four data sets. Our source code is publicly released at https://github.com/ECNU-DASE-NLP/RQP.},
  keywords={arxiv:2311.09050}
}

@article{salewski2023incontextimpersonation,
  title={In-Context Impersonation Reveals Large Language Models' Strengths and Biases},
  author={Leonard Salewski and Stephan Alaniz and Isabel Rio-Torto and Eric Schulz and Zeynep Akata},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2305.14930},
  url={https://www.semanticscholar.org/paper/19c63eade265d8a47d160098d97194b3b83d3770},
  abstract={In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases.},
  keywords={arxiv:2305.14930}
}

@article{han2023inductivereasoning,
  title={Inductive reasoning in humans and large language models},
  author={Simon J. Han and Keith Ransom and Andrew Perfors and Charles Kemp},
  year={2023},
  booktitle={Cognitive Systems Research},
  doi={10.1016/j.cogsys.2023.101155},
  url={https://www.semanticscholar.org/paper/1727c04a73ca205bf9fbfd56466f8e0da6d11433},
  keywords={arxiv:2306.06548}
}

@misc{han2023infimmevalcomplex,
  title={InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models},
  author={Xiaotian Han and Quanzeng You and Yongfei Liu and Wentao Chen and Huangjie Zheng and Khalil Mrini and Xudong Lin and Yiqi Wang and Bohan Zhai and Jianbo Yuan and Heng Wang and Hongxia Yang},
  year={2023},
  url={https://www.semanticscholar.org/paper/0c7ca22dfbe1ce02cb0f0658292499457db8ec6e},
  abstract={Multi-modal Large Language Models (MLLMs) are increasingly prominent in the field of artificial intelligence. These models not only excel in traditional vision-language tasks but also demonstrate impressive performance in contemporary multi-modal benchmarks. Although many of these benchmarks attempt to holistically evaluate MLLMs, they typically concentrate on basic reasoning tasks, often yielding only simple yes/no or multi-choice responses. These methods naturally lead to confusion and difficulties in conclusively determining the reasoning capabilities of MLLMs. To mitigate this issue, we manually curate a benchmark dataset specifically designed for MLLMs, with a focus on complex reasoning tasks. Our benchmark comprises three key reasoning categories: deductive, abductive, and analogical reasoning. The queries in our dataset are intentionally constructed to engage the reasoning capabilities of MLLMs in the process of generating answers. For a fair comparison across various MLLMs, we incorporate intermediate reasoning steps into our evaluation criteria. In instances where an MLLM is unable to produce a definitive answer, its reasoning ability is evaluated by requesting intermediate reasoning steps. If these steps align with our manual annotations, appropriate scores are assigned. This evaluation scheme resembles methods commonly used in human assessments, such as exams or assignments, and represents what we consider a more effective assessment technique compared with existing benchmarks. We evaluate a selection of representative MLLMs using this rigorously developed open-ended multi-step elaborate reasoning benchmark, designed to challenge and accurately measure their reasoning capabilities. The code and data will be released at https://infimm.github.io/InfiMM-Eval/},
  keywords={arxiv:2311.11567}
}

@article{chia2023instructevaltowards,
  title={InstructEval: Towards Holistic Evaluation of Instruction-Tuned Large Language Models},
  author={Yew Ken Chia and Pengfei Hong and Lidong Bing and Soujanya Poria},
  year={2023},
  booktitle={SCALELLM},
  doi={10.48550/arXiv.2306.04757},
  url={https://www.semanticscholar.org/paper/e47e63781c0e7a2c0504b9381b76b5d01b62c53d},
  abstract={Instruction-tuned large language models have revolutionized natural language processing and have shown great potential in applications such as conversational agents. These models, such as GPT-4, can not only master language but also solve complex tasks in areas like mathematics, coding, medicine, and law. However, there is still a lack of comprehensive understanding regarding their full potential, primarily due to the black-box nature of many models and lack of holistic evaluation. To address these challenges, we present InstructEval, a more comprehensive evaluation suite designed specifically for instruction-tuned large language models. Unlike previous works, our evaluation involves a rigorous assessment of models based on problem-solving, writing ability, and alignment to human values. We take a holistic approach to analyze various factors affecting model performance, including the pretraining foundation, instruction-tuning data, and training methods. Our findings reveal that the quality of instruction data is a crucial factor in scaling model performance. While open-source models demonstrate impressive writing abilities, there is substantial room for improvement in problem-solving and alignment.},
  keywords={arxiv:2306.04757}
}

@article{zhang2023integratingautomated,
  title={Integrating Automated Knowledge Extraction with Large Language Models for Explainable Medical Decision-Making},
  author={Haodi Zhang and Jiahong Li and Yichi Wang and Yuanfeng Song},
  year={2023},
  booktitle={IEEE International Conference on Bioinformatics and Biomedicine},
  doi={10.1109/BIBM58861.2023.10385557},
  url={https://www.semanticscholar.org/paper/6841c2dde56562350359235db1179c41097acd80},
  abstract={Large language models (LLMs) have demonstrated strong reasoning ability and inspired many previously unimaginable applications. In this paper, we aim to harness the strong reasoning capability of LLMs toward explainable medical diagnosis. As we know, deep learning has been widely adopted and shown improvement in medical diagnostics. However, it is often criticized for its lack of interpretability. To address this drawback, we propose the first method that innovatively combines Markov logic networks (MLNs) with external knowledge extracted using LLMs, aiming for improved both interpretability and accuracy. Specifically, our approach involves a new process, powered by LLMs and a search engine, to automatically collect and organize external medical knowledge. The outcome is a set of first-order logic (FOL) rules, which then become a key component for the following MLN-based diagnostic algorithm. The resulting MLN-based model can maintain the accuracy of deep networks while providing understandable reasoning for its decisions. By aiming to blend specific knowledge from the medical domain with LLM techniques, our work contributes towards the development of improved automatic diagnosis systems, with the potential for enhancing transparency and trust in medical diagnostics.}
}

@article{pan2023integratinggraphs,
  title={Integrating Graphs With Large Language Models: Methods and Prospects},
  author={Shirui Pan and Yizhen Zheng and Yixin Liu and San Murugesan},
  year={2023},
  booktitle={IEEE Intelligent Systems},
  doi={10.1109/MIS.2023.3332242},
  url={https://www.semanticscholar.org/paper/89ba59ab6b086d86ce27df32652b7499498d70fb},
  abstract={Large language models (LLMs) such as Generative Pre-trained Transformer 4 have emerged as frontrunners, showcasing unparalleled prowess in diverse applications including answering queries, code generation, and more. Parallelly, graph-structured data, intrinsic data types, are pervasive in real-world scenarios. Merging the capabilities of LLMs with graph-structured data has been a topic of keen interest. This article bifurcates such integrations into two predominant categories. The first leverages LLMs for graph learning, where LLMs can not only augment existing graph algorithms but also stand as prediction models for various graph tasks. Conversely, the second category underscores the pivotal role of graphs in advancing LLMs. Mirroring human cognition, we solve complex tasks by adopting graphs in either reasoning or collaboration. Integrating with such structures can significantly boost the performance of LLMs in various complicated tasks. We also discuss and propose open questions for integrating LLMs with graph-structured data for the future direction of the field.},
  keywords={arxiv:2310.05499}
}

@article{ban2023integratinglarge,
  title={Integrating Large Language Model for Improved Causal Discovery},
  author={Taiyu Ban and Lyuzhou Chen and Derui Lyu and Xiangyu Wang and Qinrui Zhu and Qiang Tu and Huanhuan Chen},
  year={2023},
  journal={IEEE Transactions on Artificial Intelligence},
  doi={10.1109/TAI.2025.3560927},
  url={https://www.semanticscholar.org/paper/37a25ec621139e98498252874b142c6ef3730e2a},
  abstract={Recovering the structure of causal graphical models from observational data is an essential yet challenging task for causal discovery in scientific scenarios. Domain-specific causal discovery usually relies on expert validation or prior analysis to improve the reliability of recovered causality, which is yet limited by the scarcity of expert resources. Recently, large language models (LLM) have been used for causal analysis across various domain-specific scenarios, suggesting its potential as autonomous expert roles in guiding data-based structure learning. However, integrating LLMs into causal discovery faces challenges due to inaccuracies in LLM-based reasoning on revealing the actual causal structure. To address this challenge, we propose an error-tolerant LLM-driven causal discovery framework. The error-tolerant mechanism is designed three-fold with sufficient consideration on potential inaccuracies. In the LLM-based reasoning process, an accuracy-oriented prompting strategy restricts causal analysis to a reliable range. Next, a knowledge-to-structure transition aligns LLM-derived causal statements with structural causal interactions. In the structure learning process, the goodness-of-fit to data and adherence to LLM-derived priors are balanced to further address prior inaccuracies. Evaluation of eight real-world causal structures demonstrates the efficacy of our LLM-driven approach in improving data-based causal discovery, along with its robustness to inaccurate LLM-derived priors.},
  keywords={arxiv:2306.16902}
}

@article{sun2023interactiveplanning,
  title={Interactive Planning Using Large Language Models for Partially Observable Robotic Tasks},
  author={Lingfeng Sun and Devesh K. Jha and Chiori Hori and Siddarth Jain and Radu Corcodel and Xinghao Zhu and Masayoshi Tomizuka and Diego Romeres},
  year={2023},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA57147.2024.10610981},
  url={https://www.semanticscholar.org/paper/3581d76349a569c80f331c92710839974308eb1c},
  abstract={Designing robotic agents to perform open vocabulary tasks has been the long-standing goal in robotics and AI. Recently, Large Language Models (LLMs) have achieved impressive results in creating robotic agents for performing open vocabulary tasks. However, planning for these tasks in the presence of uncertainties is challenging as it requires "chain-of-thought" reasoning, aggregating information from the environment, updating state estimates, and generating actions based on the updated state estimates. In this paper, we present an interactive planning technique for partially observable tasks using LLMs. In the proposed method, an LLM is used to collect missing information from the environment using a robot, and infer the state of the underlying problem from collected observations while guiding the robot to perform the required actions. We also use a fine-tuned Llama 2 model via self-instruct and compare its performance against a pre-trained LLM like GPT-4. Results are demonstrated on several tasks in simulation as well as real-world environments.},
  keywords={arxiv:2312.06876}
}

@article{gu2023interleavingpretrained,
  title={Interleaving Pre-Trained Language Models and Large Language Models for Zero-Shot NL2SQL Generation},
  author={Zihui Gu and Ju Fan and Nan Tang and Songyue Zhang and Yu-xin Zhang and Zui Chen and Lei Cao and Guoliang Li and Sam Madden and Xiaoyong Du},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2306.08891},
  url={https://www.semanticscholar.org/paper/71e996ff55b972946b9fe0f88394c19425f5a3ab},
  abstract={Zero-shot NL2SQL is crucial in achieving natural language to SQL that is adaptive to new environments (e.g., new databases, new linguistic phenomena or SQL structures) with zero annotated NL2SQL samples from such environments. Existing approaches either fine-tune pre-trained language models (PLMs) based on annotated data or use prompts to guide fixed large language models (LLMs) such as ChatGPT. PLMs can perform well in schema alignment but struggle to achieve complex reasoning, while LLMs is superior in complex reasoning tasks but cannot achieve precise schema alignment. In this paper, we propose a ZeroNL2SQL framework that combines the complementary advantages of PLMs and LLMs for supporting zero-shot NL2SQL. ZeroNL2SQL first uses PLMs to generate an SQL sketch via schema alignment, then uses LLMs to fill the missing information via complex reasoning. Moreover, in order to better align the generated SQL queries with values in the given database instances, we design a predicate calibration method to guide the LLM in completing the SQL sketches based on the database instances and select the optimal SQL query via an execution-based strategy. Comprehensive experiments show that ZeroNL2SQL can achieve the best zero-shot NL2SQL performance on real-world benchmarks. Specifically, ZeroNL2SQL outperforms the state-of-the-art PLM-based methods by 3.2\% to 13\% and exceeds LLM-based methods by 10\% to 20\% on execution accuracy.},
  keywords={arxiv:2306.08891}
}

@article{cai2023knowledgelarge,
  title={Is Knowledge All Large Language Models Needed for Causal Reasoning?},
  author={Hengrui Cai and Shengjie Liu and Rui Song},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2401.00139},
  url={https://www.semanticscholar.org/paper/df0db04d870e1666a64a9c92688419e7628423e5},
  abstract={This paper explores the causal reasoning of large language models (LLMs) to enhance their interpretability and reliability in advancing artificial intelligence. Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration. We propose a novel causal attribution model that utilizes ``do-operators"for constructing counterfactual scenarios, allowing us to systematically quantify the influence of input numerical data and LLMs' pre-existing knowledge on their causal reasoning processes. Our newly developed experimental setup assesses LLMs' reliance on contextual information and inherent knowledge across various domains. Our evaluation reveals that LLMs' causal reasoning ability mainly depends on the context and domain-specific knowledge provided. In the absence of such knowledge, LLMs can still maintain a degree of causal reasoning using the available numerical data, albeit with limitations in the calculations. This motivates the proposed fine-tuned LLM for pairwise causal discovery, effectively leveraging both knowledge and numerical information.},
  keywords={arxiv:2401.00139}
}

@article{balepur2023easybeing,
  title={It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning},
  author={Nishant Balepur and Shramay Palta and Rachel Rudinger},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2024.findings-acl.604},
  url={https://www.semanticscholar.org/paper/cd9f3efbe5995855e646ea1cfd368a3ed067d1a3},
  abstract={Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.},
  keywords={arxiv:2311.07532}
}

@article{sengupta2023jaisjaischat,
  title={Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models},
  author={Neha Sengupta and Sunil Kumar Sahu and Bokang Jia and Satheesh Katipomu and Haonan Li and Fajri Koto and Osama Mohammed Afzal and Samta Kamboj and O. Pandit and Rahul Pal and Lalit Pradhan and Zain Muhammad Mujahid and Massa Baali and Xudong Han and Alham Fikri Aji and Zhengzhong Liu and Andy Hock and Andrew Feldman and Jonathan Lee and A. Jackson and Preslav Nakov and Timothy Baldwin and Eric P. Xing},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.16149},
  url={https://www.semanticscholar.org/paper/5c577988ccebfea96de86678d04fd94fad367d2e},
  abstract={We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat},
  keywords={arxiv:2308.16149}
}

@article{zhao2023jiuzhangunified,
  title={JiuZhang 2.0: A Unified Chinese Pre-trained Language Model for Multi-task Mathematical Problem Solving},
  author={Wayne Xin Zhao and Kun Zhou and Beichen Zhang and Zheng Gong and Zhipeng Chen and Yuanhang Zhou and Ji-rong Wen and Jing Sha and Shijin Wang and Cong Liu and Guoping Hu},
  year={2023},
  booktitle={Knowledge Discovery and Data Mining},
  doi={10.1145/3580305.3599850},
  url={https://www.semanticscholar.org/paper/fae57797d357bfa3b39b220336d1a2e8deba5318},
  abstract={Although pre-trained language models\~{}(PLMs) have recently advanced the research progress in mathematical reasoning, they are not specially designed as a capable multi-task solver, suffering from high cost for multi-task deployment (e.g. a model copy for a task) and inferior performance on complex mathematical problems in practical applications. To address these issues, we propose JiuZhang 2.0, a unified Chinese PLM specially for multi-task mathematical problem solving. Our idea is to maintain a moderate-sized model and employ the cross-task knowledge sharing to improve the model capacity in a multi-task setting. Specially, we construct a Mixture-of-Experts (MoE) architecture for modeling mathematical text, to capture the common mathematical knowledge across tasks. For optimizing the MoE architecture, we design multi-task continual pre-training and multi-task fine-tuning strategies for multi-task adaptation. These training strategies can effectively decompose the knowledge from the task data and establish the cross-task sharing via expert networks. To further improve the general capacity of solving different complex tasks, we leverage large language models (LLMs) as complementary models to iteratively refine the generated solution by our PLM, via in-context learning. Extensive experiments have demonstrated the effectiveness of our model.},
  keywords={arxiv:2306.11027}
}

@article{kim2023kggptgeneral,
  title={KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models},
  author={Jiho Kim and Yeonsu Kwon and Yohan Jo and Edward Choi},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.11220},
  url={https://www.semanticscholar.org/paper/974f0e1a85c1ece2555718342ff2abb6bcb6a825},
  abstract={While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.},
  keywords={arxiv:2310.11220}
}

@article{ding2023knowledgecrosswords,
  title={Knowledge Crosswords: Geometric Knowledge Reasoning with Large Language Models},
  author={Wenxuan Ding and Shangbin Feng and Yuhan Liu and Zhaoxuan Tan and Vidhisha Balachandran and Tianxing He and Yulia Tsvetkov},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2024.findings-acl.154},
  url={https://www.semanticscholar.org/paper/513fdb089e079f1aa640b76da2427eee64a86439},
  abstract={We propose Knowledge Crosswords, a geometric knowledge reasoning benchmark consisting of incomplete knowledge networks bounded by structured factual constraints, where LLMs are tasked with inferring the missing facts to meet all constraints. The novel setting of geometric knowledge reasoning necessitates new LM abilities beyond existing atomic/linear multi-hop QA, such as backtracking, verifying facts and constraints, reasoning with uncertainty, and more. Knowledge Crosswords contains 2,101 individual problems, covering diverse knowledge domains, and is further divided into three difficulty levels. We conduct extensive experiments to evaluate existing LLMs and approaches on Knowledge Crosswords. Results demonstrate that baseline approaches struggle with larger knowledge networks and semantically-equivalent entity distractors. In light of their limitations, we propose two new approaches, Staged Prompting and Verify-All, to augment LLMs' abilities for error-aware backtracking and constraint verification. Our Verify-All significantly outperforms prior methods and is more robust towards problems in the hard subset. Further analysis shows that geometric knowledge reasoning poses new challenges to LLMs' knowledge abilities, particularly in robustness towards varying option orders, complex structural constraints in knowledge networks,"none of the above"scenarios, and more.},
  keywords={arxiv:2310.01290}
}

@misc{latif2023knowledgedistillation,
  title={Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments},
  author={Ehsan Latif and Luyang Fang and Ping Ma and Xiaoming Zhai},
  year={2023},
  url={https://www.semanticscholar.org/paper/631b5baa2c34f7095ccdd8761086b49148071d78},
  abstract={This study proposes a method for knowledge distillation (KD) of fine-tuned Large Language Models (LLMs) into smaller, more efficient, and accurate neural networks. We specifically target the challenge of deploying these models on resource-constrained devices. Our methodology involves training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model. This is achieved through a specialized loss function tailored to learn from the LLM's output probabilities, ensuring that the student model closely mimics the teacher's performance. To validate the performance of the KD approach, we utilized a large dataset, 7T, containing 6,684 student-written responses to science questions and three mathematical reasoning datasets with student-written responses graded by human experts. We compared accuracy with state-of-the-art (SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models. Results have shown that the KD approach has 3\% and 2\% higher scoring accuracy than ANN and TinyBERT, respectively, and comparable accuracy to the teacher model. Furthermore, the student model size is 0.03M, 4,000 times smaller in parameters and x10 faster in inferencing than the teacher model and TinyBERT, respectively. The significance of this research lies in its potential to make advanced AI technologies accessible in typical educational settings, particularly for automatic scoring.},
  keywords={arxiv:2312.15842}
}

@article{wang2023knowledgeediting,
  title={Knowledge Editing for Large Language Models: A Survey},
  author={Song Wang and Yaochen Zhu and Haochen Liu and Zaiyi Zheng and Chen Chen and Jundong Li},
  year={2023},
  booktitle={ACM Computing Surveys},
  doi={10.1145/3698590},
  url={https://www.semanticscholar.org/paper/42016f91e5b1da63174d45acb96bc89b64aa124d},
  abstract={Large Language Models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME), also known as Knowledge Editing or Model Editing, has attracted increasing attention, which aims at precisely modifying the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim at providing a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.},
  keywords={arxiv:2310.16218}
}

@article{guo2023knowledgenavigatorleveraging,
  title={KnowledgeNavigator: leveraging large language models for enhanced reasoning over knowledge graph},
  author={Tiezheng Guo and Qingwen Yang and Chen Wang and Yanyi Liu and Pan Li and Jiawei Tang and Dapeng Li and Yingyou Wen},
  year={2023},
  booktitle={Complex \& Intelligent Systems},
  doi={10.1007/s40747-024-01527-8},
  url={https://www.semanticscholar.org/paper/06f710f59e2119ed892e9291eb942847a34db91c},
  abstract={Large language models have achieved outstanding performance on various downstream tasks with their advanced understanding of natural language and zero-shot capability. However, they struggle with knowledge constraints, particularly in tasks requiring complex reasoning or extended logical sequences. These limitations can affect their performance in question answering by leading to inaccuracies and hallucinations. This paper proposes a novel framework called KnowledgeNavigator that leverages large language models on knowledge graphs to achieve accurate and interpretable multi-hop reasoning. Especially with an analysis-retrieval-reasoning process, KnowledgeNavigator searches the optimal path iteratively to retrieve external knowledge and guide the reasoning to reliable answers. KnowledgeNavigator treats knowledge graphs and large language models as flexible components that can be switched between different tasks without additional costs. Experiments on three benchmarks demonstrate that KnowledgeNavigator significantly improves the performance of large language models in question answering and outperforms all large language models-based baselines.},
  keywords={arxiv:2312.15880}
}

@article{ni2023l2cevalevaluating,
  title={L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models},
  author={Ansong Ni and Pengcheng Yin and Yilun Zhao and Martin Riddell and Troy Feng and Rui Shen and Stephen Yin and Ye Liu and Semih Yavuz and Caiming Xiong and Shafiq R. Joty and Yingbo Zhou and Dragomir R. Radev and Arman Cohan},
  year={2023},
  journal={Transactions of the Association for Computational Linguistics},
  doi={10.1162/tacl_a_00705},
  url={https://www.semanticscholar.org/paper/12db3efff4cc9e16822dd64bb1cad66f3f034f3b},
  abstract={Abstract Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs. Despite promising results, there is a notable lack of a comprehensive evaluation of these models’ language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning, and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition, we assess confidence calibration, and conduct human evaluations to identify typical failures across different tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We release the evaluation framework1 and all model outputs, hoping to lay the groundwork for further future research. All future evaluations (e.g., LLaMA-3, StarCoder2, etc) will be updated on the project website: https://l2c-eval.github.io/.},
  keywords={arxiv:2309.17446}
}

@article{chen2023lionempowering,
  title={LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge},
  author={Gongwei Chen and Leyang Shen and Rui Shao and Xiang Deng and Liqiang Nie},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.02506},
  url={https://www.semanticscholar.org/paper/98b69e478d2d4e4cf1a0befcdb27c4f220fc0a4b},
  abstract={Multimodal Large Language Models (MLLMs) have endowed LLMs with the ability to perceive and understand multimodal signals. However, most of the existing MLLMs mainly adopt vision encoders pretrained on coarsely aligned image-text pairs, leading to insufficient extraction and reasoning of visual knowledge. To address this issue, we devise a dual-Level vIsual knOwledge eNhanced Multimodal Large Language Model (LION), which empowers the MLLM by injecting visual knowledge in two levels. 1) Progressive incorporation of fine-grained spatial-aware visual knowledge. We design a vision aggregator cooperated with region-level vision-language (VL) tasks to incorporate fine-grained spatial-aware visual knowledge into the MLLM. To alleviate the conflict between imagelevel and region-level VL tasks during incorporation, we devise a dedicated stage-wise instruction-tuning strategy with mixture-of-adapters. This progressive incorporation scheme contributes to the mutual promotion between these two kinds of VL tasks. 2) Soft prompting of high-level semantic visual evidence. We facilitate the MLLM with high-level semantic visual evidence by leveraging diverse image tags. To mitigate the potential influence caused by imper-fect predicted tags, we propose a soft prompting method by embedding a learnable token into the tailored text instruction. Comprehensive experiments on several multimodal benchmarks demonstrate the superiority of our model (e.g., improvement of 5\% accuracy on VSR and 3\% CIDEr on TextCaps over InstructBLIP, 5\% accuracy on RefCOCOg over Kosmos-2).},
  keywords={arxiv:2311.11860}
}

@article{lai2023lisareasoning,
  title={LISA: Reasoning Segmentation via Large Language Model},
  author={Xin Lai and Zhuotao Tian and Yukang Chen and Yanwei Li and Yuhui Yuan and Shu Liu and Jiaya Jia},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.00915},
  url={https://www.semanticscholar.org/paper/ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7},
  abstract={Although perception systems have made remarkable ad-vancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task - reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex rea-soning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at github.com/dvlab-research/LISA.},
  keywords={arxiv:2308.00692}
}

@article{chew2023llmassistedcontent,
  title={LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding},
  author={Robert F. Chew and John Bollenbacher and Michael Wenger and Jessica Speer and Annice Kim},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2306.14924},
  url={https://www.semanticscholar.org/paper/08154f2b0534c525bad888d6aaa0bb67c8e27861},
  abstract={Deductive coding is a widely used qualitative research method for determining the prevalence of themes across documents. While useful, deductive coding is often burdensome and time consuming since it requires researchers to read, interpret, and reliably categorize a large body of unstructured text documents. Large language models (LLMs), like ChatGPT, are a class of quickly evolving AI tools that can perform a range of natural language processing and reasoning tasks. In this study, we explore the use of LLMs to reduce the time it takes for deductive coding while retaining the flexibility of a traditional content analysis. We outline the proposed approach, called LLM-assisted content analysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a publicly available deductive coding data set. Additionally, we conduct an empirical benchmark using LACA on 4 publicly available data sets to assess the broader question of how well GPT-3.5 performs across a range of deductive coding tasks. Overall, we find that GPT-3.5 can often perform deductive coding at levels of agreement comparable to human coders. Additionally, we demonstrate that LACA can help refine prompts for deductive coding, identify codes for which an LLM is randomly guessing, and help assess when to use LLMs vs. human coders for deductive coding. We conclude with several implications for future practice of deductive coding and related research methods.},
  keywords={arxiv:2306.14924}
}

@article{agashe2023llmcoordinationevaluating,
  title={LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models},
  author={Saaket Agashe and Yue Fan and Anthony Reyna and Xin Eric Wang},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.18653/v1/2025.findings-naacl.448},
  url={https://www.semanticscholar.org/paper/7f0d1740e74ce36424d64d608270077b64dfe7c0},
  abstract={Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners' beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs' Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement. Code Available at https://github.com/eric-ai-lab/llm\_coordination.},
  keywords={arxiv:2310.03903}
}

@article{lyu2023llmrecpersonalized,
  title={LLM-Rec: Personalized Recommendation via Prompting Large Language Models},
  author={Hanjia Lyu and Song Jiang and Hanqing Zeng and Yinglong Xia and Jiebo Luo},
  year={2023},
  booktitle={NAACL-HLT},
  doi={10.48550/arXiv.2307.15780},
  url={https://www.semanticscholar.org/paper/006aa1580fae5968417538c7acb4662c7b58088f},
  abstract={Text-based recommendation holds a wide range of practical applications due to its versatility, as textual descriptions can represent nearly any type of item. However, directly employing the original item descriptions may not yield optimal recommendation performance due to the lack of comprehensive information to align with user preferences. Recent advances in large language models (LLMs) have showcased their remarkable ability to harness commonsense knowledge and reasoning. In this study, we introduce a novel approach, coined LLM-Rec, which incorporates four distinct prompting strategies of text enrichment for improving personalized text-based recommendations. Our empirical experiments reveal that using LLM-augmented text significantly enhances recommendation quality. Even basic MLP (Multi-Layer Perceptron) models achieve comparable or even better results than complex content-based methods. Notably, the success of LLM-Rec lies in its prompting strategies, which effectively tap into the language model's comprehension of both general and specific item characteristics. This highlights the importance of employing diverse prompts and input augmentation techniques to boost the recommendation effectiveness of LLMs.},
  keywords={arxiv:2307.15780}
}

@article{lian2023llmgroundeddiffusion,
  title={LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models},
  author={Long Lian and Boyi Li and Adam Yala and Trevor Darrell},
  year={2023},
  booktitle={Trans. Mach. Learn. Res.},
  doi={10.48550/arXiv.2305.13655},
  url={https://www.semanticscholar.org/paper/e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0},
  abstract={Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io},
  keywords={arxiv:2305.13655}
}

@article{yang2023llm4drivesurvey,
  title={LLM4Drive: A Survey of Large Language Models for Autonomous Driving},
  author={Zhenjie Yang and Xiaosong Jia and Hongyang Li and Junchi Yan},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.01043},
  url={https://www.semanticscholar.org/paper/84d99893ee24fc825e359598d44d602c45c4865e},
  abstract={Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their"black box"nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper, we systematically review a research line about \textbackslash\{\}textit\{Large Language Models for Autonomous Driving (LLM4AD)\}. This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field. For the convenience of researchers in academia and industry, we provide real-time updates on the latest advances in the field as well as relevant open-source resources via the designated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.},
  keywords={arxiv:2311.01043}
}

@article{zhong2023llm4edaemerging,
  title={LLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation},
  author={Ruizhe Zhong and Xingbo Du and Shixiong Kai and Zhentao Tang and Siyuan Xu and Hui-Ling Zhen and Jianye Hao and Qiang Xu and M. Yuan and Junchi Yan},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2401.12224},
  url={https://www.semanticscholar.org/paper/beadfd41698c0382a5899077a8346c05ca6fe513},
  abstract={Driven by Moore's Law, the complexity and scale of modern chip design are increasing rapidly. Electronic Design Automation (EDA) has been widely applied to address the challenges encountered in the full chip design process. However, the evolution of very large-scale integrated circuits has made chip design time-consuming and resource-intensive, requiring substantial prior expert knowledge. Additionally, intermediate human control activities are crucial for seeking optimal solutions. In system design stage, circuits are usually represented with Hardware Description Language (HDL) as a textual format. Recently, Large Language Models (LLMs) have demonstrated their capability in context understanding, logic reasoning and answer generation. Since circuit can be represented with HDL in a textual format, it is reasonable to question whether LLMs can be leveraged in the EDA field to achieve fully automated chip design and generate circuits with improved power, performance, and area (PPA). In this paper, we present a systematic study on the application of LLMs in the EDA field, categorizing it into the following cases: 1) assistant chatbot, 2) HDL and script generation, and 3) HDL verification and analysis. Additionally, we highlight the future research direction, focusing on applying LLMs in logic synthesis, physical design, multi-modal feature extraction and alignment of circuits. We collect relevant papers up-to-date in this field via the following link: https://github.com/Thinklab-SJTU/Awesome-LLM4EDA.},
  keywords={arxiv:2401.12224}
}

@article{xia2023llmgamultimodal,
  title={LLMGA: Multimodal Large Language Model based Generation Assistant},
  author={Bin Xia and Shiyin Wang and Yingfan Tao and Yitong Wang and Jiaya Jia},
  year={2023},
  booktitle={European Conference on Computer Vision},
  doi={10.48550/arXiv.2311.16500},
  url={https://www.semanticscholar.org/paper/769a924d0af014acec326f50c15c5d70d258a969},
  abstract={In this paper, we introduce a Multimodal Large Language Model-based Generation Assistant (LLMGA), leveraging the vast reservoir of knowledge and proficiency in reasoning, comprehension, and response inherent in Large Language Models (LLMs) to assist users in image generation and editing. Diverging from existing approaches where Multimodal Large Language Models (MLLMs) generate fixed-size embeddings to control Stable Diffusion (SD), our LLMGA provides a detailed language generation prompt for precise control over SD. This not only augments LLM context understanding but also reduces noise in generation prompts, yields images with more intricate and precise content, and elevates the interpretability of the network. To this end, we curate a comprehensive dataset comprising prompt refinement, similar image generation, inpainting \textbackslash\{\}\&outpainting, and instruction-based editing. Moreover, we propose a two-stage training scheme. In the first stage, we train the MLLM to grasp the properties of image generation and editing, enabling it to generate detailed prompts. In the second stage, we optimize SD to align with the MLLM's generation prompts. Additionally, we propose a reference-based restoration network to alleviate texture, brightness, and contrast disparities between generated and preserved regions during inpainting and outpainting. Extensive results show that LLMGA has promising generation and editing capabilities and can enable more flexible and expansive applications in an interactive manner.},
  keywords={arxiv:2311.16500}
}

@article{lai2023llmlightlarge,
  title={LLMLight: Large Language Models as Traffic Signal Control Agents},
  author={Siqi Lai and Zhao Xu and Weijiao Zhang and Hao Liu and Hui Xiong},
  year={2023},
  booktitle={Knowledge Discovery and Data Mining},
  doi={10.1145/3690624.3709379},
  url={https://www.semanticscholar.org/paper/5609bde8e191eb61a304047d2434868e243df5be},
  abstract={Traffic Signal Control (TSC) is a crucial component in urban traffic management, aiming to optimize road network efficiency and reduce congestion. Traditional TSC methods, primarily based on transportation engineering and reinforcement learning (RL), often struggle with generalization abilities across varied traffic scenarios and lack interpretability. This paper presents LLMLight, a novel framework employing Large Language Models (LLMs) as decision-making agents for TSC. Specifically, the framework begins by instructing the LLM with a knowledgeable prompt detailing real-time traffic conditions. Leveraging the advanced generalization capabilities of LLMs, LLMLight engages a reasoning and decision-making process akin to human intuition for effective traffic control. Moreover, we build LightGPT, a specialized backbone LLM tailored for TSC tasks. By learning nuanced traffic patterns and control strategies, LightGPT enhances the LLMLight framework cost-effectively. Extensive experiments conducted on ten real-world and synthetic datasets, along with evaluations by fifteen human experts, demonstrate the exceptional effectiveness, generalization ability, and interpretability of LLMLight with LightGPT, outperforming nine baseline methods and ten advanced LLMs. Our project is available at https://github.com/usail-hkust/LLMTSCS.},
  keywords={arxiv:2312.16044}
}

@article{wei2023llmreclarge,
  title={LLMRec: Large Language Models with Graph Augmentation for Recommendation},
  author={Wei Wei and Xubin Ren and Jiabin Tang and Qinyong Wang and Lixin Su and Suqi Cheng and Junfeng Wang and Dawei Yin and Chao Huang},
  year={2023},
  booktitle={Web Search and Data Mining},
  doi={10.1145/3616855.3635853},
  url={https://www.semanticscholar.org/paper/5aa3b1009955ce2c8f896e0d5e94e06155ef1e43},
  abstract={The problem of data sparsity has long been a challenge in recommendation systems, and previous studies have attempted to address this issue by incorporating side information. However, this approach often introduces side effects such as noise, availability issues, and low data quality, which in turn hinder the accurate modeling of user preferences and adversely impact recommendation performance. In light of the recent advancements in large language models (LLMs), which possess extensive knowledge bases and strong reasoning capabilities, we propose a novel framework called LLMRec that enhances recommender systems by employing three simple yet effective LLM-based graph augmentation strategies. Our approach leverages the rich content available within online platforms (e.g., Netflix, MovieLens) to augment the interaction graph in three ways: (i) reinforcing user-item interaction egde, (ii) enhancing the understanding of item node attributes, and (iii) conducting user node profiling, intuitively from the natural language perspective. By employing these strategies, we address the challenges posed by sparse implicit feedback and low-quality side information in recommenders. Besides, to ensure the quality of the augmentation, we develop a denoised data robustification mechanism that includes techniques of noisy implicit feedback pruning and MAE-based feature enhancement that help refine the augmented data and improve its reliability. Furthermore, we provide theoretical analysis to support the effectiveness of LLMRec and clarify the benefits of our method in facilitating model optimization. Experimental results on benchmark datasets demonstrate the superiority of our LLM-based augmentation approach over state-of-the-art techniques. To ensure reproducibility, we have made our code and augmented data publicly available at: https://github.com/HKUDS/LLMRec.git.},
  keywords={arxiv:2311.00423}
}

@article{deng2023llmsmoon,
  title={LLMs to the Moon? Reddit Market Sentiment Analysis with Large Language Models},
  author={Xiang Deng and Vasilisa Bashlovkina and Feng Han and Simon Baumgartner and Michael Bendersky},
  year={2023},
  booktitle={The Web Conference},
  doi={10.1145/3543873.3587605},
  url={https://www.semanticscholar.org/paper/e4bf034670934c6b99bcc8dfcee75e9f5701c3fe},
  abstract={Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. In this work, we conduct a case study approaching this problem with semi-supervised learning using a large language model (LLM). We select Reddit as the target social media platform due to its broad coverage of topics and content types. Our pipeline first generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while training the student model using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model’s competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.}
}

@article{shao2023lmdriveclosedloop,
  title={LMDrive: Closed-Loop End-to-End Driving with Large Language Models},
  author={Hao Shao and Yuxuan Hu and Letian Wang and Steven L. Waslander and Yu Liu and Hongsheng Li},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.01432},
  url={https://www.semanticscholar.org/paper/e0b05e314372ed580d9612ef5f0ee672b17ad2e4},
  abstract={Despite significant recent progress in the field of autonomous driving, modern methods still struggle and can incur serious accidents when encountering long-tail unfore-seen events and challenging urban scenarios. On the one hand, large language models (LLM) have shown impres-sive reasoning capabilities that approach “Artificial Gen-eral Intelligence”. On the other hand, previous autonomous driving methods tend to rely on limited-format inputs (e.g., sensor data and navigation waypoints), restricting the vehi-cle's ability to understand language information and inter-act with humans. To this end, this paper introduces LM-Drive, a novel language-guided, end-to-end, closed-loop autonomous driving framework. LMDrive uniquely processes and integrates multimodal sensor data with naturallanguage instructions, enabling interaction with humans and navigation software in realistic instructional settings. To facilitate research in language-based closed-loop autonomous driving, we also publicly release the corresponding dataset which includes approximately 64K instruction-following data clips, and the LangAuto benchmark that tests the system's ability to handle complex instructions and challenging driving scenarios. Extensive closed-loop experiments are conducted to demonstrate LMDrive's effectiveness. To the best of our knowledge, we're the very first work to leverage LLMs for closed-loop end-to-end autonomous driving. Code is available on our webpage.},
  keywords={arxiv:2312.07488}
}

@article{yamauchi2023lpmlllmprompting,
  title={LPML: LLM-Prompting Markup Language for Mathematical Reasoning},
  author={Ryutaro Yamauchi and Sho Sonoda and Akiyoshi Sannai and Wataru Kumagai},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2309.13078},
  url={https://www.semanticscholar.org/paper/cf237f3a6ed3e8fd970c15bf1f0bdf94f34da4a9},
  abstract={In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.},
  keywords={arxiv:2309.13078}
}

@article{ma2023lampilotopen,
  title={LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs},
  author={Yunsheng Ma and Can Cui and Xu Cao and Wenqian Ye and Peiran Liu and Juanwu Lu and Amr Abdelraouf and Rohit Gupta and Kyungtae Han and Aniket Bera and J. Rehg and Ziran Wang},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.01434},
  url={https://www.semanticscholar.org/paper/10578bc0bdb3ebf9232931dd4961f55ba470caad},
  abstract={Autonomous driving (AD) has made significant strides in recent years. However, existing frameworks struggle to interpret and execute spontaneous user instructions, such as "overtake the car ahead.” Large Language Models (LLMs) have demonstrated impressive reasoning capabilities showing potential to bridge this gap. In this paper, we present LaMPilot, a novel framework that integrates LLMs into AD systems, enabling them to follow user instructions by generating code that leverages established functional primitives. We also introduce LaMPilot-Bench, the first bench-mark dataset specifically designed to quantitatively evaluate the efficacy of language model programs in AD. Adopting the LaMPilot framework, we conduct extensive experiments to assess the performance of off-the-shelf LLMs on LaMPilot-Bench. Our results demonstrate the potential of LLMs in handling diverse driving scenarios and following user instructions in driving. To facilitate further research in this area, we release our code and data at GitHub.com/PurdueDigitalTwin/LaMPilot.},
  keywords={arxiv:2312.04372}
}

@article{chang2023languagemodel,
  title={Language Model Behavior: A Comprehensive Survey},
  author={Tyler A. Chang and B. Bergen},
  year={2023},
  booktitle={International Conference on Computational Logic},
  doi={10.1162/coli_a_00492},
  url={https://www.semanticscholar.org/paper/12d16f426edc6ab248fb476007bd1646282d4d68},
  abstract={Transformer language models have received widespread public attention, yet their generated text is often surprising even to NLP researchers. In this survey, we discuss over 250 recent studies of English language model behavior before task-specific fine-tuning. Language models possess basic capabilities in syntax, semantics, pragmatics, world knowledge, and reasoning, but these capabilities are sensitive to specific inputs and surface features. Despite dramatic increases in generated text quality as models scale to hundreds of billions of parameters, the models are still prone to unfactual responses, commonsense errors, memorized text, and social biases. Many of these weaknesses can be framed as over-generalizations or under-generalizations of learned patterns in text. We synthesize recent results to highlight what is currently known about large language model capabilities, thus providing a resource for applied work and for research in adjacent fields that use language models.},
  keywords={arxiv:2303.11504}
}

@article{pang2023languagemodel,
  title={Language Model Self-improvement by Reinforcement Learning Contemplation},
  author={Jing-Cheng Pang and Pengyuan Wang and Kaiyuan Li and Xiong-Hui Chen and Jiacheng Xu and Zongzhang Zhang and Yang Yu},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2305.14483},
  url={https://www.semanticscholar.org/paper/c226a4acb42912054d498bcf771023b0ba2da001},
  abstract={Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text generation, and machine translation. Our experiments show that SIRLC effectively improves LLM performance without external supervision, resulting in a 5.6\% increase in answering accuracy for reasoning tasks and a rise in BERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be applied to models of different sizes, showcasing its broad applicability.},
  keywords={arxiv:2305.14483}
}

@article{shi2023languagemodels,
  title={Language Models Can Improve Event Prediction by Few-Shot Abductive Reasoning},
  author={Xiaoming Shi and Siqiao Xue and Kangrui Wang and Fan Zhou and James Y. Zhang and Jun-ping Zhou and Chenhao Tan and Hongyuan Mei},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2305.16646},
  url={https://www.semanticscholar.org/paper/aefbd1cf0837b989785096921f9f04c275b88d7d},
  abstract={Large language models have shown astonishing performance on a wide range of reasoning tasks. In this paper, we investigate whether they could reason about real-world events and help improve the prediction performance of event sequence models. We design LAMP, a framework that integrates a large language model in event prediction. Particularly, the language model performs abductive reasoning to assist an event sequence model: the event model proposes predictions on future events given the past; instructed by a few expert-annotated demonstrations, the language model learns to suggest possible causes for each proposal; a search module finds out the previous events that match the causes; a scoring function learns to examine whether the retrieved events could actually cause the proposal. Through extensive experiments on several challenging real-world datasets, we demonstrate that our framework -- thanks to the reasoning capabilities of large language models -- could significantly outperform the state-of-the-art event sequence models.},
  keywords={arxiv:2305.16646}
}

@article{sha2023languagempclarge,
  title={LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving},
  author={Hao Sha and Yao Mu and Yuxuan Jiang and Li Chen and Chenfeng Xu and Ping Luo and S. Li and Masayoshi Tomizuka and Wei Zhan and Mingyu Ding},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.03026},
  url={https://www.semanticscholar.org/paper/19933dd9e03058e686ef412262eef7696cce3e8f},
  abstract={Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-makers for intricate AD scenarios in terms of safety, efficiency, generalizability, and interoperability. We aspire for it to serve as inspiration for future research in this field. Project page: https://sites.google.com/view/llm-mpc},
  keywords={arxiv:2310.03026}
}

@article{mohammad2023largelanguage,
  title={Large Language Model (LLM) \& GPT, A Monolithic Study in Generative AI},
  author={Atif Farid Mohammad and Bryan Clark and Ramya Hegde},
  year={2023},
  booktitle={2023 Congress in Computer Science, Computer Engineering, \& Applied Computing (CSCE)},
  doi={10.1109/CSCE60160.2023.00068},
  url={https://www.semanticscholar.org/paper/7529c2813c58b1fe448cfd5d2c5835729e68996d},
  abstract={Large Language Models (LLMs) are a branch of computer science and artificial intelligence which is concerned with computer and human language interaction. It is the study of mathematical and computational modeling of various aspects of language and the development of an arsenal of systems. Large Language Models are considered an area of research and application that explores how computers can be used to comprehend and manipulate natural language text or speech to perform useful tasks. It has spread its applications in various areas such as machine translation, email spam detection, information extraction, summarization, medical, and question answering etc. Large Language Models (LLMs) have a greater contribution in the area of text pre-processing as well in the time of ChatGPT as an example. Different pre-processing steps are required to perform, such as stemming, part-of-speech (POS) tagging, chunking, parsing, information extraction, etc. to perform language processing.}
}

@article{yue2023largelanguage,
  title={Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning},
  author={Murong Yue and Jie Zhao and Min Zhang and Liang Du and Ziyu Yao},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.03094},
  url={https://www.semanticscholar.org/paper/00cccb9065f0a59e845d5b4d360ce31cf25036be},
  abstract={Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the"answer consistency"of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only 40\% of its cost.},
  keywords={arxiv:2310.03094}
}

@article{ichien2023largelanguage,
  title={Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors},
  author={Nicholas Ichien and Dušan Stamenković and K. Holyoak},
  year={2023},
  booktitle={Metaphor and Symbol},
  doi={10.1080/10926488.2024.2380348},
  url={https://www.semanticscholar.org/paper/69341c5b540cc98bc54849d03c6b70857e41a2dd},
  abstract={ABSTRACT Despite the exceptional performance of large language models (LLMs) on a wide range of tasks involving natural language processing and reasoning, there has been sharp disagreement as to whether their abilities extend to more creative human abilities. A core example is the interpretation of novel metaphors. Here we assessed the ability of GPT-4, a state-of-the-art large language model, to provide natural-language interpretations of a recent AI benchmark (Fig-QA dataset), novel literary metaphors drawn from Serbian poetry and translated into English, and entire novel English poems. GPT-4 outperformed previous AI models on the Fig-QA dataset. For metaphors drawn from Serbian poetry, human judges – blind to the fact that an AI model was involved – rated metaphor interpretations generated by GPT-4 as superior to those provided by a group of college students. In interpreting reversed metaphors, GPT-4, as well as humans, exhibited signs of sensitivity to the Gricean cooperative principle. In addition, for several novel English poems GPT-4 produced interpretations that were rated as excellent or good by a human literary critic. These results indicate that LLMs such as GPT-4 have acquired an emergent ability to interpret literary metaphors, including those embedded in novel poems.},
  keywords={arxiv:2308.01497}
}

@article{jiang2023largelanguage,
  title={Large Language Model Enhanced Multi-Agent Systems for 6G Communications},
  author={Feibo Jiang and Li Dong and Yubo Peng and Kezhi Wang and Kun Yang and Cunhua Pan and D. Niyato and O. Dobre},
  year={2023},
  booktitle={IEEE wireless communications},
  doi={10.1109/MWC.016.2300600},
  url={https://www.semanticscholar.org/paper/d183cca56404f09653ba14f952597235dc37c387},
  abstract={The rapid development of the large language model (LLM) presents huge opportunities for 6G communications – for example, network optimization and management – by allowing users to input task requirements to LLMs with natural language. However, directly applying native LLMs in 6G encounters various challenges, such as a lack of communication data and knowledge, and limited logical reasoning, evaluation, and refinement abilities. Integrating LLMs with the capabilities of retrieval, planning, memory, evaluation, and reflection in agents can greatly enhance the potential of LLMs for 6G communications. To this end, we propose CommLLM, a multi-agent system with customized communication knowledge and tools for solving communication-related tasks using natural language. This system consists of three components: multi-agent data retrieval (MDR), which employs the condensate and inference agents to refine and summarize communication knowledge from the knowledge base, expanding the knowledge boundaries of LLMs in 6G communications; multi-agent collaborative planning (MCP), which utilizes multiple planning agents to generate feasible solutions for the communication-re-lated task from different perspectives based on the retrieved knowledge; and multi-agent evaluation and reflection (MER), which utilizes the evaluation agent to assess the solutions, and applies the reflection agent and refinement agent to provide improvement suggestions for current solutions. Finally, we validate the effectiveness of the proposed multi-agent system by designing a semantic communication system as a case study of 6G communications.},
  keywords={arxiv:2312.07850}
}

@article{long2023largelanguage,
  title={Large Language Model Guided Tree-of-Thought},
  author={Jieyi Long},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2305.08291},
  url={https://www.semanticscholar.org/paper/bda605928d6ebe4db906e69ab5d343df75918727},
  abstract={In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \textbackslash\{\}url\{https://github.com/jieyilong/tree-of-thought-puzzle-solver\}.},
  keywords={arxiv:2305.08291}
}

@article{schoenegger2023largelanguage,
  title={Large Language Model Prediction Capabilities: Evidence from a Real-World Forecasting Tournament},
  author={P. Schoenegger and P. S. Park},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.13014},
  url={https://www.semanticscholar.org/paper/5a23700c5198e44a05336f7a2e7a7d7d183ad625},
  abstract={Accurately predicting the future would be an important milestone in the capabilities of artificial intelligence. However, research on the ability of large language models to provide probabilistic predictions about future events remains nascent. To empirically test this ability, we enrolled OpenAI's state-of-the-art large language model, GPT-4, in a three-month forecasting tournament hosted on the Metaculus platform. The tournament, running from July to October 2023, attracted 843 participants and covered diverse topics including Big Tech, U.S. politics, viral outbreaks, and the Ukraine conflict. Focusing on binary forecasts, we show that GPT-4's probabilistic forecasts are significantly less accurate than the median human-crowd forecasts. We find that GPT-4's forecasts did not significantly differ from the no-information forecasting strategy of assigning a 50\% probability to every question. We explore a potential explanation, that GPT-4 might be predisposed to predict probabilities close to the midpoint of the scale, but our data do not support this hypothesis. Overall, we find that GPT-4 significantly underperforms in real-world predictive tasks compared to median human-crowd forecasts. A potential explanation for this underperformance is that in real-world forecasting tournaments, the true answers are genuinely unknown at the time of prediction; unlike in other benchmark tasks like professional exams or time series forecasting, where strong performance may at least partly be due to the answers being memorized from the training data. This makes real-world forecasting tournaments an ideal environment for testing the generalized reasoning and prediction capabilities of artificial intelligence going forward.},
  keywords={arxiv:2310.13014}
}

@article{dong2023largelanguage,
  title={Large Language Model for Science: A Study on P vs. NP},
  author={Qingxiu Dong and Li Dong and Ke Xu and Guangyan Zhou and Y. Hao and Zhifang Sui and Furu Wei},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2309.05689},
  url={https://www.semanticscholar.org/paper/b65144033c6d29103879fb178d8efba610cfcd27},
  abstract={In this work, we use large language models (LLMs) to augment and accelerate research on the P versus NP problem, one of the most important open problems in theoretical computer science and mathematics. Specifically, we propose Socratic reasoning, a general framework that promotes in-depth thinking with LLMs for complex problem-solving. Socratic reasoning encourages LLMs to recursively discover, solve, and integrate problems while facilitating self-evaluation and refinement. Our pilot study on the P vs. NP problem shows that GPT-4 successfully produces a proof schema and engages in rigorous reasoning throughout 97 dialogue turns, concluding"P \$\textbackslash\{\}neq\$ NP", which is in alignment with (Xu and Zhou, 2023). The investigation uncovers novel insights within the extensive solution space of LLMs, shedding light on LLM for Science.},
  keywords={arxiv:2309.05689}
}

@article{savelka2023largelanguage,
  title={Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code},
  author={Jaromir Savelka and Arav Agarwal and Chris Bogart and M. Sakr},
  year={2023},
  booktitle={International Conference on Computer Supported Education},
  doi={10.48550/arXiv.2303.08033},
  url={https://www.semanticscholar.org/paper/c65fd7dc0558adc8d0ea135914530e26d94fbc91},
  abstract={We analyzed effectiveness of three generative pre-trained transformer (GPT) models in answering multiple-choice question (MCQ) assessments, often involving short snippets of code, from introductory and intermediate programming courses at the postsecondary level. This emerging technology stirs countless discussions of its potential uses (e.g., exercise generation, code explanation) as well as misuses in programming education (e.g., cheating). However, the capabilities of GPT models and their limitations to reason about and/or analyze code in educational settings have been under-explored. We evaluated several OpenAI's GPT models on formative and summative MCQ assessments from three Python courses (530 questions). We found that MCQs containing code snippets are not answered as successfully as those that only contain natural language. While questions requiring to fill-in a blank in the code or completing a natural language statement about the snippet are handled rather successfully, MCQs that require analysis and/or reasoning about the code (e.g., what is true/false about the snippet, or what is its output) appear to be the most challenging. These findings can be leveraged by educators to adapt their instructional practices and assessments in programming courses, so that GPT becomes a valuable assistant for a learner as opposed to a source of confusion and/or potential hindrance in the learning process.},
  keywords={arxiv:2303.08033}
}

@article{koto2023largelanguage,
  title={Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU},
  author={Fajri Koto and Nurul Aisyah and Haonan Li and Timothy Baldwin},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.04928},
  url={https://www.semanticscholar.org/paper/3f421c7defc20c21e97abad5fa92887a06ec5df8},
  abstract={Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46\% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon perform at even lower levels.},
  keywords={arxiv:2310.04928}
}

@article{wu2023largelanguage,
  title={Large Language Models Perform Diagnostic Reasoning},
  author={Cheng-Kuang Wu and Wei-Lin Chen and Hsin-Hsi Chen},
  year={2023},
  booktitle={Tiny Papers @ ICLR},
  doi={10.48550/arXiv.2307.08922},
  url={https://www.semanticscholar.org/paper/4a5af57b2056c4cc0a768d830d5427f0d1bdae33},
  abstract={We explore the extension of chain-of-thought (CoT) prompting to medical reasoning for the task of automatic diagnosis. Motivated by doctors' underlying reasoning process, we present Diagnostic-Reasoning CoT (DR-CoT). Empirical results demonstrate that by simply prompting large language models trained only on general text corpus with two DR-CoT exemplars, the diagnostic accuracy improves by 15\% comparing to standard prompting. Moreover, the gap reaches a pronounced 18\% in out-domain settings. Our findings suggest expert-knowledge reasoning in large language models can be elicited through proper promptings.},
  keywords={arxiv:2307.08922}
}

@article{pezeshkpour2023largelanguage,
  title={Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions},
  author={Pouya Pezeshkpour and Estevam Hruschka},
  year={2023},
  booktitle={NAACL-HLT},
  doi={10.48550/arXiv.2308.11483},
  url={https://www.semanticscholar.org/paper/fd81018bc72b030545a2d3f3010f3758ec4d48c3},
  abstract={Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13\% to 75\% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.},
  keywords={arxiv:2308.11483}
}

@article{zhao2023largelanguage,
  title={Large Language Models are Complex Table Parsers},
  author={Bowen Zhao and Changkai Ji and Yuejie Zhang and Wen He and Yingwen Wang and Qing Wang and Rui Feng and Xiaobo Zhang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.emnlp-main.914},
  url={https://www.semanticscholar.org/paper/5d2b77ae8508e277fe9b840a471b7dfb00e806ff},
  abstract={With the Generative Pre-trained Transformer 3.5 (GPT-3.5) exhibiting remarkable reasoning and comprehension abilities in Natural Language Processing (NLP), most Question Answering (QA) research has primarily centered around general QA tasks based on GPT, neglecting the specific challenges posed by Complex Table QA. In this paper, we propose to incorporate GPT-3.5 to address such challenges, in which complex tables are reconstructed into tuples and specific prompt designs are employed for dialogues. Specifically, we encode each cell's hierarchical structure, position information, and content as a tuple. By enhancing the prompt template with an explanatory description of the meaning of each tuple and the logical reasoning process of the task, we effectively improve the hierarchical structure awareness capability of GPT-3.5 to better parse the complex tables. Extensive experiments and results on Complex Table QA datasets, i.e., the open-domain dataset HiTAB and the aviation domain dataset AIT-QA show that our approach significantly outperforms previous work on both datasets, leading to state-of-the-art (SOTA) performance.},
  keywords={arxiv:2312.11521}
}

@article{grohs2023largelanguage,
  title={Large Language Models can accomplish Business Process Management Tasks},
  author={Michael Grohs and Luka Abb and Nourhan Elsayed and Jana-Rebecca Rehse},
  year={2023},
  booktitle={Business Process Management Workshops},
  doi={10.48550/arXiv.2307.09923},
  url={https://www.semanticscholar.org/paper/cce17289765132b6192ccf90123bb7f5ef920c8e},
  abstract={Business Process Management (BPM) aims to improve organizational activities and their outcomes by managing the underlying processes. To achieve this, it is often necessary to consider information from various sources, including unstructured textual documents. Therefore, researchers have developed several BPM-specific solutions that extract information from textual documents using Natural Language Processing techniques. These solutions are specific to their respective tasks and cannot accomplish multiple process-related problems as a general-purpose instrument. However, in light of the recent emergence of Large Language Models (LLMs) with remarkable reasoning capabilities, such a general-purpose instrument with multiple applications now appears attainable. In this paper, we illustrate how LLMs can accomplish text-related BPM tasks by applying a specific LLM to three exemplary tasks: mining imperative process models from textual descriptions, mining declarative process models from textual descriptions, and assessing the suitability of process tasks from textual descriptions for robotic process automation. We show that, without extensive configuration or prompt engineering, LLMs perform comparably to or better than existing solutions and discuss implications for future BPM research as well as practical usage.},
  keywords={arxiv:2307.09923}
}

@article{yang2023largelanguage,
  title={Large Language Models for Automated Open-domain Scientific Hypotheses Discovery},
  author={Zonglin Yang and Xinya Du and Junxian Li and Jie Zheng and Soujanya Poria and E. Cambria},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2309.02726},
  url={https://www.semanticscholar.org/paper/5aea5c8b536380c5ad1d42108c2c6767622318ee},
  abstract={Hypothetical induction is recognized as the main reasoning type when scientists make observations about the world and try to propose hypotheses to explain those observations. Past research on hypothetical induction is under a constrained setting: (1) the observation annotations in the dataset are carefully manually handpicked sentences (resulting in a close-domain setting); and (2) the ground truth hypotheses are mostly commonsense knowledge, making the task less challenging. In this work, we tackle these problems by proposing the first dataset for social science academic hypotheses discovery, with the final goal to create systems that automatically generate valid, novel, and helpful scientific hypotheses, given only a pile of raw web corpus. Unlike previous settings, the new dataset requires (1) using open-domain data (raw web corpus) as observations; and (2) proposing hypotheses even new to humanity. A multi-module framework is developed for the task, including three different feedback mechanisms to boost performance, which exhibits superior performance in terms of both GPT-4 based and expert-based evaluation. To the best of our knowledge, this is the first work showing that LLMs are able to generate novel (''not existing in literature'') and valid (''reflecting reality'') scientific hypotheses.},
  keywords={arxiv:2309.02726}
}

@article{clavi2023largelanguage,
  title={Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification},
  author={Benjamin Clavié and Alexandru Ciceu and Frederick Naylor and Guillaume Souli'e and Thomas Brightwell},
  year={2023},
  booktitle={International Conference on Applications of Natural Language to Data Bases},
  doi={10.48550/arXiv.2303.07142},
  url={https://www.semanticscholar.org/paper/aa2fa431ce1d5a8d56d138e3330d3df381d36e3a},
  abstract={This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all other models, achieving a 6\% increase in Precision@95\% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate"reasoning"in the model, and that seemingly minor aspects of the prompt significantly affect the model's performance.},
  keywords={arxiv:2303.07142}
}

@article{fei2023lawbenchbenchmarking,
  title={LawBench: Benchmarking Legal Knowledge of Large Language Models},
  author={Zhiwei Fei and Xiaoyu Shen and D. Zhu and Fengzhe Zhou and Zhuo Han and Songyang Zhang and Kai Chen and Zongwen Shen and Jidong Ge},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2309.16289},
  url={https://www.semanticscholar.org/paper/9099ee08e59cc33ed1c88d4708cf5c931bf46dc4},
  abstract={Large language models (LLMs) have demonstrated strong capabilities in various aspects. However, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. To address this gap, we propose a comprehensive evaluation benchmark LawBench. LawBench has been meticulously crafted to have precise assessment of the LLMs' legal capabilities from three cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal knowledge understanding: whether LLMs can comprehend entities, events and relationships within legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label classification (MLC), regression, extraction and generation. We perform extensive evaluations of 51 LLMs on LawBench, including 20 multilingual LLMs, 22 Chinese-oriented LLMs and 9 legal specific LLMs. The results show that GPT-4 remains the best-performing LLM in the legal domain, surpassing the others by a significant margin. While fine-tuning LLMs on legal specific text brings certain improvements, we are still a long way from obtaining usable and reliable LLMs in legal tasks. All data, model predictions and evaluation code are released in https://github.com/open-compass/LawBench/. We hope this benchmark provides in-depth understanding of the LLMs' domain-specified capabilities and speed up the development of LLMs in the legal domain.},
  keywords={arxiv:2309.16289}
}

@article{matzakos2023learningmathematics,
  title={Learning Mathematics with Large Language Models: A Comparative Study with Computer Algebra Systems and Other Tools},
  author={Nikolaos Matzakos and Spyridon Doukakis and Maria Moundridou},
  year={2023},
  journal={International Journal of Emerging Technologies in Learning (iJET)},
  doi={10.3991/ijet.v18i20.42979},
  url={https://www.semanticscholar.org/paper/f7a30e15759805555950c3341b54e95832d5a440},
  abstract={Artificial intelligence (AI) has permeated all human activities, bringing about significant changes and creating new scientific and ethical challenges. The field of education could not be an exception to this development. OpenAI’s unveiling of ChatGPT, their large language model (LLM), has sparked significant interest in the potential applications of this technology in education. This paper aims to contribute to the ongoing discussion on the role of AI in education and its potential implications for the future of learning by exploring how LLMs could be utilized in the teaching of mathematics in higher education and how they compare to the currently widely used computer algebra systems (CAS) and other mathematical tools. It argues that these innovative tools have the potential to provide functional and pedagogical opportunities that may influence changes in curriculum and assessment approaches.}
}

@article{zhang2023learningproof,
  title={Learning Proof Transformations and Its Applications in Interactive Theorem Proving},
  author={Liao Zhang and Lasse Blaauwbroek and C. Kaliszyk and Josef Urban},
  year={2023},
  booktitle={International Symposium on Frontiers of Combining Systems},
  doi={10.1007/978-3-031-43369-6_13},
  url={https://www.semanticscholar.org/paper/fea0019bc70fd54558a497d12172d524ceeef787},
  abstract={. Interactive theorem provers are today increasingly used to certify mathematical theories. To formally prove a theorem, reasoning procedures called tactics are invoked successively on the proof states starting with the initial theorem statement, transforming them into subsequent intermediate goals, and ultimately discharging all proof obligations. In this work, we develop and experimentally evaluate approaches that predict the most likely tactics that will achieve particular desired transformations of proof states. First, we design several characterizations to efficiently capture the semantics of the proof transformations. Then we use them to create large datasets on which we train state-of-the-art random forests and language models. The trained models are evaluated experimentally, and we show that our best model is able to guess the right tactic for a given proof transformation in 74\% of the cases. Finally, we use the trained methods in two applications: proof shortening and tactic suggesting. To the best of our knowledge, this is the first time that tactic synthesis is trained on proof transformations and assists interactive theorem proving in these ways.}
}

@article{chen2023learningteach,
  title={Learning To Teach Large Language Models Logical Reasoning},
  author={Meiqi Chen and Yubo Ma and Kaitao Song and Yixin Cao and Yan Zhang and Dongsheng Li},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.09158},
  url={https://www.semanticscholar.org/paper/e43e95f706762b64d4ae17a5eb72731d2bd9047a}
}

@article{abdullahi2023learningmake,
  title={Learning to Make Rare and Complex Diagnoses With Generative AI Assistance: Qualitative Study of Popular Large Language Models},
  author={Tassallah Abdullahi and Ritambhara Singh and Carsten Eickhoff},
  year={2023},
  booktitle={JMIR Medical Education},
  doi={10.2196/51391},
  url={https://www.semanticscholar.org/paper/c6b8f70704cb066d4d731cc2426ad2ab4fbb8bc0},
  abstract={Background Patients with rare and complex diseases often experience delayed diagnoses and misdiagnoses because comprehensive knowledge about these diseases is limited to only a few medical experts. In this context, large language models (LLMs) have emerged as powerful knowledge aggregation tools with applications in clinical decision support and education domains. Objective This study aims to explore the potential of 3 popular LLMs, namely Bard (Google LLC), ChatGPT-3.5 (OpenAI), and GPT-4 (OpenAI), in medical education to enhance the diagnosis of rare and complex diseases while investigating the impact of prompt engineering on their performance. Methods We conducted experiments on publicly available complex and rare cases to achieve these objectives. We implemented various prompt strategies to evaluate the performance of these models using both open-ended and multiple-choice prompts. In addition, we used a majority voting strategy to leverage diverse reasoning paths within language models, aiming to enhance their reliability. Furthermore, we compared their performance with the performance of human respondents and MedAlpaca, a generative LLM specifically designed for medical tasks. Results Notably, all LLMs outperformed the average human consensus and MedAlpaca, with a minimum margin of 5\% and 13\%, respectively, across all 30 cases from the diagnostic case challenge collection. On the frequently misdiagnosed cases category, Bard tied with MedAlpaca but surpassed the human average consensus by 14\%, whereas GPT-4 and ChatGPT-3.5 outperformed MedAlpaca and the human respondents on the moderately often misdiagnosed cases category with minimum accuracy scores of 28\% and 11\%, respectively. The majority voting strategy, particularly with GPT-4, demonstrated the highest overall score across all cases from the diagnostic complex case collection, surpassing that of other LLMs. On the Medical Information Mart for Intensive Care-III data sets, Bard and GPT-4 achieved the highest diagnostic accuracy scores, with multiple-choice prompts scoring 93\%, whereas ChatGPT-3.5 and MedAlpaca scored 73\% and 47\%, respectively. Furthermore, our results demonstrate that there is no one-size-fits-all prompting approach for improving the performance of LLMs and that a single strategy does not universally apply to all LLMs. Conclusions Our findings shed light on the diagnostic capabilities of LLMs and the challenges associated with identifying an optimal prompting strategy that aligns with each language model’s characteristics and specific task requirements. The significance of prompt engineering is highlighted, providing valuable insights for researchers and practitioners who use these language models for medical training. Furthermore, this study represents a crucial step toward understanding how LLMs can enhance diagnostic reasoning in rare and complex medical cases, paving the way for developing effective educational tools and accurate diagnostic aids to improve patient care and outcomes.}
}

@article{jiang2023legalsyllogism,
  title={Legal Syllogism Prompting: Teaching Large Language Models for Legal Judgment Prediction},
  author={Cong Jiang and Xiaolei Yang},
  year={2023},
  booktitle={International Conference on Artificial Intelligence and Law},
  doi={10.1145/3594536.3595170},
  url={https://www.semanticscholar.org/paper/1640bda76a52f8b29e012b4e98b785882fb011c2},
  abstract={Legal syllogism is a form of deductive reasoning commonly used by legal professionals to analyze cases. In this paper, we propose legal syllogism prompting (LoT), a simple prompting method to teach large language models (LLMs) for legal judgment prediction. LoT teaches only that in the legal syllogism the major premise is law, the minor premise is the fact, and the conclusion is judgment. Then the models can produce a syllogism reasoning of the case and give the judgment without any learning, fine-tuning, or examples. On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment prediction experiments with GPT-3 models. Our results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks. LoT enables the model to concentrate on the key information relevant to the judgment and to correctly understand the legal meaning of acts, as compared to other methods. Our method enables LLMs to predict judgment along with law articles and justification, which significantly enhances the explainability of models.},
  keywords={arxiv:2307.08321}
}

@article{guha2023legalbenchcollaboratively,
  title={LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models},
  author={Neel Guha and Julian Nyarko and Daniel E. Ho and Christopher Ré and Adam Chilton and Aditya Narayana and Alex Chohlas-Wood and Austin M. K. Peters and Brandon Waldon and D. Rockmore and Diego A. Zambrano and Dmitry Talisman and E. Hoque and Faiz Surani and F. Fagan and Galit Sarfaty and Gregory M. Dickinson and Haggai Porat and Jason Hegland and Jessica Wu and Joe Nudell and Joel Niklaus and John J. Nay and Jonathan H. Choi and K. Tobia and M. Hagan and Megan Ma and Michael A. Livermore and Nikon Rasumov-Rahe and Nils Holzenberger and Noam Kolt and Peter Henderson and Sean Rehaag and Sharad Goel and Shangsheng Gao and Spencer Williams and Sunny G. Gandhi and Tomer Zur and Varun J. Iyer and Zehua Li},
  year={2023},
  booktitle={Social Science Research Network},
  doi={10.48550/arXiv.2308.11462},
  url={https://www.semanticscholar.org/paper/0aa5f3e94fab50e3c19870880fdaf245bfebdfae},
  abstract={The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning -- which distinguish between its many forms -- correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This paper describes LegalBench, presents an empirical evaluation of 20 open-source and commercial LLMs, and illustrates the types of research explorations LegalBench enables.},
  keywords={arxiv:2308.11462}
}

@article{wu2023lemurintegrating,
  title={Lemur: Integrating Large Language Models in Automated Program Verification},
  author={Haoze Wu and Clark W. Barrett and Nina Narodytska},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.04870},
  url={https://www.semanticscholar.org/paper/840e9bd68aa9f991148a905e6bc9336fcd22ead6},
  abstract={The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of transition rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure and demonstrate practical improvements on a set of synthetic and competition benchmarks.},
  keywords={arxiv:2310.04870}
}

@article{zhong2023letsthink,
  title={Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation},
  author={Shan Zhong and Zhongzhan Huang and Shanghua Gao and Wushao Wen and Liang Lin and M. Zitnik and Pan Zhou},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.01258},
  url={https://www.semanticscholar.org/paper/0ef006f9900185ac6ede39352d1de1ddee7923b6},
  abstract={Chain-of-Thought (CoT) [2], [3] guides large language models (LLMs) to reason step-by-step, and can motivate their logical reasoning ability. While effective for logi-cal tasks, CoT is not conducive to creative problem-solving which often requires out-of-box thoughts and is crucial for innovation advancements. In this paper, we explore the Leap-of-Thought (LoT) abilities within LLMs - a non-sequential, creative paradigm involving strong associations and knowledge leaps. To this end, we study LLMs on the popular Oogiri game which needs participants to have good creativity and strong associative thinking for responding unexpectedly and humorously to the given image, text, or both, and thus is suitable for LoT study. Then to investi-gate LLMs' LoT ability in the Oogiri game, we first build a multimodal and multilingual Oogiri-GO dataset which contains over 130,000 samples from the Oogiri game, and observe the insufficient LoT ability or failures of most existing LLMs on the Oogiri game. Accordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve LLM's LoT ability. CLoT first formulates the Oogiri-GO dataset into LoT-oriented instruction tuning data to train pre-trained LLM for achieving certain LoT humor generation and discrimination abilities. Then CLoT designs an ex-plorative self-refinement that encourages the LLM to gener-ate more creative LoT data via exploring parallels between seemingly unrelated concepts and selects high-quality data to train itself for self-refinement. CLoT not only excels in humor generation in the Oogiri game as shown in Fig. 1 but also boosts creative abilities in various tasks like “cloud guessing game” and “divergent association task”. These findings advance our understanding and offer a pathway to improve LLMs' creative capacities for innovative applications across domains. The dataset, code, and models have been released online: https://zhongshsh.github.io/CLoT.},
  keywords={arxiv:2312.02439}
}

@article{ma2023letsreward,
  title={Let's reward step by step: Step-Level reward model as the Navigators for Reasoning},
  author={Qianli Ma and Haotian Zhou and Tingkai Liu and Jianbo Yuan and Pengfei Liu and Yang You and Hongxia Yang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.10080},
  url={https://www.semanticscholar.org/paper/44b506d9619b5f957dc2b5588801138f343c0308},
  abstract={Recent years have seen considerable advancements in multi-step reasoning with Large Language Models (LLMs). The previous studies have elucidated the merits of integrating feedback or search mechanisms during model inference to improve the reasoning accuracy. The Process-Supervised Reward Model (PRM), typically furnishes LLMs with step-by-step feedback during the training phase, akin to Proximal Policy Optimization (PPO) or reject sampling. Our objective is to examine the efficacy of PRM in the inference phase to help discern the optimal solution paths for multi-step tasks such as mathematical reasoning and code generation. To this end, we propose a heuristic greedy search algorithm that employs the step-level feedback from PRM to optimize the reasoning pathways explored by LLMs. This tailored PRM demonstrated enhanced results compared to the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH. Additionally, to explore the versatility of our approach, we develop a novel method to automatically generate step-level reward dataset for coding tasks and observed similar improved performance in the code generation tasks. Thus highlighting the robust nature of our reward-model-based approach to inference for reasoning tasks.},
  keywords={arxiv:2310.10080}
}

@article{gao2023leveragingmedical,
  title={Leveraging A Medical Knowledge Graph into Large Language Models for Diagnosis Prediction},
  author={Yanjun Gao and Ruizhe Li and John R. Caskey and Dmitriy Dligach and T. Miller and M. Churpek and M. Afshar},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.14321},
  url={https://www.semanticscholar.org/paper/99559d3d32042d7fe34153e167d5c8f40a83fcca}
}

@article{yu2023leveraginggenerative,
  title={Leveraging Generative AI and Large Language Models: A Comprehensive Roadmap for Healthcare Integration},
  author={Ping Yu and Hua Xu and Xia Hu and C. Deng},
  year={2023},
  booktitle={Healthcare},
  doi={10.3390/healthcare11202776},
  url={https://www.semanticscholar.org/paper/d0c48d3b80efb9c170e7100cb9d78d1e7f7710bf},
  abstract={Generative artificial intelligence (AI) and large language models (LLMs), exemplified by ChatGPT, are promising for revolutionizing data and information management in healthcare and medicine. However, there is scant literature guiding their integration for non-AI professionals. This study conducts a scoping literature review to address the critical need for guidance on integrating generative AI and LLMs into healthcare and medical practices. It elucidates the distinct mechanisms underpinning these technologies, such as Reinforcement Learning from Human Feedback (RLFH), including few-shot learning and chain-of-thought reasoning, which differentiates them from traditional, rule-based AI systems. It requires an inclusive, collaborative co-design process that engages all pertinent stakeholders, including clinicians and consumers, to achieve these benefits. Although global research is examining both opportunities and challenges, including ethical and legal dimensions, LLMs offer promising advancements in healthcare by enhancing data management, information retrieval, and decision-making processes. Continued innovation in data acquisition, model fine-tuning, prompt strategy development, evaluation, and system implementation is imperative for realizing the full potential of these technologies. Organizations should proactively engage with these technologies to improve healthcare quality, safety, and efficiency, adhering to ethical and legal guidelines for responsible application.}
}

@article{ishay2023leveraginglarge,
  title={Leveraging Large Language Models to Generate Answer Set Programs},
  author={Adam Ishay and Zhun Yang and Joohyung Lee},
  year={2023},
  booktitle={International Conference on Principles of Knowledge Representation and Reasoning},
  doi={10.48550/arXiv.2307.07699},
  url={https://www.semanticscholar.org/paper/4a6d7b11c4aba5a23f68856989366dd4311e960b},
  abstract={Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated exceptional performance in various natural language processing tasks and have shown the ability to solve certain reasoning problems. However, their reasoning capabilities are limited and relatively shallow, despite the application of various prompting techniques. In contrast, formal logic is adept at handling complex reasoning, but translating natural language descriptions into formal logic is a challenging task that non-experts struggle with. This paper proposes a neuro-symbolic method that combines the strengths of large language models and answer set programming. Specifically, we employ an LLM to transform natural language descriptions of logic puzzles into answer set programs. We carefully design prompts for an LLM to convert natural language descriptions into answer set programs in a step by step manner. Surprisingly, with just a few in-context learning examples, LLMs can generate reasonably complex answer set programs. The majority of errors made are relatively simple and can be easily corrected by humans, thus enabling LLMs to effectively assist in the creation of answer set programs.},
  keywords={arxiv:2307.07699}
}

@article{gao2023leveragingmedical,
  title={Leveraging Medical Knowledge Graphs Into Large Language Models for Diagnosis Prediction: Design and Application Study},
  author={Yanjun Gao and Ruizhe Li and E. Croxford and J. Caskey and Brian W. Patterson and M. Churpek and Timothy A. Miller and D. Dligach and Majid Afshar},
  year={2023},
  booktitle={JMIR AI},
  doi={10.2196/58670},
  url={https://www.semanticscholar.org/paper/6418f301fa2d72e57d82a0aaf3685af58079b134},
  abstract={Background Electronic health records (EHRs) and routine documentation practices play a vital role in patients’ daily care, providing a holistic record of health, diagnoses, and treatment. However, complex and verbose EHR narratives can overwhelm health care providers, increasing the risk of diagnostic inaccuracies. While large language models (LLMs) have showcased their potential in diverse language tasks, their application in health care must prioritize the minimization of diagnostic errors and the prevention of patient harm. Integrating knowledge graphs (KGs) into LLMs offers a promising approach because structured knowledge from KGs could enhance LLMs’ diagnostic reasoning by providing contextually relevant medical information. Objective This study introduces DR.KNOWS (Diagnostic Reasoning Knowledge Graph System), a model that integrates Unified Medical Language System–based KGs with LLMs to improve diagnostic predictions from EHR data by retrieving contextually relevant paths aligned with patient-specific information. Methods DR.KNOWS combines a stack graph isomorphism network for node embedding with an attention-based path ranker to identify and rank knowledge paths relevant to a patient’s clinical context. We evaluated DR.KNOWS on 2 real-world EHR datasets from different geographic locations, comparing its performance to baseline models, including QuickUMLS and standard LLMs (Text-to-Text Transfer Transformer and ChatGPT). To assess diagnostic reasoning quality, we designed and implemented a human evaluation framework grounded in clinical safety metrics. Results DR.KNOWS demonstrated notable improvements over baseline models, showing higher accuracy in extracting diagnostic concepts and enhanced diagnostic prediction metrics. Prompt-based fine-tuning of Text-to-Text Transfer Transformer with DR.KNOWS knowledge paths achieved the highest ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation–Longest Common Subsequence) and concept unique identifier F1-scores, highlighting the benefits of KG integration. Human evaluators found the diagnostic rationales of DR.KNOWS to be aligned strongly with correct clinical reasoning, indicating improved abstraction and reasoning. Recognized limitations include potential biases within the KG data, which we addressed by emphasizing case-specific path selection and proposing future bias-mitigation strategies. Conclusions DR.KNOWS offers a robust approach for enhancing diagnostic accuracy and reasoning by integrating structured KG knowledge into LLM-based clinical workflows. Although further work is required to address KG biases and extend generalizability, DR.KNOWS represents progress toward trustworthy artificial intelligence–driven clinical decision support, with a human evaluation framework focused on diagnostic safety and alignment with clinical standards.},
  keywords={arxiv:2308.14321}
}

@article{yang2023lidarllmexploring,
  title={LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding},
  author={Senqiao Yang and Jiaming Liu and Ray Zhang and Mingjie Pan and Zoey Guo and Xiaoqi Li and Zehui Chen and Peng Gao and Yandong Guo and Shanghang Zhang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.14074},
  url={https://www.semanticscholar.org/paper/5edf706467dc76cd09319592d18db0ad4e1fb64d},
  abstract={Recently, Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have shown promise in instruction following and 2D image understanding. While these models are powerful, they have not yet been developed to comprehend the more challenging 3D physical scenes, especially when it comes to the sparse outdoor LiDAR data. In this paper, we introduce LiDAR-LLM, which takes raw LiDAR data as input and harnesses the remarkable reasoning capabilities of LLMs to gain a comprehensive understanding of outdoor 3D scenes. The central insight of our LiDAR-LLM is the reformulation of 3D outdoor scene cognition as a language modeling problem, encompassing tasks such as 3D captioning, 3D grounding, 3D question answering, etc. Specifically, due to the scarcity of 3D LiDAR-text pairing data, we introduce a three-stage training strategy and generate relevant datasets, progressively aligning the 3D modality with the language embedding space of LLM. Furthermore, we design a View-Aware Transformer (VAT) to connect the 3D encoder with the LLM, which effectively bridges the modality gap and enhances the LLM's spatial orientation comprehension of visual features. Our experiments show that LiDAR-LLM possesses favorable capabilities to comprehend various instructions regarding 3D scenes and engage in complex spatial reasoning. LiDAR-LLM attains a 40.9 BLEU-1 on the 3D captioning task and achieves a 63.1\textbackslash\{\}\% classification accuracy and a 14.3\textbackslash\{\}\% BEV mIoU on the 3D grounding task. Web page: https://sites.google.com/view/lidar-llm},
  keywords={arxiv:2312.14074}
}

@article{azerbayev2023llemmaopen,
  title={Llemma: An Open Language Model For Mathematics},
  author={Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and S. McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and S. Welleck},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.10631},
  url={https://www.semanticscholar.org/paper/b16c7d45183b9d595ab64301be019741b1528860},
  abstract={We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.},
  keywords={arxiv:2310.10631}
}

@article{pan2023logiclmempowering,
  title={Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning},
  author={Liangming Pan and Alon Albalak and Xinyi Wang and William Yang Wang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.12295},
  url={https://www.semanticscholar.org/paper/9e9e4df2996bac794c4f04cb887df3e553bae4fd},
  abstract={Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2\% over using LLM alone with standard prompting and 18.4\% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. Code and data are publicly available at https://github.com/teacherpeterpan/Logic-LLM.},
  keywords={arxiv:2305.12295}
}

@article{jiang2023lowparameterfederated,
  title={Low-Parameter Federated Learning with Large Language Models},
  author={Jing Jiang and Xiangyang Liu and Chenyou Fan},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2307.13896},
  url={https://www.semanticscholar.org/paper/573dad7b2fca7ce72a7f0daf681391d96379ebe3},
  abstract={We study few-shot Natural Language Understanding (NLU) tasks with Large Language Models (LLMs) in federated learning (FL) scenarios. It is a challenging task due to limited labeled data and communication capacities in FL, especially with mobile devices. Recent studies show LLMs can be prompted to perform few-shot NLU tasks like sentiment analysis and arithmetic reasoning. However, the huge sizes of LLMs result in high computation and communication costs, making classical FL schemes impractical. To address these challenges, we propose Low-Parameter Federated Learning (LP-FL). LP-FL combines few-shot prompt learning from LLMs with efficient communication and federating techniques. Our approach enables federated clients to assign soft labels to unlabeled data using gradually learned knowledge from the global model. Through iterative soft-label assigning, we continually expand the labeled set during the FL process. Additionally, to reduce computation and communication costs, LP-FL utilizes the Low-Rank Adaptation (LoRA) technique for compact learnable parameter construction, efficient local model fine-tuning, and affordable global model federation. LP-FL consistently outperforms Full-Parameter Federated Learning (FP-FL) in sentiment analysis tasks across various FL settings. Its resistance to overfitting allows LP-FL to equal or surpass centralized training in few-shot scenarios.},
  keywords={arxiv:2307.13896}
}

@misc{liu2023m2ugenmultimodal,
  title={M\$\^{}\{2\}\$UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models},
  author={Shansong Liu and Atin Sakkeer Hussain and Qilong Wu and Chenshuo Sun and Ying Shan},
  year={2023},
  url={https://www.semanticscholar.org/paper/d426b5d9a20762541519d5572970df8db7e9fbcd},
  abstract={The current landscape of research leveraging large language models (LLMs) is experiencing a surge. Many works harness the powerful reasoning capabilities of these models to comprehend various modalities, such as text, speech, images, videos, etc. They also utilize LLMs to understand human intention and generate desired outputs like images, videos, and music. However, research that combines both understanding and generation using LLMs is still limited and in its nascent stage. To address this gap, we introduce a Multi-modal Music Understanding and Generation (M\$\^{}\{2\}\$UGen) framework that integrates LLM's abilities to comprehend and generate music for different modalities. The M\$\^{}\{2\}\$UGen framework is purpose-built to unlock creative potential from diverse sources of inspiration, encompassing music, image, and video through the use of pretrained MERT, ViT, and ViViT models, respectively. To enable music generation, we explore the use of AudioLDM 2 and MusicGen. Bridging multi-modal understanding and music generation is accomplished through the integration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA model to generate extensive datasets that support text/image/video-to-music generation, facilitating the training of our M\$\^{}\{2\}\$UGen framework. We conduct a thorough evaluation of our proposed framework. The experimental results demonstrate that our model achieves or surpasses the performance of the current state-of-the-art models.},
  keywords={arxiv:2311.11255}
}

@article{nathani2023multiaspectfeedback,
  title={MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models},
  author={Deepak Nathani and David Wang and Liangming Pan and W. Wang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.12426},
  url={https://www.semanticscholar.org/paper/20eecb9ead20ffe49a66588a9662336eefb20a54},
  abstract={Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20\% in Mathematical Reasoning and up to 18\% in Logical Entailment.},
  keywords={arxiv:2310.12426}
}

@article{xu2023magicinvestigation,
  title={MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration},
  author={Lin Xu and Zhiyuan Hu and Daquan Zhou and Hongyu Ren and Zhen Dong and Kurt Keutzer and See-Kiong Ng and Jiashi Feng},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2311.08562},
  url={https://www.semanticscholar.org/paper/72273f7a050529fc71c7d45c0256d2b9754f56bb},
  abstract={Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating exceptional reasoning, tool usage, and memory capabilities. As their applications expand into multi-agent environments, there arises a need for a comprehensive evaluation framework that captures LLMs’ reasoning, planning, collaboration, and other social abilities. This work introduces a novel competition-based benchmark framework specifically designed to assess LLMs within multi-agent settings, providing quantitative metrics to evaluate their judgment, reasoning, deception, self-awareness, cooperation, coordination, and rationality.We utilize two social deduction games alongside three game-theory scenarios to create diverse environments.Our frame is fortified with the probabilistic graphic modeling (PGM) method, enhancing the LLMs’ capabilities in navigating complex social and cognitive dimensions. We evaluate seven LLMs, quantitatively highlighting a significant capability gap of over threefold between the strongest, GPT o1, and the weakest, Llama-2-70B. It also confirms that our PGM enhancement boosts the abilities of all selected models by an average of 37\%. Our data and code can be found here https://github.com/cathyxl/MAgIC.},
  keywords={arxiv:2311.08562}
}

@article{yue2023mammothbuilding,
  title={MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning},
  author={Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2309.05653},
  url={https://www.semanticscholar.org/paper/a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9},
  abstract={We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16\% and 32\%. Remarkably, our MAmmoTH-7B model reaches 33\% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23\%, and the MAmmoTH-34B model achieves 44\% accuracy on MATH, even surpassing GPT-4's CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.},
  keywords={arxiv:2309.05653}
}

@article{chen2023mcckdmulticot,
  title={MCC-KD: Multi-CoT Consistent Knowledge Distillation},
  author={Hongzhan Chen and Siyue Wu and Xiaojun Quan and Rui Wang and Ming Yan and Ji Zhang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.14747},
  url={https://www.semanticscholar.org/paper/ef9079f32e806e4d297cee28f36b2321acee9eb3},
  abstract={Large language models (LLMs) have showcased remarkable capabilities in complex reasoning through chain of thought (CoT) prompting. Recently, there has been a growing interest in transferring these reasoning abilities from LLMs to smaller models. However, achieving both the diversity and consistency in rationales presents a challenge. In this paper, we focus on enhancing these two aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple rationales for each question and enforce consistency among the corresponding predictions by minimizing the bidirectional KL-divergence between the answer distributions. We investigate the effectiveness of MCC-KD with different model architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both mathematical reasoning and commonsense reasoning benchmarks. The empirical results not only confirm MCC-KD's superior performance on in-distribution datasets but also highlight its robust generalization ability on out-of-distribution datasets.},
  keywords={arxiv:2310.14747}
}

@article{chen2023meditron70bscaling,
  title={MEDITRON-70B: Scaling Medical Pretraining for Large Language Models},
  author={Zeming Chen and Alejandro Hern'andez Cano and Angelika Romanou and Antoine Bonnet and Kyle Matoba and Francesco Salvi and Matteo Pagliardini and Simin Fan and Andreas Kopf and Amirkeivan Mohtashami and Alexandre Sallinen and Alireza Sakhaeirad and Vinitra Swamy and Igor Krawczuk and Deniz Bayazit and Axel Marmet and Syrielle Montariol and Mary-Anne Hartley and Martin Jaggi and Antoine Bosselut},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.16079},
  url={https://www.semanticscholar.org/paper/ff5f0c5b6905a8c4b361a625b450e9ab417fa854},
  abstract={Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities. In this work, we improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a 6\% absolute performance gain over the best public baseline in its parameter class and 3\% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5\% of GPT-4 and 10\% of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MEDITRON model weights to drive open-source development of more capable medical LLMs.},
  keywords={arxiv:2311.16079}
}

@article{jia2023millmutual,
  title={MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion},
  author={Pengyue Jia and Yiding Liu and Xiangyu Zhao and Xiaopeng Li and Changying Hao and Shuaiqiang Wang and Dawei Yin},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2310.19056},
  url={https://www.semanticscholar.org/paper/f1a950af6e547c3c12a4defbd7120d28f540c6c1},
  abstract={Query expansion, pivotal in search engines, enhances the representation of user information needs with additional terms. While existing methods expand queries using retrieved or generated contextual documents, each approach has notable limitations. Retrieval-based methods often fail to accurately capture search intent, particularly with brief or ambiguous queries. Generation-based methods, utilizing large language models (LLMs), generally lack corpus-specific knowledge and entail high fine-tuning costs. To address these gaps, we propose a novel zero-shot query expansion framework utilizing LLMs for mutual verification. Specifically, we first design a query-query-document generation method, leveraging LLMs’ zero-shot reasoning ability to produce diverse sub-queries and corresponding documents. Then, a mutual verification process synergizes generated and retrieved documents for optimal expansion. Our proposed method is fully zero-shot, and extensive experiments on three public benchmark datasets are conducted to demonstrate its effectiveness over existing methods. Our code is available online at https://github.com/Applied-Machine-Learning-Lab/MILL to ease reproduction.},
  keywords={arxiv:2310.19056}
}

@article{zhang2023mlcopilotunleashing,
  title={MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks},
  author={Lei Zhang and Yuge Zhang and Kan Ren and Dongsheng Li and Yuqing Yang},
  year={2023},
  booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2304.14979},
  url={https://www.semanticscholar.org/paper/fce42753155280051ac64817404b4e1d3be6ebaa},
  abstract={The field of machine learning (ML) has gained widespread adoption, leading to significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time-consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art large language models to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the existing experiences of ML tasks and (ii) reason effectively to deliver promising results for new tasks. The solution generated can be used directly to achieve high levels of competitiveness.},
  keywords={arxiv:2304.14979}
}

@article{zeng2023mrgsm8kmetareasoning,
  title={MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation},
  author={Zhongshen Zeng and Pengguang Chen and Shu Liu and Haiyun Jiang and Jiaya Jia},
  year={2023},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/490e465ef0909f01a82916f4adab0c410576c866},
  abstract={In this work, we introduce a novel evaluation paradigm for Large Language Models (LLMs) that compels them to transition from a traditional question-answering role, akin to a student, to a solution-scoring role, akin to a teacher. This paradigm, focusing on"reasoning about reasoning,"hence termed meta-reasoning, shifts the emphasis from result-oriented assessments, which often neglect the reasoning process, to a more comprehensive evaluation that effectively distinguishes between the cognitive capabilities of different models. By applying this paradigm in the GSM8K dataset, we have developed the MR-GSM8K benchmark. Our extensive analysis includes several state-of-the-art models from both open-source and commercial domains, uncovering fundamental deficiencies in their training and evaluation methodologies. Notably, while models like Deepseek-v2 and Claude3-Sonnet closely competed with GPT-4 in GSM8K, their performance disparities expanded dramatically in MR-GSM8K, with differences widening to over 20 absolute points, underscoring the significant challenge posed by our meta-reasoning approach.},
  keywords={arxiv:2312.17080}
}

@article{wang2023makinglarge,
  title={Making Large Language Models Better Reasoners with Alignment},
  author={Peiyi Wang and Lei Li and Liang Chen and Feifan Song and Binghuai Lin and Yunbo Cao and Tianyu Liu and Zhifang Sui},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2309.02144},
  url={https://www.semanticscholar.org/paper/74b4b993babe99bc5f5c589c27fef0f1baba606b},
  abstract={Reasoning is a cognitive process of using evidence to reach a sound conclusion. The reasoning capability is essential for large language models (LLMs) to serve as the brain of the artificial general intelligence agent. Recent studies reveal that fine-tuning LLMs on data with the chain of thought (COT) reasoning process can significantly enhance their reasoning capabilities. However, we find that the fine-tuned LLMs suffer from an \textbackslash\{\}textit\{Assessment Misalignment\} problem, i.e., they frequently assign higher scores to subpar COTs, leading to potential limitations in their reasoning abilities. To address this problem, we introduce an \textbackslash\{\}textit\{Alignment Fine-Tuning (AFT)\} paradigm, which involves three steps: 1) fine-tuning LLMs with COT training data; 2) generating multiple COT responses for each question, and categorizing them into positive and negative ones based on whether they achieve the correct answer; 3) calibrating the scores of positive and negative responses given by LLMs with a novel constraint alignment loss. Specifically, the constraint alignment loss has two objectives: a) Alignment, which guarantees that positive scores surpass negative scores to encourage answers with high-quality COTs; b) Constraint, which keeps the negative scores confined to a reasonable range to prevent the model degradation. Beyond just the binary positive and negative feedback, the constraint alignment loss can be seamlessly adapted to the ranking situations when ranking feedback is accessible. Furthermore, we also delve deeply into recent ranking-based alignment methods, such as DPO, RRHF, and PRO, and discover that the constraint, which has been overlooked by these approaches, is also crucial for their performance. Extensive experiments on four reasoning benchmarks with both binary and ranking feedback demonstrate the effectiveness of AFT.},
  keywords={arxiv:2309.02144}
}

@article{li2023manipllmembodied,
  title={ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation},
  author={Xiaoqi Li and Mingxu Zhang and Yiran Geng and Haoran Geng and Yuxing Long and Yan Shen and Renrui Zhang and Jiaming Liu and Hao Dong},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.01710},
  url={https://www.semanticscholar.org/paper/d23662d2275498f4fa7a937dabd60fbc7faf9c14},
  abstract={Robot manipulation relies on accurately predicting contact points and end-effector directions to ensure successful operation. However, learning-based robot manipulation, trained on a limited category within a simulator, often struggles to achieve generalizability, especially when confronted with extensive categories. Therefore, we introduce an innovative approach for robot manipulation that leverages the robust reasoning capabilities of Multimodal Large Language Models (MLLMs) to enhance the stability and generalization of manipulation. By fine-tuning the injected adapters, we preserve the inherent common sense and reasoning ability of the MLLMs while equipping them with the ability for manipulation. The fundamental insight lies in the introduced fine-tuning paradigm, encompassing object category understanding, affordance prior reasoning, and object-centric pose prediction to stimulate the reasoning ability of MLLM in manipulation. During inference, our approach utilizes an RGB image and text prompt to predict the end effector's pose in chain of thoughts. After the initial contact is established, an active impedance adaptation policy is introduced to plan the upcoming way-points in a closed-loop manner. Moreover, in real world, we design a test-time adaptation (TTA) strategy for manipulation to enable the model better adapt to the current real-world scene configuration. Experiments in simulator and real-world show the promising performance of Mani-pLLM. More details and demonstrations can be found at https://sites.google.com/view/manipllm.},
  keywords={arxiv:2312.16217}
}

@article{zhou2023mathattackattacking,
  title={MathAttack: Attacking Large Language Models Towards Math Solving Ability},
  author={Zihao Zhou and Qiufeng Wang and Mingyu Jin and Jie Yao and Jianan Ye and Wei Liu and Wei Wang and Xiaowei Huang and Kaizhu Huang},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2309.01686},
  url={https://www.semanticscholar.org/paper/3886f3bd2a0af9e75bf9fa5b7db4224969dbf346},
  abstract={With the boom of Large Language Models (LLMs), the research of solving Math Word Problem (MWP) has recently made great progress. However, there are few studies to examine the robustness of LLMs in math solving ability. Instead of attacking prompts in the use of LLMs, we propose a MathAttack model to attack MWP samples which are closer to the essence of robustness in solving math problems. Compared to traditional text adversarial attack, it is essential to preserve the mathematical logic of original MWPs during the attacking. To this end, we propose logical entity recognition to identify logical entries which are then frozen. Subsequently, the remaining text are attacked by adopting a word-level attacker. Furthermore, we propose a new dataset RobustMath to evaluate the robustness of LLMs in math solving ability. Extensive experiments on our RobustMath and two another math benchmark datasets GSM8K and MultiAirth show that MathAttack could effectively attack the math solving ability of LLMs. In the experiments, we observe that (1) Our adversarial samples from higher-accuracy LLMs are also effective for attacking LLMs with lower accuracy (e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot prompts); (2) Complex MWPs (such as more solving steps, longer text, more numbers) are more vulnerable to attack; (3) We can improve the robustness of LLMs by using our adversarial samples in few-shot prompts. Finally, we hope our practice and observation can serve as an important attempt towards enhancing the robustness of LLMs in math solving ability. The code and dataset is available at: https://github.com/zhouzihao501/MathAttack.},
  keywords={arxiv:2309.01686}
}

@article{wang2023mathcoderseamless,
  title={MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning},
  author={Ke Wang and Houxing Ren and Aojun Zhou and Zimu Lu and Sichun Luo and Weikang Shi and Renrui Zhang and Linqi Song and Mingjie Zhan and Hongsheng Li},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.03731},
  url={https://www.semanticscholar.org/paper/cddb552f6c3464a54a02b0b64b2d1af56c086606},
  abstract={The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2\%) and GSM8K (83.9\%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/MathCoder.},
  keywords={arxiv:2310.03731}
}

@article{imani2023mathpromptermathematical,
  title={MathPrompter: Mathematical Reasoning using Large Language Models},
  author={Shima Imani and Liang Du and H. Shrivastava},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2303.05398},
  url={https://www.semanticscholar.org/paper/b626560f19f815808a289ef5c24a17c57320da70},
  abstract={Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose ‘MathPrompter’, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the ‘MultiArith’ dataset (78.7\% - 92.5\%) evaluated using 175B parameter GPT-based LLM.},
  keywords={arxiv:2303.05398}
}

@article{lu2023mathvistaevaluating,
  title={MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  author={Pan Lu and Hritik Bansal and Tony Xia and Jiacheng Liu and Chun-yue Li and Hannaneh Hajishirzi and Hao Cheng and Kai-Wei Chang and Michel Galley and Jianfeng Gao},
  year={2023},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/8946891e94831adc8cddb0d32311cce2445c96d2},
  abstract={Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9\%, substantially outperforming Bard, the second-best performer, by 15.1\%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4\%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.},
  keywords={arxiv:2310.02255}
}

@article{romeraparedes2023mathematicaldiscoveries,
  title={Mathematical discoveries from program search with large language models},
  author={Bernardino Romera-Paredes and M. Barekatain and Alexander Novikov and Matej Balog and M. P. Kumar and Emilien Dupont and Francisco J. R. Ruiz and J. Ellenberg and Pengming Wang and Omar Fawzi and Pushmeet Kohli and Alhussein Fawzi and Josh Grochow and Andrea Lodi and Jean-Baptiste Mouret and Talia Ringer and Tao Yu},
  year={2023},
  booktitle={Nature},
  doi={10.1038/s41586-023-06924-6},
  url={https://www.semanticscholar.org/paper/d32ba88571141ed0ebe7aeefbaa4ccaf8cda7be3},
  abstract={Large language models (LLMs) have demonstrated tremendous capabilities in solving complex tasks, from quantitative reasoning to understanding natural language. However, LLMs sometimes suffer from confabulations (or hallucinations), which can result in them making plausible but incorrect statements1,2. This hinders the use of current large models in scientific discovery. Here we introduce FunSearch (short for searching in the function space), an evolutionary procedure based on pairing a pretrained LLM with a systematic evaluator. We demonstrate the effectiveness of this approach to surpass the best-known results in important problems, pushing the boundary of existing LLM-based approaches3. Applying FunSearch to a central problem in extremal combinatorics—the cap set problem—we discover new constructions of large cap sets going beyond the best-known ones, both in finite dimensional and asymptotic cases. This shows that it is possible to make discoveries for established open problems using LLMs. We showcase the generality of FunSearch by applying it to an algorithmic problem, online bin packing, finding new heuristics that improve on widely used baselines. In contrast to most computer search approaches, FunSearch searches for programs that describe how to solve a problem, rather than what the solution is. Beyond being an effective and scalable strategy, discovered programs tend to be more interpretable than raw solutions, enabling feedback loops between domain experts and FunSearch, and the deployment of such programs in real-world applications. FunSearch makes discoveries in established open problems using large language models by searching for programs describing how to solve a problem, rather than what the solution is.}
}

@article{ni2023mechagentslarge,
  title={MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge},
  author={Bo Ni and Markus J. Buehler},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.08166},
  url={https://www.semanticscholar.org/paper/da17ec7b2867c3e02951c5598936b891ebe865ce},
  abstract={Solving mechanics problems using numerical methods requires comprehensive intelligent capability of retrieving relevant knowledge and theory, constructing and executing codes, analyzing the results, a task that has thus far mainly been reserved for humans. While emerging AI methods can provide effective approaches to solve end-to-end problems, for instance via the use of deep surrogate models or various data analytics strategies, they often lack physical intuition since knowledge is baked into the parametric complement through training, offering less flexibility when it comes to incorporating mathematical or physical insights. By leveraging diverse capabilities of multiple dynamically interacting large language models (LLMs), we can overcome the limitations of conventional approaches and develop a new class of physics-inspired generative machine learning platform, here referred to as MechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for elasticity problems, via autonomous collaborations. A two-agent team can effectively write, execute and self-correct code, in order to apply finite element methods to solve classical elasticity problems in various flavors (different boundary conditions, domain geometries, meshes, small/finite deformation and linear/hyper-elastic constitutive laws, and others). For more complex tasks, we construct a larger group of agents with enhanced division of labor among planning, formulating, coding, executing and criticizing the process and results. The agents mutually correct each other to improve the overall team-work performance in understanding, formulating and validating the solution. Our framework shows the potential of synergizing the intelligence of language models, the reliability of physics-based modeling, and the dynamic collaborations among diverse agents, opening novel avenues for automation of solving engineering problems.},
  keywords={arxiv:2311.08166}
}

@article{umapathi2023medhaltmedical,
  title={Med-HALT: Medical Domain Hallucination Test for Large Language Models},
  author={Logesh Kumar Umapathi and Ankit Pal and Malaikannan Sankarasubbu},
  year={2023},
  booktitle={Conference on Computational Natural Language Learning},
  doi={10.48550/arXiv.2307.15343},
  url={https://www.semanticscholar.org/paper/3b0792f6d7f6aa6aadd316e73943116afef2979b},
  abstract={This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs’ problem-solving and information retrieval abilities. Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io},
  keywords={arxiv:2307.15343}
}

@article{tang2023medagentslarge,
  title={MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning},
  author={Xiangru Tang and Anni Zou and Zhuosheng Zhang and Yilun Zhao and Xingyao Zhang and Arman Cohan and Mark B. Gerstein},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2311.10537},
  url={https://www.semanticscholar.org/paper/44d16a076c00ecada3d425203377e4ec951c4ed0},
  abstract={Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose MedAgents, a novel multi-disciplinary collaboration framework for the medical domain. MedAgents leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MedAgents framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at https://github.com/gersteinlab/MedAgents.},
  keywords={arxiv:2311.10537}
}

@article{cai2023medbenchlargescale,
  title={MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models},
  author={Yan Cai and Linlin Wang and Ye Wang and Gerard de Melo and Ya Zhang and Yanfeng Wang and Liang He},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2312.12806},
  url={https://www.semanticscholar.org/paper/6887f052c78f016fbf9cbb0c4f887e5c14069651},
  abstract={The emergence of various medical large language models (LLMs) in the medical domain has highlighted the need for unified evaluation standards, as manual evaluation of LLMs proves to be time-consuming and labor-intensive. To address this issue, we introduce MedBench, a comprehensive benchmark for the Chinese medical domain, comprising 40,041 questions sourced from authentic examination exercises and medical reports of diverse branches of medicine. In particular, this benchmark is composed of four key components: the Chinese Medical Licensing Examination, the Resident Standardization Training Examination, the Doctor In-Charge Qualification Examination, and real-world clinic cases encompassing examinations, diagnoses, and treatments. MedBench replicates the educational progression and clinical practice experiences of doctors in Mainland China, thereby establish- ing itself as a credible benchmark for assessing the mastery of knowledge and reasoning abilities in medical language learning models. We perform extensive experiments and conduct an in-depth analysis from diverse perspectives, which culminate in the following findings: (1) Chinese medical LLMs underperform on this benchmark, highlighting the need for significant advances in clinical knowledge and diagnostic precision. (2) Several general-domain LLMs surprisingly possess considerable medical knowledge. These findings elucidate both the capabilities and limitations of LLMs within the context of MedBench, with the ultimate goal of aiding the medical research community.},
  keywords={arxiv:2312.12806}
}

@article{kim2023memoryefficientfinetuning,
  title={Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization},
  author={Jeonghoon Kim and J. H. Lee and Sungdong Kim and Joonsuk Park and Kang Min Yoo and S. Kwon and Dongsoo Lee},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2305.14152},
  url={https://www.semanticscholar.org/paper/a10843d1349fff8d2a7d9722f800802187fef67f},
  abstract={Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.},
  keywords={arxiv:2305.14152}
}

@article{wei2023menatqadataset,
  title={MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models},
  author={Yifan Wei and Yisong Su and Huanhuan Ma and Xiaoyan Yu and Fangyu Lei and Yuanzhe Zhang and Jun Zhao and Kang Liu},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.05157},
  url={https://www.semanticscholar.org/paper/ff2eecb21972eb287064f98db1a4487c62bd7566},
  abstract={Large language models (LLMs) have shown nearly saturated performance on many natural language processing (NLP) tasks. As a result, it is natural for people to believe that LLMs have also mastered abilities such as time understanding and reasoning. However, research on the temporal sensitivity of LLMs has been insufficiently emphasized. To fill this gap, this paper constructs Multiple Sensitive Factors Time QA (MenatQA), which encompasses three temporal factors (scope factor, order factor, counterfactual factor) with total 2,853 samples for evaluating the time comprehension and reasoning abilities of LLMs. This paper tests current mainstream LLMs with different parameter sizes, ranging from billions to hundreds of billions. The results show most LLMs fall behind smaller temporal reasoning models with different degree on these factors. In specific, LLMs show a significant vulnerability to temporal biases and depend heavily on the temporal information provided in questions. Furthermore, this paper undertakes a preliminary investigation into potential improvement strategies by devising specific prompts and leveraging external tools. These approaches serve as valuable baselines or references for future research endeavors.},
  keywords={arxiv:2310.05157}
}

@article{wang2023metareasoningsemanticssymbol,
  title={Meta-Reasoning: Semantics-Symbol Deconstruction For Large Language Models},
  author={Yiming Wang and Zhuosheng Zhang and Rui Wang},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2306.17820},
  url={https://www.semanticscholar.org/paper/8568d7bd9dfb5ba0b91940b938b44a88fafdf95b},
  abstract={Neural-symbolic methods have demonstrated efficiency in enhancing the reasoning abilities of large language models (LLMs). However, existing methods mainly rely on syntactically mapping natural languages to complete formal languages like Python and SQL. Those methods require that reasoning tasks be convertible into programs, which cater to the computer execution mindset and deviate from human reasoning habits. To broaden symbolic methods' applicability and adaptability in the real world, we propose the Meta-Reasoning from a linguistic perspective. This method empowers LLMs to deconstruct reasoning-independent semantic information into generic symbolic representations, thereby efficiently capturing more generalized reasoning knowledge. We conduct extensive experiments on more than ten datasets encompassing conventional reasoning tasks like arithmetic, symbolic, and logical reasoning, and the more complex interactive reasoning tasks like theory-of-mind reasoning. Experimental results demonstrate that Meta-Reasoning significantly enhances in-context reasoning accuracy, learning efficiency, out-of-domain generalization, and output stability compared to the Chain-of-Thought technique. Code and data are publicly available at \textbackslash\{\}url\{https://github.com/Alsace08/Meta-Reasoning\}.},
  keywords={arxiv:2306.17820}
}

@article{yu2023metamathbootstrap,
  title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  author={L. Yu and Weisen Jiang and Han Shi and Jincheng Yu and Zhengying Liu and Yu Zhang and James T. Kwok and Zheng Li and Adrian Weller and Weiyang Liu},
  year={2023},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/77b1f1c6d1658d120456b9046667cf009ceb39ce},
  abstract={Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4\% on GSM8K and 19.4\% on MATH, exceeding the state-of-the-art models of the same size by 11.5\% and 8.7\%. Particularly, MetaMath-70B achieves an accuracy of 82.3\% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.},
  keywords={arxiv:2309.12284}
}

@article{wang2023metacognitiveprompting,
  title={Metacognitive Prompting Improves Understanding in Large Language Models},
  author={Yuqing Wang and Yun Zhao},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2308.05342},
  url={https://www.semanticscholar.org/paper/896ca0a68e4d33d76a7366bcab85eb7d2605a8c4},
  abstract={In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. Recent advancements in prompting have enhanced reasoning in logic-intensive tasks for LLMs, yet the nuanced understanding abilities of these models, crucial for processing and interpreting complex information, remain underexplored. In this study, we introduce Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. We conduct extensive experiments on four prevalent LLMs: Llama2, PaLM2, GPT-3.5, and GPT-4, across ten natural language understanding (NLU) datasets from GLUE, SuperGLUE, BLUE, and LexGLUE benchmarks. Additionally, we compare our method with chain-of-thought prompting and its advanced versions. The results show that GPT-4 consistently excels across all tasks, while other models have shown significant progress in some tasks when used in conjunction with MP. Furthermore, MP consistently outperforms existing prompting methods in both general and domain-specific NLU tasks. This study underscores the potential to amplify the understanding abilities of LLMs and highlights the benefits of mirroring human introspective reasoning in NLU tasks.},
  keywords={arxiv:2308.05342}
}

@article{liang2023mintboosting,
  title={MinT: Boosting Generalization in Mathematical Reasoning via Multi-view Fine-tuning},
  author={Zhenwen Liang and Dian Yu and Xiaoman Pan and Wenlin Yao and Qingkai Zeng and Xiangliang Zhang and Dong Yu},
  year={2023},
  booktitle={International Conference on Language Resources and Evaluation},
  doi={10.48550/arXiv.2307.07951},
  url={https://www.semanticscholar.org/paper/c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36},
  abstract={Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on distilling knowledge from powerful yet inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different “views” that may help each other and leverage them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables relatively small LMs to outperform prior approaches that heavily rely on knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promising generalization ability across various views and datasets, and the capability to learn from inaccurate or incomplete noisy data. We hope our multi-view training paradigm could inspire future studies in other machine reasoning domains.},
  keywords={arxiv:2307.07951}
}

@article{sileo2023mindgamestargeting,
  title={MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic},
  author={Damien Sileo and Antoine Lernould},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.03353},
  url={https://www.semanticscholar.org/paper/b6ccdd0eb776eee6b317d235e457f20175f380ff},
  abstract={Theory of Mind (ToM) is a critical component of intelligence, yet accurately measuring it continues to be a subject of debate. Prior research has attempted to apply human ToM assessments to natural language processing models using either human-created standardized tests or rule-based templates. However, these methods primarily focus on simplistic reasoning and require further validation. In this study, we utilize dynamic epistemic logic, which has established overlaps with ToM, to generate more intricate problems. We also introduce novel verbalization techniques to express these problems using natural language. Our findings indicate that certain language model scaling (from 70M to 6B and 350M to 174B) does not consistently yield results better than random chance. While GPT-4 demonstrates improved epistemic reasoning capabilities, there is still room for enhancement. Our code and datasets are publicly available https://github.com/antoinelrnld/modlog https://huggingface.co/datasets/sileod/mindgames},
  keywords={arxiv:2305.03353}
}

@article{wen2023mindmapknowledge,
  title={MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models},
  author={Yilin Wen and Zifeng Wang and Jimeng Sun},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2308.09729},
  url={https://www.semanticscholar.org/paper/ca261cb681b082e90ca6c7a9d325b4265ed1dc28},
  abstract={Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \textbackslash\{\}method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \textbackslash\{\}\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.},
  keywords={arxiv:2308.09729}
}

@article{guan2023mitigatinglarge,
  title={Mitigating Large Language Model Hallucinations via Autonomous Knowledge Graph-based Retrofitting},
  author={Xinyan Guan and Yanjiang Liu and Hongyu Lin and Yaojie Lu and Ben He and Xianpei Han and Le Sun},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2311.13314},
  url={https://www.semanticscholar.org/paper/612cb53584b23e2066631d143e09b471852afaeb},
  abstract={Incorporating factual knowledge in knowledge graph is regarded as a promising approach for mitigating the hallucination of large language models (LLMs). Existing methods usually only use the user's input to query the knowledge graph, thus failing to address the factual hallucination generated by LLMs during its reasoning process. To address this problem, this paper proposes Knowledge Graph-based Retrofitting (KGR), a new framework that incorporates LLMs with KGs to mitigate factual hallucination during the reasoning process by retrofitting the initial draft responses of LLMs based on the factual knowledge stored in KGs. Specifically, KGR leverages LLMs to extract, select, validate, and retrofit factual statements within the model-generated responses, which enables an autonomous knowledge verifying and refining procedure without any additional manual efforts. Experiments show that KGR can significantly improve the performance of LLMs on factual QA benchmarks especially when involving complex reasoning processes, which demonstrates the necessity and effectiveness of KGR in mitigating hallucination and enhancing the reliability of LLMs.},
  keywords={arxiv:2311.13314}
}

@article{li2023mixeddistillation,
  title={Mixed Distillation Helps Smaller Language Model Better Reasoning},
  author={Chenglin Li and Qianglong Chen and Wang Caiyu and Zhang Yin},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.10730},
  url={https://www.semanticscholar.org/paper/1bd9466f0bb10d29a16f614943ec7823e13cb210},
  abstract={While large language models (LLMs) have demonstrated exceptional performance in recent natural language processing (NLP) tasks, their deployment poses substantial challenges due to high computational and memory demands in real-world applications. Recent studies have focused on enhancing smaller models through knowledge distillation from LLMs, yielding promising results. However, these models often struggle to match the performance of LLMs, especially in tasks that require reasoning. In this work, we introduce Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models. Our experimental results show that MD significantly enhances the single-path and multi-path reasoning ability of smaller models in various tasks. In terms of accuracy and generality of reasoning tasks, the model generated by it exceeds the comprehensive performance of two individually distilled models. Notably, LLaMA2-7B and CodeLlama-7B using MD achieved remarkable improvements of (84.5\%) and (85.5\%), respectively, outperforming GPT-3.5-Turbo by (2.5\%) and (3.5\%), on the SVAMP benchmark.},
  keywords={arxiv:2312.10730}
}

@article{si2023mixtureprompt,
  title={Mixture of Prompt Experts for Generalizable and Interpretable Question Answering},
  author={Chenglei Si and Weijia Shi and Chen Zhao and Luke S. Zettlemoyer and Jordan L. Boyd-Graber},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2305.14628},
  url={https://www.semanticscholar.org/paper/700f9c8a76d7af0fc550d119aa1d1164a496055e}
}

@article{li2023modelscopeagentbuilding,
  title={ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models},
  author={Chenliang Li and Hehong Chen and Mingshi Yan and Weizhou Shen and Haiyang Xu and Zhikai Wu and Zhicheng Zhang and Wenmeng Zhou and Yingda Chen and Chen Cheng and Hongzhu Shi and Ji Zhang and Fei Huang and Jingren Zhou},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2309.00986},
  url={https://www.semanticscholar.org/paper/e2f1f04f648a8863d11439aa4c80ee65d6caccda},
  abstract={Large language models (LLMs) have recently demonstrated remarkable capabilities to comprehend human intentions, engage in reasoning, and design planning-like behavior. To further unleash the power of LLMs to accomplish complex tasks, there is a growing trend to build agent framework that equips LLMs, such as ChatGPT, with tool-use abilities to connect with massive external APIs. In this work, we introduce ModelScope-Agent, a general and customizable agent framework for real-world applications, based on open-source LLMs as controllers. It provides a user-friendly system library, with customizable engine design to support model training on multiple open-source LLMs, while also enabling seamless integration with both model APIs and common APIs in a unified way. To equip the LLMs with tool-use abilities, a comprehensive framework has been proposed spanning over tool-use data collection, tool retrieval, tool registration, memory control, customized model training, and evaluation for practical real-world applications. Finally, we showcase ModelScopeGPT, a real-world intelligent assistant of ModelScope Community based on the ModelScope-Agent framework, which is able to connect open-source LLMs with more than 1000 public AI models and localized community knowledge in ModelScope. The ModelScope-Agent library\textbackslash\{\}footnote\{https://github.com/modelscope/modelscope-agent\} and online demo\textbackslash\{\}footnote\{https://modelscope.cn/studios/damo/ModelScopeGPT/summary\} are now publicly available.},
  keywords={arxiv:2309.00986}
}

@article{liao2023modelingcomplex,
  title={Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent},
  author={Haoran Liao and Qinyi Du and Shaohua Hu and Hao He and Yanyan Xu and Jidong Tian and Yaohui Jin},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.08926},
  url={https://www.semanticscholar.org/paper/7017c58e19f4db0c38040935cc9fb7b7090a466d},
  abstract={Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements, associate domain knowledge, perform compound logical reasoning, and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs, thus leading to confusion in generation. In this work, we explore the potential of enhancing LLMs with agents by meticulous decomposition and modeling of mathematical reasoning process. Specifically, we propose a formal description of the mathematical solving and extend LLMs with an agent-based zero-shot framework named \$\textbackslash\{\}bf\{P\}\$lanner-\$\textbackslash\{\}bf\{R\}\$easoner-\$\textbackslash\{\}bf\{E\}\$xecutor-\$\textbackslash\{\}bf\{R\}\$eflector (PRER). We further provide and implement two MathAgents that define the logical forms and inherent relations via a pool of actions in different grains and orientations: MathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with humankind. Experiments on miniF2F and MATH have demonstrated the effectiveness of PRER and proposed MathAgents, achieving an increase of \$12.3\textbackslash\{\}\%\$(\$53.9\textbackslash\{\}\%\textbackslash\{\}xrightarrow\{\}66.2\textbackslash\{\}\%\$) on the MiniF2F, \$9.2\textbackslash\{\}\%\$ (\$49.8\textbackslash\{\}\%\textbackslash\{\}xrightarrow\{\}59.0\textbackslash\{\}\%\$) on MATH, and \$13.2\textbackslash\{\}\%\$(\$23.2\textbackslash\{\}\%\textbackslash\{\}xrightarrow\{\}35.4\textbackslash\{\}\%\$) for level-5 problems of MATH against GPT-4. Further analytical results provide more insightful perspectives on exploiting the behaviors of LLMs as agents.},
  keywords={arxiv:2312.08926}
}

@article{vishwamitra2023moderatingwaves,
  title={Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models},
  author={Nishant Vishwamitra and Keyan Guo and Farhan Tajwar Romit and Isabelle Ondracek and Long Cheng and Ziming Zhao and Hongxin Hu},
  year={2023},
  booktitle={IEEE Symposium on Security and Privacy},
  doi={10.1109/SP54263.2024.00181},
  url={https://www.semanticscholar.org/paper/4dbae943822dcb21d3d1dd757b70740615bb4172},
  abstract={Online hate is an escalating problem that negatively impacts the lives of Internet users, and is also subject to rapid changes due to evolving events, resulting in new waves of online hate that pose a critical threat. Detecting and mitigating these new waves present two key challenges: it demands reasoning-based complex decision-making to determine the presence of hateful content, and the limited availability of training samples hinders updating the detection model. To address this critical issue, we present a novel framework called HateGuard for effectively moderating new waves of online hate. HateGuard employs a reasoning-based approach that leverages the recently introduced chain-of-thought (CoT) prompting technique, harnessing the capabilities of large language models (LLMs). HateGuard further achieves prompt-based zero-shot detection by automatically generating and updating detection prompts with new derogatory terms and targets in new wave samples to effectively address new waves of online hate. To demonstrate the effectiveness of our approach, we compile a new dataset consisting of tweets related to three recently witnessed new waves: the 2022 Russian invasion of Ukraine, the 2021 insurrection of the US Capitol, and the COVID-19 pandemic. Our studies reveal crucial longitudinal patterns in these new waves concerning the evolution of events and the pressing need for techniques to rapidly update existing moderation tools to counteract them. Comparative evaluations against state-of-the-art approaches illustrate the superiority of our framework, showcasing a substantial 10.59\% to 88\% improvement in detecting the three new waves of online hate. Our work highlights the severe threat posed by the emergence of new waves of online hate and represents a paradigm shift in addressing this threat practically.},
  keywords={arxiv:2312.15099}
}

@article{sprueill2023montecarlo,
  title={Monte Carlo Thought Search: Large Language Model Querying for Complex Scientific Reasoning in Catalyst Design},
  author={Henry W. Sprueill and Carl N. Edwards and Mariefel V. Olarte and Udishnu Sanyal and Heng Ji and Sutanay Choudhury},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.14420},
  url={https://www.semanticscholar.org/paper/2129c6edc2593bf4adb5bc2772fdb042bdf14070},
  abstract={Discovering novel catalysts requires complex reasoning involving multiple chemical properties and resultant trade-offs, leading to a combinatorial growth in the search space. While large language models (LLM) have demonstrated novel capabilities for chemistry through complex instruction following capabilities and high quality reasoning, a goal-driven combinatorial search using LLMs has not been explored in detail. In this work, we present a Monte Carlo Tree Search-based approach that improves beyond state-of-the-art chain-of-thought prompting variants to augment scientific reasoning. We introduce two new reasoning datasets: 1) a curation of computational chemistry simulations, and 2) diverse questions written by catalysis researchers for reasoning about novel chemical conversion processes. We improve over the best baseline by 25.8\textbackslash\{\}\% and find that our approach can augment scientist's reasoning and discovery process with novel insights.},
  keywords={arxiv:2310.14420}
}

@article{abdulhai2023moralfoundations,
  title={Moral Foundations of Large Language Models},
  author={Marwa Abdulhai and Gregory Serapio-Garcia and Clé-ment Crepy and Dasha Valter and John F. Canny and Natasha Jaques},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.15337},
  url={https://www.semanticscholar.org/paper/650fa5cda36ac2c4c4b5ca4f72ac7b3ab3c3236c},
  abstract={Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary in the weight they place on these dimensions when making moral decisions, in part due to their cultural upbringing and political ideology. As large language models (LLMs) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values. We analyze known LLMs and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. Finally, we show that we can adversarially select prompts that encourage the moral to exhibit a particular set of moral foundations, and that this can affect the model’s behavior on downstream tasks. These findings help illustrate the potential risks and unintended consequences of LLMs assuming a particular moral stance.},
  keywords={arxiv:2310.15337}
}

@article{li2023mugglemathassessing,
  title={MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning},
  author={Chengpeng Li and Zheng Yuan and Guanting Dong and Keming Lu and Jiancan Wu and Chuanqi Tan and Xiang Wang and Chang Zhou},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2024.acl-long.551},
  url={https://www.semanticscholar.org/paper/84c03d0ec9dd00de3dc7ea7577ceefc9f093c564},
  abstract={In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create two new dataset AugGSM8K and AugMATH, by complicating and diversifying the queries and sampling multiple reasoning paths from GSM8K and MATH. We obtained a series of LLMs called MuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath substantially achieves new state-of-the-art on GSM8K and MATH. A log-linear relationship and a segmented log-linear are presented between MuggleMath's performance and the amount of augmented data on GSM8K and MATH, respectively. We also find that it is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization. We release our codes and augmented data in https://github.com/OFA-Sys/gsm8k-ScRel.},
  keywords={arxiv:2310.05506}
}

@article{tsao2023multiagentreasoning,
  title={Multi-Agent Reasoning with Large Language Models for Effective Corporate Planning},
  author={Wen-Kwang Tsao},
  year={2023},
  booktitle={2023 International Conference on Computational Science and Computational Intelligence (CSCI)},
  doi={10.1109/CSCI62032.2023.00065},
  url={https://www.semanticscholar.org/paper/caec517eaebe39c815c8b123aaf6265dace83796},
  abstract={Large Language Models (LLMs) have demonstrated significant capabilities in natural language processing tasks. In this paper, we explore the application of LLMs within a business context. Specifically, we employ LLMs to devise a sales strategy geared towards maximizing customer values (benefits and satisfaction). This sales plan encompasses five iterative stages: market landscape survey, customer profiling, product usage analysis, sales strategy formulation, and crafting persuasive pitches and materials. We leverage LLMs to supplement the limited data available to the company, aiming to enhance the efficacy of each stage and optimize KPIs, including the value-oriented sales conversion and profitability. Due to confidentiality and trade secret concerns, we blend artificial data with genuine data to ensure customer anonymity and protect sales playbooks. Despite these precautions, we effectively demonstrate our methodology of harnessing LLMs to refine the sales planning procedure.}
}

@article{zhang2023multimodalchainofthought,
  title={Multimodal Chain-of-Thought Reasoning in Language Models},
  author={Zhuosheng Zhang and Aston Zhang and Mu Li and Hai Zhao and G. Karypis and Alexander J. Smola},
  year={2023},
  booktitle={Trans. Mach. Learn. Res.},
  doi={10.48550/arXiv.2302.00923},
  url={https://www.semanticscholar.org/paper/780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050},
  abstract={Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have primarily focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With Multimodal-CoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark. Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https://github.com/amazon-science/mm-cot.},
  keywords={arxiv:2302.00923}
}

@article{wang2023newtonlarge,
  title={NEWTON: Are Large Language Models Capable of Physical Reasoning?},
  author={Yi Ru Wang and Jiafei Duan and Dieter Fox and S. Srinivasa},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.07018},
  url={https://www.semanticscholar.org/paper/ae8aabebad0c3ecae165ec05c18a2072ed360d1e},
  abstract={Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of several mainstream language models across foundational, explicit, and implicit reasoning tasks. Through extensive empirical analysis, our results highlight the capabilities of LLMs for physical reasoning. We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50\% vs. 84\%). Furthermore, the NEWTON platform demonstrates its potential for evaluating and enhancing language models, paving the way for their integration into physically grounded settings, such as robotic manipulation. Project site: https://newtonreasoning.github.io},
  keywords={arxiv:2310.07018}
}

@article{fan2023nphardevaldynamic,
  title={NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes},
  author={Lizhou Fan and Wenyue Hua and Lingyao Li and Haoyang Ling and Yongfeng Zhang and Libby Hemphill},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2312.14890},
  url={https://www.semanticscholar.org/paper/a2d069eb8c772e51292d52e297a464a5eb51979f},
  abstract={Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class. These questions are meticulously chosen to represent a wide range of complexity class below the NP-hard complexity class, offering a rigorous measure of the reasoning ability of LLMs. Through this study, we shed light on the current state of reasoning in LLMs, providing an objective and rigorous perspective through the comparison of LLMs' performance across complex classes. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https://github.com/casmlab/NPHardEval.},
  keywords={arxiv:2312.14890}
}

@article{zhou2023navgptexplicit,
  title={NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models},
  author={Gengze Zhou and Yicong Hong and Qi Wu},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2305.16986},
  url={https://www.semanticscholar.org/paper/8199c9d55dd998f69f703e0ad250ca0697e3ad27},
  abstract={Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goals, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models. Code is available at: https://github.com/GengzeZhou/NavGPT.},
  keywords={arxiv:2305.16986}
}

@article{jha2023neurosymbolic,
  title={Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving},
  author={Sumit Kumar Jha and Susmit Jha and Patrick Lincoln and Nathaniel D. Bastian and Alvaro Velasquez and Rickard Ewetz and Sandeep Neema},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2309.16436},
  url={https://www.semanticscholar.org/paper/1c89d8672a3742672850fa46f1e8ec51f3261019},
  abstract={Generative large language models (LLMs) with instruct training such as GPT-4 can follow human-provided instruction prompts and generate human-like responses to these prompts. Apart from natural language responses, they have also been found to be effective at generating formal artifacts such as code, plans, and logical specifications from natural language prompts. Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan, and other formal artifacts produced by LLMs can be catastrophic. We posit that we can use the satisfiability modulo theory (SMT) solvers as deductive reasoning engines to analyze the generated solutions from the LLMs, produce counterexamples when the solutions are incorrect, and provide that feedback to the LLMs exploiting the dialog capability of instruct-trained LLMs. This interaction between inductive LLMs and deductive SMT solvers can iteratively steer the LLM to generate the correct response. In our experiments, we use planning over the domain of blocks as our synthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo, Davinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our method allows the user to communicate the planning problem in natural language; even the formulation of queries to SMT solvers is automatically generated from natural language. Thus, the proposed technique can enable non-expert users to describe their problems in natural language, and the combination of LLMs and SMT solvers can produce provably correct solutions.},
  keywords={arxiv:2309.16436}
}

@article{xu2023trainstill,
  title={No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function},
  author={Haotian Xu},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2309.03224},
  url={https://www.semanticscholar.org/paper/537335d9aad0ddbaef93e7f88b0db096671ef6ec},
  abstract={Large language models (LLMs) demonstrate impressive language understanding and contextual learning abilities, making them suitable for natural language processing (NLP) tasks and complex mathematical reasoning. However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions. To overcome this limitation and enhance the mathematical reasoning capabilities of fine-tuned LLMs without additional fine-tuning steps, we propose a method that incorporates Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning. Specifically, we re-formulate the fine-tuned LLMs into a Residual-based Energy Model (Residual-EBM) and employ noise contrastive estimation to estimate the energy function's parameters. We then utilize MCTS with the energy function as a path verifier to search the output space and evaluate the reasoning path. Through extensive experiments on two mathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the exceptional capabilities of our method, which significantly improves the pass@1 metric of the fine-tuned model without requiring additional fine-tuning or reinforcement learning with human feedback alignment.},
  keywords={arxiv:2309.03224}
}

@article{liu2023novicelearner,
  title={Novice Learner and Expert Tutor: Evaluating Math Reasoning Abilities of Large Language Models with Misconceptions},
  author={Naiming Liu and Shashank Sonkar and Zichao Wang and Simon Woodhead and R. Baraniuk},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.02439},
  url={https://www.semanticscholar.org/paper/518c3eae36d00ef612a623f335a595e8577655aa},
  abstract={We propose novel evaluations for mathematical reasoning capabilities of Large Language Models (LLMs) based on mathematical misconceptions. Our primary approach is to simulate LLMs as a novice learner and an expert tutor, aiming to identify the incorrect answer to math question resulted from a specific misconception and to recognize the misconception(s) behind an incorrect answer, respectively. Contrary to traditional LLMs-based mathematical evaluations that focus on answering math questions correctly, our approach takes inspirations from principles in educational learning sciences. We explicitly ask LLMs to mimic a novice learner by answering questions in a specific incorrect manner based on incomplete knowledge; and to mimic an expert tutor by identifying misconception(s) corresponding to an incorrect answer to a question. Using simple grade-school math problems, our experiments reveal that, while LLMs can easily answer these questions correctly, they struggle to identify 1) the incorrect answer corresponding to specific incomplete knowledge (misconceptions); 2) the misconceptions that explain particular incorrect answers. Our study indicates new opportunities for enhancing LLMs' math reasoning capabilities, especially on developing robust student simulation and expert tutoring models in the educational applications such as intelligent tutoring systems.},
  keywords={arxiv:2310.02439}
}

@article{alkhamissi2023optrexploring,
  title={OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models},
  author={Badr AlKhamissi and Siddharth Verma and Ping Yu and Zhijing Jin and Asli Celikyilmaz and Mona T. Diab},
  year={2023},
  booktitle={NLRSE},
  doi={10.18653/v1/2023.nlrse-1.10},
  url={https://www.semanticscholar.org/paper/c218cd1772999517b137bbbc9872c4f67e540b7f},
  abstract={We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model’s performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which reasoning skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4\%) and Analogical (+13.9\%) reasoning, as well as skills that exhibit negligible or negative effects.},
  keywords={arxiv:2305.12001}
}

@article{yu2023outcomesupervisedvalue,
  title={OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning},
  author={Fei Yu and Anningzhe Gao and Benyou Wang},
  year={2023},
  booktitle={NAACL-HLT},
  doi={10.18653/v1/2024.findings-naacl.55},
  url={https://www.semanticscholar.org/paper/288e64e8adb23d81e291a2cb51e3a56b315023b7},
  abstract={Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer. To reduce error propagation, guided decoding is employed to direct the LM decoding on a step-by-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a \$\textbackslash\{\}textit\{value estimation\}\$ problem in planning. Inspired by the findings that \$\textbackslash\{\}textit\{outcome supervision for guided decoding essentially acts as a value model\}\$, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurate conclusions. Furthermore, the OVM eliminates the need for labor-intensive annotations of step-level correctness, thereby significantly enhancing its scalability. Our experiments on two multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our \$\textbackslash\{\}textbf\{OVM-7B model achieves state-of-the-art results among LLMs up to 13B parameters\}\$; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training value models for multi-step reasoning tasks and provide theoretical justification for its advantage in value estimation for guided decoding.},
  keywords={arxiv:2311.09724}
}

@article{valmeekam2023planningabilities,
  title={On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)},
  author={Karthik Valmeekam and S. Sreedharan and Matthew Marquez and Alberto Olmo Hernandez and Subbarao Kambhampati},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2302.06706},
  url={https://www.semanticscholar.org/paper/85996f9fc312777f487dd51bf9e96bb3704c2fb7},
  abstract={Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) how good LLMs are by themselves in generating and validating simple plans in commonsense planning tasks (of the type that humans are generally quite good at) and (2) how good LLMs are in being a source of heuristic guidance for other agents--either AI planners or human planners--in their planning tasks. To investigate these questions in a systematic rather than anecdotal manner, we start by developing a benchmark suite based on the kinds of domains employed in the International Planning Competition. On this benchmark, we evaluate LLMs in three modes: autonomous, heuristic and human-in-the-loop. Our results show that LLM's ability to autonomously generate executable plans is quite meager, averaging only about 3\% success rate. The heuristic and human-in-the-loop modes show slightly more promise. In addition to these results, we also make our benchmark and evaluation tools available to support investigations by research community.},
  keywords={arxiv:2302.06706}
}

@article{reese2023limitationslarge,
  title={On the limitations of large language models in clinical diagnosis},
  author={J. Reese and D. Danis and J. H. Caufield and T. Groza and E. Casiraghi and G. Valentini and C. Mungall and Peter N. Robinson},
  year={2023},
  booktitle={medRxiv},
  doi={10.1101/2023.07.13.23292613},
  url={https://www.semanticscholar.org/paper/4153b7fd52c2a65a0a8ef005bd8831b30b5f0b93},
  abstract={Background: The potential of large language models (LLM) such as GPT to support complex tasks such as differential diagnosis has been a subject of debate, with some ascribing near sentient abilities to the models and others claiming that LLMs merely perform "autocomplete on steroids". A recent study reported that the Generative Pretrained Transformer 4 (GPT-4) model performed well in complex differential diagnostic reasoning. The authors assessed the performance of GPT-4 in identifying the correct diagnosis in a series of case records from the New England Journal of Medicine. The authors constructed prompts based on the clinical presentation section of the case reports, and compared the results of GPT-4 to the actual diagnosis. GPT-4 returned the correct diagnosis as a part of its response in 64\% of cases, with the correct diagnosis being at rank 1 in 39\% of cases. However, such concise but comprehensive narratives of the clinical course are not typically available in electronic health records (EHRs). Further, if they were available, EHR records contain identifying information whose transmission is prohibited by Health Insurance Portability and Accountability Act (HIPAA) regulations. Methods: To assess the expected performance of GPT on comparable datasets that can be generated by text mining and by design cannot contain identifiable information, we parsed the texts of the case reports and extracted Human Phenotype Ontology (HPO) terms, from which prompts for GPT were constructed that contain largely the same clinical abnormalities but lack the surrounding narrative. Results: While the performance of GPT-4 on the original narrative-based text was good, with the final diagnosis being included in its differential in 29/75 cases (38.7\%; rank 1 in 17.3\% of cases; mean rank of 3.4), the performance of GPT-4 on the feature-based approach that includes the major clinical abnormalities without additional narrative texas substantially worse, with GPT-4 including the final diagnosis in its differential in 8/75 cases (10.7\%; rank 1 in 4.0\% of cases; mean rank of 3.9). Interpretation: We consider the feature-based queries to be a more appropriate test of the performance of GPT-4 in diagnostic tasks, since it is unlikely that the narrative approach can be used in actual clinical practice. Future research and algorithmic development is needed to determine the optimal approach to leveraging LLMs for clinical diagnosis.}
}

@article{paster2023openwebmathopen,
  title={OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text},
  author={Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.06786},
  url={https://www.semanticscholar.org/paper/e92d6b93371c5dc6f3f6fb917f925d6c2fae5492},
  abstract={There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known open source web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and LaTeX content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B parameter language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, openly released on the Hugging Face Hub, will help spur advances in the reasoning abilities of large language models.},
  keywords={arxiv:2310.06786}
}

@article{ahmaditeshnizi2023optimusoptimization,
  title={OptiMUS: Optimization Modeling Using MIP Solvers and large language models},
  author={Ali AhmadiTeshnizi and Wenzhi Gao and Madeleine Udell},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.06116},
  url={https://www.semanticscholar.org/paper/37a30e97ae043075f09738984a59991ff20ecf0c},
  abstract={Optimization problems are pervasive across various sectors, from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers, as the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. We introduce OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve MILP problems from their natural language descriptions. OptiMUS is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of generated solutions. To benchmark our agent, we present NLP4LP, a novel dataset of linear programming (LP) and mixed integer linear programming (MILP) problems. Our experiments demonstrate that OptiMUS solves nearly twice as many problems as a basic LLM prompting strategy. OptiMUS code and NLP4LP dataset are available at \textbackslash\{\}href\{https://github.com/teshnizi/OptiMUS\}\{https://github.com/teshnizi/OptiMUS\}},
  keywords={arxiv:2310.06116}
}

@article{shang2023pbllmpartially,
  title={PB-LLM: Partially Binarized Large Language Models},
  author={Yuzhang Shang and Zhihang Yuan and Qiang Wu and Zhen Dong},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.00034},
  url={https://www.semanticscholar.org/paper/ce15b4255bcf79781906c7830973a9df39e7fe24},
  abstract={This paper explores network binarization, a radical form of quantization, compressing model weights to a single bit, specifically for Large Language Models (LLMs) compression. Due to previous binarization methods collapsing LLMs, we propose a novel approach, Partially-Binarized LLM (PB-LLM), which can achieve extreme low-bit quantization while maintaining the linguistic reasoning capacity of quantized LLMs. Specifically, our exploration first uncovers the ineffectiveness of naive applications of existing binarization algorithms and highlights the imperative role of salient weights in achieving low-bit quantization. Thus, PB-LLM filters a small ratio of salient weights during binarization, allocating them to higher-bit storage, i.e., partially-binarization. PB-LLM is extended to recover the capacities of quantized LMMs, by analyzing from the perspective of post-training quantization (PTQ) and quantization-aware training (QAT). Under PTQ, combining the concepts from GPTQ, we reconstruct the binarized weight matrix guided by the Hessian matrix and successfully recover the reasoning capacity of PB-LLM in low-bit. Under QAT, we freeze the salient weights during training, explore the derivation of optimal scaling factors crucial for minimizing the quantization error, and propose a scaling mechanism based on this derived scaling strategy for residual binarized weights. Those explorations and the developed methodologies significantly contribute to rejuvenating the performance of low-bit quantized LLMs and present substantial advancements in the field of network binarization for LLMs.The code is available at https://github.com/hahnyuan/BinaryLLM.},
  keywords={arxiv:2310.00034}
}

@article{sun2023pearlprompting,
  title={PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents},
  author={Simeng Sun and Y. Liu and Shuo Wang and Chenguang Zhu and Mohit Iyyer},
  year={2023},
  booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2305.14564},
  url={https://www.semanticscholar.org/paper/4ee96f0757e517928590a2300af5d40ba768a5a7},
  abstract={Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND\_EVENT, FIND\_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents.},
  keywords={arxiv:2305.14564}
}

@article{driess2023palmeembodied,
  title={PaLM-E: An Embodied Multimodal Language Model},
  author={Danny Driess and F. Xia and Mehdi S. M. Sajjadi and Corey Lynch and A. Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Q. Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and P. Sermanet and Daniel Duckworth and S. Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Peter R. Florence},
  year={2023},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2303.03378},
  url={https://www.semanticscholar.org/paper/38fe8f324d2162e63a967a9ac6648974fc4c66f3},
  abstract={Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
  keywords={arxiv:2303.03378}
}

@article{ren2023pangutowards,
  title={PanGu-Σ: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing},
  author={Xiaozhe Ren and Pingyi Zhou and Xinfan Meng and Xinjing Huang and Yadao Wang and Weichao Wang and Pengfei Li and Xiaoda Zhang and A. V. Podolskiy and G. Arshinov and A. Bout and Irina Piontkovskaya and Jiansheng Wei and Xin Jiang and Teng Su and Qun Liu and Jun Yao},
  year={2023},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/362cbfd0d05e139cd6cf049754098a6e1520b910},
  abstract={The scaling of large language models has greatly improved natural language understanding, generation, and reasoning. In this work, we develop a system that trained a trillion-parameter language model on a cluster of Ascend 910 AI processors and MindSpore framework, and present the language model with 1.085T parameters named PanGu-\{\textbackslash\{\}Sigma\}. With parameter inherent from PanGu-\{\textbackslash\{\}alpha\}, we extend the dense Transformer model to sparse one with Random Routed Experts (RRE), and efficiently train the model over 329B tokens by using Expert Computation and Storage Separation(ECSS). This resulted in a 6.3x increase in training throughput through heterogeneous computing. Our experimental findings show that PanGu-\{\textbackslash\{\}Sigma\} provides state-of-the-art performance in zero-shot learning of various Chinese NLP downstream tasks. Moreover, it demonstrates strong abilities when fine-tuned in application data of open-domain dialogue, question answering, machine translation and code generation.},
  keywords={arxiv:2303.10845}
}

@article{lin2023paralinguisticsenhancedlarge,
  title={Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue},
  author={Guan-Ting Lin and Prashanth Gurunath Shivakumar and Ankur Gandhe and Chao-Han Huck Yang and Yile Gu and Shalini Ghosh and A. Stolcke and Hung-yi Lee and I. Bulyko},
  year={2023},
  booktitle={IEEE International Conference on Acoustics, Speech, and Signal Processing},
  doi={10.1109/ICASSP48485.2024.10446933},
  url={https://www.semanticscholar.org/paper/f5593021b2f485b9738246b34145176011df0c80},
  abstract={Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditioning. We utilize the Switchboard-1 corpus, including its sentiment labels as the paralinguistic attribute, as our spoken dialogue dataset. Experimental results indicate the proposed serialized multitasking method outperforms typical sequence classification techniques on current and response sentiment classification. Furthermore, leveraging conversational context and speech embeddings significantly improves both response text generation and sentiment prediction. Our proposed framework achieves relative improvements of 6.7\%, 12.0\%, and 3.5\% in current sentiment accuracy, response sentiment accuracy, and response text BLEU score, respectively.},
  keywords={arxiv:2312.15316}
}

@article{giannos2023performancechatgpt,
  title={Performance of ChatGPT on UK Standardized Admission Tests: Insights From the BMAT, TMUA, LNAT, and TSA Examinations},
  author={P. Giannos and Orestis Delardas},
  year={2023},
  booktitle={JMIR Medical Education},
  doi={10.2196/47737},
  url={https://www.semanticscholar.org/paper/b1a365943652c2245c82e61a37362ba77fb108dc},
  abstract={Background Large language models, such as ChatGPT by OpenAI, have demonstrated potential in various applications, including medical education. Previous studies have assessed ChatGPT’s performance in university or professional settings. However, the model’s potential in the context of standardized admission tests remains unexplored. Objective This study evaluated ChatGPT’s performance on standardized admission tests in the United Kingdom, including the BioMedical Admissions Test (BMAT), Test of Mathematics for University Admission (TMUA), Law National Aptitude Test (LNAT), and Thinking Skills Assessment (TSA), to understand its potential as an innovative tool for education and test preparation. Methods Recent public resources (2019-2022) were used to compile a data set of 509 questions from the BMAT, TMUA, LNAT, and TSA covering diverse topics in aptitude, scientific knowledge and applications, mathematical thinking and reasoning, critical thinking, problem-solving, reading comprehension, and logical reasoning. This evaluation assessed ChatGPT’s performance using the legacy GPT-3.5 model, focusing on multiple-choice questions for consistency. The model’s performance was analyzed based on question difficulty, the proportion of correct responses when aggregating exams from all years, and a comparison of test scores between papers of the same exam using binomial distribution and paired-sample (2-tailed) t tests. Results The proportion of correct responses was significantly lower than incorrect ones in BMAT section 2 (P<.001) and TMUA paper 1 (P<.001) and paper 2 (P<.001). No significant differences were observed in BMAT section 1 (P=.2), TSA section 1 (P=.7), or LNAT papers 1 and 2, section A (P=.3). ChatGPT performed better in BMAT section 1 than section 2 (P=.047), with a maximum candidate ranking of 73\% compared to a minimum of 1\%. In the TMUA, it engaged with questions but had limited accuracy and no performance difference between papers (P=.6), with candidate rankings below 10\%. In the LNAT, it demonstrated moderate success, especially in paper 2’s questions; however, student performance data were unavailable. TSA performance varied across years with generally moderate results and fluctuating candidate rankings. Similar trends were observed for easy to moderate difficulty questions (BMAT section 1, P=.3; BMAT section 2, P=.04; TMUA paper 1, P<.001; TMUA paper 2, P=.003; TSA section 1, P=.8; and LNAT papers 1 and 2, section A, P>.99) and hard to challenging ones (BMAT section 1, P=.7; BMAT section 2, P<.001; TMUA paper 1, P=.007; TMUA paper 2, P<.001; TSA section 1, P=.3; and LNAT papers 1 and 2, section A, P=.2). Conclusions ChatGPT shows promise as a supplementary tool for subject areas and test formats that assess aptitude, problem-solving and critical thinking, and reading comprehension. However, its limitations in areas such as scientific and mathematical knowledge and applications highlight the need for continuous development and integration with conventional learning strategies in order to fully harness its potential.}
}

@article{zhao2023pipelinechainofthought,
  title={Pipeline Chain-of-Thought: A Prompt Method for Large Language Model Relation Extraction},
  author={Hangtian Zhao and Hankiz Yilahun and Askar Hamdulla},
  year={2023},
  booktitle={International Conference on Asian Language Processing},
  doi={10.1109/IALP61005.2023.10337264},
  url={https://www.semanticscholar.org/paper/c202f2679dfc8fc4af0967d1a801d72109761763},
  abstract={The development of language models has been influencing approaches to relation extraction (RE) problems. Although large language models (LLMs) have demonstrated breakthrough potential in certain aspects, they are still in the exploratory stage for RE tasks. Currently, the mainstream approach to improving the RE performance of LLMs is through prompt fine-tuning, but most methods require providing entity information in the prompt, which effectively only allows the LLMs to perform relationship classification tasks. We propose the Pipeline Chain-of-Thought (Pipeline-COT), which breaks down the RE task into steps and transforms it into reasoning tasks that have flat scaling curves, thereby enabling the use of Chain-of-Thought (COT) to enhance model inference. In addition, our method utilizes n-shot samples to provide signals for the Bayesian inference of the model by prompting the LLMs to focus on specific concepts to generate answers. We evaluated pipeline-COT on the Chinese dataset DuIE2.0, and compared with baseline methods that require including entity information in the prompt, our method still shows competitive performance.}
}

@article{wang2023planandsolveprompting,
  title={Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models},
  author={Lei Wang and Wanyu Xu and Yihuai Lan and Zhiqiang Hu and Yunshi Lan and Roy Ka-Wei Lee and Ee-Peng Lim},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2305.04091},
  url={https://www.semanticscholar.org/paper/62176de125738e3b95850d1227bac81fd646b78e},
  abstract={Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with “Let’s think step by step” as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.},
  keywords={arxiv:2305.04091}
}

@article{zheng2023progressivehintprompting,
  title={Progressive-Hint Prompting Improves Reasoning in Large Language Models},
  author={Chuanyang Zheng and Zhengying Liu and Enze Xie and Zhenguo Li and Yu Li},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2304.09797},
  url={https://www.semanticscholar.org/paper/261549439aebdda72b648ecc462448fd24857ac1},
  abstract={The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2\% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17\% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1\% ->91.9\%), GSM8K (92\% ->95.5\%), AQuA (76.4\% ->79.9\%) and MATH (50.3\% ->53.9\%).},
  keywords={arxiv:2304.09797}
}

@article{shi2023promptspace,
  title={Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models},
  author={Fobo Shi and Peijun Qing and D. Yang and Nan Wang and Youbo Lei and H. Lu and Xiaodong Lin},
  year={2023},
  booktitle={NAACL-HLT},
  doi={10.48550/arXiv.2306.03799},
  url={https://www.semanticscholar.org/paper/2d338cdd12091814dec11155d3f6f848d7bab4d8},
  abstract={Prompt engineering is an essential technique for enhancing the abilities of large language models (LLMs) by providing explicit and specific instructions. It enables LLMs to excel in various tasks, such as arithmetic reasoning, question answering, summarization, relation extraction, machine translation, and sentiment analysis. Researchers have been actively exploring different prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and In-context learning. However, an unresolved problem arises from the fact that current approaches lack a solid mathematical solution for determining optimal prompts. To address this issue in prompt engineering, we propose a new and effective approach called Prompt Space. Our methodology utilizes text embeddings to obtain basis vectors by matrix decomposition, and then constructs a space for representing all prompts. Prompt Space significantly outperforms state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably, without the help of the CoT method and the prompt"Let's think step by step", Prompt Space shows superior performance over the few-shot method. Overall, our approach provides a robust and effective mathematical framework for selecting simple and effective prompts. This advancement marks a significant step towards improving prompt engineering for a wide variety of applications in LLMs. Our code is publicly available at \textbackslash\{\}textcolor\{blue\}\{\textbackslash\{\}url\{https://github.com/YouBLEI/Prompt-Space\}\}},
  keywords={arxiv:2306.03799}
}

@article{feng2023promptingneed,
  title={Prompting Is All You Need: Automated Android Bug Replay with Large Language Models},
  author={Sidong Feng and Chunyang Chen},
  year={2023},
  booktitle={International Conference on Software Engineering},
  doi={10.1145/3597503.3608137},
  url={https://www.semanticscholar.org/paper/48385ded07af641da331c05f6ea3f93694a08425},
  abstract={Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current au-tomated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and predefined vocabulary lists. In-spired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a devel-oper. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3\% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers' bug replay capabilities.},
  keywords={arxiv:2306.01987}
}

@article{feng2023promptingyour,
  title={Prompting Is All Your Need: Automated Android Bug Replay with Large Language Models},
  author={Sidong Feng and Chunyang Chen},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2306.01987},
  url={https://www.semanticscholar.org/paper/9c00fd1e5c058c29608f7b1b71d0ebcf50a4ecf7}
}

@article{liang2023promptinglarge,
  title={Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation},
  author={Yuanyuan Liang and Jianing Wang and Hanlun Zhu and Lei Wang and Weining Qian and Yunshi Lan},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.08395},
  url={https://www.semanticscholar.org/paper/eafc97d979e790a84329f0a49d1b627bd5979499},
  abstract={The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.},
  keywords={arxiv:2310.08395}
}

@article{deng2023promptingevaluating,
  title={Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration},
  author={Yang Deng and Wenqiang Lei and Hongru Wang and Tat-seng Chua},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.13626},
  url={https://www.semanticscholar.org/paper/13c85adfa950651ffcd91ef3018fa30801b74472},
  abstract={Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems.},
  keywords={arxiv:2305.13626}
}

@article{tanneru2023quantifyinguncertainty,
  title={Quantifying Uncertainty in Natural Language Explanations of Large Language Models},
  author={Sree Harsha Tanneru and Chirag Agarwal and Himabindu Lakkaraju},
  year={2023},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  doi={10.48550/arXiv.2311.03533},
  url={https://www.semanticscholar.org/paper/ad402080a4aa66ef3c57a46ce4685a47a3cc0a61},
  abstract={Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- \$\textbackslash\{\}textit\{Verbalized Uncertainty\}\$ and \$\textbackslash\{\}textit\{Probing Uncertainty\}\$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.},
  keywords={arxiv:2311.03533}
}

@article{li2023queryresponse,
  title={Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization},
  author={Chengpeng Li and Zheng Yuan and Hongyi Yuan and Guanting Dong and Keming Lu and Jiancan Wu and Chuanqi Tan and Xiang Wang and Chang Zhou},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.05506},
  url={https://www.semanticscholar.org/paper/fb8330d41315c471edb42e799de18386a9a0b1a2}
}

@article{tian2023promptingreview,
  title={R3 Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context},
  author={Qingyuan Tian and Hanlun Zhu and Lei Wang and Yang Li and Yunshi Lan},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.16535},
  url={https://www.semanticscholar.org/paper/4d2a0840abd2eef795a4e14538271e06f8536b07},
  abstract={With the help of Chain-of-Thought (CoT) prompting, Large Language Models (LLMs) have achieved remarkable performance on various reasoning tasks. However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated. Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction. Inspired by interactive CoT method, where intermediate reasoning steps are promoted by multiple rounds of interaction between users and LLMs, we propose a novel prompting method, namely R\$\^{}3\$ prompting, for CoT reasoning under noisy context. Specifically, R\$\^{}3\$ prompting interacts with LLMs to perform key sentence extraction, variable declaration and answer prediction, which corresponds to a thought process of reviewing, rephrasing and resolving. The responses generated at the last interaction will perform as hints to guide toward the responses of the next interaction. Our experiments show that R\$\^{}3\$ prompting significantly outperforms existing CoT prompting methods on five reasoning tasks under noisy context. With GPT-3.5-turbo, we observe 3.7\% accuracy improvement on average on the reasoning tasks under noisy context compared to the most competitive prompting baseline. More analyses and ablation studies show the robustness and generalization of R\$\^{}3\$ prompting method in solving reasoning tasks in LLMs under noisy context.},
  keywords={arxiv:2310.16535}
}

@article{tagliabue2023realresilience,
  title={REAL: Resilience and Adaptation using Large Language Models on Autonomous Aerial Robots},
  author={Andrea Tagliabue and Kota Kondo and Tong Zhao and Mason B. Peterson and Claudius T. Tewari and Jonathan P. How},
  year={2023},
  booktitle={IEEE Conference on Decision and Control},
  doi={10.1109/CDC56724.2024.10885890},
  url={https://www.semanticscholar.org/paper/cc25dc73b0a479094744b2cbf24e84d0b109e4f4},
  abstract={Large Language Models (LLMs) pre-trained on internet-scale datasets have shown impressive capabilities in code understanding, synthesis and in processing extended sequences of symbols, often presented in natural language. This work aims to explore new opportunities in long-term reasoning, natural language comprehension, and the available prior knowledge of LLMs for increased resilience and adaptation in autonomous mobile robots. We introduce REAL, an approach for REsilience and Adaptation using LLMs. REAL interfaces LLMs with the mission planning and control framework of an autonomous robot. The LLM employed by REAL provides (i) a source of prior knowledge to increase resilience for challenging scenarios that the system has not been explicitly designed for; (ii) a way to interpret natural language and other log/diagnostic information available in the autonomy stack, for mission planning; (iii) a way to adapt the control inputs using minimal user-provided prior knowledge about the robot. We integrate REAL in the autonomy stack of a real multirotor, querying onboard an offboard LLM at about \$1.0-0.1 \textbackslash\{\}mathrm\{\~{}Hz\}\$ as part of the robot’s mission planning and control feedback loops. We provide a demonstration of capabilities by showcasing in real-world experiments the ability of the LLM to reduce the position tracking errors of a multirotor, and decision-making to avoid potentially dangerous scenarios (e.g., robot oscillates) that are not explicitly accounted for in the initial prompt design.},
  keywords={arxiv:2311.01403}
}

@article{jiang2023respromptresidual,
  title={RESPROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models},
  author={Song Jiang and Zahra Shakeri and Aaron Chan and Maziar Sanjabi and Hamed Firooz and Yinglong Xia and Bugra Akyildiz and Yizhou Sun and Jinchao Li and Qifan Wang and Asli Celikyilmaz},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2310.04743},
  url={https://www.semanticscholar.org/paper/be9447ccc05a0e8a07321272778c7574173cf00e},
  abstract={Chain-of-thought (CoT) has impressively unlocked the reasoning potential of large language models (LLMs). Yet, it falls short when tackling problems that require multiple reasoning steps. This limitation arises from the complex nature of multi-step reasoning processes: later stages often depend not only on the immediately preceding step, but also on the results from several steps earlier. Such complexities indicate the reasoning process is naturally a graph. The almost linear structure of CoT, however, struggles to capture this complex reasoning graph. To address this challenge, we propose Residual Connection Prompting (ResPrompt), a new prompting strategy that advances multi-step reasoning in LLMs. The core of our idea is to reconstruct the reasoning graph within prompts. We achieve this by integrating necessary connections–links present in reasoning graph but missing in the linear CoT flow–into the prompts. Termed “residual connections”, these links can transform linear CoT into the complex reasoning graphs that multi-step problems entail. On benchmarks across math, sequential, and commonsense domains, ResPrompt demonstrates clear improvements in multi-step reasoning compared with CoT. Through extensive ablation studies and analyses, we pinpoint how to effectively build residual connections and also identify situations where it might be unnecessary.},
  keywords={arxiv:2310.04743}
}

@article{xu2023rereadingimproves,
  title={Re-Reading Improves Reasoning in Large Language Models},
  author={Xiaohan Xu and Chongyang Tao and Tao Shen and Can Xu and Hongbo Xu and Guodong Long and Jian-Guang Lou},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2024.emnlp-main.871},
  url={https://www.semanticscholar.org/paper/4b508ba98a180f27fd93b702d7044adad91620eb},
  abstract={To enhance the reasoning capabilities of off-the-shelf Large Language Models (LLMs), we introduce a simple, yet general and effective prompting method, RE2, i.e., Re-Reading the question as input. Unlike most thought-eliciting prompting methods, such as Chain-of-Thought (CoT), which aim to elicit the reasoning process in the output, RE2 shifts the focus to the input by processing questions twice, thereby enhancing the understanding process. Consequently, RE2 demonstrates strong generality and compatibility with most thought-eliciting prompting methods, including CoT. Crucially, RE2 facilitates a “bidirectional” encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass. We begin with a preliminary empirical study as the foundation of RE2, illustrating its potential to enable “bidirectional” attention mechanisms. We then evaluate RE2 on extensive reasoning benchmarks across 14 datasets, spanning 112 experiments, to validate its effectiveness and generality. Our findings indicate that, with the exception of a few scenarios on vanilla ChatGPT, RE2 consistently enhances the reasoning performance of LLMs through a simple re-reading strategy. Further analyses reveal RE2’s adaptability, showing how it can be effectively integrated with different LLMs, thought-eliciting prompting, and ensemble strategies.},
  keywords={arxiv:2309.06275}
}

@article{gaur2023reasoninglarge,
  title={Reasoning in Large Language Models Through Symbolic Math Word Problems},
  author={Vedant Gaur and Nikunj Saunshi},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2023.findings-acl.364},
  url={https://www.semanticscholar.org/paper/1e2eba005ccd8ab7a668a525c5b43245853bdaf1},
  abstract={Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a"concise explanation"of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP\_Sym dataset will be released for future research on symbolic math problems.},
  keywords={arxiv:2308.01906}
}

@article{luo2023reasoninggraphs,
  title={Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning},
  author={Linhao Luo and Yuan-Fang Li and Gholamreza Haffari and Shirui Pan},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.01061},
  url={https://www.semanticscholar.org/paper/b47e96762351b2dbf7e863ece4640df6194bcc0c},
  abstract={Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.},
  keywords={arxiv:2310.01061}
}

@article{hao2023reasoningwith,
  title={Reasoning with Language Model is Planning with World Model},
  author={Shibo Hao and Yi Gu and Haodi Ma and Joshua Jiahua Hong and Zhen Wang and D. Wang and Zhiting Hu},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.14992},
  url={https://www.semanticscholar.org/paper/5dbffedcabe3fa43060ebbe2b1789500edfd871f},
  abstract={Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal \$\textbackslash\{\}textit\{world model\}\$ to predict the world \$\textbackslash\{\}textit\{state\}\$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, \$\textbackslash\{\}underline\{R\}\$easoning vi\$\textbackslash\{\}underline\{a\}\$ \$\textbackslash\{\}underline\{P\}\$lanning \$\textbackslash\{\}textbf\{(RAP)\}\$. RAP repurposes the LLM as both a world model and a reasoning agent, and incorporates a principled planning algorithm (based on Monto Carlo Tree Search) for strategic exploration in the vast reasoning space. During reasoning, the LLM (as agent) incrementally builds a reasoning tree under the guidance of the LLM (as world model) and task-specific rewards, and obtains a high-reward reasoning path efficiently with a proper balance between exploration \$\textbackslash\{\}textit\{vs.\}\$ exploitation. We apply RAP to a variety of challenging reasoning problems including plan generation, math reasoning, and logical inference. Empirical results on these tasks demonstrate the superiority of RAP over various strong baselines, including CoT and least-to-most prompting with self-consistency. RAP on LLAMA-33B surpasses CoT on GPT-4 with 33\% relative improvement in a plan generation setting.},
  keywords={arxiv:2305.14992}
}

@article{jiang2023reasoninglmenabling,
  title={ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph},
  author={Jinhao Jiang and Kun Zhou and Wayne Xin Zhao and Yaliang Li and Ji-Rong Wen},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.emnlp-main.228},
  url={https://www.semanticscholar.org/paper/1866cf8e89f5f3375ca6abc019bdf963bd486d56},
  abstract={Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph\~{}(KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model\~{}(PLM) to model the question, and a graph neural network\~{}(GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions. To solve it, we aim to simplify the above two-module approach, and develop a more capable PLM that can directly support subgraph reasoning for KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning, and also adopt an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions. After adaptation, the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments show that ReasoningLM surpasses state-of-the-art models by a large margin, even with fewer updated parameters and less training data. Our codes and data are publicly available at\~{}\textbackslash\{\}url\{https://github.com/RUCAIBox/ReasoningLM\}.},
  keywords={arxiv:2401.00158}
}

@article{cui2023receivereason,
  title={Receive, Reason, and React: Drive as You Say, With Large Language Models in Autonomous Vehicles},
  author={Can Cui and Yunsheng Ma and Xu Cao and Wenqian Ye and Ziran Wang},
  year={2023},
  booktitle={IEEE Intelligent Transportation Systems Magazine},
  doi={10.1109/MITS.2024.3381793},
  url={https://www.semanticscholar.org/paper/10158879cdb64ce7d3f7bb5572c4617ea808602e},
  abstract={The fusion of human-centric design and artificial intelligence capabilities has opened up new possibilities for next-generation autonomous vehicles that go beyond traditional transportation. These vehicles can dynamically interact with passengers and adapt to their preferences. This article proposes a novel framework that leverages large language models (LLMs) to enhance the decision-making process in autonomous vehicles. By utilizing LLMs’ contextual understanding abilities with specialized tools, we aim to integrate the language and reasoning capabilities of LLMs into autonomous vehicles. Our research includes experiments in HighwayEnv, a collection of environments for autonomous driving and tactical decision-making tasks, to explore LLMs’ interpretation, interaction, and reasoning in various scenarios. We also examine some well-defined real-time personalized driving tasks, demonstrating how LLMs can influence driving behaviors based on drivers’ verbal commands. Our empirical results highlight the substantial advantages of utilizing chain-of-thought prompting, leading to improved driving decisions and showing the potential for LLMs to enhance personalized driving experiences through ongoing verbal feedback. The proposed framework aims to transform autonomous vehicle operations, offering personalized support, transparent decision making, and continuous learning to enhance safety and effectiveness. We achieve user-centric, transparent, and adaptive autonomous driving ecosystems supported by the integration of LLMs into autonomous vehicles. Experiment videos are available at https://youtube.com/playlist?list=PLgcRcf9w8BmLJi\_fqTGq-7KCZsbpEIE4a\&si=dhH9lgaeSmB5K94t.},
  keywords={arxiv:2310.08034}
}

@article{huang2023recommenderagent,
  title={Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations},
  author={Xu Huang and Jianxun Lian and Yuxuan Lei and Jing Yao and Defu Lian and Xing Xie},
  year={2023},
  booktitle={ACM Trans. Inf. Syst.},
  doi={10.1145/3731446},
  url={https://www.semanticscholar.org/paper/c237a22698223e4060d83027f399f4fb2aa24291},
  abstract={Recommender models capture ever-changing user preferences by training with in-domain user behavior data. These models are typically lightweight, facilitating real-time and large-scale online services. However, these models often falter when tasked with providing more sophisticated functionalities, such as offering explanations or engaging in conversations. Recently, large language models (LLMs) have emerged as a significant advancement towards artificial general intelligence, demonstrating impressive capabilities in instruction comprehension, reasoning, and human interaction. Unfortunately, LLMs lack the understanding of domain-specific item catalogs and behavioral patterns, especially in areas that deviate from general world knowledge, such as online e-commerce. This limitation makes them unsuitable to function as recommender models directly. In this article, we bridge the gap between recommender models and LLMs, combining their respective strengths to create an interactive recommender system. We present an efficient framework, termed as InteRecAgent, which utilizes LLMs as the brain and recommender models as instrumental tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. To overcome specific challenges associated with LLM-based agents for recommender systems, we enhance three core components, covering memory mechanism, task planning, and tool learning abilities. The InteRecAgent empowers traditional recommender systems, like ID-based matrix factorization models, to evolve into versatile and interactive systems with a natural language interface through the integration of LLMs. Experimental results derived from three public datasets demonstrate that the InteRecAgent delivers strong performance as a conversational recommender system, surpassing general LLMs such as GPT-4.},
  keywords={arxiv:2308.16505}
}

@article{fan2023recommendersystems,
  title={Recommender Systems in the Era of Large Language Models (LLMs)},
  author={Wenqi Fan and Zihuai Zhao and Jiatong Li and Yunqing Liu and Xiaowei Mei and Yiqi Wang and Jiliang Tang and Qing Li},
  year={2023},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  doi={10.1109/TKDE.2024.3392335},
  url={https://www.semanticscholar.org/paper/a35f1315e91513ff0bec0c488fe175214fd9636c},
  abstract={With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an indispensable and important component, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have achieved significant advancements in enhancing recommender systems, these DNN-based methods still exhibit some limitations, such as inferior capabilities to effectively capture textual side information about users and items, difficulties in generalization to various recommendation scenarios, and reasoning on their predictions, etc. Meanwhile, the development of Large Language Models (LLMs), such as ChatGPT and GPT-4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization capabilities and reasoning skills. As a result, recent studies have actively attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems. Therefore, in this survey, we comprehensively review LLM-empowered recommender systems from various perspectives including pre-training, fine-tuning, and prompting paradigms. More specifically, we first introduce the representative methods to learn user and item representations, leveraging LLMs as feature encoders. Then, we systematically review the emerging advanced techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss the promising future directions in this emerging field.},
  keywords={arxiv:2307.02046}
}

@article{rajasekharan2023reliablenatural,
  title={Reliable Natural Language Understanding with Large Language Models and Answer Set Programming},
  author={Abhiramon Rajasekharan and Yankai Zeng and Parth Padalkar and Gopal Gupta},
  year={2023},
  booktitle={International Conference on Logic Programming},
  doi={10.4204/EPTCS.385.27},
  url={https://www.semanticscholar.org/paper/b549716038bb167fcc65e3f9f725013d8b2ea649},
  abstract={Humans understand language by extracting information (meaning) from sentences, combining it with existing commonsense knowledge, and then performing reasoning to draw conclusions. While large language models (LLMs) such as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a variety of NLP tasks, they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question. In order to emulate humans better, we propose STAR, a framework that combines LLMs with Answer Set Programming (ASP). We show how LLMs can be used to effectively extract knowledge -- represented as predicates -- from language. Goal-directed ASP is then employed to reliably reason over this knowledge. We apply the STAR framework to three different NLU tasks requiring reasoning: qualitative reasoning, mathematical reasoning, and goal-directed conversation. Our experiments reveal that STAR is able to bridge the gap of reasoning in NLU tasks, leading to significant performance improvements, especially for smaller LLMs, i.e., LLMs with a smaller number of parameters. NLU applications developed using the STAR framework are also explainable: along with the predicates generated, a justification in the form of a proof tree can be produced for a given output.},
  keywords={arxiv:2302.03780}
}

@article{hodel2023responseemergent,
  title={Response: Emergent analogical reasoning in large language models},
  author={Damian Hodel and Jevin D. West},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.16118},
  url={https://www.semanticscholar.org/paper/c32873b62b7e186a56b941c688ef7cf64e6289d0},
  abstract={In their recent Nature Human Behaviour paper,"Emergent analogical reasoning in large language models,"(Webb, Holyoak, and Lu, 2023) the authors argue that"large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems."In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve simplest variations of the original tasks, whereas human performance remains consistently high across all modified versions. Zero-shot reasoning is an extraordinary claim that requires extraordinary evidence. We do not see that evidence in our experiments. To strengthen claims of humanlike reasoning such as zero-shot reasoning, it is important that the field develop approaches that rule out data memorization.},
  keywords={arxiv:2308.16118}
}

@article{liu2023rethinkingtabular,
  title={Rethinking Tabular Data Understanding with Large Language Models},
  author={Tianyang Liu and Fei Wang and Muhao Chen},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2312.16702},
  url={https://www.semanticscholar.org/paper/60f35bfe967dbce2c8694de8d283de01cc3766c2},
  abstract={Large Language Models (LLMs) have shown to be capable of various tasks, yet their capability in interpreting and reasoning over tabular data remains an underexplored area. In this context, this study investigates from three core perspectives: the robustness of LLMs to structural perturbations in tables, the comparative analysis of textual and symbolic reasoning on tables, and the potential of boosting model performance through the aggregation of multiple reasoning pathways. We discover that structural variance of tables presenting the same content reveals a notable performance decline, particularly in symbolic reasoning tasks. This prompts the proposal of a method for table structure normalization. Moreover, textual reasoning slightly edges out symbolic reasoning, and a detailed error analysis reveals that each exhibits different strengths depending on the specific tasks. Notably, the aggregation of textual and symbolic reasoning pathways, bolstered by a mix self-consistency mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6\% on WikiTableQuestions, representing a substantial advancement over previous existing table processing paradigms of LLMs.},
  keywords={arxiv:2312.16702}
}

@article{gao2023retrievalaugmentedgeneration,
  title={Retrieval-Augmented Generation for Large Language Models: A Survey},
  author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Qianyu Guo and Meng Wang and Haofen Wang},
  year={2023},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5},
  abstract={Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.},
  keywords={arxiv:2312.10997}
}

@article{feng2023retrievalgenerationsynergy,
  title={Retrieval-Generation Synergy Augmented Large Language Models},
  author={Zhangyin Feng and Xiaocheng Feng and Dezhi Zhao and Maojin Yang and Bing Qin},
  year={2023},
  booktitle={IEEE International Conference on Acoustics, Speech, and Signal Processing},
  doi={10.1109/ICASSP48485.2024.10448015},
  url={https://www.semanticscholar.org/paper/08dd6f6adf5657516d53db1946780a5d31a10676},
  abstract={Large language models augmented with task-relevant documents have demonstrated impressive performance on knowledge-intensive tasks. However, regarding how to obtain effective documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to generate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four question answering datasets, including single-hop QA and multi-hop QA tasks. Empirical results show that our method significantly improves the reasoning ability of large language models and outperforms previous baselines.},
  keywords={arxiv:2310.05149}
}

@article{liu2023retrievalaugmentedmultimodal,
  title={Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models},
  author={Bingshuai Liu and Chenyang Lyu and Zijun Min and Zhanyu Wang and Jinsong Su and Longyue Wang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.01714},
  url={https://www.semanticscholar.org/paper/34802b1f153d436d5ddb428642b8ae415485269e},
  abstract={The advancement of Large Language Models (LLMs) has brought substantial attention to the Chain of Thought (CoT) approach, primarily due to its ability to enhance the capability of LLMs on complex reasoning tasks. Moreover, the significance of CoT approaches extends to the application of LLMs for multi-modal tasks. However, the selection of optimal CoT demonstration examples in multi-modal reasoning remains less explored for LLMs due to the inherent complexity of multi-modal examples. In this paper, we introduce a novel approach that addresses this challenge by using retrieval mechanisms to dynamically and automatically select demonstration examples based on cross-modal and intra-modal similarities. Furthermore, we employ a Stratified Sampling method of categorising demonstration examples into groups based on their types and then retrieving examples from different groups respectively to promote the diversity of demonstration examples. Through a series of experiments on two popular benchmark datasets: ScienceQA and MathVista, we demonstrate that our approach significantly improves the performance of GPT-4 by 6\% on ScienceQA and 12.9\% on MathVista, and enhances the performance of GPT-4V on two datasets by 2.7\%, substantially improving the performance of the most advanced LLMs and LMMs for complex multi-modal reasoning tasks.},
  keywords={arxiv:2312.01714}
}

@article{burnell2023revealingstructure,
  title={Revealing the structure of language model capabilities},
  author={Ryan Burnell and Hank Hao and Andrew R. A. Conway and José Hernández Orallo},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2306.10062},
  url={https://www.semanticscholar.org/paper/00557300321dc60998e0f42853f4bba52d6e53db},
  abstract={Building a theoretical understanding of the capabilities of large language models (LLMs) is vital for our ability to predict and explain the behavior of these systems. Here, we investigate the structure of LLM capabilities by extracting latent capabilities from patterns of individual differences across a varied population of LLMs. Using a combination of Bayesian and frequentist factor analysis, we analyzed data from 29 different LLMs across 27 cognitive tasks. We found evidence that LLM capabilities are not monolithic. Instead, they are better explained by three well-delineated factors that represent reasoning, comprehension and core language modeling. Moreover, we found that these three factors can explain a high proportion of the variance in model performance. These results reveal a consistent structure in the capabilities of different LLMs and demonstrate the multifaceted nature of these capabilities. We also found that the three abilities show different relationships to model properties such as model size and instruction tuning. These patterns help refine our understanding of scaling laws and indicate that changes to a model that improve one ability might simultaneously impair others. Based on these findings, we suggest that benchmarks could be streamlined by focusing on tasks that tap into each broad model ability.},
  keywords={arxiv:2306.10062}
}

@article{liu2023reviewergptexploratory,
  title={ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing},
  author={Ryan Liu and Nihar B. Shah},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2306.00622},
  url={https://www.semanticscholar.org/paper/62729cff7dda7614f648a84e8967076d8878a5ff},
  abstract={Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks: 1. Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors. 2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 \{checklist question, paper\} pairs, the LLM had an 86.6\% accuracy. 3. Choosing the"better"paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs. Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.},
  keywords={arxiv:2306.00622}
}

@article{mandi2023rocodialectic,
  title={RoCo: Dialectic Multi-Robot Collaboration with Large Language Models},
  author={Zhao Mandi and Shreeya Jain and Shuran Song},
  year={2023},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA57147.2024.10610855},
  url={https://www.semanticscholar.org/paper/c5d18dbb92d0cd5393baa1e69de33d6922ac3e57},
  abstract={We propose a novel approach to multi-robot collaboration that harnesses the power of pre-trained large language models (LLMs) for both high-level communication and low-level path planning. Robots are equipped with LLMs to discuss and collectively reason task strategies. They generate sub-task plans and task space waypoint paths, which are used by a multi-arm motion planner to accelerate trajectory planning. We also provide feedback from the environment, such as collision checking, and prompt the LLM agents to improve their plan and waypoints in-context. For evaluation, we introduce RoCoBench, a 6-task benchmark covering a wide range of multi-robot collaboration scenarios, accompanied by a text-only dataset that evaluates LLMs’ agent representation and reasoning capability. We experimentally demonstrate the effectiveness of our approach — it achieves high success rates across all tasks in RoCoBench and adapts to variations in task semantics. Our dialog setup offers high interpretability and flexibility — in real world experiments, we show RoCo easily incorporates human-in-the-loop, where a user can communicate and collaborate with a robot agent to complete tasks together.},
  keywords={arxiv:2307.04738}
}

@article{you2023robotenabledconstruction,
  title={Robot-Enabled Construction Assembly with Automated Sequence Planning based on ChatGPT: RoboGPT},
  author={Hengxu You and Yang Ye and Tianyu Zhou and Qi Zhu and Jing Du},
  year={2023},
  booktitle={Buildings},
  doi={10.48550/arXiv.2304.11018},
  url={https://www.semanticscholar.org/paper/798868aa3d1cb0f83a3f97265a8e0d17d649e1de},
  abstract={Robot-based assembly in construction has emerged as a promising solution to address numerous challenges such as increasing costs, labor shortages, and the demand for safe and efficient construction processes. One of the main obstacles in realizing the full potential of these robotic systems is the need for effective and efficient sequence planning for construction tasks. Current approaches, including mathematical and heuristic techniques or machine learning methods, face limitations in their adaptability and scalability to dynamic construction environments. To expand the current robot system’s sequential understanding ability, this paper introduces RoboGPT, a novel system that leverages the advanced reasoning capabilities of ChatGPT, a large language model, for automated sequence planning in robot-based assembly applied to construction tasks. The proposed system adapts ChatGPT for construction sequence planning and demonstrates its feasibility and effectiveness through experimental evaluation including two case studies and 80 trials involving real construction tasks. The results show that RoboGPT-driven robots can handle complex construction operations and adapt to changes on the fly. This paper contributes to the ongoing efforts to enhance the capabilities and performance of robot-based assembly systems in the construction industry, and it paves the way for further integration of large language model technologies in the field of construction robotics.},
  keywords={arxiv:2304.11018}
}

@article{ren2023robotsthat,
  title={Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners},
  author={Allen Z. Ren and Anushri Dixit and Alexandra Bodrova and Sumeet Singh and Stephen Tu and Noah Brown and Peng Xu and L. Takayama and F. Xia and Jacob Varley and Zhenjia Xu and Dorsa Sadigh and Andy Zeng and Anirudha Majumdar},
  year={2023},
  booktitle={Conference on Robot Learning},
  doi={10.48550/arXiv.2307.01928},
  url={https://www.semanticscholar.org/paper/d1500f1dbd62e26ef0753f31e845078f58479968},
  abstract={Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io},
  keywords={arxiv:2307.01928}
}

@article{tian2023promptingreview,
  title={R³ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context},
  author={Qingyuan Tian and Hanlun Zhu and Lei Wang and Yang Li and Yunshi Lan},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  url={https://www.semanticscholar.org/paper/9d8f79d0b72e0982c1177bb9758a6c4b641fa5bb}
}

@article{gao2023socialnetworksimulation,
  title={S3: Social-network Simulation System with Large Language Model-Empowered Agents},
  author={Chen Gao and Xiaochong Lan and Zhi-jie Lu and Jinzhu Mao and J. Piao and Huandong Wang and Depeng Jin and Yong Li},
  year={2023},
  booktitle={Social Science Research Network},
  doi={10.48550/arXiv.2307.14984},
  url={https://www.semanticscholar.org/paper/221a72a3631ebf8b555c27bc864338390611feb1},
  abstract={Social network simulation plays a crucial role in addressing various challenges within social science. It offers extensive applications such as state prediction, phenomena explanation, and policy-making support, among others. In this work, we harness the formidable human-like capabilities exhibited by large language models (LLMs) in sensing, reasoning, and behaving, and utilize these qualities to construct the S\$\^{}3\$ system (short for \$\textbackslash\{\}textbf\{S\}\$ocial network \$\textbackslash\{\}textbf\{S\}\$imulation \$\textbackslash\{\}textbf\{S\}\$ystem). Adhering to the widely employed agent-based simulation paradigm, we employ prompt engineering and prompt tuning techniques to ensure that the agent's behavior closely emulates that of a genuine human within the social network. Specifically, we simulate three pivotal aspects: emotion, attitude, and interaction behaviors. By endowing the agent in the system with the ability to perceive the informational environment and emulate human actions, we observe the emergence of population-level phenomena, including the propagation of information, attitudes, and emotions. We conduct an evaluation encompassing two levels of simulation, employing real-world social network data. Encouragingly, the results demonstrate promising accuracy. This work represents an initial step in the realm of social network simulation empowered by LLM-based agents. We anticipate that our endeavors will serve as a source of inspiration for the development of simulation systems within, but not limited to, social science.},
  keywords={arxiv:2307.14984}
}

@article{tang2023salmonntowards,
  title={SALMONN: Towards Generic Hearing Abilities for Large Language Models},
  author={Changli Tang and Wenyi Yu and Guangzhi Sun and Xianzhao Chen and Tian Tan and Wei Li and Lu Lu and Zejun Ma and Chao Zhang},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.13289},
  url={https://www.semanticscholar.org/paper/f72be31de9f9a09d4410fd38bc717efe43444827},
  abstract={Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning etc. SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning etc. The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities. To our knowledge, SALMONN is the first model of its type and can be regarded as a step towards AI with generic hearing abilities. The source code, model checkpoints and data are available at https://github.com/bytedance/SALMONN.},
  keywords={arxiv:2310.13289}
}

@article{zheng2023sglangefficient,
  title={SGLang: Efficient Execution of Structured Language Model Programs},
  author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph Gonzalez and Clark W. Barrett and Ying Sheng},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.52202/079017-2000},
  url={https://www.semanticscholar.org/paper/eb95c327260725498404eb43ec370d419b8d92c7},
  abstract={Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4x higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang},
  keywords={arxiv:2312.07104}
}

@article{lin2023sphinxjoint,
  title={SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models},
  author={Ziyi Lin and Chris Liu and Renrui Zhang and Peng Gao and Longtian Qiu and Han Xiao and Han Qiu and Chen Lin and Wenqi Shao and Keqin Chen and Jiaming Han and Siyuan Huang and Yichi Zhang and Xuming He and Hongsheng Li and Y. Qiao},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.07575},
  url={https://www.semanticscholar.org/paper/76a3f4a79ae9a00db2f2b5f6877021d8deb96ada},
  abstract={We present SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings. First, for stronger vision-language alignment, we unfreeze the large language model (LLM) during pre-training, and introduce a weight mix strategy between LLMs trained by real-world and synthetic data. By directly integrating the weights from two domains, the mixed LLM can efficiently incorporate diverse semantics with favorable robustness. Then, to enable multi-purpose capabilities, we mix a variety of tasks for joint visual instruction tuning, and design task-specific instructions to avoid inter-task conflict. In addition to the basic visual question answering, we include more challenging tasks such as region-level understanding, caption grounding, document layout detection, and human pose estimation, contributing to mutual enhancement over different scenarios. Additionally, we propose to extract comprehensive visual embeddings from various network architectures, pre-training paradigms, and information granularity, providing language models with more robust image representations. Based on our proposed joint mixing, SPHINX exhibits superior multi-modal understanding capabilities on a wide range of applications. On top of this, we further propose an efficient strategy aiming to better capture fine-grained appearances of high-resolution images. With a mixing of different scales and high-resolution sub-images, SPHINX attains exceptional visual parsing and reasoning performance on existing evaluation benchmarks. We hope our work may cast a light on the exploration of joint mixing in future MLLM research. Code is released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.},
  keywords={arxiv:2311.07575}
}

@article{zhong2023suradapterenhancing,
  title={SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models},
  author={Shan Zhong and Zhongzhan Huang and Wushao Wen and Jinghui Qin and Liang Lin},
  year={2023},
  booktitle={ACM Multimedia},
  doi={10.1145/3581783.3611863},
  url={https://www.semanticscholar.org/paper/3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6},
  abstract={Diffusion models, which have emerged to become popular text-to-image generation models, can produce high-quality and content-rich images guided by textual prompts. However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation. To improve the capacities for narrative prompts, we propose a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models. To reach this goal, we first collect and annotate a new dataset SURD which consists of more than 57,000 semantically corrected multi-modal samples. Each sample contains a simple narrative prompt, a complex keyword-based prompt, and a high-quality image. Then, we align the semantic representation of narrative prompts to the complex prompts and transfer knowledge of large language models (LLMs) to our SUR-adapter via knowledge distillation so that it can acquire the powerful semantic understanding and reasoning capabilities to build a high-quality textual semantic representation for text-to-image generation. We conduct experiments by integrating multiple LLMs and popular pre-trained diffusion models to show the effectiveness of our approach in enabling diffusion models to understand and reason concise natural language without image quality degradation. Our approach can make text-to-image diffusion models easier to use with better user experience, which demonstrates our approach has the potential for further advancing the development of user-friendly text-to-image generation models by bridging the semantic gap between simple narrative prompts and complex keyword-based prompts. The code is released at https://github.com/Qrange-group/SUR-adapter.},
  keywords={arxiv:2305.05189}
}

@article{rajvanshi2023saynavgrounding,
  title={SayNav: Grounding Large Language Models for Dynamic Planning to Navigation in New Environments},
  author={Abhinav Rajvanshi and Karan Sikka and Xiao Lin and Bhoram Lee and Han-Pang Chiu and Alvaro Velasquez},
  year={2023},
  booktitle={International Conference on Automated Planning and Scheduling},
  doi={10.48550/arXiv.2309.04077},
  url={https://www.semanticscholar.org/paper/5a9d4bcffa9989cac4139b2844358884ae023e8d},
  abstract={Semantic reasoning and dynamic planning capabilities are crucial for an autonomous agent to perform complex navigation tasks in unknown environments. It requires a large amount of common-sense knowledge, that humans possess, to succeed in these tasks. We present SayNav, a new approach that leverages human knowledge from Large Language Models (LLMs) for efficient generalization to complex navigation tasks in unknown large-scale environments. SayNav uses a novel grounding mechanism, that incrementally builds a 3D scene graph of the explored environment as inputs to LLMs, for generating feasible and contextually appropriate high-level plans for navigation. The LLM-generated plan is then executed by a pre-trained low-level planner, that treats each planned step as a short-distance point-goal navigation sub-task. SayNav dynamically generates step-by-step instructions during navigation and continuously refines future steps based on newly perceived information. We evaluate SayNav on multi-object navigation (MultiON) task, that requires the agent to utilize a massive amount of human knowledge to efficiently search multiple different objects in an unknown environment. We also introduce a benchmark dataset for MultiON task employing ProcTHOR framework that provides large photo-realistic indoor environments with variety of objects. SayNav achieves state-of-the-art results and even outperforms an oracle based baseline with strong ground-truth assumptions by more than 8\% in terms of success rate, highlighting its ability to generate dynamic plans for successfully locating objects in large-scale new environments. The code, benchmark dataset and demonstration videos are accessible at https://www.sri.com/ics/computer-vision/saynav.},
  keywords={arxiv:2309.04077}
}

@article{yuan2023scalingrelationship,
  title={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models},
  author={Zheng Yuan and Hongyi Yuan and Cheng Li and Guanting Dong and Chuanqi Tan and Chang Zhou},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.01825},
  url={https://www.semanticscholar.org/paper/91206346edbe28abb606d7b3425cd455d4019d4f},
  abstract={Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\textbackslash\{\}\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\textbackslash\{\}\% significantly.},
  keywords={arxiv:2308.01825}
}

@article{wang2023scibenchevaluating,
  title={SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models},
  author={Xiaoxuan Wang and Ziniu Hu and Pan Lu and Yanqiao Zhu and Jieyu Zhang and Satyen Subramaniam and Arjun R. Loomba and Shichang Zhang and Yizhou Sun and Wei Wang},
  year={2023},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2307.10635},
  url={https://www.semanticscholar.org/paper/4993258852711c4e3d0011325ac3db680eae84f4},
  abstract={Most of the existing Large Language Model (LLM) benchmarks on scientific problem reasoning focus on problems grounded in high-school subjects and are confined to elementary algebraic operations. To systematically examine the reasoning capabilities required for solving complex scientific problems, we introduce an expansive benchmark suite SciBench for LLMs. SciBench contains a carefully curated dataset featuring a range of collegiate-level scientific problems from mathematics, chemistry, and physics domains. Based on the dataset, we conduct an in-depth benchmarking study of representative open-source and proprietary LLMs with various prompting strategies. The results reveal that the current LLMs fall short of delivering satisfactory performance, with the best overall score of merely 43.22\%. Furthermore, through a detailed user study, we categorize the errors made by LLMs into ten problem-solving abilities. Our analysis indicates that no single prompting strategy significantly outperforms the others and some strategies that demonstrate improvements in certain problem-solving skills could result in declines in other skills. We envision that SciBench will catalyze further developments in the reasoning abilities of LLMs, thereby ultimately contributing to scientific research and discovery.},
  keywords={arxiv:2307.10635}
}

@article{xu2023searchinthechaininteractively,
  title={Search-in-the-Chain: Interactively Enhancing Large Language Models with Search for Knowledge-intensive Tasks},
  author={Shicheng Xu and Liang Pang and Huawei Shen and Xueqi Cheng and Tat-Seng Chua},
  year={2023},
  booktitle={The Web Conference},
  doi={10.1145/3589334.3645363},
  url={https://www.semanticscholar.org/paper/d47b9407cf0938e9f15ce019031d7cd63b308a01},
  abstract={Making the contents generated by Large Language Model (LLM), accurate, credible and traceable is crucial, especially in complex knowledge-intensive tasks that require multi-step reasoning and each step needs knowledge to solve. Retrieval-augmented generation is good potential to solve this problem. However, where and how to introduce Information Retrieval (IR) to LLM is a big challenge. Previous work has the problems that wrong knowledge retrieved by IR misleads the LLM and interaction between IR and LLM breaks the reasoning chain of LLM. This paper proposes a novel framework named Search-in-the-Chain (SearChain) for the interaction between LLM and IR to solve the challenges. First, LLM generates the reasoning chain named Chain-of-Query (CoQ) where each node consists of an IR-oriented query-answer pair. Second, IR verifies the answer of each node of CoQ. It corrects the answer that is not consistent with the retrieved information when IR gives high confidence, which improves the credibility. Third, LLM can indicate its missing knowledge in CoQ and rely on IR to provide this knowledge to LLM. These operations improve the accuracy in terms of reasoning and knowledge. Finally, SearChain generates the reasoning process and marks references to supporting documents for each reasoning step, which improves traceability. Interaction with IR in SearChain forms a novel reasoning path based on a tree, which enables LLM to dynamically modify the direction of reasoning. Experiments show that SearChain outperforms state-of-the-art baselines on complex knowledge-intensive tasks including multi-hop Q\&A, slot filling, fact checking, and long-form Q\&A.},
  keywords={arxiv:2304.14732}
}

@article{zheng2023secretsrlhf,
  title={Secrets of RLHF in Large Language Models Part I: PPO},
  author={Rui Zheng and Shihan Dou and Songyang Gao and Wei Shen and Wei-Yuan Shen and Bing Wang and Yan Liu and Senjie Jin and Qin Liu and Limao Xiong and Luyao Chen and Zhiheng Xi and Yuhao Zhou and Nuo Xu and Wen-De Lai and Minghao Zhu and Rongxiang Weng and Wen-Chun Cheng and Cheng Chang and Zhangyue Yin and Yuan Hua and Haoran Huang and Tianxiang Sun and Hang Yan and Tao Gui and Qi Zhang and Xipeng Qiu and Xuanjing Huang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2307.04964},
  url={https://www.semanticscholar.org/paper/548278897d46a54958909bb23bcaecf63e24fadf},
  abstract={Large language models (LLMs) have formulated a blueprint for the advancement of artificial general intelligence. Its primary objective is to function as a human-centric (helpful, honest, and harmless) assistant. Alignment with humans assumes paramount significance, and reinforcement learning with human feedback (RLHF) emerges as the pivotal technological paradigm underpinning this pursuit. Current technical routes usually include \textbackslash\{\}textbf\{reward models\} to measure human preferences, \textbackslash\{\}textbf\{Proximal Policy Optimization\} (PPO) to optimize policy model outputs, and \textbackslash\{\}textbf\{process supervision\} to improve step-by-step reasoning capabilities. However, due to the challenges of reward design, environment interaction, and agent training, coupled with huge trial and error cost of large language models, there is a significant barrier for AI researchers to motivate the development of technical alignment and safe landing of LLMs. The stable training of RLHF has still been a puzzle. In the first report, we dissect the framework of RLHF, re-evaluate the inner workings of PPO, and explore how the parts comprising PPO algorithms impact policy agent training. We identify policy constraints being the key factor for the effective implementation of the PPO algorithm. Therefore, we explore the PPO-max, an advanced version of PPO algorithm, to efficiently improve the training stability of the policy model. Based on our main results, we perform a comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT. The absence of open-source implementations has posed significant challenges to the investigation of LLMs alignment. Therefore, we are eager to release technical reports, reward models and PPO codes, aiming to make modest contributions to the advancement of LLMs.},
  keywords={arxiv:2307.04964}
}

@article{chen2023thinkconfirm,
  title={See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning},
  author={Zhenfang Chen and Qinhong Zhou and Yikang Shen and Yining Hong and Hao Zhang and Chuang Gan},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2301.05226},
  url={https://www.semanticscholar.org/paper/6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc},
  abstract={Large pre-trained vision and language models have demonstrated remarkable capacities for various tasks. However, solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect the external world knowledge, and perform step-by-step reasoning to answer the questions correctly. To this end, we propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge-based visual reasoning. IPVR contains three stages, see, think and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our IPVR enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines.},
  keywords={arxiv:2301.05226}
}

@article{nascimento2023selfadaptivelarge,
  title={Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems},
  author={N. Nascimento and Paulo Alencar and Donald D. Cowan},
  year={2023},
  booktitle={2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C)},
  doi={10.1109/ACSOS-C58168.2023.00048},
  url={https://www.semanticscholar.org/paper/1f9822022f586e375461660db792f23e891c7123},
  abstract={The complexity of managing multiagent systems (MASs) in autonomic computing can be mitigated using a self-adaptation approach, where systems are equipped to monitor and adjust themselves based on specific concerns. Communication in these systems is key given that in scenarios involving agent interaction, it enhances cooperation and reduces coordination challenges by enabling direct, clear information exchange. However, the tasks of boosting communication expressiveness within MASs and logically processing a multitude of variables in dynamic environments are still challenging. This paper presents a novel strategy: integrating large language models (LLMs) like GPT-based technologies into MASs to boost communication and agent autonomy. Our proposal encompasses the development of a novel LLM/GPT-based agent architecture, focusing not only on advanced conversation features but also on the reasoning and decision-making capacities of these models. This is grounded in the MAPE-K model, known for supporting system adaptability in dynamic environments. We illustrate our approach through a marketplace scenario. This work represents a paradigm shift in MAS self-adaptation, utilizing LLMs' capabilities and indicating further research opportunities to assess LLMs' applicability in more complex MAS scenarios. This could pave the way for more potent problem-solving capabilities and refined communication within MASs.},
  keywords={arxiv:2307.06187}
}

@article{xi2023selfpolishenhance,
  title={Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement},
  author={Zhiheng Xi and Senjie Jin and Yuhao Zhou and Rui Zheng and Songyang Gao and Tao Gui and Qi Zhang and Xuanjing Huang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.14497},
  url={https://www.semanticscholar.org/paper/9a9b1e2968302eb882870537d4af6e2c722dfd1a},
  abstract={To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems. In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable. We also explore several automatic prompting varients and propose the Self-Polish prompt bank for the community. SP is orthogonal to all other prompting methods of answer/reasoning side like CoT, allowing for seamless integration with state-of-the-art techniques for further improvement. Thorough experiments show that the proposed method attains notable and consistent effectiveness on five reasoning benchmarks across different models. Furthermore, our method also showcases impressive performance on robustness evaluation. Codes and prompts are available at https://github.com/WooooDyy/Self-Polish.},
  keywords={arxiv:2305.14497}
}

@article{song2023selfrefinedlarge,
  title={Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics},
  author={Jiayang Song and Zhehua Zhou and Jiawei Liu and Chunrong Fang and Zhan Shu and Lei Ma},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2309.06687},
  url={https://www.semanticscholar.org/paper/6b4da2023c3a11aea6e3ccb9ab13e594833c47eb},
  abstract={Although Deep Reinforcement Learning (DRL) has achieved notable success in numerous robotic applications, designing a high-performing reward function remains a challenging task that often requires substantial manual input. Recently, Large Language Models (LLMs) have been extensively adopted to address tasks demanding in-depth common-sense knowledge, such as reasoning and planning. Recognizing that reward function design is also inherently linked to such knowledge, LLM offers a promising potential in this context. Motivated by this, we propose in this work a novel LLM framework with a self-refinement mechanism for automated reward function design. The framework commences with the LLM formulating an initial reward function based on natural language inputs. Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process. We examine the performance of our proposed framework through a variety of continuous robotic control tasks across three diverse robotic systems. The results indicate that our LLM-designed reward functions are able to rival or even surpass manually designed reward functions, highlighting the efficacy and applicability of our approach.},
  keywords={arxiv:2309.06687}
}

@article{wang2023selfpromptedchainofthought,
  title={Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning},
  author={Jinyuan Wang and Junlong Li and Hai Zhao},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.13552},
  url={https://www.semanticscholar.org/paper/0aaf7a76507248d80f65b6a49e200d2370bcb2c9},
  abstract={In open-domain question-answering (ODQA), most existing questions require single-hop reasoning on commonsense. To further extend this task, we officially introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop questions with explicit reasoning steps in open-domain setting. Recently, large language models (LLMs) have found significant utility in facilitating ODQA without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts the reasoning capability of LLMs to a greater extent with manual or automated paradigms. However, existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs. In this paper, we propose Self-prompted Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT selection and self-prompted inference via in-context learning. Extensive experiments on four multi-hop question-answering benchmarks show that our proposed SP-CoT not only significantly surpasses the previous SOTA methods on large-scale (175B) LLMs, but also nearly doubles the zero-shot performance of small-scale (13B) LLMs. Further analysis reveals the remarkable capability of SP-CoT to elicit direct and concise intermediate reasoning steps by recalling \$\textbackslash\{\}sim\$50\textbackslash\{\}\% of intermediate answers on MuSiQue-Ans dataset.},
  keywords={arxiv:2310.13552}
}

@article{elhafsi2023semanticanomaly,
  title={Semantic anomaly detection with large language models},
  author={Amine Elhafsi and Rohan Sinha and Christopher Agia and E. Schmerling and I. Nesnas and M. Pavone},
  year={2023},
  booktitle={Autonomous Robots},
  doi={10.1007/s10514-023-10132-6},
  url={https://www.semanticscholar.org/paper/735ea38114bd2406a8bbc7f060cba1fc7a254d89},
  abstract={As robots acquire increasingly sophisticated skills and see increasingly complex and varied environments, the threat of an edge case or anomalous failure is ever present. For example, Tesla cars have seen interesting failure modes ranging from autopilot disengagements due to inactive traffic lights carried by trucks to phantom braking caused by images of stop signs on roadside billboards. These system-level failures are not due to failures of any individual component of the autonomy stack but rather system-level deficiencies in semantic reasoning. Such edge cases, which we call semantic anomalies, are simple for a human to disentangle yet require insightful reasoning. To this end, we study the application of large language models (LLMs), endowed with broad contextual understanding and reasoning capabilities, to recognize such edge cases and introduce a monitoring framework for semantic anomaly detection in vision-based policies. Our experiments apply this framework to a finite state machine policy for autonomous driving and a learned policy for object manipulation. These experiments demonstrate that the LLM-based monitor can effectively identify semantic anomalies in a manner that shows agreement with human reasoning. Finally, we provide an extended discussion on the strengths and weaknesses of this approach and motivate a research outlook on how we can further use foundation models for semantic anomaly detection. Our project webpage can be found at https://sites.google.com/view/llm-anomaly-detection.},
  keywords={arxiv:2305.11307}
}

@article{su2023semistructuredchainofthought,
  title={Semi-Structured Chain-of-Thought: Integrating Multiple Sources of Knowledge for Improved Language Model Reasoning},
  author={Xin Su and Tiep Le and Steven Bethard and Phillip Howard},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2311.08505},
  url={https://www.semanticscholar.org/paper/159ee595bbc453637f7d524d1a9ca9d58a0c10d2},
  abstract={An important open question in the use of large language models for knowledge-intensive tasks is how to effectively integrate knowledge from three sources: the model’s parametric memory, external structured knowledge, and external unstructured knowledge. Most existing prompting methods either rely on one or two of these sources, or require repeatedly invoking large language models to generate similar or identical content. In this work, we overcome these limitations by introducing a novel semi-structured prompting approach that seamlessly integrates the model’s parametric memory with unstructured knowledge from text documents and structured knowledge from knowledge graphs. Experimental results on open-domain multi-hop question answering datasets demonstrate that our prompting method significantly surpasses existing techniques, even exceeding those that require fine-tuning.},
  keywords={arxiv:2311.08505}
}

@article{chen2023skillsincontextprompting,
  title={Skills-in-Context Prompting: Unlocking Compositionality in Large Language Models},
  author={Jiaao Chen and Xiaoman Pan and Dian Yu and Kaiqiang Song and Xiaoyang Wang and Dong Yu and Jianshu Chen},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.00304},
  url={https://www.semanticscholar.org/paper/447bbdbeb5dfa9252b51a833eafe5e8f4d3b632e},
  abstract={We investigate how to elicit compositional generalization capabilities in large language models (LLMs). Compositional generalization empowers LLMs to solve complex problems by combining foundational skills, a critical reasoning ability akin to human intelligence. However, even the most advanced LLMs currently struggle with this form of reasoning. We examine this problem within the framework of in-context learning and find that demonstrating both foundational skills and compositional examples grounded in these skills within the same prompt context is crucial. We refer to this prompt structure as skills-in-context (SKiC). With as few as two exemplars, this in-context learning structure enables LLMs to tackle more challenging problems requiring innovative skill combinations, achieving near-perfect systematic generalization across a broad range of tasks. Intriguingly, SKiC also unlocks the latent potential of LLMs, allowing them to more actively utilize pre-existing internal skills acquired during earlier pretraining stages to solve complex reasoning problems. The SKiC structure is robust across different skill constructions and exemplar choices and demonstrates strong transferability to new tasks. Finally, inspired by our in-context learning study, we show that fine-tuning LLMs with SKiC-style data can elicit zero-shot weak-to-strong generalization, enabling the models to solve much harder problems directly with standard prompting.},
  keywords={arxiv:2308.00304}
}

@article{huang2023smarteditexploring,
  title={SmartEdit: Exploring Complex Instruction-Based Image Editing with Multimodal Large Language Models},
  author={Yuzhou Huang and Liangbin Xie and Xintao Wang and Ziyang Yuan and Xiaodong Cun and Yixiao Ge and Jiantao Zhou and Chao Dong and Rui Huang and Ruimao Zhang and Ying Shan},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.00799},
  url={https://www.semanticscholar.org/paper/388b0f44faf0a14cc402c2554ec36a868cf59129},
  abstract={Current instruction-based image editing methods, such as InstructPix2Pix, often fail to produce satisfactory results in complex scenarios due to their dependence on the simple CLIP text encoder in diffusion models. To rectify this, this paper introduces SmartEdit, a novel approach of instruction-based image editing that leverages Multimodal Large Language Models (MLLMs) to enhance its understanding and reasoning capabilities. However, direct integration of these elements still faces challenges in situations requiring complex reasoning. To mitigate this, we propose a Bidirectional Interaction Module (BIM) that enables comprehensive bidirectional information interactions between the input image and the MLLM output. During training, we initially incorporate perception data to boost the perception and understanding capabilities of diffusion models. Subsequently, we demonstrate that a small amount of complex instruction editing data can effectively stimulate SmartEdit’ s editing capabilities for more complex instructions. We further construct a new evaluation dataset, Reason-Edit, specifically tailored for complex instruction-based image editing. Both quantitative and qualitative results on this evaluation dataset indicate that our SmartEdit surpasses previous methods, paving the way for the practical application of complex instruction-based image editing.},
  keywords={arxiv:2312.06739}
}

@article{he2023socrevallarge,
  title={SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation},
  author={Hangfeng He and Hongming Zhang and Dan Roth},
  year={2023},
  booktitle={NAACL-HLT},
  doi={10.48550/arXiv.2310.00074},
  url={https://www.semanticscholar.org/paper/12dc76b5f04fb1fc34f94f26632d193305a97e7e},
  abstract={To comprehensively gauge the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains as references to assess the model-derived chains. However, such"gold-standard"human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning evaluation metrics, while eliminating the need for human-crafted reasoning chains as references, often require fine-tuning with human-derived chains before evaluation, complicating the process and questioning their adaptability to other datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes. Leveraging the Socratic method, we develop SocREval (\{\textbackslash\{\}bf Soc\}ratic Method-Inspired \{\textbackslash\{\}bf R\}easoning \{\textbackslash\{\}bf Eval\}uation), a novel approach for prompt design in reference-free reasoning evaluation. Empirical results from four human annotated datasets reveal that SocREval significantly improves GPT-4's performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated efficacy, SocREval, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis.},
  keywords={arxiv:2310.00074}
}

@article{fu2023specializingsmaller,
  title={Specializing Smaller Language Models towards Multi-Step Reasoning},
  author={Yao Fu and Hao-Chun Peng and Litu Ou and Ashish Sabharwal and Tushar Khot},
  year={2023},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2301.12726},
  url={https://www.semanticscholar.org/paper/fbd49b25bdab98c171af49962a41139c73dacbde},
  abstract={The surprising ability of Large Language Models (LLMs) to perform well on complex reasoning with only few-shot chain-of-thought prompts is believed to emerge only in very large-scale models (100+ billion parameters). We show that such abilities can, in fact, be distilled down from GPT-3.5 (\$\textbackslash\{\}ge\$ 175B) to T5 variants (\$\textbackslash\{\}le\$ 11B). We propose model specialization, to specialize the model's ability towards a target task. The hypothesis is that large models (commonly viewed as larger than 100B) have strong modeling power, but are spread on a large spectrum of tasks. Small models (commonly viewed as smaller than 10B) have limited model capacity, but if we concentrate their capacity on a specific target task, the model can achieve a decent improved performance. We use multi-step math reasoning as our testbed because it is a very typical emergent ability. We show two important aspects of model abilities: (1). there exists a very complex balance/ tradeoff between language models' multi-dimensional abilities; (2). by paying the price of decreased generic ability, we can clearly lift up the scaling curve of models smaller than 10B towards a specialized multi-step math reasoning ability. We further give comprehensive discussions about important design choices for better generalization, including the tuning data format, the start model checkpoint, and a new model selection method. We hope our practice and discoveries can serve as an important attempt towards specialized smaller models in the new research paradigm set by LLMs.},
  keywords={arxiv:2301.12726}
}

@article{jeoung2023stereomapquantifying,
  title={StereoMap: Quantifying the Awareness of Human-like Stereotypes in Large Language Models},
  author={Sullam Jeoung and Yubin Ge and Jana Diesner},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.13673},
  url={https://www.semanticscholar.org/paper/549da43aacc3ef5986a126dd9154b7772594b76b},
  abstract={Large Language Models (LLMs) have been observed to encode and perpetuate harmful associations present in the training data. We propose a theoretically grounded framework called StereoMap to gain insights into their perceptions of how demographic groups have been viewed by society. The framework is grounded in the Stereotype Content Model (SCM); a well-established theory from psychology. According to SCM, stereotypes are not all alike. Instead, the dimensions of Warmth and Competence serve as the factors that delineate the nature of stereotypes. Based on the SCM theory, StereoMap maps LLMs' perceptions of social groups (defined by socio-demographic features) using the dimensions of Warmth and Competence. Furthermore, the framework enables the investigation of keywords and verbalizations of reasoning of LLMs' judgments to uncover underlying factors influencing their perceptions. Our results show that LLMs exhibit a diverse range of perceptions towards these groups, characterized by mixed evaluations along the dimensions of Warmth and Competence. Furthermore, analyzing the reasonings of LLMs, our findings indicate that LLMs demonstrate an awareness of social disparities, often stating statistical data and research findings to support their reasoning. This study contributes to the understanding of how LLMs perceive and represent social groups, shedding light on their potential biases and the perpetuation of harmful associations.},
  keywords={arxiv:2310.13673}
}

@article{jiayang2023storyanalogyderiving,
  title={StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding},
  author={Cheng Jiayang and Lin Qiu and Tszho Chan and Tianqing Fang and Weiqi Wang and Chunkit Chan and Dongyu Ru and Qipeng Guo and Hongming Zhang and Yangqiu Song and Yue Zhang and Zheng Zhang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.12874},
  url={https://www.semanticscholar.org/paper/64410909714f421c153ac123f975f86cc15c1fec},
  abstract={Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, \textbackslash\{\}textsc\{StoryAnalogy\}, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on \textbackslash\{\}textsc\{StoryAnalogy\}, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30\% accuracy in multiple-choice questions (compared to over 85\% accuracy for humans). Furthermore, we observe that the data in \textbackslash\{\}textsc\{StoryAnalogy\} can improve the quality of analogy generation in LLMs, where a fine-tuned FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.},
  keywords={arxiv:2310.12874}
}

@article{shen2023storygptvlarge,
  title={StoryGPT-V: Large Language Models as Consistent Story Visualizers},
  author={Xiaoqian Shen and Mohamed Elhoseiny},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52734.2025.01239},
  url={https://www.semanticscholar.org/paper/e49cb2ab3a7990e3d05042197ae8b3fd934453de},
  abstract={Recent generative models have demonstrated impressive capabilities in generating realistic and visually pleasing images grounded on textual prompts. Nevertheless, a significant challenge remains in applying these models for the more intricate task of story visualization. Since it requires resolving pronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution, and ensuring consistent characters and background synthesis across frames. Yet, the emerging Large Language Model (LLM) showcases robust reasoning abilities to navigate through ambiguous references and process extensive sequences. Therefore, we introduce StoryGPT-V, which leverages the merits of the latent diffusion (LDM) and LLM to produce images with consistent and high-quality characters grounded on given story descriptions. First, we train a character-aware LDM, which takes character-augmented semantic embedding as input and includes the supervision of the cross-attention map using character segmentation masks, aiming to enhance character generation accuracy and faithfulness. In the second stage, we enable an alignment between the output of LLM and the character-augmented embedding residing in the input space of the first-stage model. This harnesses the reasoning ability of LLM to address ambiguous references and the comprehension capability to memorize the context. We conduct comprehensive experiments on two visual story visualization benchmarks. Our model reports superior quantitative results and consistently generates accurate characters of remarkable quality with low memory consumption. Our code is publicly available at: https://xiaoqian-shen.github.io/StoryGPT-V.},
  keywords={arxiv:2312.02252}
}

@article{lor2023strategicbehavior,
  title={Strategic Behavior of Large Language Models: Game Structure vs. Contextual Framing},
  author={Nunzio Lorè and Babak Heydari},
  year={2023},
  booktitle={Social Science Research Network},
  doi={10.48550/arXiv.2309.05898},
  url={https://www.semanticscholar.org/paper/17e8f71dfdfa03f5df6e5ed7b40ec3b396f17a79},
  abstract={This paper investigates the strategic decision-making capabilities of three Large Language Models (LLMs): GPT-3.5, GPT-4, and LLaMa-2, within the framework of game theory. Utilizing four canonical two-player games -- Prisoner's Dilemma, Stag Hunt, Snowdrift, and Prisoner's Delight -- we explore how these models navigate social dilemmas, situations where players can either cooperate for a collective benefit or defect for individual gain. Crucially, we extend our analysis to examine the role of contextual framing, such as diplomatic relations or casual friendships, in shaping the models' decisions. Our findings reveal a complex landscape: while GPT-3.5 is highly sensitive to contextual framing, it shows limited ability to engage in abstract strategic reasoning. Both GPT-4 and LLaMa-2 adjust their strategies based on game structure and context, but LLaMa-2 exhibits a more nuanced understanding of the games' underlying mechanics. These results highlight the current limitations and varied proficiencies of LLMs in strategic decision-making, cautioning against their unqualified use in tasks requiring complex strategic reasoning.},
  keywords={arxiv:2309.05898}
}

@article{gandhi2023strategicreasoning,
  title={Strategic Reasoning with Language Models},
  author={Kanishk Gandhi and Dorsa Sadigh and Noah D. Goodman},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2305.19165},
  url={https://www.semanticscholar.org/paper/f1a3cd5cc340f47e3e966709f7dfddef23460aa2},
  abstract={Strategic reasoning enables agents to cooperate, communicate, and compete with other agents in diverse situations. Existing approaches to solving strategic games rely on extensive training, yielding strategies that do not generalize to new scenarios or games without retraining. Large Language Models (LLMs), with their ability to comprehend and generate complex, context-rich language, could prove powerful as tools for strategic gameplay. This paper introduces an approach that uses pretrained LLMs with few-shot chain-of-thought examples to enable strategic reasoning for AI agents. Our approach uses systematically generated demonstrations of reasoning about states, values, and beliefs to prompt the model. Using extensive variations of simple matrix games, we show that strategies that are derived based on systematically generated prompts generalize almost perfectly to new game structures, alternate objectives, and hidden information. Additionally, we demonstrate our approach can lead to human-like negotiation strategies in realistic scenarios without any extra training or fine-tuning. Our results highlight the ability of LLMs, guided by systematic reasoning demonstrations, to adapt and excel in diverse strategic scenarios.},
  keywords={arxiv:2305.19165}
}

@article{jiang2023structgptgeneral,
  title={StructGPT: A General Framework for Large Language Model to Reason over Structured Data},
  author={Jinhao Jiang and Kun Zhou and Zican Dong and Keming Ye and Wayne Xin Zhao and Ji-rong Wen},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.09645},
  url={https://www.semanticscholar.org/paper/e0f27336698c84709bd60b6b7f4ce588cbae66bf},
  abstract={In this paper, we study how to improve the zero-shot reasoning ability of large language models\~{}(LLMs) over structured data in a unified way. Inspired by the study on tool augmentation for LLMs, we develop an \textbackslash\{\}emph\{Iterative Reading-then-Reasoning\~{}(IRR)\} approach for solving question answering tasks based on structured data, called \textbackslash\{\}textbf\{StructGPT\}. In our approach, we construct the specialized function to collect relevant evidence from structured data (\textbackslash\{\}ie \textbackslash\{\}emph\{reading\}), and let LLMs concentrate the reasoning task based on the collected information (\textbackslash\{\}ie \textbackslash\{\}emph\{reasoning\}). Specially, we propose an \textbackslash\{\}emph\{invoking-linearization-generation\} procedure to support LLMs in reasoning on the structured data with the help of the external interfaces. By iterating this procedures with provided interfaces, our approach can gradually approach the target answer to a given query. Extensive experiments conducted on three types of structured data demonstrate the effectiveness of our approach, which can significantly boost the performance of ChatGPT and achieve comparable performance against the full-data supervised-tuning baselines. Our codes and data are publicly available at\~{}\textbackslash\{\}url\{https://github.com/RUCAIBox/StructGPT\}.},
  keywords={arxiv:2305.09645}
}

@article{ouyang2023structuredchemistry,
  title={Structured Chemistry Reasoning with Large Language Models},
  author={Siru Ouyang and Zhuosheng Zhang and Bing Yan and Xuan Liu and Jiawei Han and Lianhui Qin},
  year={2023},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2311.09656},
  url={https://www.semanticscholar.org/paper/e56aa728aaa32c087c8f7bc56a7eb225675dd8ae},
  abstract={Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes). Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways. Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality. On this basis, we introduce StructChem, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs' chemical reasoning capability. Testing across four chemistry areas -- quantum chemistry, mechanics, physical chemistry, and kinetics -- StructChem substantially enhances GPT-4's performance, with up to 30\textbackslash\{\}\% peak improvement. Our analysis also underscores the unique difficulties of precise grounded reasoning in science with LLMs, highlighting a need for more research in this area. Code is available at \textbackslash\{\}url\{https://github.com/ozyyshr/StructChem\}.},
  keywords={arxiv:2311.09656}
}

@article{shao2023syntheticprompting,
  title={Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models},
  author={Zhihong Shao and Yeyun Gong and Yelong Shen and Minlie Huang and Nan Duan and Weizhu Chen},
  year={2023},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2302.00618},
  url={https://www.semanticscholar.org/paper/69619a2a47faee7a29ec596db13172e2a42ff921},
  abstract={Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.},
  keywords={arxiv:2302.00618}
}

@article{chen2023tevalevaluating,
  title={T-Eval: Evaluating the Tool Utilization Capability of Large Language Models Step by Step},
  author={Zehui Chen and Weihua Du and Wenwei Zhang and Kuikun Liu and Jiangning Liu and Miao Zheng and Jingming Zhuo and Songyang Zhang and Dahua Lin and Kai Chen and Feng Zhao},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2024.acl-long.515},
  url={https://www.semanticscholar.org/paper/caf60d1120c2d5a894098f01b51d2e2ad32301d7},
  abstract={Large language models (LLM) have achieved remarkable performance on various NLP tasks and are augmented by tools for broader applications. Yet, how to evaluate and analyze the tool-utilization capability of LLMs is still under-explored. In contrast to previous works that evaluate models holistically, we comprehensively decompose the tool utilization into multiple sub-processes, including instruction following, planning, reasoning, retrieval, understanding, and review. Based on that, we further introduce T-Eval to evaluate the tool utilization capability step by step. T-Eval disentangles the tool utilization evaluation into several sub-domains along model capabilities, facilitating the inner understanding of both holistic and isolated competency of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of various LLMs. T-Eval not only exhibits consistency with the outcome-oriented evaluation but also provides a more fine-grained analysis of the capabilities of LLMs, providing a new perspective in LLM evaluation on tool-utilization ability. The benchmark will be available at https://github.com/open-compass/T-Eval.},
  keywords={arxiv:2312.14033}
}

@article{wang2023tsciqteaching,
  title={T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering},
  author={Lei Wang and Yilang Hu and Jiabang He and Xingdong Xu and Ning Liu and Hui-juan Liu and Hengtao Shen},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2305.03453},
  url={https://www.semanticscholar.org/paper/3f758a13d3703b02bdf977f9189230276064da42},
  abstract={Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the external essential information missed. To address these issues, we propose a novel method termed T-SciQ that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a novel data mixing strategy to produce more effective teaching data samples for simple and complex science question answer problems. Extensive experimental results show that our T-SciQ method achieves a new state-of-the-art performance on the ScienceQA benchmark, with an accuracy of 96.18\%. Moreover, our approach outperforms the most powerful fine-tuned baseline by 4.5\%. The code is publicly available at https://github.com/T-SciQ/T-SciQ.},
  keywords={arxiv:2305.03453}
}

@article{sui2023tap4llmtable,
  title={TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning},
  author={Yuan Sui and Jiaru Zou and Mengyu Zhou and Xinyi He and Lun Du and Shi Han and Dongmei Zhang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2312.09039},
  url={https://www.semanticscholar.org/paper/00a67af3b7dc785b4813b61d232cc76b4fb2b189},
  abstract={Table reasoning tasks have shown remarkable progress with the development of large language models (LLMs), which involve interpreting and drawing conclusions from tabular data based on natural language (NL) questions. Existing solutions mainly tested on smaller tables face scalability issues and struggle with complex queries due to incomplete or dispersed data across different table sections. To alleviate these challenges, we propose TAP4LLM as a versatile pre-processor suite for leveraging LLMs in table-based tasks effectively. It covers several distinct components: (1) table sampling to decompose large tables into manageable sub-tables based on query semantics, (2) table augmentation to enhance tables with additional knowledge from external sources or models, and (3) table packing\&serialization to convert tables into various formats suitable for LLMs' understanding. In each module, we design and compare several common methods under various usage scenarios, aiming to shed light on the best practices for leveraging LLMs for table-reasoning tasks. Our experiments show that our method improves LLMs' reasoning capabilities in various tabular tasks and enhances the interaction between LLMs and tabular data by employing effective pre-processing.},
  keywords={arxiv:2312.09039}
}

@article{wang2023tracecomprehensive,
  title={TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models},
  author={Xiao Wang and Yuan Zhang and Tianze Chen and Songyang Gao and Senjie Jin and Xianjun Yang and Zhiheng Xi and Rui Zheng and Yicheng Zou and Tao Gui and Qi Zhang and Xuanjing Huang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.06762},
  url={https://www.semanticscholar.org/paper/a64067c6c4286fc60f4430829ae6b18519c088e3},
  abstract={Aligned large language models (LLMs) demonstrate exceptional capabilities in task-solving, following instructions, and ensuring safety. However, the continual learning aspect of these aligned LLMs has been largely overlooked. Existing continual learning benchmarks lack sufficient challenge for leading aligned LLMs, owing to both their simplicity and the models' potential exposure during instruction tuning. In this paper, we introduce TRACE, a novel benchmark designed to evaluate continual learning in LLMs. TRACE consists of 8 distinct datasets spanning challenging tasks including domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning. All datasets are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Our experiments show that after training on TRACE, aligned LLMs exhibit significant declines in both general ability and instruction-following capabilities. For example, the accuracy of llama2-chat 13B on gsm8k dataset declined precipitously from 28.8\textbackslash\{\}\% to 2\textbackslash\{\}\% after training on our datasets. This highlights the challenge of finding a suitable tradeoff between achieving performance on specific tasks while preserving the original prowess of LLMs. Empirical findings suggest that tasks inherently equipped with reasoning paths contribute significantly to preserving certain capabilities of LLMs against potential declines. Motivated by this, we introduce the Reasoning-augmented Continual Learning (RCL) approach. RCL integrates task-specific cues with meta-rationales, effectively reducing catastrophic forgetting in LLMs while expediting convergence on novel tasks.},
  keywords={arxiv:2310.06762}
}

@article{wang2023trambenchmarking,
  title={TRAM: Benchmarking Temporal Reasoning for Large Language Models},
  author={Yuqing Wang and Yun Zhao},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2310.00835},
  url={https://www.semanticscholar.org/paper/811f451f1991ec5508e67d00375ca4f5d05e0eeb},
  abstract={Reasoning about time is essential for understanding the nuances of events described in natural language. Previous research on this topic has been limited in scope, characterized by a lack of standardized benchmarks that would allow for consistent evaluations across different studies. In this paper, we introduce TRAM, a temporal reasoning benchmark composed of ten datasets, encompassing various temporal aspects of events such as order, arithmetic, frequency, and duration, designed to facilitate a comprehensive evaluation of the TeR capabilities of large language models (LLMs). We evaluate popular LLMs like GPT-4 and Llama2 in zero-shot and few-shot scenarios, and establish baselines with BERT-based and domain-specific models. Our findings indicate that the best-performing model lags significantly behind human performance. It is our aspiration that TRAM will spur further progress in enhancing the TeR capabilities of LLMs.},
  keywords={arxiv:2310.00835}
}

@article{zheng2023takestep,
  title={Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models},
  author={Huaixiu Steven Zheng and Swaroop Mishra and Xinyun Chen and Heng-Tze Cheng and E. Chi and Quoc V. Le and Denny Zhou},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.06117},
  url={https://www.semanticscholar.org/paper/0786c88990235414611478099e43611542d973b0},
  abstract={We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7\% and 11\% respectively, TimeQA by 27\%, and MuSiQue by 7\%.},
  keywords={arxiv:2310.06117}
}

@article{fatemi2023talklike,
  title={Talk like a Graph: Encoding Graphs for Large Language Models},
  author={Bahare Fatemi and Jonathan J. Halcrow and Bryan Perozzi},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.04560},
  url={https://www.semanticscholar.org/paper/9283b8c7e6ad6ae86be059a26595de0d7a427a10},
  abstract={Graphs are a powerful tool for representing and analyzing complex relationships in real-world applications such as social networks, recommender systems, and computational finance. Reasoning on graphs is essential for drawing inferences about the relationships between entities in a complex system, and to identify hidden patterns and trends. Despite the remarkable progress in automated reasoning with natural text, reasoning on graphs with large language models (LLMs) remains an understudied problem. In this work, we perform the first comprehensive study of encoding graph-structured data as text for consumption by LLMs. We show that LLM performance on graph reasoning tasks varies on three fundamental levels: (1) the graph encoding method, (2) the nature of the graph task itself, and (3) interestingly, the very structure of the graph considered. These novel results provide valuable insight on strategies for encoding graphs as text. Using these insights we illustrate how the correct choice of encoders can boost performance on graph reasoning tasks inside LLMs by 4.8\% to 61.8\%, depending on the task.},
  keywords={arxiv:2310.04560}
}

@article{ding2023taskmotion,
  title={Task and Motion Planning with Large Language Models for Object Rearrangement},
  author={Yan Ding and Xiaohan Zhang and Chris Paxton and Shiqi Zhang},
  year={2023},
  booktitle={IEEE/RJS International Conference on Intelligent RObots and Systems},
  doi={10.1109/IROS55552.2023.10342169},
  url={https://www.semanticscholar.org/paper/e4be613cc875e61b8c1c6c60d958f1c20d12d6c0},
  abstract={Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop},
  keywords={arxiv:2303.06247}
}

@article{saparov2023testinggeneral,
  title={Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples},
  author={Abulhair Saparov and Richard Yuanzhe Pang and Vishakh Padmakumar and Nitish Joshi and Seyed Mehran Kazemi and Najoung Kim and He He},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2305.15269},
  url={https://www.semanticscholar.org/paper/c58325547156a70cb27c148e5b57738ca9ce79aa},
  abstract={Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction.},
  keywords={arxiv:2305.15269}
}

@article{sun2023textclassification,
  title={Text Classification via Large Language Models},
  author={Xiaofei Sun and Xiaoya Li and Jiwei Li and Fei Wu and Shangwei Guo and Tianwei Zhang and Guoyin Wang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.findings-emnlp.603},
  url={https://www.semanticscholar.org/paper/6001dce1c8f63350263e013e0e6ff69816f0a9af},
  abstract={Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification. This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning. In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for \$k\$NN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM's generalization ability and the task-specific evidence provided by the full labeled dataset. Remarkably, CARP yields new SOTA performances on 4 out of 5 widely-used text-classification benchmarks, 97.39 (+1.24) on SST-2, 96.40 (+0.72) on AGNews, 98.78 (+0.25) on R8 and 96.95 (+0.6) on R52, and a performance comparable to SOTA on MR (92.39 v.s. 93.3). More importantly, we find that CARP delivers impressive abilities on low-resource and domain-adaptation setups. Specifically, using 16 examples per class, CARP achieves comparable performances to supervised models with 1,024 examples per class.},
  keywords={arxiv:2305.08377}
}

@article{shridhar2023refinementrefine,
  title={The ART of LLM Refinement: Ask, Refine, and Trust},
  author={Kumar Shridhar and Koustuv Sinha and Andrew Cohen and Tianlu Wang and Ping Yu and Ramakanth Pasunuru and Mrinmaya Sachan and Jason Weston and Asli Celikyilmaz},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2311.07961},
  url={https://www.semanticscholar.org/paper/b7db52a4ad0211bf0b18195d6d247b089da26048},
  abstract={Large Language Models (LLMs) have demonstrated remarkable generative abilities, but can they judge the quality of their own generations and self-improve?A popular concept, referred to as *self-refinement*, postulates that LLMs can detect and correct the errors in their generations when asked to do so. However, recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved. To address this, we propose a reasoning with a refinement strategy called *ART: Ask, Refine, and Trust*, which *asks* necessary questions to decide when an LLM should *refine* its output, and uses it to affirm or deny *trust* in its refinement by ranking the refinement and the initial prediction. On two multistep reasoning tasks of mathematical word problems (GSM8K) and question answering (StrategyQA), *ART* achieves a performance gain of +5 points over self-refinement baselines, while using a much smaller model as the decision maker. We believe that *ART* with smaller models, making refinement decisions can be a cost-effective alternative to fine-tuning LLMs.},
  keywords={arxiv:2311.07961}
}

@article{li2023causalreasoning,
  title={The Causal Reasoning Ability of Open Large Language Model: A Comprehensive and Exemplary Functional Testing},
  author={Shunhang Li and Gang Zhou and Zhi-Bo Li and Ji-Cang Lu and Ningbo Huang},
  year={2023},
  booktitle={International Conference on Software Quality, Reliability and Security},
  doi={10.1109/QRS60937.2023.00032},
  url={https://www.semanticscholar.org/paper/5a17155867261aff3773fd5f8600be05e1dd4356},
  abstract={As the intelligent software, the development and application of large language models are extremely hot topics recently, bringing tremendous changes to general AI and software industry. Nonetheless, large language models, especially open source ones, incontrollably suffer from some potential software quality issues such as instability, inaccuracy, and insecurity, making software testing necessary. In this paper, we propose the first solution for functional testing of open large language models to check full-scene availability and conclude empirical principles for better steering large language models, particularly considering their black box and intelligence properties. Specifically, we focus on the model’s causal reasoning ability, which is the core of artificial intelligence but almost ignored by most previous work. First, for comprehensive evaluation, we deconstruct the causal reasoning capability into five dimensions and summary the forms of causal reasoning task as causality identification and causality matching. Then, rich datasets are introduced and further modified to generate test cases along with different ability dimensions and task forms to improve the testing integrity. Moreover, we explore the ability boundary of open large language models in two usage modes: prompting and lightweight fine-tuning. Our work conducts comprehensive functional testing on the causal reasoning ability of open large language models, establishes benchmarks, and derives empirical insights for practical usage. The proposed testing solution can be transferred to other similar evaluation tasks as a general framework for large language models or their derivations.}
}

@article{jacob2023consensusgame,
  title={The Consensus Game: Language Model Generation via Equilibrium Search},
  author={Athul Paul Jacob and Yikang Shen and Gabriele Farina and Jacob Andreas},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.09139},
  url={https://www.semanticscholar.org/paper/5baeb40d658ba6f2f26ecd64b6ce594077a9725d},
  abstract={When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate outputs). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game - which we term the CONSENSUS GAME - in which a GENERATOR seeks to communicate an abstract correctness parameter using natural language sentences to a DISCRIMINATOR. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call EQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading comprehension, commonsense reasoning, mathematical problem-solving, and dialog), EQUILIBRIUM-RANKING consistently, and sometimes substantially, improves performance over existing LM decoding procedures - on multiple benchmarks, we observe that applying EQUILIBRIUM-RANKING to LLaMA-7B outperforms the much larger LLaMA-65B and PaLM-540B models. These results highlight the promise of game-theoretic tools for addressing fundamental challenges of truthfulness and consistency in LMs.},
  keywords={arxiv:2310.09139}
}

@article{fernandes2023devilerrors,
  title={The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation},
  author={Patrick Fernandes and Daniel Deutsch and M. Finkelstein and Parker Riley and André F. T. Martins and Graham Neubig and Ankush Garg and J. Clark and Markus Freitag and Orhan Firat},
  year={2023},
  booktitle={Conference on Machine Translation},
  doi={10.48550/arXiv.2308.07286},
  url={https://www.semanticscholar.org/paper/fd80f7f3673fc6ca02f192d5d73426f11a4be659},
  abstract={Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.},
  keywords={arxiv:2308.07286}
}

@article{li2023hitchhikersguide,
  title={The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models},
  author={Haonan Li and Yu Hao and Yizhuo Zhai and Zhiyun Qian},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.00245},
  url={https://www.semanticscholar.org/paper/6968552aedfdcdd4cbdc480fd7afd44f4f9b90c4},
  abstract={Static analysis is a widely used technique in software engineering for identifying and mitigating bugs. However, a significant hurdle lies in achieving a delicate balance between precision and scalability. Large Language Models (LLMs) offer a promising alternative, as recent advances demonstrate remarkable capabilities in comprehending, generating, and even debugging code. Yet, the logic of bugs can be complex and require sophisticated reasoning and a large analysis scope spanning multiple functions. Therefore, at this point, LLMs are better used in an assistive role to complement static analysis. In this paper, we take a deep dive into the open space of LLM-assisted static analysis, using use-before-initialization (UBI) bugs as a case study. To this end, we develop LLift, a fully automated framework that interfaces with both a static analysis tool and an LLM. By carefully designing the framework and the prompts, we are able to overcome a number of challenges, including bug-specific modeling, the large problem scope, the non-deterministic nature of LLMs, etc. Tested in a real-world scenario analyzing nearly a thousand potential UBI bugs produced by static analysis, LLift demonstrates a potent capability, showcasing a reasonable precision (50\%) and appearing to have no missing bugs. It even identified 13 previously unknown UBI bugs in the Linux kernel. This research paves the way for new opportunities and methodologies in using LLMs for bug discovery in extensive, real-world datasets.},
  keywords={arxiv:2308.00245}
}

@article{liu2023magicinvestigating,
  title={The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code},
  author={Xiao Liu and Da Yin and Chen Zhang and Yansong Feng and Dongyan Zhao},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2305.19213},
  url={https://www.semanticscholar.org/paper/498d1406fc4cddb05cd46477793f2e726a6fe238},
  abstract={Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations.},
  keywords={arxiv:2305.19213}
}

@article{sharma2023truththere,
  title={The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction},
  author={Pratyusha Sharma and Jordan T. Ash and Dipendra Misra},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2312.13558},
  url={https://www.semanticscholar.org/paper/5c7a21e9262b62f0a27fefdc8b1270dfdcbd3912},
  abstract={Transformer-based Large Language Models (LLMs) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing amounts of data. This work, however, demonstrates the surprising result that it is often possible to significantly improve the performance of LLMs by selectively removing higher-order components of their weight matrices. This simple intervention, which we call LAyer-SElective Rank reduction (LASER), can be done on a model after training has completed, and requires no additional parameters or data. We show extensive experiments demonstrating the generality of this finding across language models and datasets, and provide in-depth analyses offering insights into both when LASER is effective and the mechanism by which it operates.},
  keywords={arxiv:2312.13558}
}

@article{spathis2023firststep,
  title={The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models},
  author={Dimitris Spathis and F. Kawsar},
  year={2023},
  booktitle={J. Am. Medical Informatics Assoc.},
  doi={10.48550/arXiv.2309.06236},
  url={https://www.semanticscholar.org/paper/0b778079946764292de3771a489d5ce9e1868a8b},
  abstract={OBJECTIVES
Large language models (LLMs) have demonstrated remarkable generalization and across diverse tasks, leading individuals to increasingly use them as personal assistants due to their emerging reasoning capabilities. Nevertheless, a notable obstacle emerges when including numerical/temporal data into these prompts, such as data sourced from wearables or electronic health records. LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships. This article discusses the challenges of representing and tokenizing temporal data. It argues that naively passing timeseries to LLMs can be ineffective due to the modality gap between numbers and text.


MATERIALS AND METHODS
We conduct a case study by tokenizing a sample mobile sensing dataset using the OpenAI tokenizer. We also review recent works that feed timeseries data into LLMs for human-centric tasks, outlining common experimental setups like zero-shot prompting and few-shot learning.


RESULTS
The case study shows that popular LLMs split timestamps and sensor values into multiple nonmeaningful tokens, indicating they struggle with temporal data. We find that preliminary works rely heavily on prompt engineering and timeseries aggregation to "ground" LLMs, hinting that the "modality gap" hampers progress. The literature was critically analyzed through the lens of models optimizing for expressiveness versus parameter efficiency. On one end of the spectrum, training large domain-specific models from scratch is expressive but not parameter-efficient. On the other end, zero-shot prompting of LLMs is parameter-efficient but lacks expressiveness for temporal data.


DISCUSSION
We argue tokenizers are not optimized for numerical data, while the scarcity of timeseries examples in training corpora exacerbates difficulties. We advocate balancing model expressiveness and computational efficiency when integrating temporal data. Prompt tuning, model grafting, and improved tokenizers are highlighted as promising directions.


CONCLUSION
We underscore that despite promising capabilities, LLMs cannot meaningfully process temporal data unless the input representation is addressed. We argue that this paradigm shift in how we leverage pretrained models will particularly affect the area of biomedical signals, given the lack of modality-specific foundation models.},
  keywords={arxiv:2309.06236}
}

@article{yeadon2023impactphysics,
  title={The impact of AI in physics education: a comprehensive review from GCSE to university levels},
  author={W. Yeadon and Tom Hardy},
  year={2023},
  booktitle={The Physical Educator},
  doi={10.1088/1361-6552/ad1fa2},
  url={https://www.semanticscholar.org/paper/e2ffb7b4215cbe7b5d06be4a37aacedc8762fd50},
  abstract={With the rapid evolution of artificial intelligence (AI), its potential implications for higher education have become a focal point of interest. This study delves into the capabilities of AI in physics education and offers actionable AI policy recommendations. Using openAI’s flagship gpt-3.5-turbo large language model (LLM), we assessed its ability to answer 1337 physics exam questions spanning general certificate of secondary education (GCSE), A-Level, and introductory university curricula. We employed various AI prompting techniques: Zero Shot, in context learning, and confirmatory checking, which merges chain of thought reasoning with reflection. The proficiency of gpt-3.5-turbo varied across academic levels: it scored an average of 83.4\% on GCSE, 63.8\% on A-Level, and 37.4\% on university-level questions, with an overall average of 59.9\% using the most effective prompting technique. In a separate test, the LLM’s accuracy on 5000 mathematical operations was found to be 45.2\%. When evaluated as a marking tool, the LLM’s concordance with human markers averaged at 50.8\%, with notable inaccuracies in marking straightforward questions, like multiple-choice. Given these results, our recommendations underscore caution: while current LLMs can consistently perform well on physics questions at earlier educational stages, their efficacy diminishes with advanced content and complex calculations. LLM outputs often showcase novel methods not in the syllabus, excessive verbosity, and miscalculations in basic arithmetic. This suggests that at university, there’s no substantial threat from LLMs for non-invigilated physics questions. However, given the LLMs’ considerable proficiency in writing physics essays and coding abilities, non-invigilated examinations of these skills in physics are highly vulnerable to automated completion by LLMs. This vulnerability also extends to pysics questions pitched at lower academic levels. It is thus recommended that educators be transparent about LLM capabilities with their students, while emphasizing caution against overreliance on their output due to its tendency to sound plausible but be incorrect.},
  keywords={arxiv:2309.05163}
}

@article{quelle2023perilspromises,
  title={The perils and promises of fact-checking with large language models},
  author={Dorian Quelle and Alexandre Bovet},
  year={2023},
  booktitle={Frontiers Artif. Intell.},
  doi={10.3389/frai.2024.1341697},
  url={https://www.semanticscholar.org/paper/f72b6f2fee42c3f46bc3b7f8f3dbcdc32c38f119},
  abstract={Automated fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large language models (LLMs) like GPT-4 are increasingly trusted to write academic papers, lawsuits, and news articles and to verify information, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Understanding the capacities and limitations of LLMs in fact-checking tasks is therefore essential for ensuring the health of our information ecosystem. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.},
  keywords={arxiv:2310.13549}
}

@article{li2023theorymind,
  title={Theory of Mind for Multi-Agent Collaboration via Large Language Models},
  author={Huao Li and Yu Quan Chong and Simon Stepputtis and Joseph Campbell and Dana Hughes and Michael Lewis and Katia P. Sycara},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.emnlp-main.13},
  url={https://www.semanticscholar.org/paper/e17c58d7a48b6b811df023484161a3b9c03e0d6b},
  abstract={While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.},
  keywords={arxiv:2310.10701}
}

@article{li2023thinkoutside,
  title={Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation},
  author={Xinyu Li and Jiang-Tian Xue and Zheng Xie and Ming Li},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2305.10679},
  url={https://www.semanticscholar.org/paper/d53f4cdddd7494d5f6e64d81f627691f6d7dff95},
  abstract={Code generation aims to automatically generate source code from high-level task specifications, which can significantly increase productivity of software engineering. Recently, approaches based on large language models (LLMs) have shown remarkable code generation abilities on simple tasks. However, generate code for more complex tasks, such as competition-level problems, remains challenging. In this paper, we introduce Brainstorm framework for code generation. It leverages a brainstorming step that generates and selects diverse thoughts on the problem to facilitate algorithmic reasoning, where the thoughts are possible blueprint of solving the problem. We demonstrate that Brainstorm significantly enhances the ability of LLMs to solve competition-level programming problems, resulting in a more than 50\% increase in the pass@\$k\$ metrics for ChatGPT on the CodeContests benchmark, achieving state-of-the-art performance. Furthermore, our experiments conducted on LeetCode contests show that our framework boosts the ability of ChatGPT to a level comparable to that of human programmers.},
  keywords={arxiv:2305.10679}
}

@article{wilf2023thinktwice,
  title={Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities},
  author={Alex Wilf and Sihyun Shawn Lee and Paul Pu Liang and Louis-philippe Morency},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2311.10227},
  url={https://www.semanticscholar.org/paper/0aa150619e07fa41492517368beaaf8ae56fe061},
  abstract={Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory"Simulation Theory"to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory's notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs' ToM capabilities.},
  keywords={arxiv:2311.10227}
}

@article{sun2023thinkongraphdeep,
  title={Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph},
  author={Jiashuo Sun and Chengjin Xu and Lumingyuan Tang and Sai Wang and Chen Lin and Yeyun Gong and Lionel M. Ni and H. Shum and Jian Guo},
  year={2023},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/c7184f9a914dbfbad59faa5aeaf9ba7019dfcf74},
  abstract={Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``\$\textbackslash\{\}hbox\{LLM\}\textbackslash\{\}otimes\textbackslash\{\}hbox\{KG\}\$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training.},
  keywords={arxiv:2307.07697}
}

@article{yu2023thoughtpropagation,
  title={Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models},
  author={Junchi Yu and Ran He and Rex Ying},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.03965},
  url={https://www.semanticscholar.org/paper/3784fd84b61d482b52f7ef72aac66bcb886b892b},
  abstract={Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \textbackslash\{\}textit\{from scratch\}. To address these issues, we propose \textbackslash\{\}textbf\{\textbackslash\{\}textit\{Thought Propagation\} (TP)\}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\textbackslash\{\}\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\textbackslash\{\}\% improvement of human preference in Creative Writing, and 15\textbackslash\{\}\% enhancement in the task completion rate of LLM-Agent Planning.},
  keywords={arxiv:2310.03965}
}

@article{ott2023thoughtsourcecentral,
  title={ThoughtSource: A central hub for large language model reasoning data},
  author={Simon Ott and Konstantin Hebenstreit and Valentin Li'evin and C. Hother and M. Moradi and Maximilian Mayrhauser and Robert Praas and O. Winther and M. Samwald},
  year={2023},
  booktitle={Scientific Data},
  doi={10.1038/s41597-023-02433-3},
  url={https://www.semanticscholar.org/paper/edc9bf11c4810a77f00ccb96130ff67ee578391e},
  abstract={Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to ‘hallucinate’ facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates seven scientific/medical, three general-domain and five math word question answering datasets.},
  keywords={arxiv:2301.11596}
}

@article{yen2023threequestions,
  title={Three Questions Concerning the Use of Large Language Models to Facilitate Mathematics Learning},
  author={An-Zi Yen and Wei-Ling Hsu},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.13615},
  url={https://www.semanticscholar.org/paper/e1a10caa6571602980f488822e2f7e88e311f160},
  abstract={Due to the remarkable language understanding and generation abilities of large language models (LLMs), their use in educational applications has been explored. However, little work has been done on investigating the pedagogical ability of LLMs in helping students to learn mathematics. In this position paper, we discuss the challenges associated with employing LLMs to enhance students' mathematical problem-solving skills by providing adaptive feedback. Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions' rationales when attempting to correct students' answers. Three research questions are formulated.},
  keywords={arxiv:2310.13615}
}

@article{zhuang2023throughlens,
  title={Through the Lens of Core Competency: Survey on Evaluation of Large Language Models},
  author={Ziyu Zhuang and Qiguang Chen and Longxuan Ma and Mingda Li and Yi Han and Yushan Qian and Haopeng Bai and Zixian Feng and Weinan Zhang and Ting Liu},
  year={2023},
  booktitle={China National Conference on Chinese Computational Linguistics},
  doi={10.48550/arXiv.2308.07902},
  url={https://www.semanticscholar.org/paper/451a657dabf80ebc43f6a3be518250b2cd5dfe1a},
  abstract={“From pre-trained language model (PLM) to large language model (LLM), the field of naturallanguage processing (NLP) has witnessed steep performance gains and wide practical uses. Theevaluation of a research field guides its direction of improvement. However, LLMs are extremelyhard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inade-quate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficultto keep up with the wide range of applications in real-world scenarios. To tackle these problems,existing works proposed various benchmarks to better evaluate LLMs. To clarify the numerousevaluation tasks in both academia and industry, we investigate multiple papers concerning LLMevaluations. We summarize 4 core competencies of LLM, including reasoning, knowledge, relia-bility, and safety. For every competency, we introduce its definition, corresponding benchmarks,and metrics. Under this competency architecture, similar tasks are combined to reflect corre-sponding ability, while new tasks can also be easily added into the system. Finally, we give oursuggestions on the future direction of LLM’s evaluation.”},
  keywords={arxiv:2308.07902}
}

@article{jin2023timellmtime,
  title={Time-LLM: Time Series Forecasting by Reprogramming Large Language Models},
  author={Ming Jin and Shiyu Wang and Lintao Ma and Zhixuan Chu and James Y. Zhang and X. Shi and Pin-Yu Chen and Yuxuan Liang and Yuan-Fang Li and Shirui Pan and Qingsong Wen},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.01728},
  url={https://www.semanticscholar.org/paper/16f01c1b3ddd0b2abd5ddfe4fdb3f74767607277},
  abstract={Time series forecasting holds significant importance in many real-world dynamic systems and has been extensively studied. Unlike natural language process (NLP) and computer vision (CV), where a single large model can tackle multiple tasks, models for time series forecasting are often specialized, necessitating distinct designs for different tasks and applications. While pre-trained foundation models have made impressive strides in NLP and CV, their development in time series domains has been constrained by data sparsity. Recent studies have revealed that large language models (LLMs) possess robust pattern recognition and reasoning abilities over complex sequences of tokens. However, the challenge remains in effectively aligning the modalities of time series data and natural language to leverage these capabilities. In this work, we present Time-LLM, a reprogramming framework to repurpose LLMs for general time series forecasting with the backbone language models kept intact. We begin by reprogramming the input time series with text prototypes before feeding it into the frozen LLM to align the two modalities. To augment the LLM's ability to reason with time series data, we propose Prompt-as-Prefix (PaP), which enriches the input context and directs the transformation of reprogrammed input patches. The transformed time series patches from the LLM are finally projected to obtain the forecasts. Our comprehensive evaluations demonstrate that Time-LLM is a powerful time series learner that outperforms state-of-the-art, specialized forecasting models. Moreover, Time-LLM excels in both few-shot and zero-shot learning scenarios.},
  keywords={arxiv:2310.01728}
}

@article{chu2023timebenchcomprehensive,
  title={TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models},
  author={Zheng Chu and Jingchang Chen and Qianglong Chen and Weijiang Yu and Haotian Wang and Ming Liu and Bing Qin},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2311.17667},
  url={https://www.semanticscholar.org/paper/f37d1ef3c4fd85f608439d239306a3b3302e3add},
  abstract={Grasping the concept of time is a fundamental facet of human cognition, indispensable for truly comprehending the intricacies of the world. Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark. To address this, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena. TimeBench provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models. We conduct extensive experiments on GPT-4, LLaMA2, and other popular LLMs under various settings. Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning. Besides, LLMs exhibit capability discrepancies across different reasoning categories. Furthermore, we thoroughly analyze the impact of multiple aspects on temporal reasoning and emphasize the associated challenges. We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning. Resources are available at: https://github.com/zchuz/TimeBench},
  keywords={arxiv:2311.17667}
}

@article{ren2023timechattimesensitive,
  title={TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding},
  author={Shuhuai Ren and Linli Yao and Shicheng Li and Xu Sun and Lu Hou},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.01357},
  url={https://www.semanticscholar.org/paper/eca8a3e6383e3618e0bc984382e08c09be3cca6c},
  abstract={This work proposes TimeChat, a time-sensitive multi-modal large language model specifically designed for long video understanding. Our model incorporates two key architectural contributions: (1) a timestamp-aware frame encoder that binds visual content with the timestamp of each frame, and (2) a sliding video Q-Former that produces a video token sequence of varying lengths to accommodate videos of various durations. Additionally, we construct an instruction-tuning dataset, encompassing 6 tasks and a total of 125K instances, to further enhance TimeChat's instruction-following performance. Experiment results across various video understanding tasks, such as dense captioning, temporal grounding, and highlight detection, demonstrate TimeChat's strong zero-shot temporal localization and reasoning capabilities. For example, it achieves +9.2 F1 score and +2.8 CIDEr on YouCook2, +5.8 HIT@1 on QVHighlights, and +27.5 R@1 (I oU=0.5) on Charades-STA, compared to state-of-the-art video large language models, holding the potential to serve as a versatile video assistant for long-form video comprehension tasks and satisfy realistic user requirements.11Our code and dataset are available at https://github.com/RenShuhuai-Andy/TimeChat.},
  keywords={arxiv:2312.02051}
}

@article{gou2023toratoolintegrated,
  title={ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving},
  author={Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Minlie Huang and Nan Duan and Weizhu Chen},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2309.17452},
  url={https://www.semanticscholar.org/paper/b272513916b45c8517d289d7abee4a53e6832187},
  abstract={Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13\%-19\% absolute improvements on average. Notably, ToRA-7B reaches 44.6\% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22\% absolute. ToRA-Code-34B is also the first open-source model that achieves an accuracy exceeding 50\% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.},
  keywords={arxiv:2309.17452}
}

@article{qin2023toolllmfacilitating,
  title={ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs},
  author={Yujia Qin and Shi Liang and Yining Ye and Kunlun Zhu and Lan Yan and Ya-Ting Lu and Yankai Lin and Xin Cong and Xiangru Tang and Bill Qian and Sihan Zhao and Runchu Tian and Ruobing Xie and Jie Zhou and Marc H. Gerstein and Dahai Li and Zhiyuan Liu and Maosong Sun},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2307.16789},
  url={https://www.semanticscholar.org/paper/0bfc804e31eecfd77f45e4ee7f4d629fffdcd628},
  abstract={Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.},
  keywords={arxiv:2307.16789}
}

@article{tan2023towardsbenchmarking,
  title={Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models},
  author={Qingyu Tan and H. Ng and Lidong Bing},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2306.08952},
  url={https://www.semanticscholar.org/paper/11daaaedd317ae23c7de7df506572d9155017ae3},
  abstract={Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.},
  keywords={arxiv:2306.08952}
}

@article{wang2023towardscodable,
  title={Towards Codable Text Watermarking for Large Language Models},
  author={Lean Wang and Wenkai Yang and Deli Chen and Hao Zhou and Yankai Lin and Fandong Meng and Jie Zhou and Xu Sun},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2307.15992},
  url={https://www.semanticscholar.org/paper/3aff8ba10ab822ff30d57f1d4a0c71af3facaca6}
}

@article{chen2023towardsendtoend,
  title={Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond},
  author={Liang Chen and Yichi Zhang and Shuhuai Ren and Haozhe Zhao and Zefan Cai and Yuchi Wang and Tianyu Liu and Baobao Chang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.02071},
  url={https://www.semanticscholar.org/paper/bee68767debbdc96d6f75947e544a8be98b869e3},
  abstract={In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3\%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26\%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.},
  keywords={arxiv:2310.02071}
}

@article{yang2023towardsinterpretable,
  title={Towards Interpretable Mental Health Analysis with Large Language Models},
  author={Kailai Yang and Shaoxiong Ji and Tianlin Zhang and Qianqian Xie and Zi-Zhou Kuang and Sophia Ananiadou},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.emnlp-main.370},
  url={https://www.semanticscholar.org/paper/5d879530c443dd06d3686f31d32cfe34c7ade9bc},
  abstract={The latest large language models (LLMs) such as ChatGPT, exhibit strong capabilities in automated mental health analysis. However, existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability. To bridge these gaps, we comprehensively evaluate the mental health analysis and emotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore the effects of different prompting strategies with unsupervised and distantly supervised emotional information. Based on these prompts, we explore LLMs for interpretable mental health analysis by instructing them to generate explanations for each of their decisions. We convey strict human evaluations to assess the quality of the generated explanations, leading to a novel dataset with 163 human-assessed explanations. We benchmark existing automatic evaluation metrics on this dataset to guide future related works. According to the results, ChatGPT shows strong in-context learning ability but still has a significant gap with advanced task-specific methods. Careful prompt engineering with emotional cues and expert-written few-shot examples can also effectively improve performance on mental health analysis. In addition, ChatGPT generates explanations that approach human performance, showing its great potential in explainable mental health analysis.},
  keywords={arxiv:2304.03347}
}

@article{xi2023towardsopenworld,
  title={Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models},
  author={Yunjia Xi and Weiwen Liu and Jianghao Lin and Jieming Zhu and Bo Chen and Ruiming Tang and Weinan Zhang and Rui Zhang and Yong Yu},
  year={2023},
  booktitle={ACM Conference on Recommender Systems},
  doi={10.1145/3640457.3688104},
  url={https://www.semanticscholar.org/paper/c5481668f78ab0c8ef2de9230f2fc1ce27eea6e4},
  abstract={Recommender system plays a vital role in various online services. However, its insulated nature of training and deploying separately within a specific closed domain limits its access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capabilities. Nevertheless, previous attempts to directly use LLMs as recommenders cannot meet the inference latency demand of industrial recommender systems. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs — the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments show that KAR significantly outperforms the state-of-the-art baselines and is compatible with a wide range of recommendation algorithms. We deploy KAR to Huawei’s news and music recommendation platforms and gain a 7\% and 1.7\% improvement in the online A/B test, respectively.},
  keywords={arxiv:2306.10933}
}

@article{xu2023towardsreasoning,
  title={Towards Reasoning in Large Language Models via Multi-Agent Peer Review Collaboration},
  author={Zhenran Xu and Senbao Shi and Baotian Hu and Jindi Yu and Dongfang Li and Min Zhang and Yuxiang Wu},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.08152},
  url={https://www.semanticscholar.org/paper/4014253368133c01bfc0383660c518d11afccad2},
  abstract={Large Language Models (LLMs) have shown remarkable capabilities in general natural language processing tasks but often fall short in complex reasoning tasks. Recent studies have explored human-like problem-solving strategies, such as self-correct, to push further the boundary of single-model reasoning ability. In this work, we let a single model"step outside the box"by engaging multiple models to correct each other. We introduce a multi-agent collaboration strategy that emulates the academic peer review process. Each agent independently constructs its own solution, provides reviews on the solutions of others, and assigns confidence levels to its reviews. Upon receiving peer reviews, agents revise their initial solutions. Extensive experiments on three different types of reasoning tasks show that our collaboration approach delivers superior accuracy across all ten datasets compared to existing methods. Further study underscores the effectiveness of integrating confidence in reviews, demonstrates the superiority of feedback exchange over mere solution sharing, and highlights the role of capability and diversity in fostering successful collaboration.},
  keywords={arxiv:2311.08152}
}

@article{feng2023towardsrevealing,
  title={Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective},
  author={Guhao Feng and Yuntian Gu and Bohang Zhang and Haotian Ye and Di He and Liwei Wang},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2305.15408},
  url={https://www.semanticscholar.org/paper/c2260403fd5cb2de73491323433e48b6ec36872c},
  abstract={Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language format. Moreover, we show LLMs with CoT are capable of solving a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, extensive experiments on four tasks show that, while Transformers always fail to predict the answers directly, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.},
  keywords={arxiv:2305.15408}
}

@article{tan2023towardsrobust,
  title={Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning},
  author={Qingyu Tan and Hwee Tou Ng and Lidong Bing},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2311.09821},
  url={https://www.semanticscholar.org/paper/066dc4d3550dce456856344acb1434a5ef46ac5d},
  abstract={Knowledge in the real world is being updated constantly. However, it is costly to frequently update large language models (LLMs). Therefore, it is crucial for LLMs to understand the concept of temporal knowledge. However, prior works on temporal question answering (TQA) did not emphasize multi-answer and multi-hop types of temporal reasoning. In this paper, we propose a complex temporal question-answering dataset Complex-TR that focuses on multi-answer and multi-hop temporal reasoning. Besides, we also propose a novel data augmentation strategy to improve the complex temporal reasoning capability and robustness of LLMs. We conducted experiments on multiple temporal QA datasets. Experimental results show that our method is able to improve LLMs' performance on temporal QA benchmarks by significant margins. Our code and data are released at: https://github.com/nusnlp/complex-tr.},
  keywords={arxiv:2311.09821}
}

@article{dou2023towardsunderstanding,
  title={Towards Understanding the Capability of Large Language Models on Code Clone Detection: A Survey},
  author={Shihan Dou and Junjie Shan and Haoxiang Jia and Wenhao Deng and Zhiheng Xi and Wei He and Yueming Wu and Tao Gui and Yang Liu and Xuanjing Huang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.01191},
  url={https://www.semanticscholar.org/paper/1e26b42669b060a3850e4766dea0db6e3c85cdec},
  abstract={Code cloning, the duplication of code fragments, is common in software development. While some reuse aids productivity, excessive cloning hurts maintainability and introduces bugs. Hence, automatic code clone detection is vital. Meanwhile, large language models (LLMs) possess diverse code-related knowledge, making them versatile for various software engineering challenges. However, LLMs' performance in code clone detection is unclear and needs more study for accurate assessment. In this paper, we provide the first comprehensive evaluation of LLMs for clone detection, covering different clone types, languages, and prompts. We find advanced LLMs excel in detecting complex semantic clones, surpassing existing methods. Adding intermediate reasoning steps via chain-of-thought prompts noticeably enhances performance. Additionally, representing code as vector embeddings, especially with text encoders, effectively aids clone detection.Lastly, the ability of LLMs to detect code clones differs among various programming languages. Our study suggests that LLMs have potential for clone detection due to their language capabilities, offering insights for developing robust LLM-based methods to enhance software engineering.},
  keywords={arxiv:2308.01191}
}

@article{mcduff2023towardsaccurate,
  title={Towards accurate differential diagnosis with large language models},
  author={Daniel McDuff and Mike Schaekermann and Tao Tu and Anil Palepu and Amy Wang and Jake Garrison and Karan Singhal and Yash Sharma and Shekoofeh Azizi and Kavita Kulkarni and Le Hou and Yong Cheng and Yun Liu and S. Mahdavi and Sushant Prakash and Anupam Pathak and Christopher Semturs and Shwetak N. Patel and D. Webster and Ewa Dominowska and Juraj Gottweis and Joelle K. Barral and Katherine Chou and G. Corrado and Yossi Matias and Jacob Sunshine and A. Karthikesalingam and Vivek Natarajan},
  year={2023},
  booktitle={Nature},
  doi={10.1038/s41586-025-08869-4},
  url={https://www.semanticscholar.org/paper/76ab9bc1cbf529d1cfe5963f24069539c3199745},
  abstract={A comprehensive differential diagnosis is a cornerstone of medical care that is often reached through an iterative process of interpretation that combines clinical history, physical examination, investigations and procedures. Interactive interfaces powered by large language models present new opportunities to assist and automate aspects of this process1. Here we introduce the Articulate Medical Intelligence Explorer (AMIE), a large language model that is optimized for diagnostic reasoning, and evaluate its ability to generate a differential diagnosis alone or as an aid to clinicians. Twenty clinicians evaluated 302 challenging, real-world medical cases sourced from published case reports. Each case report was read by two clinicians, who were randomized to one of two assistive conditions: assistance from search engines and standard medical resources; or assistance from AMIE in addition to these tools. All clinicians provided a baseline, unassisted differential diagnosis prior to using the respective assistive tools. AMIE exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1\% versus 33.6\%, P = 0.04). Comparing the two assisted study arms, the differential diagnosis quality score was higher for clinicians assisted by AMIE (top-10 accuracy 51.7\%) compared with clinicians without its assistance (36.1\%; McNemar’s test: 45.7, P < 0.01) and clinicians with search (44.4\%; McNemar’s test: 4.75, P = 0.03). Further, clinicians assisted by AMIE arrived at more comprehensive differential lists than those without assistance from AMIE. Our study suggests that AMIE has potential to improve clinicians’ diagnostic reasoning and accuracy in challenging cases, meriting further real-world evaluation for its ability to empower physicians and widen patients’ access to specialist-level expertise. Diagnostic reasoning using an optimized large language model with a dataset comprising real-world medical cases exhibited improved differential diagnostic performance as an assistive tool for clinicians over search engines and standard medical resources.},
  keywords={arxiv:2312.00164}
}

@article{zheng2023towardsunderstanding,
  title={Towards an understanding of large language models in software engineering tasks},
  author={Zibin Zheng and Kai-Chun Ning and Jiachi Chen and Yanlin Wang and Wenqing Chen and Lianghong Guo and Weicheng Wang},
  year={2023},
  booktitle={Empirical Software Engineering},
  doi={10.1007/s10664-024-10602-0},
  url={https://www.semanticscholar.org/paper/681f9009e22c947007b53455e9f8f22e29209010},
  abstract={Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.},
  keywords={arxiv:2308.11396}
}

@article{xie2023translatingnatural,
  title={Translating Natural Language to Planning Goals with Large-Language Models},
  author={Yaqi Xie and Chenyao Yu and Tongyao Zhu and Jinbin Bai and Ze Gong and Harold Soh},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2302.05128},
  url={https://www.semanticscholar.org/paper/3ad346ae7af5c30964c4916dbcee798f72e1bdb7},
  abstract={Recent large language models (LLMs) have demonstrated remarkable performance on a variety of natural language processing (NLP) tasks, leading to intense excitement about their applicability across various domains. Unfortunately, recent work has also shown that LLMs are unable to perform accurate reasoning nor solve planning problems, which may limit their usefulness for robotics-related tasks. In this work, our central question is whether LLMs are able to translate goals specified in natural language to a structured planning language. If so, LLM can act as a natural interface between the planner and human users; the translated goal can be handed to domain-independent AI planners that are very effective at planning. Our empirical results on GPT 3.5 variants show that LLMs are much better suited towards translation rather than planning. We find that LLMs are able to leverage commonsense knowledge and reasoning to furnish missing details from under-specified goals (as is often the case in natural language). However, our experiments also reveal that LLMs can fail to generate goals in tasks that involve numerical or physical (e.g., spatial) reasoning, and that LLMs are sensitive to the prompts used. As such, these models are promising for translation to structured planning languages, but care should be taken in their use.},
  keywords={arxiv:2302.05128}
}

@article{yao2023treethoughts,
  title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models},
  author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and T. Griffiths and Yuan Cao and Karthik Narasimhan},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2305.10601},
  url={https://www.semanticscholar.org/paper/2f3822eb380b5e753a6d579f31dfc3ec4c4a0820},
  abstract={Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\% of tasks, our method achieved a success rate of 74\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
  keywords={arxiv:2305.10601}
}

@article{mo2023treeuncertain,
  title={Tree of Uncertain Thoughts Reasoning for Large Language Models},
  author={Shentong Mo and Miao Xin},
  year={2023},
  booktitle={IEEE International Conference on Acoustics, Speech, and Signal Processing},
  doi={10.1109/ICASSP48485.2024.10448355},
  url={https://www.semanticscholar.org/paper/875d71bae61a66f7e65a2b6d363b7a0a27a6ed25},
  abstract={While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or "thoughts". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) — a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs’ diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model’s precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence underscores TouT’s superiority over both ToT and chain-of-thought prompting methods.},
  keywords={arxiv:2309.07694}
}

@article{chen2023trustareasoning,
  title={Trusta: Reasoning about Assurance Cases with Formal Methods and Large Language Models},
  author={Zezhong Chen and Yu Deng and Wenjie Du},
  year={2023},
  booktitle={Science of Computer Programming},
  doi={10.48550/arXiv.2309.12941},
  url={https://www.semanticscholar.org/paper/63980a765ae610ab4cab3a21e035b051407bfc9d},
  abstract={Assurance cases can be used to argue for the safety of products in safety engineering. In safety-critical areas, the construction of assurance cases is indispensable. Trustworthiness Derivation Trees (TDTs) enhance assurance cases by incorporating formal methods, rendering it possible for automatic reasoning about assurance cases. We present Trustworthiness Derivation Tree Analyzer (Trusta), a desktop application designed to automatically construct and verify TDTs. The tool has a built-in Prolog interpreter in its backend, and is supported by the constraint solvers Z3 and MONA. Therefore, it can solve constraints about logical formulas involving arithmetic, sets, Horn clauses etc. Trusta also utilizes large language models to make the creation and evaluation of assurance cases more convenient. It allows for interactive human examination and modification. We evaluated top language models like ChatGPT-3.5, ChatGPT-4, and PaLM 2 for generating assurance cases. Our tests showed a 50\%-80\% similarity between machine-generated and human-created cases. In addition, Trusta can extract formal constraints from text in natural languages, facilitating an easier interpretation and validation process. This extraction is subject to human review and correction, blending the best of automated efficiency with human insight. To our knowledge, this marks the first integration of large language models in automatic creating and reasoning about assurance cases, bringing a novel approach to a traditional challenge. Through several industrial case studies, Trusta has proven to quickly find some subtle issues that are typically missed in manual inspection, demonstrating its practical value in enhancing the assurance case development process.},
  keywords={arxiv:2309.12941}
}

@article{liu2023trustworthyllms,
  title={Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment},
  author={Yang Liu and Yuanshun Yao and Jean-François Ton and Xiaoying Zhang and Ruocheng Guo and Hao Cheng and Yegor Klochkov and Muhammad Faaiz Taufiq and Hanguang Li},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.05374},
  url={https://www.semanticscholar.org/paper/7142e920b6b9355d9cbacc9450818f912eca138e},
  abstract={Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.},
  keywords={arxiv:2308.05374}
}

@article{li2023turningdust,
  title={Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data},
  author={Yiwei Li and Peiwen Yuan and Shaoxiong Feng and Boyuan Pan and Bin Sun and Xinglin Wang and Heda Wang and Kan Li},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2312.12832},
  url={https://www.semanticscholar.org/paper/9529a6b2032bd08ea4e34954b7a7d2b8b5d5f66b},
  abstract={Large Language Models (LLMs) have performed well on various reasoning tasks, but their inaccessibility and numerous parameters hinder wide application in practice. One promising way is distilling the reasoning ability from LLMs to small models by the generated chain-of-thought reasoning paths. In some cases, however, LLMs may produce incorrect reasoning chains, especially when facing complex mathematical problems. Previous studies only transfer knowledge from positive samples and drop the synthesized data with wrong answers. In this work, we illustrate the merit of negative data and propose a model specialization framework to distill LLMs with negative samples besides positive ones. The framework consists of three progressive steps, covering from training to inference stages, to absorb knowledge from negative data. We conduct extensive experiments across arithmetic reasoning tasks to demonstrate the role of negative data in distillation from LLM.},
  keywords={arxiv:2312.12832}
}

@article{binz2023turninglarge,
  title={Turning large language models into cognitive models},
  author={Marcel Binz and Eric Schulz},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2306.03917},
  url={https://www.semanticscholar.org/paper/56caaf598c1bf36a24385f30ca775b94cf215b6b},
  abstract={Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that -- after finetuning them on data from psychological experiments -- these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become generalist cognitive models, thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole.},
  keywords={arxiv:2306.03917}
}

@article{gandhi2023understandingsocial,
  title={Understanding Social Reasoning in Language Models with Language Models},
  author={Kanishk Gandhi and Jan-Philipp Franken and Tobias Gerstenberg and Noah D. Goodman},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2306.15448},
  url={https://www.semanticscholar.org/paper/f590cbb28e4994f62e94bf9400a9cb33e99922fa},
  abstract={As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.},
  keywords={arxiv:2306.15448}
}

@article{khare2023understandingeffectiveness,
  title={Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities},
  author={Avishree Khare and Saikat Dutta and Ziyang Li and Alaia Solko-Breslin and R. Alur and Mayur Naik},
  year={2023},
  booktitle={International Conference on Information Control Systems \& Technologies},
  doi={10.1109/ICST62969.2025.10988968},
  url={https://www.semanticscholar.org/paper/c782bd29db2fd79d5511ccdb78ba907accb348d1},
  abstract={Security vulnerabilities in modern software are prevalent and harmful. While automated vulnerability detection techniques have made promising progress, their scalability and applicability remain challenging. The remarkable performance of Large Language Models (LLMs), such as GPT-4 and CodeLlama, on code-related tasks has prompted recent works to explore if LLMs can be used to detect security vulnerabilities. In this paper, we perform a more comprehensive study by examining a larger and more diverse set of datasets, languages, and LLMs, and qualitatively evaluating detection performance across prompts and vulnerability classes. Concretely, we evaluate the effectiveness of 16 pre-trained LLMs on 5,000 code samples-1,000 randomly selected each from five diverse security datasets. These balanced datasets encompass synthetic and real-world projects in Java and C/C++ and cover 25 distinct vulnerability classes. Our results show that LLMs across all scales and families show modest effectiveness in end-to-end reasoning about vul-nerabilities, obtaining an average accuracy of 62.8\% and F1 score of 0.71 across all datasets. LLMs are significantly better at detecting vulnerabilities that typically only need intra-procedural reasoning, such as OS Command Injection and NULL Pointer Dereference. Moreover, LLMs report higher accuracies on these vulnerabilities than popular static analysis tools, such as CodeQL. We find that advanced prompting strategies that involve step-by-step analysis significantly improve performance of LLMs on real-world datasets in terms of F1 score (by up to 0.18 on average). Interestingly, we observe that LLMs show promising abilities at performing parts of the analysis correctly, such as identifying vulnerability-related specifications (e.g., sources and sinks) and leveraging natural language information to understand code behavior (e.g., to check if code is sanitized). We believe our insights can motivate future work on LLM-augmented vulnerability detection systems.},
  keywords={arxiv:2311.16169}
}

@article{masry2023unichartuniversal,
  title={UniChart: A Universal Vision-language Pretrained Model for Chart Comprehension and Reasoning},
  author={Ahmed Masry and P. Kavehzadeh and Do Xuan Long and Enamul Hoque and Shafiq R. Joty},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.14761},
  url={https://www.semanticscholar.org/paper/8ab4863393fceb41d0fa77d632ace4c80a57a154},
  abstract={Charts are very popular for analyzing data, visualizing key insights and answering complex reasoning questions about data. To facilitate chart-based data analysis using natural language, several downstream tasks have been introduced recently such as chart question answering and chart summarization. However, most of the methods that solve these tasks use pretraining on language or vision-language tasks that do not attempt to explicitly model the structure of the charts (e.g., how data is visually encoded and how chart elements are related to each other). To address this, we first build a large corpus of charts covering a wide variety of topics and visual styles. We then present UniChart, a pretrained model for chart comprehension and reasoning. UniChart encodes the relevant text, data, and visual elements of charts and then uses a chart-grounded text decoder to generate the expected output in natural language. We propose several chart-specific pretraining tasks that include: (i) low-level tasks to extract the visual elements (e.g., bars, lines) and data from charts, and (ii) high-level tasks to acquire chart understanding and reasoning skills. We find that pretraining the model on a large corpus with chart-specific low- and high-level tasks followed by finetuning on three down-streaming tasks results in state-of-the-art performance on three downstream tasks.},
  keywords={arxiv:2305.14761}
}

@article{chen2023universalselfconsistency,
  title={Universal Self-Consistency for Large Language Model Generation},
  author={Xinyun Chen and Renat Aksitov and Uri Alon and Jie Ren and Kefan Xiao and Pengcheng Yin and Sushant Prakash and Charles Sutton and Xuezhi Wang and Denny Zhou},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.17311},
  url={https://www.semanticscholar.org/paper/5f66d1a667eec13b5d337c3fc5619bcef95092bd},
  abstract={Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.},
  keywords={arxiv:2311.17311}
}

@article{wang2023unleashingcognitive,
  title={Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration},
  author={Zhenhailong Wang and Shaoguang Mao and Wenshan Wu and Tao Ge and Furu Wei and Heng Ji},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2307.05300},
  url={https://www.semanticscholar.org/paper/a863e5b80f5d79a9fc9d73e41bc7f7c91250a571}
}

@article{wang2023unleashingemergent,
  title={Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration},
  author={Zhenhailong Wang and Shaoguang Mao and Wenshan Wu and Tao Ge and Furu Wei and Heng Ji},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.18653/v1/2024.naacl-long.15},
  url={https://www.semanticscholar.org/paper/434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa},
  abstract={Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds’ strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.},
  keywords={arxiv:2307.05300}
}

@article{bello2023unravelingenigma,
  title={Unraveling the Enigma: how can ChatGPT perform so well with language understanding, reasoning, and knowledge processing without having real knowledge or logic?},
  author={Fabio Di Bello},
  year={2023},
  booktitle={ABOUTOPEN},
  doi={10.33393/ao.2023.2618},
  url={https://www.semanticscholar.org/paper/bc782beec8a9e0e3e56e0623136b42a04130369a},
  abstract={Artificial Intelligence (AI) has made significant progress in various domains, but the quest for machines to truly understand natural language has been challenging. Traditional mainstream approaches to AI, while valuable, often struggled to achieve human-level language comprehension. However, the emergence of neural networks and the subsequent adoption of the downstream approach have revolutionized the field, as demonstrated by the powerful and successful language model, ChatGPT.
The deep learning algorithms utilized in large language models (LLMs) differ significantly from those employed in traditional neural networks.
This article endeavors to provide a valuable and insightful exploration of the functionality and performance of generative AI. It aims to accomplish this by offering a comprehensive, yet simplified, analysis of the underlying mathematical models used by systems such as ChatGPT. The primary objective is to explore the diverse performance capabilities of these systems across some important domains such as clinical practice. The article also sheds light on the existing gaps and limitations that impact the quality and reliability of generated answers. Furthermore, it delves into potential strategies aimed at improving the reliability and cognitive aspects of generative AI systems.}
}

@article{zhang2023usercentricconversational,
  title={User-Centric Conversational Recommendation: Adapting the Need of User with Large Language Models},
  author={Gangyi Zhang},
  year={2023},
  booktitle={ACM Conference on Recommender Systems},
  doi={10.1145/3604915.3608885},
  url={https://www.semanticscholar.org/paper/b878cf9c7a1b7886233e907147f06208d324e992},
  abstract={Conversational recommender systems (CRS) promise to provide a more natural user experience for exploring and discovering items of interest through ongoing conversation. However, effectively modeling and adapting to users’ complex and changing preferences remains challenging. This research develops user-centric methods that focus on understanding and adapting to users throughout conversations to provide the most helpful recommendations. First, a graph-based Conversational Path Reasoning (CPR) framework is proposed that represents dialogs as interactive reasoning over a knowledge graph to capture nuanced user interests and explain recommendations. To further enhance relationship modeling, graph neural networks are incorporated for improved representation learning. Next, to address uncertainty in user needs, the Vague Preference Multi-round Conversational Recommendation (VPMCR) scenario and matching Adaptive Vague Preference Policy Learning (AVPPL) solution are presented using reinforcement learning to tailor recommendations to evolving preferences. Finally, opportunities to leverage large language models are discussed to further advance user experiences via advanced user modeling, policy learning, and response generation. Overall, this research focuses on designing conversational recommender systems that continuously understand and adapt to users’ ambiguous, complex and changing needs during natural conversations.}
}

@article{lin2023usinglarge,
  title={Using Large Language Models to Provide Explanatory Feedback to Human Tutors},
  author={Jionghao Lin and Danielle R. Thomas and Feifei Han and Shivang Gupta and Wei Tan and Ngoc Dang Nguyen and K. Koedinger},
  year={2023},
  booktitle={Human-AI Math Tutoring@AIED},
  doi={10.48550/arXiv.2306.15498},
  url={https://www.semanticscholar.org/paper/8dec1f7434b548e1c135ac9e2a26c4b36588cca0},
  abstract={Research demonstrates learners engaging in the process of producing explanations to support their reasoning, can have a positive impact on learning. However, providing learners real-time explanatory feedback often presents challenges related to classification accuracy, particularly in domain-specific environments, containing situationally complex and nuanced responses. We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise. This work-in-progress demonstrates considerable accuracy in binary classification for corrective feedback of effective, or effort-based (F1 score = 0.811), and ineffective, or outcome-based (F1 score = 0.350), praise responses. More notably, we introduce progress towards an enhanced approach of providing explanatory feedback using large language model-facilitated named entity recognition, which can provide tutors feedback, not only while engaging in lessons, but can potentially suggest real-time tutor moves. Future work involves leveraging large language models for data augmentation to improve accuracy, while also developing an explanatory feedback interface.},
  keywords={arxiv:2306.15498}
}

@article{jain2023vcoderversatile,
  title={VCoder: Versatile Vision Encoders for Multimodal Large Language Models},
  author={Jitesh Jain and Jianwei Yang and Humphrey Shi},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.02644},
  url={https://www.semanticscholar.org/paper/1c9bab7ab072c619133c936b5b85160e5373e638},
  abstract={Humans possess the remarkable skill of Visual Perception, the ability to see and understand the seen, helping them make sense of the visual world and, in turn, reason. Multimodal Large Language Models (MLLM) have recently achieved impressive performance on vision-language tasks ranging from visual question-answering and image captioning to visual reasoning and image generation. However, when prompted to identify or count (perceive) the entities in a given image, existing MLLM systems fail. Working towards developing an accurate MLLM system for perception and reasoning, we propose using Versatile vision encoders (VCoder) as perception eyes for Multimodal LLMs. We feed the VCoder with perception modalities such as segmentation or depth maps, improving the MLLM's perception abilities. Secondly, we leverage the images from COCO and outputs from off-the-shelf vision perception models to create our COCO Segmentation Text (COST) dataset for training and evaluating MLLMs on the object perception task. Thirdly, we introduce metrics to assess the object perception abilities in MLLMs on our COST dataset. Lastly, we provide extensive experimental evidence proving the VCoder's improved object-level perception skills over existing Multimodal LLMs, including GPT-4V. We open-source our dataset, code, and models to promote research.},
  keywords={arxiv:2312.14233}
}

@article{toh2023veritymathadvancing,
  title={VerityMath: Advancing Mathematical Reasoning by Self-Verification Through Unit Consistency},
  author={Vernon Toh and Ratish Puduppully and Nancy Chen},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.07172},
  url={https://www.semanticscholar.org/paper/3f8f428d3249d53dd0452283163a4a4124f9bc9c},
  abstract={Large Language Models (LLMs), combined with program-based solving techniques, are increasingly demonstrating proficiency in mathematical reasoning. For example, closed-source models such as OpenAI GPT-4 and Claude show excellent results in solving math word problems. However, progress in math word problem-solving for open-source LLMs is limited, and the challenges these models face are not well-studied. In this paper, we study the performance of strong open-source LLMs, including Llama 2 (7B), Code Llama (7B), and Mistral (7B) on math word problems using program-based solving techniques. Specifically, we analyze the outputs of these models when applied to math word problems and identify a category of problems that pose a significant challenge, particularly those involving quantities spanning multiple units. To address this issue, we propose a systematic approach by defining the units for each quantity and ensuring the consistency of these units during mathematical operations. We developed Unit Consistency Programs (UCPs), an annotated dataset of math word problems, each paired with programs containing unit specifications and unit verification routines. We fine-tuned Llama 2 (7B), Code Llama (7B), and Mistral (7B) models with UCPs to produce theirVerityMath variants. Our findings indicate that our approach, which incorporates unit consistency, currently slightly underperforms compared to an approach that does not. To understand the reasons behind this, we conduct an in-depth error analysis and suggest options for future improvements. Our code and dataset are available at https://github.com/vernontoh/VerityMath.},
  keywords={arxiv:2311.07172}
}

@article{zhou2023vicorbridging,
  title={ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models},
  author={KAI-QING Zhou and Kwonjoon Lee and Teruhisa Misu and X. Wang},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2310.05872},
  url={https://www.semanticscholar.org/paper/33095b1334bed852e3652bd9d7da3f4df0cdf485},
  abstract={In our work, we explore the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) on visual commonsense reasoning (VCR) problems. We find that VLMs and LLMs-based decision pipelines are good at different kinds of VCR problems. Pre-trained VLMs exhibit strong performance for problems involving understanding the literal visual content, which we noted as visual commonsense understanding (VCU). For problems where the goal is to infer conclusions beyond image content, which we noted as visual commonsense inference (VCI), VLMs face difficulties, while LLMs, given sufficient visual evidence, can use commonsense to infer the answer well. We empirically validate this by letting LLMs classify VCR problems into these two categories and show the significant difference between VLM and LLM with image caption decision pipelines on two subproblems. Moreover, we identify a challenge with VLMs' passive perception, which may miss crucial context information, leading to incorrect reasoning by LLMs. Based on these, we suggest a collaborative approach, named ViCor, where pre-trained LLMs serve as problem classifiers to analyze the problem category, then either use VLMs to answer the question directly or actively instruct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences. We evaluate our framework on two VCR benchmark datasets and outperform all other methods that do not require in-domain fine-tuning.},
  keywords={arxiv:2310.05872}
}

@article{tang2023videounderstanding,
  title={Video Understanding with Large Language Models: A Survey},
  author={Yunlong Tang and Jing Bi and Siting Xu and Luchuan Song and Susan Liang and Teng Wang and Daoan Zhang and Jie An and Jingyang Lin and Rongyi Zhu and A. Vosoughi and Chao Huang and Zeliang Zhang and Feng Zheng and Jianguo Zhang and Ping Luo and Jiebo Luo and Chenliang Xu},
  year={2023},
  journal={IEEE transactions on circuits and systems for video technology (Print)},
  doi={10.48550/arXiv.2312.17432},
  url={https://www.semanticscholar.org/paper/5f58863dd6474d6f127be995b5871e7c60f2792f},
  abstract={With the burgeoning growth of online video platforms and the escalating volume of video content, the demand for proficient video understanding tools has intensified markedly. Given the remarkable capabilities of large language models (LLMs) in language and multimodal tasks, this survey provides a detailed overview of recent advancements in video understanding that harness the power of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended multi-granularity (general, temporal, and spatiotemporal) reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding. We examine the unique characteristics and capabilities of Vid-LLMs, categorizing the approaches into three main types: Video Analyzer x LLM, Video Embedder x LLM, and (Analyzer + Embedder) x LLM. Furthermore, we identify five sub-types based on the functions of LLMs in Vid-LLMs: LLM as Summarizer, LLM as Manager, LLM as Text Decoder, LLM as Regressor, and LLM as Hidden Layer. Furthermore, this survey presents a comprehensive study of the tasks, datasets, benchmarks, and evaluation methodologies for Vid-LLMs. Additionally, it explores the expansive applications of Vid-LLMs across various domains, highlighting their remarkable scalability and versatility in real-world video understanding challenges. Finally, it summarizes the limitations of existing Vid-LLMs and outlines directions for future research. For more information, readers are recommended to visit the repository at https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.},
  keywords={arxiv:2312.17432}
}

@article{chen2023videollmmodeling,
  title={VideoLLM: Modeling Video Sequence with Large Language Models},
  author={Guo Chen and Yin-Dong Zheng and Jiahao Wang and Jilan Xu and Yifei Huang and Junting Pan and Yi Wang and Yali Wang and Y. Qiao and Tong Lu and Limin Wang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2305.13292},
  url={https://www.semanticscholar.org/paper/f9bfc6d9ba1665b73af3323d46c7642b852759ef},
  abstract={With the exponential growth of video data, there is an urgent need for automated technology to analyze and comprehend video content. However, existing video understanding models are often task-specific and lack a comprehensive capability of handling diverse tasks. The success of large language models (LLMs) like GPT has demonstrated their impressive abilities in sequence causal reasoning. Building upon this insight, we propose a novel framework called VideoLLM that leverages the sequence reasoning capabilities of pre-trained LLMs from natural language processing (NLP) for video sequence understanding. VideoLLM incorporates a carefully designed Modality Encoder and Semantic Translator, which convert inputs from various modalities into a unified token sequence. This token sequence is then fed into a decoder-only LLM. Subsequently, with the aid of a simple task head, our VideoLLM yields an effective unified framework for different kinds of video understanding tasks. To evaluate the efficacy of VideoLLM, we conduct extensive experiments using multiple LLMs and fine-tuning methods. We evaluate our VideoLLM on eight tasks sourced from four different datasets. The experimental results demonstrate that the understanding and reasoning capabilities of LLMs can be effectively transferred to video understanding tasks. We release the code at https://github.com/cg1177/VideoLLM.},
  keywords={arxiv:2305.13292}
}

@article{hu2023visualprogram,
  title={Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models},
  author={Yushi Hu and Otilia Stretcu and Chun-Ta Lu and Krishnamurthy Viswanathan and K. Hata and Enming Luo and Ranjay Krishna and Ariel Fuxman},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.00916},
  url={https://www.semanticscholar.org/paper/f32ea390686b1eee3ba5b53c7a85e9e9385d4b94},
  abstract={Solving complex visual tasks such as “Who invented the musical instrument on the right?” involves a composition of skills: understanding space, recognizing instruments, and also retrieving prior knowledge. Recent work shows promise by decomposing such tasks using a large language model (LLM) into an executable program that invokes specialized vision models. However, generated programs are error-prone: they omit necessary steps, include spurious ones, and are unable to recover when the specialized models give incor-rect outputs. Moreover, they require loading multiple models, incurring high latency and computation costs. We propose Visual Program Distillation (VPD), an instruction tuning framework that produces a vision-language model (VLM) ca-pable of solving complex visual tasks with a single forward pass. VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one. It translates each correct program into a language description of the reasoning steps, which are then distilled into a VLM. Exten-sive experiments show that VPD improves the VLM's ability to count, understand spatial relations, and reason compositionally. Our VPD-trained PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE, and Hateful Memes. An evaluation with human annotators also confirms that VPD improves model response factuality and consistency. Finally, experiments on content moderation demonstrate that VPD is also helpful for adaptation to real-world applications with limited data.},
  keywords={arxiv:2312.03052}
}

@article{buschoff2023visualcognition,
  title={Visual cognition in multimodal large language models},
  author={Luca M. Schulze Buschoff and Elif Akata and Matthias Bethge and Eric Schulz},
  year={2023},
  booktitle={Nature Machine Intelligence},
  doi={10.1038/s42256-024-00963-y},
  url={https://www.semanticscholar.org/paper/302b2594cdffe82eb59f9e7d9e9629e43185c57a},
  abstract={A chief goal of artificial intelligence is to build machines that think like people. Yet it has been argued that deep neural network architectures fail to accomplish this. Researchers have asserted these models’ limitations in the domains of causal reasoning, intuitive physics and intuitive psychology. Yet recent advancements, namely the rise of large language models, particularly those designed for visual processing, have rekindled interest in the potential to emulate human-like cognitive abilities. This paper evaluates the current state of vision-based large language models in the domains of intuitive physics, causal reasoning and intuitive psychology. Through a series of controlled experiments, we investigate the extent to which these modern models grasp complex physical interactions, causal relationships and intuitive understanding of others’ preferences. Our findings reveal that, while some of these models demonstrate a notable proficiency in processing and interpreting visual data, they still fall short of human capabilities in these areas. Our results emphasize the need for integrating more robust mechanisms for understanding causality, physical dynamics and social cognition into modern-day, vision-based language models, and point out the importance of cognitively inspired benchmarks. Modern vision-based language models face challenges with complex physical interactions, causal reasoning and intuitive psychology. Schulze Buschoff and colleagues demonstrate that while some models exhibit proficient visual data processing capabilities, they fall short of human performance in these cognitive domains.},
  keywords={arxiv:2311.16093}
}

@article{guo2023whatlarge,
  title={What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks},
  author={Taicheng Guo and Kehan Guo and B. Nan and Zhengwen Liang and Zhichun Guo and N. Chawla and O. Wiest and Xiangliang Zhang},
  year={2023},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/20d7965c0b282a0cd7f990e435d0f6bc9535bbc6},
  abstract={Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4, GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs' performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.},
  keywords={arxiv:2305.18365}
}

@article{chen2023whenlarge,
  title={When large language models meet personalization: perspectives of challenges and opportunities},
  author={Jin Chen and Zheng Liu and Xu Huang and Chenwang Wu and Qi Liu and Gangwei Jiang and Yuanhao Pu and Yuxuan Lei and Xiaolong Chen and Xingmei Wang and Defu Lian and Enhong Chen},
  year={2023},
  booktitle={World wide web (Bussum)},
  doi={10.1007/s11280-024-01276-1},
  url={https://www.semanticscholar.org/paper/7d46a13a1edd02dd6ae2b9f713e6f91ea001dfb4},
  abstract={The advent of large language models marks a revolutionary breakthrough in artificial intelligence. With the unprecedented scale of training and model parameters, the capability of large language models has been dramatically improved, leading to human-like performances in understanding, language synthesizing, common-sense reasoning, etc. Such a major leap forward in general AI capacity will fundamentally change the pattern of how personalization is conducted. For one thing, it will reform the way of interaction between humans and personalization systems. Instead of being a passive medium of information filtering, like conventional recommender systems and search engines, large language models present the foundation for active user engagement. On top of such a new foundation, users’ requests can be proactively explored, and users’ required information can be delivered in a natural, interactable, and explainable way. For another thing, it will also considerably expand the scope of personalization, making it grow from the sole function of collecting personalized information to the compound function of providing personalized services. By leveraging large language models as a general-purpose interface, the personalization systems may compile user’s requests into plans, calls the functions of external tools (e.g., search engines, calculators, service APIs, etc.) to execute the plans, and integrate the tools’ outputs to complete the end-to-end personalization tasks. Today, large language models are still being rapidly developed, whereas the application in personalization is largely unexplored. Therefore, we consider it to be right the time to review the challenges in personalization and the opportunities to address them with large language models. In particular, we dedicate this perspective paper to the discussion of the following aspects: the development and challenges for the existing personalization system, the newly emerged capabilities of large language models, and the potential ways of making use of large language models for personalization.},
  keywords={arxiv:2307.16376}
}

@article{wang2023wherewould,
  title={Where Would I Go Next? Large Language Models as Human Mobility Predictors},
  author={Xinglei Wang and Meng Fang and Zichao Zeng and Tao Cheng},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.15197},
  url={https://www.semanticscholar.org/paper/f4eea3bda63af47bd6ddc0b5a34ab04c558f0da2},
  abstract={Accurate human mobility prediction underpins many important applications across a variety of domains, including epidemic modelling, transport planning, and emergency responses. Due to the sparsity of mobility data and the stochastic nature of people's daily activities, achieving precise predictions of people's locations remains a challenge. While recently developed large language models (LLMs) have demonstrated superior performance across numerous language-related tasks, their applicability to human mobility studies remains unexplored. Addressing this gap, this article delves into the potential of LLMs for human mobility prediction tasks. We introduce a novel method, LLM-Mob, which leverages the language understanding and reasoning capabilities of LLMs for analysing human mobility data. We present concepts of historical stays and context stays to capture both long-term and short-term dependencies in human movement and enable time-aware prediction by using time information of the prediction target. Additionally, we design context-inclusive prompts that enable LLMs to generate more accurate predictions. Comprehensive evaluations of our method reveal that LLM-Mob excels in providing accurate and interpretable predictions, highlighting the untapped potential of LLMs in advancing human mobility prediction techniques. We posit that our research marks a significant paradigm shift in human mobility modelling, transitioning from building complex domain-specific models to harnessing general-purpose LLMs that yield accurate predictions through language instructions. The code for this work is available at https://github.com/xlwang233/LLM-Mob.},
  keywords={arxiv:2308.15197}
}

@article{tutunov2023largelanguage,
  title={Why Can Large Language Models Generate Correct Chain-of-Thoughts?},
  author={Rasul Tutunov and Antoine Grosnit and Juliusz Ziomek and Jun Wang and Haitham Bou-Ammar},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.13571},
  url={https://www.semanticscholar.org/paper/4ac776e01407c439d4b7d9764de9288193a1748f},
  abstract={This paper delves into the capabilities of large language models (LLMs), specifically focusing on advancing the theoretical comprehension of chain-of-thought prompting. We investigate how LLMs can be effectively induced to generate a coherent chain of thoughts. To achieve this, we introduce a two-level hierarchical graphical model tailored for natural language generation. Within this framework, we establish a compelling geometrical convergence rate that gauges the likelihood of an LLM-generated chain of thoughts compared to those originating from the true language. Our findings provide a theoretical justification for the ability of LLMs to produce the correct sequence of thoughts (potentially) explaining performance gains in tasks demanding reasoning skills.},
  keywords={arxiv:2310.13571}
}

@article{luo2023wizardmathempowering,
  title={WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct},
  author={Haipeng Luo and Qingfeng Sun and Can Xu and Pu Zhao and Jian-Guang Lou and Chongyang Tao and Xiubo Geng and Qingwei Lin and Shifeng Chen and Dongmei Zhang},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2308.09583},
  url={https://www.semanticscholar.org/paper/dd18782960f9ee4c66b79e1518b342ad3f8d19e7},
  abstract={Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM},
  keywords={arxiv:2308.09583}
}

@article{saidi2023mathpvslarge,
  title={math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories},
  author={Hassen Saidi and Susmit Jha and T. Sahai},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.17064},
  url={https://www.semanticscholar.org/paper/4a830a6cba4ec8c87c10348955b6bb633f401c0b},
  abstract={As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few. While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the capabilities of proof assistants, specifically PVS, with LLMs, enabling a bridge between textual descriptions in academic papers and formal specifications in PVS. By harnessing the PVS environment, coupled with data ingestion and conversion mechanisms, we envision an automated process, called \textbackslash\{\}emph\{math-PVS\}, to extract and formalize mathematical theorems from research papers, offering an innovative tool for academic review and discovery.},
  keywords={arxiv:2310.17064}
}

@article{cohen2022thisunicorn,
  title={"This is my unicorn, Fluffy": Personalizing frozen vision-language representations},
  author={Niv Cohen and Rinon Gal and E. Meirom and Gal Chechik and Y. Atzmon},
  year={2022},
  booktitle={European Conference on Computer Vision},
  doi={10.48550/arXiv.2204.01694},
  url={https://www.semanticscholar.org/paper/0791a0441e1f672c43aecb2d6708fbc8725c8cad},
  abstract={Large Vision\&Language models pretrained on web-scale data provide representations that are invaluable for numerous V\&L problems. However, it is unclear how they can be used for reasoning about user-specific visual concepts in unstructured language. This problem arises in multiple domains, from personalized image retrieval to personalized interaction with smart devices. We introduce a new learning setup called Personalized Vision\&Language (PerVL) with two new benchmark datasets for retrieving and segmenting user-specific"personalized"concepts"in the wild". In PerVL, one should learn personalized concepts (1) independently of the downstream task (2) allowing a pretrained model to reason about them with free language, and (3) does not require personalized negative examples. We propose an architecture for solving PerVL that operates by extending the input vocabulary of a pretrained model with new word embeddings for the new personalized concepts. The model can then reason about them by simply using them in a sentence. We demonstrate that our approach learns personalized visual concepts from a few examples and can effectively apply them in image retrieval and semantic segmentation using rich textual queries.},
  keywords={arxiv:2204.01694}
}

@article{stolfo2022causalframework,
  title={A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models},
  author={Alessandro Stolfo and Zhijing Jin and Kumar Shridhar and B. Scholkopf and Mrinmaya Sachan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.12023},
  url={https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe},
  abstract={We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution.By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems.Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.},
  keywords={arxiv:2210.12023}
}

@article{snchez2022clusteringapproach,
  title={A Clustering Approach for the Optimal Siting of Recharging Stations in the Electric Vehicle Routing Problem with Time Windows},
  author={Danny García Sánchez and Alejandra Tabares and L. Faria and Juan Carlos Rivera and J. Franco},
  year={2022},
  booktitle={Energies},
  doi={10.3390/en15072372},
  url={https://www.semanticscholar.org/paper/f1164514c7180331c3b059c19eab5169c9c921a7},
  abstract={Transportation has been incorporating electric vehicles (EVs) progressively. EVs do not produce air or noise pollution, and they have high energy efficiency and low maintenance costs. In this context, the development of efficient techniques to overcome the vehicle routing problem becomes crucial with the proliferation of EVs. The vehicle routing problem concerns the freight capacity and battery autonomy limitations in different delivery-service scenarios, and the challenge of best locating recharging stations. This work proposes a mixed-integer linear programming model to solve the electric location routing problem with time windows (E-LRPTW) considering the state of charge, freight and battery capacities, and customer time windows in the decision model. A clustering strategy based on the k-means algorithm is proposed to divide the set of vertices (EVs) into small areas and define potential sites for recharging stations, while reducing the number of binary variables. The proposed model for E-LRPTW was implemented in Python and solved using mathematical modeling language AMPL together with CPLEX. Performed tests on instances with 5 and 10 clients showed a large reduction in the time required to find the solution (by about 60 times in one instance). It is concluded that the strategy of dividing customers by sectors has the potential to be applied and generate solutions for larger geographical areas and numbers of recharging stations, and determine recharging station locations as part of planning decisions in more realistic scenarios.}
}

@misc{desogus2022contributionrelationship,
  title={A Contribution on Relationship Banking. Economic, Anthropological and Mathematical Reasoning, Empirical Evidence from Italy},
  author={Marco Desogus and Elisa Casu},
  year={2022},
  url={https://www.semanticscholar.org/paper/dd88cc9b1f6d71ef82631b4e1c98c077ccdf291a}
}

@article{wang2022hybridgenetic,
  title={A Hybrid Genetic Algorithm for Flexible Job Shop Scheduling Problem},
  author={Xianglong Wang and Changyi Liu},
  year={2022},
  booktitle={2022 5th World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM)},
  doi={10.1109/WCMEIM56910.2022.10021523},
  url={https://www.semanticscholar.org/paper/3cff420b5a41a06291b68f5b6600935c090f8ad8},
  abstract={Partially flexible job shop scheduling problem (P-FJSP) is a NP Hard problem more complex than fully flexi-ble job shop scheduling problem (T -FJSP). In this paper, the mathematical model of flexible job shop scheduling is established with the goal of minimizing the maximum completion time (makespan). It combines the local search ability of simu-lated annealing algorithm and the global search ability of ge-netic algorithm. In the process of chromosome decoding, greedy decoding method is used to get a better scheduling solution as far as possible. The hybrid scheduling algorithm is implemented based on Visual Studio and C \# language. Finally, 8×8 classic scheduling instance are used for simulation scheduling experiments to verify that the hybrid genetic algorithm proposed in this paper is effective in solving large-scale FJSP.}
}

@article{zhang2022multilayerattention,
  title={A Multi-Layer Attention Network for Visual Commonsense Reasoning},
  author={Wenqi Zhang and Yongchao Gao and Heng Qian and Hongli Lyu},
  year={2022},
  booktitle={International Conference on Data Science and Information Technology},
  doi={10.1109/DSIT55514.2022.9943834},
  url={https://www.semanticscholar.org/paper/0e0f20f3af3650b5a97b0ec3f046ba8160b45279},
  abstract={Visual Commonsense Reasoning (VCR) is a challenging multimodal task involving several research fields such as vision, cognition, and reasoning, which combines images and natural language for reasoning. Existing VCR methods focus on global attention or use pre-training models, but these methods lack attention to local features of visual and language. In this paper, a multi-layer attention network is proposed for the VCR task, including an intra-modal attention module and an inter-modal attention module. The intra-modal attention module complements important features of visual and language modalities with fine-grained visual attention to improve the relevance of visual and language. The inter-modal attention module captures the internal dependencies between visual and language. Finally, the two modules are integrated into an end-to-end reasoning framework. Experiments on the VCR large-scale dataset show that the proposed method exhibits a decent improvement in the VCR task and illustrates the effectiveness of the method on three subtasks.}
}

@article{ricci2022petrinetbasedapproach,
  title={A Petri-Net-Based Approach for Enhancing Clinical Reasoning in Medical Education},
  author={F. Ricci and F. Consorti and F. Pecoraro and D. Luzi and Oscar Tamburis},
  year={2022},
  journal={IEEE Transactions on Learning Technologies},
  doi={10.1109/tlt.2022.3157391},
  url={https://www.semanticscholar.org/paper/98b0fb67a6fb222998e3449621f3f5eecaed758e},
  abstract={Medical students are called to acquire competence to manage disease in its dynamic evolution over time, learning to analyze how clinical conditions evolve in a patient's history and how each condition interferes with the evolution of the other coexisting conditions. In this article, the health issue network (HIN) approach is introduced as a formal language based on Petri nets (PNs) to model properties that are particularly apposite for the graphical representation of HIN evolutionary paths. Moreover, the PNs’ underlying mathematical model allows users to draw coherent and well-formed graphs representing rather complex clinical cases. Finally, HIN can be easily integrated into a simulation environment to support case-based learning activities and assessment. The examples of the exercises provided in this article show, on the one hand, the ways the introduced methodology is figured out and implemented; on the other hand, they outline the variety of learning questions that users may deal with when deploying the HIN approach.}
}

@article{ekong2022ratiocinativestudy,
  title={A Ratiocinative Study and Assessment of W. V. O. Quine’s “Criterion of Ontological Commitment”},
  author={Joseph T. Ekong},
  year={2022},
  journal={International Journal of Philosophy},
  doi={10.47941/ijp.1052},
  url={https://www.semanticscholar.org/paper/7b6955111d3bd91b13e7a9c7fbdfd75d43825c36},
  abstract={Purpose: This work has three main objectives: Firstly, it offers an elucidation of the notion of ontological commitment. Secondly, it assesses the adequacy of the criterion of ontological commitment for different languages. Thirdly, it offers some speculative and evaluative remarks regarding the significance of Quine’s criterion of ontological commitment. Many ontologists, within the analytic tradition, often appeal to Quine's criterion of ontological commitment, when debating whether an assertion or theory implies the existence of a certain entity. Regarding his goal in formulating this criterion, he says that the criterion does not aim to help us discover what it is that there is, but only what a theory says there is: “I look to variables and quantification for evidence as to what a theory says that there is, not for evidence as to what there is” (Quine, 1960: 225). Its most popular formulation, using textual evidence from Quine's oeuvre, is: “To be is to be the value of a bound variable,” (Quine, 1961: 15). However, this formulation is susceptible to gross misunderstanding, especially if one is influenced by the formalities and technical maneuvers of model theory. In mathematical logic, model theory is the study of the relationship between formal theories (a collection of sentences in a formal language expressing statements about a mathematical structure), and their models (those structures in which the statements of the theory hold). Model theory is a branch of mathematical logic where we study mathematical structures by considering the first-order sentences true in those structures and the sets definable by first-order formulas. Model theory studies the relations between sentences of a formal language and the interpretations (or ‘structures’) which make these sentences true or false. It offers precise definitions of truth, logical truth and consequence, meanings and modalities. 
Methodology: This work is expository, analytic, critical and evaluative in its methodology. Of course, there are familiar philosophical problems which are within the discursive framework of ‘ontology,’ often phrased by asking if something or some category of things are “real,” or whether “they exist,” concretely. An outstanding example is provided by the traditional problem of universals, which issues in the nominalist-realist controversy, as to the real existence of universals, or of abstract entities such as classes (in the mathematical sense) or propositions (in the abstract sense, referring to the content of an assertion in abstraction from the particular words used to convey it). 
Results: In as much as one might agree with Quine’s Criterion of Ontological Commitment, one might also opine that it is nonetheless a feature of first-order language (i.e. the language embodied in first-order logic; a symbolized reasoning process comprising relations, functions and constants, in which each sentence or statement is broken down into a subject and a predicate. In this regard, the predicate modifies or defines the properties of the subject) that there should be an exact correspondence between the ontological commitments carried by a sentence and the objects that must be counted among the values of the variables in order for the sentence to be true. However, this in itself is not a reason for thinking that such a feature will generalize beyond first-order languages. It is possible for Quine’s Criterion to degenerate, when the language contains atomic predicates expressing extrinsic properties. 
Unique Contribution to theory, practice and policy: Based on Quine’s analysis, a theory is committed to those and only those entities that in the last analysis serve as the values of its bound variables. Thus, ordinary first-order theory commits one to an ontology only of individuals (particulars), whereas higher order logic commits one to the existence of sets, i.e. of collections of definite and distinct entities (or, alternatively, of properties and relations). Likewise, if bound first-order variables are assumed to range over sets (as they do in set theory), a commitment to the existence of these sets is incurred. Admittedly, the precise import of Quine’s criterion of ontological commitment, however, is not completely clear, nor is it clear in what other sense one is perhaps committed by a theory to those entities that are named or otherwise referred to in it, but not quantified over in it. However, it despite its limitations, it has made is possible for one to measure the ontological cost of theories, an important component in deciding which theories to accept, thus offering a partial foundation for theory choice.}
}

@article{li2022scenariobasedexploration,
  title={A Scenario-based Exploration of Expected Usefulness, Privacy Concerns, and Adoption Likelihood of Learning Analytics},
  author={X. Li and M. Rosson and Jenay Robert},
  year={2022},
  booktitle={ACM Conference on Learning @ Scale},
  doi={10.1145/3491140.3528271},
  url={https://www.semanticscholar.org/paper/067b8489b028d931b751cb9413225b761e51dcf3},
  abstract={Learning analytics has become a robust research area in the last decade, as innovative analytic models of learning data have been created with the goal of enhancing teaching and learning. However, barriers to large scale adoption of such technologies in higher education still exist. In recent years, a strand of research has begun to investigate stakeholders' expectations of learning analytics, hoping to find ways to integrate the innovations into everyday teaching practices. For instance, studies have investigated instructors' ideas about how learning analytics might be helpful, as well as concerns about student data privacy. However, most studies have taken a general approach rather than considering instructors' day-to-day experiences. Using survey methods, we presented instructors with hypothetical scenarios of learning analytics in use across disciplines, class sizes, teaching activities, and types of student data. We asked for ratings of both usefulness and privacy concerns for each proposed teaching situation. Our respondents considered scenarios involving learning outcomes-related data (e.g. grades) to be more useful than those that involve student interactions (e.g. language, social activity). In contrast, privacy concerns were lower for outcomes-oriented scenarios than interactions-focused scenarios. An interesting new finding was a negative correlation of usefulness and privacy; we discuss this in the context of instructors' possible cost-benefit reasoning. We reflect on our findings with respect to future efforts in developing and fielding learning analytics tools.}
}

@article{lu2022surveydeep,
  title={A Survey of Deep Learning for Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Wenhao Yu and S. Welleck and Kai-Wei Chang},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10535},
  url={https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d},
  abstract={Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.},
  keywords={arxiv:2212.10535}
}

@article{hu2022surveyknowledge,
  title={A Survey of Knowledge Enhanced Pre-Trained Language Models},
  author={Linmei Hu and Zeyi Liu and Ziwang Zhao and Lei Hou and Liqiang Nie and Juanzi Li},
  year={2022},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  doi={10.1109/TKDE.2023.3310002},
  url={https://www.semanticscholar.org/paper/a26623d52d24e03044a158cddad931ec5ab7304c},
  abstract={Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are categorized into KG-based and retrieval-based methods. Finally, we point out some promising future directions of KE-PLMs.},
  keywords={arxiv:2211.05994}
}

@article{zhou2022surveyneural,
  title={A Survey on Neural Open Information Extraction: Current Status and Future Directions},
  author={Shaowen Zhou and Yu Bowen and Aixin Sun and Cheng Long and Jingyang Li and Haiyang Yu and Jianguo Sun},
  year={2022},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2205.11725},
  url={https://www.semanticscholar.org/paper/5de6ecf62f14c9263882f9f30d6448df9efd34e0},
  abstract={Open Information Extraction (OpenIE) facilitates domain-independent discovery of relational facts from large corpora. The technique well suits many open-world natural language understanding scenarios, such as automatic knowledge base construction, open-domain question answering, and explicit reasoning. Thanks to the rapid development in deep learning technologies, numerous neural OpenIE architectures have been proposed and achieve considerable performance improvement. In this survey, we provide an extensive overview of the state-of-the-art neural OpenIE models, their key design decisions, strengths and weakness. Then, we discuss limitations of current solutions and the open issues in OpenIE problem itself. Finally we list recent trends that could help expand its scope and applicability, setting up promising directions for future research in OpenIE. To our best knowledge, this paper is the first review on neural OpenIE.},
  keywords={arxiv:2205.11725}
}

@article{wankmller2022comparisonapproaches,
  title={A comparison of approaches for imbalanced classification problems in the context of retrieving relevant documents for an analysis},
  author={Sandra Wankmüller},
  year={2022},
  journal={Journal of Computational Social Science},
  doi={10.1007/s42001-022-00191-7},
  url={https://www.semanticscholar.org/paper/a12e9a6863c8453787575172599389d2ddcd9f62},
  abstract={One of the first steps in many text-based social science studies is to retrieve documents that are relevant for an analysis from large corpora of otherwise irrelevant documents. The conventional approach in social science to address this retrieval task is to apply a set of keywords and to consider those documents to be relevant that contain at least one of the keywords. But the application of incomplete keyword lists has a high risk of drawing biased inferences. More complex and costly methods such as query expansion techniques, topic model-based classification rules, and active as well as passive supervised learning could have the potential to more accurately separate relevant from irrelevant documents and thereby reduce the potential size of bias. Yet, whether applying these more expensive approaches increases retrieval performance compared to keyword lists at all, and if so, by how much, is unclear as a comparison of these approaches is lacking. This study closes this gap by comparing these methods across three retrieval tasks associated with a data set of German tweets (Linder in SSRN, 2017. https://doi.org/10.2139/ssrn.3026393 ), the Social Bias Inference Corpus (SBIC) (Sap et al. in Social bias frames: reasoning about social and power implications of language. In: Jurafsky et al. (eds) Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, p 5477–5490, 2020. https://doi.org/10.18653/v1/2020.aclmain.486 ), and the Reuters-21578 corpus (Lewis in Reuters-21578 (Distribution 1.0). [Data set], 1997. http://www.daviddlewis.com/resources/testcollections/reuters21578/ ). Results show that query expansion techniques and topic model-based classification rules in most studied settings tend to decrease rather than increase retrieval performance. Active supervised learning, however, if applied on a not too small set of labeled training instances (e.g. 1000 documents), reaches a substantially higher retrieval performance than keyword lists.},
  keywords={arxiv:2205.01600}
}

@article{wang2022comparisonthree,
  title={A comparison of three approaches to covariate effects on latent factors},
  author={Ze Wang},
  year={2022},
  booktitle={Large-scale Assessments in Education},
  doi={10.1186/s40536-022-00148-2},
  url={https://www.semanticscholar.org/paper/c40412109167ae57baab6505edf9b628efca6d3a},
  abstract={In educational and psychological research, it is common to use latent factors to represent constructs and then to examine covariate effects on these latent factors. Using empirical data, this study applied three approaches to covariate effects on latent factors: the multiple-indicator multiple-cause (MIMIC) approach, multiple group confirmatory factor analysis (MG-CFA) approach, and the structural equation model trees (SEM Trees) approach. The MIMIC approach directly models covariate effects on latent factors. The MG-CFA approach allows testing of measurement invariance before latent factor means could be compared. The more recently developed SEM Trees approach partitions the sample into homogenous subsets based on the covariate space; model parameters are estimated separately for each subgroup. We applied the three approaches using an empirical dataset extracted from the eighth-grade U.S. data from the Trends in International Mathematics and Science Study 2019 database. All approaches suggested differences among mathematics achievement categories for the latent factor of mathematics self-concept. In addition, language spoken at home did not seem to affect students’ mathematics self-concept. Despite these general findings, the three approaches provided different pieces of information regarding covariate effects. For all models, we appropriately considered the complex data structure and sampling weights following recent recommendations for analyzing large-scale assessment data.}
}

@misc{alemany2022methodologycharacterize,
  title={A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America},
  author={L. A. Alemany and Luciana Benotti and Hernán Maina and Luc'ia M. Gonz'alez and Mariela Rajngewerc and Lautaro Mart'inez and Jos'e L. S'anchez and M. Schilman and Guido Ivetta and Alexia Halvorsen and Amanda Rojo and M. Bordone and Beatriz Busaniche},
  year={2022},
  url={https://www.semanticscholar.org/paper/a82a08b5e6a11f4d6fdff95dd30177957ed7855e},
  abstract={Automated decision-making systems, especially those based on natural language processing, are pervasive in our lives. They are not only behind the internet search engines we use daily, but also take more critical roles: selecting candidates for a job, determining suspects of a crime, diagnosing autism and more. Such automated systems make errors, which may be harmful in many ways, be it because of the severity of the consequences (as in health issues) or because of the sheer number of people they affect. When errors made by an automated system affect a population more than others, we call the system \textbackslash\{\}textit\{biased\}. Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to audit them. In this paper, we present a methodology that spells out how social scientists, domain experts, and machine learning experts can collaboratively explore biases and harmful stereotypes in word embeddings and large language models. Our methodology is based on the following principles: * focus on the linguistic manifestations of discrimination on word embeddings and language models, not on the mathematical properties of the models * reduce the technical barrier for discrimination experts\%, be it social scientists, domain experts or other * characterize through a qualitative exploratory process in addition to a metric-based approach * address mitigation as part of the training process, not as an afterthought},
  keywords={arxiv:2207.06591}
}

@article{kim2022novelmodular,
  title={A novel modular modeling approach for understanding different electromechanics between left and right heart in rat},
  author={Nari Kim and Julius D. Pronto and D. Nickerson and A. Taberner and Peter J. Hunter},
  year={2022},
  booktitle={Frontiers in Physiology},
  doi={10.3389/fphys.2022.965054},
  url={https://www.semanticscholar.org/paper/2fba0d7b1293e4b13180fb3bccf86ed52ddcaf70},
  abstract={While ion channels and transporters involved in excitation-contraction coupling have been linked and constructed as comprehensive computational models, validation of whether each individual component of a model can be reused has not been previously attempted. Here we address this issue while using a novel modular modeling approach to investigate the underlying mechanism for the differences between left ventricle (LV) and right ventricle (RV). Our model was developed from modules constructed using the module assembly principles of the CellML model markup language. The components of three existing separate models of cardiac function were disassembled as to create smaller modules, validated individually, and then the component parts were combined into a new integrative model of a rat ventricular myocyte. The model was implemented in OpenCOR using the CellML standard in order to ensure reproducibility. Simulated action potential (AP), Ca2+ transient, and tension were in close agreement with our experimental measurements: LV AP showed a prolonged duration and a more prominent plateau compared with RV AP; Ca2+ transient showed prolonged duration and slow decay in LV compared to RV; the peak value and relaxation of tension were larger and slower, respectively, in LV compared to RV. Our novel approach of module-based mathematical modeling has established that the ionic mechanisms underlying the APs and Ca2+ handling play a role in the variation in force production between ventricles. This simulation process also provides a useful way to reuse and elaborate upon existing models in order to develop a new model.}
}

@article{mi2022reviewdevelopment,
  title={A review: development of named entity recognition (NER) technology for aeronautical information intelligence},
  author={Baigang Mi and Fan Yi},
  year={2022},
  booktitle={Artificial Intelligence Review},
  doi={10.1007/s10462-022-10197-2},
  url={https://www.semanticscholar.org/paper/ca2da2420fd25c8633641542730d3f0867c50f60}
}

@article{poythress2022semioticanalysis,
  title={A semiotic analysis of multiple systems of logic: using tagmemic theory to assess the usefulness and limitations of formal logics, and to produce a mathematical lattice model including multiple systems of logic},
  author={V. Poythress},
  year={2022},
  journal={Semiotica: Journal of the International Association for Semiotic Studies},
  doi={10.1515/sem-2020-0051},
  url={https://www.semanticscholar.org/paper/606db29a9d5cad5cd06b8eeb1f8beee390c87ca4},
  abstract={Abstract Tagmemic theory as a semiotic theory can be used to analyze multiple systems of logic and to assess their strengths and weaknesses. This analysis constitutes an application of semiotics and also a contribution to understanding of the nature of logic within the context of human meaning. Each system of logic is best adapted to represent one portion of human rationality. Acknowledging this correlation between systems and their targets helps explain the usefulness of more than one system. Among these systems, the two-valued system of classical logic takes its place. All the systems of logic can be incorporated into a complex mathematical model that has a place for each system and that represents a larger whole in human reasoning. The model can represent why tight formal systems of logic can be applied in some contexts with great success, but in other contexts are not directly applicable. The result suggests that human reasoning is innately richer than any one formal system of logic.}
}

@misc{song2022thesissubmitted,
  title={A thesis submitted to the Faculty of Graduate and Postdoctoral Affairs in partial fulfillment of the requirements for the degree of Master of Arts},
  author={Charlene Song},
  year={2022},
  url={https://www.semanticscholar.org/paper/25be22274b72f1337e977d94d0c94026d13a67d0}
}

@article{markta2022accuracypupils,
  title={ACCURACY OF PUPILS´ SELF-ASSESSMENT},
  author={Švamberk Šauerová Markéta and Smetáčková Irena},
  year={2022},
  booktitle={EduPort},
  doi={10.21062/edp.2022.009},
  url={https://www.semanticscholar.org/paper/fcfabc1d551304cde28a4f0658ed20e36559e05f},
  abstract={In this study, we investigated the accuracy of pupils´ self-assessment in two main school domains – mathematics and Czech language. The analysis explores whether pupils are able to evaluate adequately their own results in the didactic tests and then use some individual parameters to explain the level of self-assessment. The aim of the study was to analyze whether groups of pupils with different self-assessments of school tasks in the Czech language and mathematics (significant underestimation, adequate self-assessment, significant overestimation) differ in some of the cognitive skills studied. Our study questions were as follows: (1) Do pupils assess their achievements in particular school tasks accurately, or inaccurately? (2) Do pupils´ self-assessments differ in mathematics and language? (3) Do the pupil´s self-assessment correlate with individual parameters? The main tool used in the study was a didactic test on mathematics and a didactic test on the Czech language based on the Czech National Curricula Document and created by an expert team. In addition, Raven's Color Progressive Matrices (CPM), Similarities from the Wechsler Intelligence (WISC-SIM), and the Rey-Osterrieth Complex Figure (ROCF) were used. Considering the nature of the data, the non-parametric Kruskal-Wallis ANOVA was used. The present study is a part of the larger research project, involving 29 primary school classes, 657 pupils in total. Based on the data obtained, it can be concluded that the accuracy of pupils' self-assessments is low, while the accuracy of pupils' self-assessments in mathematics and Czech language differs (in mathematics there are more children with more accurate estimates and more pupils who underestimate themselves, in Czech language there are more pupils who overestimate their performance. Statistically significant differences were observed in the domains of Raven's Color Progressive Matrices and Rey-Osterrieth Figure, and in terms of the focus of each test, it could be concluded that there are significant differences between the groups in the domain of non-verbal reasoning skills and in the domain of analytical and organizational perceptual activity and memory. In the area of verbal intellectual abilities, there were no significant differences between the groups.}
}

@article{ji2022afrbertattentionbased,
  title={AFR-BERT: Attention-based mechanism feature relevance fusion multimodal sentiment analysis model},
  author={Mingyu Ji and Jiawei Zhou and Wei Ning},
  year={2022},
  booktitle={PLoS ONE},
  doi={10.1371/journal.pone.0273936},
  url={https://www.semanticscholar.org/paper/918f34bd4274316d684dd6c267b13fe010a74a6e},
  abstract={Multimodal sentiment analysis is an essential task in natural language processing which refers to the fact that machines can analyze and recognize emotions through logical reasoning and mathematical operations after learning multimodal emotional features. For the problem of how to consider the effective fusion of multimodal data and the relevance of multimodal data in multimodal sentiment analysis, we propose an attention-based mechanism feature relevance fusion multimodal sentiment analysis model (AFR-BERT). In the data pre-processing stage, text features are extracted using the pre-trained language model BERT (Bi-directional Encoder Representation from Transformers), and the BiLSTM (Bi-directional Long Short-Term Memory) is used to obtain the internal information of the audio. In the data fusion phase, the multimodal data fusion network effectively fuses multimodal features through the interaction of text and audio information. During the data analysis phase, the multimodal data association network analyzes the data by exploring the correlation of fused information between text and audio. In the data output phase, the model outputs the results of multimodal sentiment analysis. We conducted extensive comparative experiments on the publicly available sentiment analysis datasets CMU-MOSI and CMU-MOSEI. The experimental results show that AFR-BERT improves on the classical multimodal sentiment analysis model in terms of relevant performance metrics. In addition, ablation experiments and example analysis show that the multimodal data analysis network in AFR-BERT can effectively capture and analyze the sentiment features in text and audio.}
}

@article{gulwani2022aiassistedprogramming,
  title={AI-assisted programming: applications, user experiences, and neuro-symbolic techniques (keynote)},
  author={Sumit Gulwani},
  year={2022},
  booktitle={ESEC/SIGSOFT FSE},
  doi={10.1145/3540250.3569444},
  url={https://www.semanticscholar.org/paper/11230f03465d8ab073815397717d8afa3f3dae1c}
}

@article{yu2022alertadapt,
  title={ALERT: Adapt Language Models to Reasoning Tasks},
  author={Ping Yu and Tianlu Wang and O. Yu. Golovneva and Badr AlKhamissi and Gargi Ghosh and Mona T. Diab and Asli Celikyilmaz},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.08286},
  url={https://www.semanticscholar.org/paper/95c11cc5820ba32c60d5f2671f6567b9914a4978},
  abstract={Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.To address this question, we introduce \{pasted macro ‘OUR’\}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. \{pasted macro ‘OUR’\}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using \{pasted macro ‘OUR’\}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.},
  keywords={arxiv:2212.08286}
}

@article{2022algorithmmethod,
  title={ALGORITHM METHOD IN TEACHING RUSSIAN AT SECONDARY SCHOOL},
  author={Юлия Владимировна Подкина},
  year={2022},
  booktitle={Tomsk state pedagogical university bulletin},
  doi={10.23951/1609-624x-2022-6-80-87},
  url={https://www.semanticscholar.org/paper/ded393f5b3432f3d0b9258fff2b9db33b204bf84},
  abstract={Введение. Обучение русскому языку в средней школе, развитие речи и формирование орфографических и пунктуационных навыков – важная задача, которая сопряжена с рядом трудностей. Эффективному изучению русского языка в общеобразовательной школе зачастую препятствуют такие факторы, как плохая усидчивость, отсутствие интереса к предмету, билингвизм и другое. Метод алгоритмизированного представления правил русской орфографии и пунктуации способствует наилучшему усвоению учебного материала и позволяет повысить качество обучения русскому языку школьников среднего и старшего звена. Цель − обоснование эффективности метода алгоритма в обучении русскому языку детей общеобразовательных средних школ, рассмотрение примерных моделей обучающих алгоритмов. Материал и методы. В работе применялись теоретические методы (моделирование, анализ, синтез); эмпирические методы (наблюдение, сравнение, эксперимент). Результаты и обсуждение. Простое заучивание правил не всегда приводит к повышению грамотности учащихся. Метод алгоритма предусматривает совместное с учениками составление алгоритмизированных схем различных видов, которые иллюстрируют изучаемое правило, позволяют пошагово отработать механизм рассуждения при выполнении орфографических и пунктуационных заданий. Такой подход способствует достижению высокого качества знаний путем систематической отработки практических навыков с помощью схем, адаптируемых под потребности каждого ребенка. Обучающий алгоритм может иметь разные виды: от четко сформулированной схемы (похожей на математический пример) до красочной иллюстрации, которая будет понятна детям с творческими способностями. Заключение. Метод алгоритма применяют для изучения практически любого правила русской орфографии и пунктуации. В созданной совместно с учащимися схеме должно быть отведено место для исключений и для примеров, которые ребенок впишет самостоятельно. При создании обучающей схемы школьник является активным соавтором. Схема никогда не является замкнутой системой. Она дорабатывается и совершенствуется в процессе практической деятельности учащихся. У детей из одного класса схемы могут быть совершенно различны, так как усовершенствованы и доработаны самостоятельно под руководством учителя.
 Introduction. Teaching Russian in secondary school, speech development and the formation of spelling and punctuation skills is an important task that involves a number of difficulties. Effective study of the Russian language in a secondary school is often hindered by factors such as poor perseverance, lack of interest in the subject, bilingualism, and more. Russian Russian spelling rules algorithmized representation method is considered in this paper, which allows to improve the quality of teaching Russian to middle and senior school students. The purpose is to substantiate the effectiveness of the algorithm method in teaching the Russian language to children of secondary schools, to consider approximate models of training algorithms. Material and methods. Theoretical methods (modeling, analysis, synthesis) were used in the work; empirical methods (observation, comparison, experiment). Results and discussion. Simple memorizing of the rules does not always lead to increased literacy of students. The algorithm method provides for the joint compilation of algorithmic schemes of various types with students, which illustrate the rule being studied, allow you to work out the mechanism of reasoning step by step when performing spelling and punctuation tasks. This approach contributes to the achievement of a high quality of knowledge through the systematic development of practical skills with the help of schemes adapted to the needs of each child. The training algorithm can have different types: from a clearly formulated scheme (similar to a mathematical example) up to a colorful illustration that will be understandable to children with creative abilities. Conclusion. The algorithm method can be applied to study almost any rule of Russian spelling and punctuation. In the scheme created jointly with the students, there should be a place for exceptions and for examples that the child will enter independently. When creating a training scheme, the student is an active co-author. A circuit is never a closed system. It is being refined and improved in the process of practical activity of students. For children from the same class, the schemes can be completely different, as they have been improved and finalized independently.}
}

@article{mare2022updatethermal,
  title={AN UPDATE OF THERMAL ERROR COMPENSATION MODEL VIA ON-MACHINE MEASUREMENT},
  author={M. Mareš and O. Horejš and Michal Straka and J. Švéda and Tomáš Kozlok},
  year={2022},
  journal={MM Science Journal},
  doi={10.17973/mmsj.2022_12_2022150},
  url={https://www.semanticscholar.org/paper/796f47a4059604f27ad57c3760cc7ebea9f6a020},
  abstract={Software compensation is state-of-the-art technology used to reduce CNC machine tool thermal errors, and it belongs to a key intelligent functions of modern machine tools. However, a pretrained and nonadaptive model may not be accurate and robust enough for long-term application. This research presents a transfer function based thermal error compensation model updated via on-machine measurement. A mathematical model is implemented into the machine management software of a large horizontal machining centre to compensate for thermal errors in real time using C\#/C++ programming language. The results show that after the thermal error compensation model is updated via on-machine measurement, the prediction accuracy, measured as peak-to-peak values, and the normalized root mean squared error are significantly improved. The prediction accuracy of the compensation model updated via on-machine measurement strongly depends on the sampling interval of the on machine measurements.}
}

@misc{nam2022achievingunderstanding,
  title={Achieving and Understanding Out-of-Distribution Generalization in Systematic Reasoning in Small-Scale Transformers},
  author={A. Nam and Mustafa Abdool and Trevor Maxfield and James L. McClelland},
  year={2022},
  url={https://www.semanticscholar.org/paper/8283064365ae7594d891e8b7daf36fd37ca809b0},
  abstract={Out-of-distribution generalization (OODG) is a longstanding challenge for neural networks. This challenge is quite apparent in tasks with well-defined variables and rules, where explicit use of the rules could solve problems independently of the particular values of the variables, but networks tend to be tied to the range of values sampled in their training data. Large transformer-based language models have pushed the boundaries on how well neural networks can solve previously unseen problems, but their complexity and lack of clarity about the relevant content in their training data obfuscates how they achieve such robustness. As a step toward understanding how transformer-based systems generalize, we explore the question of OODG in small scale transformers trained with examples from a known distribution. Using a reasoning task based on the puzzle Sudoku, we show that OODG can occur on a complex problem if the training set includes examples sampled from the whole distribution of simpler component tasks. Successful generalization depends on carefully managing positional alignment when absolute position encoding is used, but we find that suppressing sensitivity to absolute positions overcomes this limitation. Taken together our results represent a small step toward understanding and promoting systematic generalization in transformers.},
  keywords={arxiv:2210.03275}
}

@article{hppner2022advantagesdisadvantages,
  title={Advantages and disadvantages of (dedicated) model transformation languages},
  author={S. Höppner and Yves Haas and Matthias Tichy and Katharina Juhnke},
  year={2022},
  booktitle={Empirical Software Engineering},
  doi={10.1007/s10664-022-10194-7},
  url={https://www.semanticscholar.org/paper/d96fa397010fa107aadcedbff577feead334e3be},
  abstract={Model driven development envisages the use of model transformations to evolve models. Model transformation languages, developed for this task, are touted with many benefits over general purpose programming languages. However, a large number of these claims have not yet been substantiated. They are also made without the context necessary to be able to critically assess their merit or built meaningful empirical studies around them. The objective of our work is to elicit the reasoning, influences and background knowledge that lead people to assume benefits or drawbacks of model transformation languages. We conducted a large-scale interview study involving 56 participants from research and industry. Interviewees were presented with claims about model transformation languages and were asked to provide reasons for their assessment thereof. We qualitatively analysed the responses to find factors that influence the properties of model transformation languages as well as explanations as to how exactly they do so. Our interviews show, that general purpose expressiveness of GPLs, domain specific capabilities of MTLs as well as tooling all have strong influences on how people view properties of model transformation languages. Moreover, the Choice of MTL, the Use Case for which a transformation should be developed as well as the Skill s of involved stakeholders have a moderating effect on the influences, by changing the context to consider. There is a broad body of experience, that suggests positive and negative influences for properties of MTLs. Our data suggests, that much needs to be done in order to convey the viability of model transformation languages. Efforts to provide more empirical substance need to be undergone and lacklustre language capabilities and tooling need to be improved upon. We suggest several approaches for this that can be based on the results of the presented study.},
  keywords={arxiv:2201.13348}
}

@article{abramson2022applicationpseudologlikelihoods,
  title={An Application of Pseudo-Log-Likelihoods to Natural Language Scoring},
  author={Darren Abramson and Ali Emami},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/16bf88a6d172699cb9a26a6936efb4941e3f3c13},
  abstract={Language models built using semi-supervised machine learning on large corpora of natural language have very quickly enveloped the fields of natural language generation and understanding. In this paper we apply a zero-shot approach independently developed by a number of researchers now gaining recognition as a significant alternative to fine-tuning for evaluation on common sense tasks. A language model with relatively few parameters and training steps compared to a more recent language model (T5) can outperform it on a recent large data set (TimeDial), while displaying robustness in its performance across a similar class of language tasks. Surprisingly, this result is achieved by using a hyperparameter-free zero-shot method with the smaller model, compared to fine-tuning to the larger model. We argue that robustness of the smaller model ought to be understood in terms of compositionality, in a sense that we draw from recent literature on a class of similar models. We identify a practical cost for our method and model: high GPU-time for natural language evaluation. The zero-shot measurement technique that produces remarkable stability, both for ALBERT and other BERT variants, is an application of pseudo-log-likelihoods to masked language models for the relative measurement of probability for substitution alternatives in forced choice language tasks such as the Winograd Schema Challenge, Winogrande, and others. One contribution of this paper is to bring together a number of similar, but independent strands of research. We produce some absolute state-of-the-art results for common sense reasoning in binary choice tasks, performing better than any published result in the literature, including fine-tuned efforts. We show a remarkable consistency of the model's performance under adversarial settings, which we argue is best explained by the model's compositionality of representations.},
  keywords={arxiv:2201.09377}
}

@article{zhang2022empiricalinvestigation,
  title={An Empirical Investigation of Commonsense Self-Supervision with Knowledge Graphs},
  author={Jiarui Zhang and Filip Ilievski and Kaixin Ma and Jonathan M Francis and A. Oltramari},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2205.10661},
  url={https://www.semanticscholar.org/paper/651ae53112e73b02440773727b68cedbf8322705},
  abstract={Self-supervision based on the information extracted from large knowledge graphs has been shown to improve the generalization of language models, in zero-shot evaluation on various downstream language reasoning tasks. Since these improvements are reported in aggregate, however, little is known about (i) how to select the appropriate knowledge for solid performance across tasks, (ii) how to combine this knowledge with neural language models, and (iii) how these pairings affect granular task performance. In this paper, we study the effect of knowledge sampling strategies and sizes that can be used to generate synthetic data for adapting language models. We study the effect of different synthetic datasets on language models with various architectures and sizes. The resulting models are evaluated against four task properties: domain overlap, answer similarity, vocabulary overlap, and answer length. Our experiments show that encoder-decoder models benefit from more data to learn from, whereas sampling strategies that balance across different aspects yield best performance. Most of the improvement occurs on questions with short answers and dissimilar answer candidates, which corresponds to the characteristics of the data used for pre-training.},
  keywords={arxiv:2205.10661}
}

@article{khan2022executableformal,
  title={An Executable Formal Model of the VHDL in Isabelle/HOL},
  author={Wilayat Khan and Zhé Hóu and David Sanán and J. Nebhen and Yang Liu and Alwen Tiu},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/37b0b6db785f8c37460e2bb80da138c1443af5b4},
  abstract={In the hardware design process, hardware components are usually described in a hardware description language. Most of the hardware description languages, such as Verilog and VHDL, do not have mathematical foundation and hence are not fit for formal reasoning about the design. To enable formal reasoning in one of the most commonly used description language VHDL, we define a formal model of the VHDL language in Isabelle/HOL. Our model targets the functional part of VHDL designs used in industry, specifically the design of the LEON3 processor's integer unit. We cover a wide range of features in the VHDL language that are usually not modelled in the literature and define a novel operational semantics for it. Furthermore, our model can be exported to OCaml code for execution, turning the formal model into a VHDL simulator. We have tested our simulator against simple designs used in the literature, as well as the div32 module in the LEON3 design. The Isabelle/HOL code is publicly available: https://zhehou.github.io/apps/VHDLModel.zip},
  keywords={arxiv:2202.04192}
}

@article{katra2022experimentationframework,
  title={An Experimentation Framework for Specification and Verification of Web Services},
  author={Szymon Katra and Wiktor B. Daszczuk and Danny Czejdo},
  year={2022},
  booktitle={Conference on Computer Science and Information Systems},
  doi={10.15439/2022F188},
  url={https://www.semanticscholar.org/paper/9fbe3dc7a2229a5435fc7ace6978550af5ac3268},
  abstract={Designing and implementing Web Services constitutes a large and constantly growing part of the information technology market. Web Services have specific scenarios in which distributed processes and network resources are used. This aspect of services requires integration with the model checkers. This article presents the experimentation framework in which services can be specified and then formally analyzed for deadlock-freedom, achievement of process goals, and similar features. Rybu4WS language enriches the basic Rybu language with the ability to use variables in processes, service calls between servers, new structural instructions, and other constructions known to programmers while remaining in line with declarative, mathematical IMDS formalism. Additionally, the development environment allows simulation of a counterexample or a witness - obtained as a result of the model checking - in a similar way to traditional debuggers.}
}

@article{jeon2022informationtheoreticanalysis,
  title={An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws},
  author={Hong Jun Jeon and Benjamin Van Roy},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.01365},
  url={https://www.semanticscholar.org/paper/dab053b7713b77ab09f50b90b3176607912e913a},
  abstract={We study the compute-optimal trade-off between model and training data set sizes for large neural networks. Our result suggests a linear relation similar to that supported by the empirical analysis of chinchilla. While that work studies transformer-based large language models trained on the MassiveText corpus gopher, as a starting point for development of a mathematical theory, we focus on a simpler learning model and data generating process, each based on a neural network with a sigmoidal output unit and single hidden layer of ReLU activation units. We introduce general error upper bounds for a class of algorithms which incrementally update a statistic (for example gradient descent). For a particular learning model inspired by barron 1993, we establish an upper bound on the minimal information-theoretically achievable expected error as a function of model and data set sizes. We then derive allocations of computation that minimize this bound. We present empirical results which suggest that this approximation correctly identifies an asymptotic linear compute-optimal scaling. This approximation also generates new insights. Among other things, it suggests that, as the input dimension or latent space complexity grows, as might be the case for example if a longer history of tokens is taken as input to a language model, a larger fraction of the compute budget should be allocated to growing the learning model rather than training data.},
  keywords={arxiv:2212.01365}
}

@article{bellomarini2022overviewvadalog,
  title={An Overview of Vadalog: a System for Reasoning over Large Knowledge Graphs},
  author={Luigi Bellomarini and Davide Benedetto and Emanuel Sallinger},
  year={2022},
  booktitle={Sistemi Evoluti per Basi di Dati},
  url={https://www.semanticscholar.org/paper/83dc0eca1a453e2970d32923bb48bb84976bd968}
}

@article{amaliyah2022analisiskesulitan,
  title={Analisis Kesulitan Belajar Matematika dalam Menyelesaikan Soal Cerita di Kelas IV Sekolah Dasar Negeri Pakujaya 02},
  author={Aam Amaliyah and Luthfia Nur Maulida and N. Safitri and Ratri Hersita Dewi and Sabgi Wulan Septiara},
  year={2022},
  booktitle={ALSYS},
  doi={10.58578/alsys.v2i3.386},
  url={https://www.semanticscholar.org/paper/5023bebd78bb5f55a0d706f94b27f718b9c83cfc},
  abstract={Students with learning difficulties in mathematics often make mistakes in solving story problems on fractional material. This research uses descriptive qualitative. The purpose of this study was to determine the types of learning difficulties in mathematics experienced by students, the factors that influence learning difficulties, and to reveal the efforts that can be made to overcome the difficulties in learning mathematics in grade IV Pakujaya 02 State Elementary School. Data collection techniques were observation and interviews. . Based on data analysis and discussion, students experienced errors, namely: 1. Understanding the problem, namely errors in interpreting language and making mathematical models. The reason is incomplete/wrong reasoning and low student ability. 2. Planning for problem solving is an error in connecting one concept with another concept. The cause of this error is the humanistic thinking of students. 3. Implement problem solving planning, namely errors in implementing incorrect formulas. Errors in this aspect are caused by incomplete or incorrect reasoning and students' humanistic thinking.}
}

@article{shidqiya2022analysisstudents,
  title={Analysis of Students’ Mathematical Thinking Ability in Terms of Self Efficacy},
  author={Adiba Idlal Shidqiya and Sukestiyarno Sukestiyarno},
  year={2022},
  journal={Unnes Journal of Mathematics Education},
  doi={10.15294/ujme.v11i3.58772},
  url={https://www.semanticscholar.org/paper/fa5b5d97f15b5244e34a49e44317a1822b3e0daa},
  abstract={Mathematical thinking ability must be owned by students to solve various problems. Students are considered capable of fulfilling the indicators of mathematical thinking ability properly if they are balanced with good self-efficacy abilities. This research method is qualitative which aims to find new indicators and describe mathematical thinking ability in terms of self-efficacy and provide recommendations for teachers. The research subjects were six students from the first year of senior high school using purposive sampling. Indicators of mathematical thinking ability, include 1) Reasoning: identifying concepts and problems; 2) Generalizing: demonstrating mathematical ideas in writing and using mathematical language to express ideas correctly; 3) Critical Thinking: using representations to create mathematical models; 4) Problem Solving: planning problem solving strategies, implementing and checking results. 5) Communicating: revealing the results of problem solving. The results: 1) low self-efficacy’s students were only able to master reasoning; 2) moderate self-efficacy’s students are able to master reasoning, generalizing, and critical thinking; 3) high self-efficacy’s students are able to master all indicators. Recommendations for teachers are by giving opportunity to low self-efficacy’s students to speak in public, give appreciation for their efforts and reprimand if it doesn’t lower their confidence when they make mistakes.}
}

@article{yu2022analysiscorrelation,
  title={Analysis of the Correlation between Academic Performance and Learning Motivation in English Course under a Corpus-Data-Driven Blended Teaching Model},
  author={Lan Yu and Jun Shen},
  year={2022},
  booktitle={Scientific Programming},
  doi={10.1155/2022/3407270},
  url={https://www.semanticscholar.org/paper/6f554d023d8e403e5ee70268e55f5b2fe1be574e},
  abstract={To explore the correlation between academic performance and learning motivation in English course under a corpus-data-driven blended teaching model, this study set research objects as 62 year-2020-enrolled undergraduate students majoring in English from a university in Jinan City, Shandong Province, eastern China. According to their previous frequencies of using information technology to learn English, these 62 students were divided into two groups: practice group with high frequency and control group with low frequency, with 31 students in each group. The two groups of students were taught 3 English lessons per week for a total of 15 weeks by the exact same teachers using a corpus-data-driven blended teaching model. The students’ English academic performances were assessed by well-organized final tests, and their English learning motivations were measured by a motivation scale and questionnaires. The results show that the correlation coefficients between the average score of motivation questionnaires, intrinsic motivation factors, extrinsic motivation factors, and the average score of academic performances in practice group were 0.894, 0.682, and 0.724, respectively, while those in control group were 0.749, 0.836, and 0.904. In all the above correlation analyses, the significance level is 0.01, and all coefficient values are higher than critical value. Hence, there is a positive correlation between learning motivation and academic performance of the two groups of subjects. It is found that the corpus-data-driven blended teaching model has a significant impact on college students’ English academic performance and learning motivation, and it has a positive effect on the improvement of their English academic performance and the cultivation of learning motivation. In general, the key to this teaching model lies in reasoning and acquisition by analyzing the language provided by the corpus, and the whole process of data-driven learning is student-centered. Students are exposed to a large number of authentic language knowledge and cultural information, which promotes the sensitivity to relevant points. The results of this paper provide a reference for further research on the analysis of the correlation between academic performance and learning motivation in English course under the corpus-data-driven blended teaching model.}
}

@article{kumar2022answerlevelcalibration,
  title={Answer-level Calibration for Free-form Multiple Choice Question Answering},
  author={Sawan Kumar},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2022.acl-long.49},
  url={https://www.semanticscholar.org/paper/a5584d2d9b0de9e1692241d46d0c70942919cd60},
  abstract={Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks. This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem. While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration. In this work, we consider the question answering format, where we need to choose from a set of (free-form) textual choices of unspecified lengths given a context. We present ALC (Answer-Level Calibration), where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context. We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks. Further, we show that popular datasets potentially favor models biased towards easy cues which are available independent of the context. We analyze such biases using an associated F1-score. Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability.}
}

@article{zhou2022applicationthreeflow,
  title={Application of Three-Flow Fusion Technology Based on Modelica in Thermal Power Digital Twin},
  author={Dongyan Zhou and Haidong Gao and Wenyu Wang and Jun Cao and Wenfei Yang and Ruirui Zeng and Yuan He},
  year={2022},
  journal={IEEE Journal of Radio Frequency Identification},
  doi={10.1109/JRFID.2022.3205855},
  url={https://www.semanticscholar.org/paper/5391cf3bf8f2cc858ee1a532be7d9e2e6b6f6983},
  abstract={Thermal power plants gather large energy infrastructure; therefore, massive historical and real-time data of equipment operation will be generated in daily operation. Digital industrialization puts forward higher requirements for the use of big data than simple tasks, such as generating reports. MWorks is a multidomain unified modeling and simulation platform based on the Modelica language. In this study, MWorks is used to realize the modeling of multidomain systems, including electrical, thermal, mechanical, fluid, and heat transfer, in power plants. The test, calibration, verification, parameter optimization, and fault diagnosis of the thermal power plant mathematical models, which are historical and real-time data-driven, are discussed. The technology of three-flow fusion, including material flow, energy flow, and information flow, and its application in thermal power digital twin are explored.}
}

@article{alghamdi2022armathdataset,
  title={ArMATH: a Dataset for Solving Arabic Math Word Problems},
  author={Reem Alghamdi and Zhenwen Liang and Xiangliang Zhang},
  year={2022},
  booktitle={International Conference on Language Resources and Evaluation},
  url={https://www.semanticscholar.org/paper/4aca69be58a271b1be45ec7ebb3586569cec50b0}
}

@article{kar2022arggenprompting,
  title={ArgGen: Prompting Text Generation Models for Document-Level Event-Argument Aggregation},
  author={Debanjana Kar and S. Sarkar and Pawan Goyal},
  year={2022},
  booktitle={AACL/IJCNLP},
  doi={10.18653/v1/2022.findings-aacl.37},
  url={https://www.semanticscholar.org/paper/61f49465c0d53663ad5264c8f683c6724d31eef1},
  abstract={Most of the existing discourse-level Information Extraction tasks have been modeled to be extractive in nature. However, we argue that extracting information from larger bodies of discourse-like documents requires more natural language understanding and reasoning capabilities. In our work, we propose the novel task of document-level event argument aggregation which generates consolidated event-arguments at a document-level with minimal loss of information. More specifically, we focus on generating precise document-level information frames in a multilingual setting using prompt-based methods. In this paper, we show the effectiveness of prompt-based text generation approach to generate document-level argument spans in a low-resource and zero-shot setting. We also release the first of its kind multilingual event argument aggregation dataset that can be lever-aged in other related multilingual text generation tasks as well: https://github.com/}
}

@article{tewes2022artificialintelligence,
  title={Artificial Intelligence in the American Healthcare Industry: Looking Forward to 2030},
  author={F. Tewes},
  year={2022},
  journal={Journal of Medical Research and Surgery},
  doi={10.52916/jmrs224089},
  url={https://www.semanticscholar.org/paper/6baa97e2ca007eb2eeb51490f604d2bfd767fa0c},
  abstract={Artificial intelligence (AI) has the potential to speed up the exponential growth of cutting-edge technology, much way the Internet did. Due to intense competition from the private sector, governments, and businesspeople around the world, the Internet has already reached its peak as an exponential technology. In contrast, artificial intelligence is still in its infancy, and people all over the world are unsure of how it will impact their lives in the future. Artificial intelligence, is a field of technology that enables robots and computer programmes to mimic human intellect by teaching a predetermined set of software rules to learn by repetitive learning from experience and slowly moving toward maximum performance. Although this intelligence is still developing, it has already demonstrated five different levels of independence. Utilized initially to resolve issues. Next, think about solutions. Third, respond to inquiries. Fourth, use data analytics to generate forecasts. Fifth, make tactical recommendations. Massive data sets and "iterative algorithms," which use lookup tables and other data structures like stacks and queues to solve issues, make all of this possible. Iteration is a strategy where software rules are regularly adjusted to patterns in the data for a certain number of iterations. The artificial intelligence continuously makes small, incremental improvements that result in exponential growth, which enables the computer to become incredibly proficient at whatever it is trained to do. For each round of data processing, the artificial intelligence tests and measures its performance to develop new expertise. In order to address complicated problems, artificial intelligence aims to create computer systems that can mimic human behavior and exhibit human-like thought processes [1]. Artificial intelligence technology is being developed to give individualized medication in the field of healthcare. By 2030, six different artificial intelligence sectors will have considerably improved healthcare delivery through the utilization of larger, more accessible data sets. The first is machine learning. This area of artificial intelligence learns automatically and produces improved results based on identifying patterns in the data, gaining new insights, and enhancing the outcomes of whatever activity the system is intended to accomplish. It does this without being trained to learn a particular topic. Here are several instances of machine learning in the healthcare industry. The first is the IBM Watson Genomics, which aids in rapid disease diagnosis and identification by fusing cognitive computing with genome-based tumour sequencing. Second, a project called Nave Bayes allows for the prediction of diabetes years before an official diagnosis, before it results in harm to the kidneys, the heart, and the nerves. Third, employing two machine learning approaches termed classification and clustering to analyse the Indian Liver Patient Data (ILPD) set in order to predict liver illness before this organ that regulates metabolism becomes susceptible to chronic hepatitis, liver cancer, and cirrhosis [2]. Second, deep learning. Deep learning employs artificial intelligence to learn from data processing, much like machine learning does. Deep learning, on the other hand, makes use of synthetic neural networks that mimic human brain function to analyse data, identify relationships between the data, and provide outputs based on positive and negative reinforcement. For instance, in the fields of Magnetic Resonance Imaging (MRI) and Computed Tomography (CT), deep learning aids in the processes of picture recognition and object detection. Deep learning algorithms for the early identification of Alzheimer's, diabetic retinopathy, and breast nodule ultrasound detection are three applications of this cutting-edge technology in the real world. Future developments in deep learning will make considerable improvements in pathology and radiology pictures [3]. Third, neural networks. The artificial intelligence system can now accept massive data sets, find patterns within the data, and respond to queries regarding the information processed because the computer learning process resembles a network of neurons in the human brain. Let's examine a few application examples that are now applicable to the healthcare sector. According to studies from John Hopkins University, surgical errors are a major contributor to medical malpractice claims since they happen more than 4,000 times a year in just the United States due to the human error of surgeons. Neural networks can be used in robot-assisted surgery to model and plan procedures, evaluate the abilities of the surgeon, and streamline surgical activities. In one study of 379 orthopaedic patients, it was discovered that robotic surgery using neural networks results in five times fewer complications than surgery performed by a single surgeon. Another application of neural networks is in visualising diagnostics, which was proven to physicians by Harvard University researchers who inserted an image of a gorilla to x-rays. Of the radiologists who saw the images, 83\% did not recognise the gorilla. The Houston Medical Research Institute has created a breast cancer early detection programme that can analyse mammograms with 99 percent accuracy and offer diagnostic information 30 times faster than a human [4]. Cognitive computing is the fourth. Aims to replicate the way people and machines interact, showing how a computer may operate like the human brain when handling challenging tasks like text, speech, or image analysis. Large volumes of patient data have been analysed, with the majority of the research to date focusing on cancer, diabetes, and cardiovascular disease. Companies like Google, IBM, Facebook, and Apple have shown interest in this work. Cognitive computing made up the greatest component of the artificial market in 2020, with 39\% of the total [5]. Hospitals made up 42\% of the market for cognitive computing end users because of the rising demand for individualised medical data. IBM invested more than \$1 billion on the development of the WATSON analytics platform ecosystem and collaboration with startups committed to creating various cloud and application-based systems for the healthcare business in 2014 because it predicted the demand for cognitive computing in this sector. Natural Language Processing (NLP) is the fifth. This area of artificial intelligence enables computers to comprehend and analyse spoken language. The initial phase of this pre-processing is to divide the data up into more manageable semantic units, which merely makes the information simpler for the NLP system to understand. Clinical trial development is experiencing exponential expansion in the healthcare sector thanks to NLP. First, the NLP uses speech-to-text dictation and structured data entry to extract clinical data at the point of care, reducing the need for manual assessment of complex clinical paperwork. Second, using NLP technology, healthcare professionals can automatically examine enormous amounts of unstructured clinical and patient data to select the most suitable patients for clinical trials, perhaps leading to an improvement in the patients' health [6]. Computer vision comes in sixth. Computer vision, an essential part of artificial intelligence, uses visual data as input to process photos and videos continuously in order to get better results faster and with higher quality than would be possible if the same job were done manually. Simply put, doctors can now diagnose their patients with diseases like cancer, diabetes, and cardiovascular disorders more quickly and at an earlier stage. Here are a few examples of real-world applications where computer vision technology is making notable strides. Mammogram images are analysed by visual systems that are intended to spot breast cancer at an early stage. Automated cell counting is another example from the real world that dramatically decreases human error and raises concerns about the accuracy of the results because they might differ greatly depending on the examiner's experience and degree of focus. A third application of computer vision in the real world is the quick and painless early-stage tumour detection enabled by artificial intelligence. Without a doubt, computer vision has the unfathomable potential to significantly enhance how healthcare is delivered. Other than for visual data analysis, clinicians can use this technology to enhance their training and skill development. Currently, Gramener is the top company offering medical facilities and research organisations computer vision solutions [7]. The usage of imperative rather than functional programming languages is one of the key difficulties in creating artificial intelligence software. As artificial intelligence starts to increase exponentially, developers employing imperative programming languages must assume that the machine is stupid and supply detailed instructions that are subject to a high level of maintenance and human error. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures of research and development. As a result, software developers have contributed to the unreasonably high cost of medical care. Functional programming languages, on the other hand, demand that the developer use their problem-solving abilities as though the computer were a mathematician. As a result, compared to the number of lines of code needed by the programme to perform the same operation, mathematical functions are orders of magnitude shorter. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures o}
}

@article{zimmerman2022assessingphysics,
  title={Assessing physics quantitative literacy in algebra-based physics: lessons learned},
  author={Charlotte Zimmerman and Andrew McCarty and Suzanne White Brahmia and Alexis Olsho and Mieke De Cock and A. Boudreaux and Trevor I. Smith and Philip Eaton},
  year={2022},
  booktitle={Physics Education Research Conference Proceedings},
  doi={10.1119/perc.2022.pr.zimmerman},
  url={https://www.semanticscholar.org/paper/8f16d42771af2c1227c7a4cf6ad219e54351c9f7},
  abstract={Physics quantitative literacy (PQL)—applying familiar mathematics in novel ways in the context of physics— is ubiquitous across physics classrooms. The Physics Inventory for Quantitative Literacy, or PIQL, is a recently published reasoning inventory that can be used to assess PQL from calculus-based introductory physics through upper division courses (White Brahmia et al. 2021). There remains a need, however, for assessment of quantitative reasoning at the algebra-based level which includes not only algebra-based college courses but also pre-college physics courses. We present recent work adapting the PIQL to an algebra-based context towards developing the GERQN—the Generalized Equation-based Reasoning inventory for Quantities and Negativity. We report lessons learned from our efforts to adapt items from the calculus-based PIQL to the algebra-based GERQN, and provide examples of how items were revised to be within students proximal zone. We also report on our experience translating the GERQN into Flemish as part of a larger, on-going research project, and what we learned about language accessibility for native and non-native English speakers alike for developing assessment items, curricular materials, and when speaking with students.}
}

@misc{kogan2022assessingacademic,
  title={Assessing the Academic Recovery of Ohio Students: An Analysis of Spring 2022 Ohio State Tests},
  author={Vladimir Kogan},
  year={2022},
  url={https://www.semanticscholar.org/paper/76ac1af061d5d2ccf19d748ca8b744a9461260f3}
}

@article{gao2022attributedtext,
  title={Attributed Text Generation via Post-hoc Research and Revision},
  author={Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Zhao and N. Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.08726},
  url={https://www.semanticscholar.org/paper/4ef5410ec4b546eda642fe786cc1bdbb5a7251e1}
}

@misc{wu2022autoformalizationneural,
  title={Autoformalization for Neural Theorem Proving},
  author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
  year={2022},
  url={https://www.semanticscholar.org/paper/15e767aa26a14455da95a2b2f11e3d59f2c250f6}
}

@article{wu2022autoformalizationwith,
  title={Autoformalization with Large Language Models},
  author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2205.12615},
  url={https://www.semanticscholar.org/paper/c28e95a06dfcf13fc65a1cac83722f53e34f12a5},
  abstract={Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion (\$25.3\textbackslash\{\}\%\$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from \$29.6\textbackslash\{\}\%\$ to \$35.2\textbackslash\{\}\%\$.},
  keywords={arxiv:2205.12615}
}

@article{zhang2022automaticchain,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alexander J. Smola},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/90350aa626bed47b02d0c162462e5b0ca82be6b2},
  abstract={Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like"Let's think step by step"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the"Let's think step by step"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
  keywords={arxiv:2210.03493}
}

@article{shridhar2022automaticgeneration,
  title={Automatic Generation of Socratic Subquestions for Teaching Math Word Problems},
  author={Kumar Shridhar and Jakub Macina and Mennatallah El-Assady and Tanmay Sinha and Manu Kapur and Mrinmaya Sachan},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2211.12835},
  url={https://www.semanticscholar.org/paper/e6745fb621481ccb0ed53c267a37292e499c1b42},
  abstract={Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers.In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning.On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.},
  keywords={arxiv:2211.12835}
}

@article{xiao2022auxiliaryteaching,
  title={Auxiliary Teaching System of Higher Mathematics Based on Random Matrix Model},
  author={Yabin Xiao and Bing Zhou and Dan-ni He and Jingzhong Liu},
  year={2022},
  booktitle={Mathematical Problems in Engineering},
  doi={10.1155/2022/7983989},
  url={https://www.semanticscholar.org/paper/869011d58c272450dc8cf95bd4f81601e17b8511},
  abstract={With the development of computer technology, computers have become a part of people’s lives and the Internet has connected the world’s networks as a whole. Computer technology is changing people’s study, life, and work. People’s traditional education mode, thinking, content, method, and talent training program have a significant impact. The development from traditional to computer technology-based teaching methods has brought new developments and leaps in educational technology. This paper analyzes the research background, significance, and research status of the advanced mathematics auxiliary teaching system, introduces the related technologies and development modes used in the development of the system, and especially discusses the access database technology by ADO and the mathematical expression based on MathML language. Secondly, starting from the actual teaching, we analyze the functional requirements and performance requirements of the system in detail and make detailed planning and design for the system architecture, database selection, functional modules, etc. The design and implementation process of this teaching system are summarized. The teaching strategy inference engine is the key to the personalization and intelligence of the ICAI system. According to the learning models provided by different students, the system designs a corresponding teaching sequence for the learners by controlling the meta-knowledge of the domain knowledge base. The teaching strategy inference engine cuts the domain knowledge tree, selects the knowledge points suitable for the student, and sorts the selected knowledge points reasonably to generate an optimal teaching sequence. According to the students’ learning situation, combined with the teaching rules in the teaching rule library, the students’ grades are dynamically adjusted, so as to select new learning content for students and provide teaching suggestions in time. The student model is the premise of the ICAI system to achieve individualization and intelligence. The system makes a comprehensive evaluation and diagnosis of students through fuzzy comprehensive evaluation and fuzzy reasoning. On this basis, a cognitive student model is established, which is the teaching strategy that provided the basis for the formulation.}
}

@misc{an2022bevbertmultimodal,
  title={BEVBert: Multimodal Map Pre-training for Language-guided Navigation},
  author={Dongyan An and Yuankai Qi and Yangguang Li and Yan Huang and Liangsheng Wang and T. Tan and Jing Shao},
  year={2022},
  url={https://www.semanticscholar.org/paper/d7abc3bcf368c7c0e3487da7cecae1ac209a7284},
  abstract={Large-scale pre-training has shown promising results on the vision-and-language navigation (VLN) task. However, most existing pre-training methods employ discrete panoramas to learn visual-textual associations. This requires the model to implicitly correlate incomplete, duplicate observations within the panoramas, which may impair an agent's spatial understanding. Thus, we propose a new map-based pre-training paradigm that is spatial-aware for use in VLN. Concretely, we build a local metric map to explicitly aggregate incomplete observations and remove duplicates, while modeling navigation dependency in a global topological map. This hybrid design can balance the demand of VLN for both short-term reasoning and long-term planning. Then, based on the hybrid map, we devise a pre-training framework to learn a multimodal map representation, which enhances spatial-aware cross-modal reasoning thereby facilitating the language-guided navigation goal. Extensive experiments demonstrate the effectiveness of the map-based pre-training route for VLN, and the proposed method achieves state-of-the-art on four VLN benchmarks.},
  keywords={arxiv:2212.04385}
}

@article{chen2022btpkbasedlearning,
  title={BTPK-based learning: An Interpretable Method for Named Entity Recognition},
  author={Yulin Chen and Zelai Yao and Haixiao Chi and D. Gabbay and Bo Yuan and Bruno Bentzen and Beishui Liao},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/811151315ac5fefb1629a4d02c0274370db468a7}
}

@article{hua2022bayesvarbrulunified,
  title={BayesVarbrul: a unified multidimensional analysis of language change in a speaker community},
  author={Xia Hua},
  year={2022},
  journal={Journal of Language Evolution},
  doi={10.1093/jole/lzac004},
  url={https://www.semanticscholar.org/paper/c294f2479c50d70b8e6b32b630e197aea1d9309d},
  abstract={
 Exchange in ideas between language evolution and biological evolution has a long history, due to a shared theoretical foundation between language and biology as two evolving systems. Both systems evolve in terms of the frequency of a variant in a population for each of a large number of variables, that is how often a particular variant of a language variable is used in a speaker community and how many individuals in a biological population carry a particular variant of a gene. The way these frequencies change has been modelled under a similar mathematical framework. Here, I show how we can use concepts from genome wide association studies that identify the source of natural selection and the genes under selection in a biological population to study how social factors affect the usage of language variables in a speaker community or how some social groups use some language variables differently from other groups. Using the Gurindji Kriol language as a case study, I show how this approach unifies existing mathematical and statistical tools in studying language evolution over a large number of speakers and a large number of language variables, which provides a promising link between micro- and macro-evolution in language. The approach is named BayesVarbrul and is ready to apply to datasets other than the Gurindji Kriol dataset, including existing corpus data. The code and the instructions are available at https://github.com/huaxia1985/BayesVarbrul.}
}

@misc{si2022benchmarkinggpt3,
  title={Benchmarking GPT-3 For Closed-Book QA: Strengths and Weaknesses},
  author={Chenglei Si and Naman Molri and Gurmehar Cheema and Elliot Huang and Arjun Akkiraju},
  year={2022},
  url={https://www.semanticscholar.org/paper/b9a0bc80aa136027327697fe40189792a32c8b0c}
}

@article{gopinath2022benchmarkinglargescale,
  title={Benchmarking Large-Scale ACOPF Solutions and Optimality Bounds},
  author={S. Gopinath and H. Hijazi},
  year={2022},
  booktitle={IEEE Power \& Energy Society General Meeting},
  doi={10.1109/PESGM48719.2022.9916662},
  url={https://www.semanticscholar.org/paper/dea559bde46a1b9efc64c4418eebb3e9f3b775b7},
  abstract={We present the results of a comprehensive bench-marking effort aimed at evaluating and comparing state-of-the-art open-source tools for solving the Alternating-Current Optimal Power Flow (ACOPF) problem. Our numerical experiments include all instances found in the public library PGLIB with network sizes up to 30,000 nodes. The benchmarked tools span a number of programming languages (Python, Julia, Matlab/Octave, and C++), nonlinear optimization solvers (Ipopt, MIPS, and INLP) as well as different mathematical modeling tools (JuMP and Gravity). We also present state-of-the-art optimality bounds obtained using sparsity-exploiting semidefinite programming approaches and corresponding computational times.},
  keywords={arxiv:2203.11328}
}

@article{gokhale2022benchmarkingspatial,
  title={Benchmarking Spatial Relationships in Text-to-Image Generation},
  author={Tejas Gokhale and Hamid Palangi and Besmira Nushi and Vibhav Vineet and E. Horvitz and Ece Kamar and Chitta Baral and Yezhou Yang},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.10015},
  url={https://www.semanticscholar.org/paper/4bf77d64b860ed0cd84a63aecd92a3cb295b88ee},
  abstract={Spatial understanding is a fundamental aspect of computer vision and integral for human-level reasoning about images, making it an important component for grounded language understanding. While recent text-to-image synthesis (T2I) models have shown unprecedented improvements in photorealism, it is unclear whether they have reliable spatial understanding capabilities. We investigate the ability of T2I models to generate correct spatial relationships among objects and present VISOR, an evaluation metric that captures how accurately the spatial relationship described in text is generated in the image. To benchmark existing models, we introduce a dataset, \$\textbackslash\{\}mathrm\{SR\}\_\{2D\}\$, that contains sentences describing two or more objects and the spatial relationships between them. We construct an automated evaluation pipeline to recognize objects and their spatial relationships, and employ it in a large-scale evaluation of T2I models. Our experiments reveal a surprising finding that, although state-of-the-art T2I models exhibit high image quality, they are severely limited in their ability to generate multiple objects or the specified spatial relations between them. Our analyses demonstrate several biases and artifacts of T2I models such as the difficulty with generating multiple objects, a bias towards generating the first object mentioned, spatially inconsistent outputs for equivalent relationships, and a correlation between object co-occurrence and spatial understanding capabilities. We conduct a human study that shows the alignment between VISOR and human judgement about spatial understanding. We offer the \$\textbackslash\{\}mathrm\{SR\}\_\{2D\}\$ dataset and the VISOR metric to the community in support of T2I reasoning research.},
  keywords={arxiv:2212.10015}
}

@article{srivastava2022beyondimitation,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and A. Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmuller and Andrew M. Dai and A. La and Andrew Kyle Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and A. Tabassum and Arul Menezes and Arun Kirubarajan and A. Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakacs and B. R. Roberts and B. S. Loe and Barret Zoph and Bartlomiej Bojanowski and Batuhan Ozyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and B. Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and C'esar Ferri Ram'irez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and D. Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and D. Gonz'alez and Danielle R. Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and D. Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and E. D. Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodolà and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and E. Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Mart'inez-Plumed and Francesca Happ'e and François Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Xinyue Wang and Gonzalo Jaimovitch-L'opez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and H. Bogar and Henry Shevlin and Hinrich Schutze and H. Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and John Kernion and Jacob Hilton and Jaehoon Lee and J. Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Koco'n and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Narain Sohl-Dickstein and Jason Phang and Jason Wei and J. Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Oluwadara Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Jane W Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jorg Frohberg and Jos Rozen and J. Hernández-Orallo and Joseph Boudeman and J. Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and K. Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and K. Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and K. Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-philippe Morency and Luca Moschella and Luca Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Col'on and Luke Metz and Lutfi Kerem cSenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ram’irez Quintana and M. Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and M. Schubert and Medina Baitemirova and Melody Arnaud and M. McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and M. Strube and Michal Swkedrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Monica Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and T. MukundVarma and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and N. Keskar and Niveditha Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and P. Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and P. Eckersley and Phu Mon Htut and P. Hwang and P. Milkowski and P. Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphael Milliere and Rhythm Garg and Richard Barnes and R. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan Le Bras and Rosanne Liu and Rowan Jacobs and Rui Zhang and R. Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and R. Teehan and Rylan Yang and Sahib Singh and Saif Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi S. Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and S. Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima Debnath and Siamak Shakeri and Simon Thormeyer and S. Melzi and Siva Reddy and S. Makini and Soo-Hwan Lee and Spencer Bradley Torene and Sriharsha Hatwar and S. Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T Piantadosi and Stuart M. Shieber and Summer Misherghi and S. Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsunori Hashimoto and Te-Lin Wu and T. Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and T. Kornev and T. Tunduny and Tobias Gerstenberg and T. Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and V. Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and W. Fedus and W. Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yu Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881},
  abstract={Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit"breakthrough"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
  keywords={arxiv:2206.04615}
}

@article{jung2022blankcollapse,
  title={Blank Collapse: Compressing CTC emission for the faster decoding},
  author={Minkyu Jung and Ohhyeok Kwon and S. Seo and Soonshin Seo},
  year={2022},
  booktitle={Interspeech},
  doi={10.48550/arXiv.2210.17017},
  url={https://www.semanticscholar.org/paper/6498d95d3f988e684bc6a70004decbefec655222},
  abstract={Connectionist Temporal Classification (CTC) model is a very efficient method for modeling sequences, especially for speech data. In order to use CTC model as an Automatic Speech Recognition (ASR) task, the beam search decoding with an external language model like n-gram LM is necessary to obtain reasonable results. In this paper we analyze the blank label in CTC beam search deeply and propose a very simple method to reduce the amount of calculation resulting in faster beam search decoding speed. With this method, we can get up to 78\% faster decoding speed than ordinary beam search decoding with a very small loss of accuracy in LibriSpeech datasets. We prove this method is effective not only practically by experiments but also theoretically by mathematical reasoning. We also observe that this reduction is more obvious if the accuracy of the model is higher.},
  keywords={arxiv:2210.17017}
}

@article{wan2022bridgingbetween,
  title={Bridging the Gap between Recognition-level Pre-training and Commonsensical Vision-language Tasks},
  author={Yue Wan and Yueen Ma and Haoxuan You and Zhecan Wang and Shih-Fu Chang},
  year={2022},
  booktitle={CSRR},
  doi={10.18653/v1/2022.csrr-1.4},
  url={https://www.semanticscholar.org/paper/ac13afe6c5f456a1f250e03e3258f5cfecc99373},
  abstract={Large-scale visual-linguistic pre-training aims to capture the generic representations from multimodal features, which are essential for downstream vision-language tasks. Existing methods mostly focus on learning the semantic connections between visual objects and linguistic content, which tend to be recognitionlevel information and may not be sufficient for commonsensical reasoning tasks like VCR. In this paper, we propose a novel commonsensical vision-language pre-training framework to bridge the gap. We first augment the conventional image-caption pre-training datasets with commonsense inferences from a visuallinguistic GPT-2. To pre-train models on image, caption and commonsense inferences together, we propose two new tasks: masked commonsense modeling (MCM) and commonsense type prediction (CTP). To reduce the shortcut effect between captions and commonsense inferences, we further introduce the domain-wise adaptive masking that dynamically adjusts the masking ratio. Experimental results on downstream tasks, VCR and VQA, show the improvement of our pre-training strategy over previous methods. Human evaluation also validates the relevance, informativeness, and diversity of the generated commonsense inferences. Overall, we demonstrate the potential of incorporating commonsense knowledge into the conventional recognition-level visual-linguistic pre-training.}
}

@misc{leemann2022oherencevaluation,
  title={C OHERENCE E VALUATION OF V ISUAL C ONCEPTS WITH O BJECTS AND L ANGUAGE},
  author={Tobias Leemann and Yao Rong and Stefan Kraft and Enkelejda Kasneci and Gjergji Kasneci},
  year={2022},
  url={https://www.semanticscholar.org/paper/75e3f61b69dc6bc8bfb7fd28aa1001edbbc8eab4}
}

@article{raman2022capecorrective,
  title={CAPE: Corrective Actions from Precondition Errors using Large Language Models},
  author={S. S. Raman and Vanya Cohen and Eric Rosen and Ifrah Idrees and D. Paulius and Stefanie Tellex},
  year={2022},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA57147.2024.10611376},
  url={https://www.semanticscholar.org/paper/c82d8d80ea68400adb7faebb2f1cff38dd83093a},
  abstract={Extracting knowledge and reasoning from large language models (LLMs) offers a path to designing intelligent robots. Common approaches that leverage LLMs for planning are unable to recover when actions fail and resort to retrying failed actions without resolving the underlying cause. We propose a novel approach (CAPE) that generates corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans through few-shot reasoning on action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while maintaining semantic correctness and minimizing re-prompting. In VirtualHome, CAPE improves a human-annotated plan correctness metric from 28.89\% to 49.63\% over SayCan, whilst achieving competitive executability. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves correctness by 76.49\% with higher executability compared to SayCan. Our approach enables embodied agents to follow natural language commands and robustly recover from failures.},
  keywords={arxiv:2211.09935}
}

@article{lindstrm2022clevrmathdataset,
  title={CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning},
  author={Adam Dahlgren Lindström and Savitha Sam Abraham},
  year={2022},
  booktitle={International Workshop on Neural-Symbolic Learning and Reasoning},
  doi={10.48550/arXiv.2208.05358},
  url={https://www.semanticscholar.org/paper/74cc3d340039c67bdabaef090d1386fe2c5376ca},
  abstract={We introduce CLEVR-Math, a multi-modal math word problems dataset consisting of simple math word problems involving addition/subtraction, represented partly by a textual description and partly by an image illustrating the scenario. The text describes actions performed on the scene that is depicted in the image. Since the question posed may not be about the scene in the image, but about the state of the scene before or after the actions are applied, the solver envision or imagine the state changes due to these actions. Solving these word problems requires a combination of language, visual and mathematical reasoning. We apply state-of-the-art neural and neuro-symbolic models for visual question answering on CLEVR-Math and empirically evaluate their performances. Our results show how neither method generalise to chains of operations. We discuss the limitations of the two in addressing the task of multi-modal word problem solving.},
  keywords={arxiv:2208.05358}
}

@article{dong2022corrpusdetecting,
  title={CORRPUS: Detecting Story Inconsistencies via Codex-Bootstrapped Neurosymbolic Reasoning},
  author={Yi Dong and Lara J. Martin and Chris Callison-Burch},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.10754},
  url={https://www.semanticscholar.org/paper/4bea09d4c897fb201c032b9eb605a943b1e70435}
}

@misc{krell2022crosscontaminationaccelerating,
  title={CROSS-CONTAMINATION: ACCELERATING LARGE LANGUAGE MODELS WITHOUT IMPACTING PERFORMANCE},
  author={M. M. Krell and Matej Kosec},
  year={2022},
  url={https://www.semanticscholar.org/paper/85cac89ba01a07f3dbf6dbb1e0c56067a3105714}
}

@article{lin2022curriculumlearning,
  title={CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction},
  author={Jiaju Lin and Qin Chen and Jie Zhou and Jian Jin and Liangye He},
  year={2022},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2205.00498},
  url={https://www.semanticscholar.org/paper/65d88194a902332b78dd5a7b919fa577bfa7ee9f},
  abstract={Implicit event argument extraction (EAE) aims to identify arguments that could scatter over the document. Most previous work focuses on learning the direct relations between arguments and the given trigger, while the implicit relations with long-range dependency are not well studied. Moreover, recent neural network based approaches rely on a large amount of labeled data for training, which is unavailable due to the high labelling cost. In this paper, we propose a Curriculum learning based Prompt tuning (CUP) approach, which resolves implicit EAE by four learning stages. The stages are defined according to the relations with the trigger node in a semantic graph, which well captures the long-range dependency between arguments and the trigger. In addition, we integrate a prompt-based encoder-decoder model to elicit related knowledge from pre-trained language models (PLMs) in each stage, where the prompt templates are adapted with the learning progress to enhance the reasoning for arguments. Experimental results on two well-known benchmark datasets show the great advantages of our proposed approach. In particular, we outperform the state-of-the-art models in both fully-supervised and low-data scenarios.},
  keywords={arxiv:2205.00498}
}

@article{willig2022foundationmodels,
  title={Can Foundation Models Talk Causality?},
  author={Moritz Willig and M. Zecevic and D. Dhami and K. Kersting},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2206.10591},
  url={https://www.semanticscholar.org/paper/6745381bfa99a3b979766cca05e91559f1b770e3},
  abstract={Foundation models are subject to an ongoing heated debate, leaving open the question of progress towards AGI and dividing the community into two camps: the ones who see the arguably impressive results as evidence to the scaling hypothesis, and the others who are worried about the lack of interpretability and reasoning capabilities. By investigating to which extent causal representations might be captured by these large scale language models, we make a humble efforts towards resolving the ongoing philosophical conflicts.},
  keywords={arxiv:2206.10591}
}

@article{tefnik2022incontextlearners,
  title={Can In-context Learners Learn a Reasoning Concept from Demonstrations?},
  author={Michal Tefnik and Marek Kadlcík},
  year={2022},
  booktitle={NLRSE},
  doi={10.18653/v1/2023.nlrse-1.8},
  url={https://www.semanticscholar.org/paper/e7cfc3362dd85b17c747e9f9636749696f87a88b},
  abstract={Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations.However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input.However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models’ ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.To disentangle models’ in-context learning ability independent of models’ memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in few-shot demonstrations.We find that smaller models are more sensitive to the presented concepts. While some of the models are able to benefit from concept-presenting demonstrations for each assessed concept, we find that none of the assessed in-context learners can benefit from all presented reasoning concepts consistently, leaving the in-context concept learning an open challenge.},
  keywords={arxiv:2212.01692}
}

@article{behnamghader2022retrieveraugmentedlanguage,
  title={Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model},
  author={Parishad BehnamGhader and Santiago Miret and Siva Reddy},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2212.09146},
  url={https://www.semanticscholar.org/paper/e4758d05c3d4231dd30c656330e156ccc9dbb07b},
  abstract={Augmenting pretrained language models with retrievers to select the supporting documents has shown promise in effectively solving common NLP problems, including language modeling and question answering, in an interpretable way. In this paper, we first study the strengths and weaknesses of different retriever-augmented language models (REALM, \$k\$NN-LM, FiD coupled with DPR, and ATLAS and Flan-T5 coupled with Contriever) in reasoning over the retrieved statements in different tasks. We show how the retrieve-then-read models' limitations in reasoning are rooted both in the retriever module as well as the language model. Our experimental results demonstrate that the similarity metric used by the retrievers is generally insufficient for reasoning tasks. Additionally, we show that the language models in retriever-augmented models do not take the complicated relations between the statements into account, which leads to poor reasoning performance even when using the larger models. Moreover, we analyze the reasoning performance of large language models using multihop retrieval but we only observe minor improvements. Overall, this shows great room for further research in this area.},
  keywords={arxiv:2212.09146}
}

@article{schlegel2022transformersreason,
  title={Can Transformers Reason in Fragments of Natural Language?},
  author={Viktor Schlegel and Kamen V. Pavlov and Ian Pratt-Hartmann},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2211.05417},
  url={https://www.semanticscholar.org/paper/8ee376114a43432399554be39a79c1a2b6c65d51},
  abstract={State-of-the-art deep-learning-based approaches to Natural Language Processing (NLP) are credited with various capabilities that involve reasoning with natural language texts. \%However, reasoning in this setting is often ill-defined and shallow. In this paper we carry out a large-scale empirical study investigating the detection of formally valid inferences in controlled fragments of natural language for which the satisfiability problem becomes increasingly complex. We find that, while transformer-based language models perform surprisingly well in these scenarios, a deeper analysis reveals that they appear to overfit to superficial patterns in the data rather than acquiring the logical principles governing the reasoning in these fragments.},
  keywords={arxiv:2211.05417}
}

@article{carette2022centralsubmonads,
  title={Central Submonads and Notions of Computation},
  author={T. Carette and Louis Lemonnier and V. Zamdzhiev},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2207.09190},
  url={https://www.semanticscholar.org/paper/af36ab7fe4e10aad2e01be5dcde0784241742832}
}

@article{wei2022chainthought,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5},
  abstract={We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  keywords={arxiv:2201.11903}
}

@misc{unknown2022chartingspace,
  title={Charting the Space of Quantum Field Theories},
  year={2022},
  url={https://www.semanticscholar.org/paper/cb39717895b9fa4cd2cc59748d59e20ce5eb4521}
}

@article{wang2022chiqalarge,
  title={ChiQA: A Large Scale Image-based Real-World Question Answering Dataset for Multi-Modal Understanding},
  author={Bingning Wang and Feiya Lv and Ting Yao and Yiming Yuan and Jin Ma and Yu Luo and Haijin Liang},
  year={2022},
  booktitle={International Conference on Information and Knowledge Management},
  doi={10.1145/3511808.3557258},
  url={https://www.semanticscholar.org/paper/730efc9d93a2b34f02a98aa46d9357f05111fd99},
  abstract={Visual question answering is an important task in both natural language and vision understanding. However, in most of the public visual question answering datasets such as VQA, CLEVR, the questions are human generated that specific to the given image, such as 'What color are her eyes?'. The human generated crowdsourcing questions are relatively simple and sometimes have the bias toward certain entities or attributes [1, 55]. In this paper, we introduce a new question answering dataset based on image-ChiQA. It contains the real-world queries issued by internet users, combined with several related open-domain images. The system should determine whether the image could answer the question or not. Different from previous VQA datasets, the questions are real-world image-independent queries that are more various and unbiased. Compared with previous image-retrieval or image-caption datasets, the ChiQA not only measures the relatedness but also measures the answerability, which demands more fine-grained vision and language reasoning. ChiQA contains more than 40K questions and more than 200K question-images pairs. A three-level 2/1/0 label is assigned to each pair indicating perfect answer, partially answer and irrelevant. Data analysis shows ChiQA requires a deep understanding of both language and vision, including grounding, comparisons, and reading. We evaluate several state-of-the-art visual-language models such as ALBEF, demonstrating that there is still a large room for improvements on ChiQA.},
  keywords={arxiv:2208.03030}
}

@article{wilson2022classificationopenended,
  title={Classification of open-ended responses to a research-based assessment using natural language processing},
  author={Joseph Wilson and Benjamin Pollard and J. M. Aiken and Marcos D. Caballero and H. Lewandowski},
  year={2022},
  booktitle={Physical Review Physics Education Research},
  doi={10.1103/physrevphyseducres.18.010141},
  url={https://www.semanticscholar.org/paper/932cb50f541c3141fabe156ecf3bbafb0aa61c29},
  abstract={Surveys have long been used in physics education research to understand student reasoning and inform course improvements. However, to make analysis of large sets of responses practical, most surveys use a closed-response format with a small set of potential responses. Open-ended formats, such as written free response, can provide deeper insights into student thinking, but take much longer to analyze, especially with a large number of responses. Here, we explore natural language processing as a computational solution to this problem. We create a machine learning model that can take student responses from the Physics Measurement Questionnaire as input, and output a categorization of student reasoning based on different reasoning paradigms. Our model yields classifications with the same level of agreement as that between two humans categorizing the data, but can be done by a computer, and thus can be scaled for large datasets. In this work, we describe the algorithms and methodologies used to create, train, and test our natural language processing system. We also present the results of the analysis and discuss the utility of these approaches for analyzing open-response data in education research. DOI: 10.1103/PhysRevPhysEducRes.18.010141}
}

@article{dong2022corrpuscodebased,
  title={CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding},
  author={Yi Dong and Lara J. Martin and Chris Callison-Burch},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2023.findings-acl.832},
  url={https://www.semanticscholar.org/paper/76f54657eb0893a0b203da57dcf0b4fffeebfc2c},
  abstract={Story generation and understanding -- as with all NLG/NLU tasks -- has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI Task 2 and Re\^{}3) with minimal hand engineering. We hope that this work can help highlight the importance of symbolic representations and specialized prompting for LLMs as these models require some guidance for performing reasoning tasks properly.},
  keywords={arxiv:2212.10754}
}

@article{kim2022cosimcommonsense,
  title={CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination},
  author={Hyounghun Kim and Abhaysinh Zala and Mohit Bansal},
  year={2022},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2207.03961},
  url={https://www.semanticscholar.org/paper/153b51c7871f82c8966a8d744d3630ef791f00f4},
  abstract={As humans, we can modify our assumptions about a scene by imagining alternative objects or concepts in our minds. For example, we can easily anticipate the implications of the sun being overcast by rain clouds (e.g., the street will get wet) and accordingly prepare for that. In this paper, we introduce a new dataset called Commonsense Reasoning for Counterfactual Scene Imagination (CoSIm) which is designed to evaluate the ability of AI systems to reason about scene change imagination. To be specific, in this multimodal task/dataset, models are given an image and an initial question-response pair about the image. Next, a counterfactual imagined scene change (in textual form) is applied, and the model has to predict the new response to the initial question based on this scene change. We collect 3.5K high-quality and challenging data instances, with each instance consisting of an image, a commonsense question with a response, a description of a counterfactual change, a new response to the question, and three distractor responses. Our dataset contains various complex scene change types (such as object addition/removal/state change, event description, environment change, etc.) that require models to imagine many different scenarios and reason about the changed scenes. We present a baseline model based on a vision-language Transformer (i.e., LXMERT) and ablation studies. Through human evaluation, we demonstrate a large human-model performance gap, suggesting room for promising future work on this challenging, counterfactual multimodal task.},
  keywords={arxiv:2207.03961}
}

@article{liang2022codepolicies,
  title={Code as Policies: Language Model Programs for Embodied Control},
  author={Jacky Liang and Wenlong Huang and F. Xia and Peng Xu and Karol Hausman and Brian Ichter and Peter R. Florence and Andy Zeng},
  year={2022},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA48891.2023.10160591},
  url={https://www.semanticscholar.org/paper/91deaf9d324c8feafc189da0da03e60a60287bca},
  abstract={Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  keywords={arxiv:2209.07753}
}

@article{sahu2022codequeriesdataset,
  title={CodeQueries: A Dataset of Semantic Queries over Code},
  author={Surya Prakash Sahu and Madhurima Mandal and Shikhar Bharadwaj and Aditya Kanade and Petros Maniatis and S. Shevade},
  year={2022},
  booktitle={International Symposium on Electronic Commerce},
  doi={10.1145/3641399.3641408},
  url={https://www.semanticscholar.org/paper/cd937849a314b3e5eb4862a3b55aa823811a5996},
  abstract={Developers often have questions about semantic aspects of code they are working on, e.g., “Is there a class whose parent classes declare a conflicting attribute?”. Answering them requires understanding code semantics such as attributes and inheritance relation of classes. An answer to such a question should identify code spans constituting the answer (e.g., the declaration of the subclass) as well as supporting facts (e.g., the definitions of the conflicting attributes). The existing work on question-answering over code has considered yes/no questions or method-level context. We contribute a labeled dataset, called CodeQueries, of semantic queries over Python code. Compared to the existing datasets, in CodeQueries, the queries are about code semantics, the context is file level and the answers are code spans. We curate the dataset based on queries supported by a widely-used static analysis tool, CodeQL, and include both positive and negative examples, and queries requiring single-hop and multi-hop reasoning. To assess the value of our dataset, we evaluate baseline neural approaches. We study a large language model (GPT3.5-Turbo) in zero-shot and few-shot settings on a subset of CodeQueries. We also evaluate a BERT style model (CuBERT) with fine-tuning. We find that these models achieve limited success on CodeQueries. CodeQueries is thus a challenging dataset to test the ability of neural models, to understand code semantics, in the extractive question-answering setting.},
  keywords={arxiv:2209.08372}
}

@article{zhao2022collaborativereasoning,
  title={Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation},
  author={Xueliang Zhao and Yuxuan Wang and Chongyang Tao and Chenshuo Wang and Dongyan Zhao},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.12460},
  url={https://www.semanticscholar.org/paper/256fd60c692ebe12fe2bbf65d46722f511aa3117},
  abstract={We study video-grounded dialogue generation, where a response is generated based on the dialogue context and the associated video. The primary challenges of this task lie in (1) the difficulty of integrating video data into pre-trained language models (PLMs) which presents obstacles to exploiting the power of large-scale pre-training; and (2) the necessity of taking into account the complementarity of various modalities throughout the reasoning process. Although having made remarkable progress in video-grounded dialogue generation, existing methods still fall short when it comes to integrating with PLMs in a way that allows information from different modalities to complement each other. To alleviate these issues, we first propose extracting pertinent information from videos and turning it into reasoning paths that are acceptable to PLMs. Additionally, we propose a multi-agent reinforcement learning method to collaboratively perform reasoning on different modalities (i.e., video and dialogue context). Empirical experiment results on two public datasets indicate that the proposed model can significantly outperform state-of-the-art models by large margins on both automatic and human evaluations.},
  keywords={arxiv:2210.12460}
}

@article{gouhar2022combininglocal,
  title={Combining local and global approaches to ascertain semantic similarity},
  author={Shahrukh Gouhar and Anupam Misra and Radha Rathore and Mansoor Ali Shaik and Dr. Subhasis Dasgupta},
  year={2022},
  booktitle={2022 IEEE India Council International Subsections Conference (INDISCON)},
  doi={10.1109/INDISCON54605.2022.9862898},
  url={https://www.semanticscholar.org/paper/1f9ae8a3f6be60d10e9d1d3eeecc0a8fda0404b2},
  abstract={Interviewing potential candidates is both time consuming and resource intensive. This is particularly prominent in organizations which go for large scale recruitment processes. In the current study, a client based application has been proposed for interviewing candidates for data science profiles where interviewee’s answers are scored using machine learning. Different approaches were tried with pretrained models but as the application was very much domain specific, those models did not provide good results. Hence a custom embedding layer was built on open source data science textbooks. These embeddings were used with Gated Recurrent Units (GRU) to capture a local approach (subject specific) in the interview answers. However, this neglected the nuances of the English language involved in critical reasoning. Hence Bi-directional Encoder Representation from Transformers (BERT) was employed to capture the global approach (interaction between words in the English language) in the interview answers. The similarity scores from these two approaches were ensembled into a machine learning model which allotted the final score to the interviewee’s answer. The proposed method outperformed the pretrained models with significant margin when tested with the validation data.}
}

@misc{albalak2022commonsensereasoning,
  title={Commonsense Reasoning for Conversational AI: A Survey of Recent Datasets and Benchmarks},
  author={Alon Albalak and Varun R. Embar and Yi-Lin Tuan and L. Getoor and Ankur Bapna and Gokhan Tur and Dilek Hakkani-Tur and Larry Heck. 2017 and Lisa Bauer and Yicheng Wang and Mohit Bansal and Antoine Bosselut and Hannah Rashkin and Maarten Sap and Chaitanya Malaviya and Asli Celikyilmaz and Yejin Choi and Yulong Chen and Y. Liu and Liang Chen and Leyang Cui and Yu Wu and Shujie Liu and Yue-Feng Zhang and J. Devlin and Ming-Wei Chang and Kenton Lee and Leilei Gan and Yating Zhang and Kun Kuang and Shuo Lin Yuan and Changlong Li and Xiaozhong Sun and Liu Fei and Wu and R. Speer and Joshua Chin and Catherine Havasi and Kai Sun and Dian Yu and Jianshu Chen and Dong Yu and Jai Desai and Aaron Wade and Haoran Li and Asli Celikyil-879 maz and Yashar Mehdad and Dragomir R. Radev and Geng Tu and Ji-Rong Wen and Cheng Liu and Dazhi Jiang and A. Stolcke and Lynn Voss and Dilek Peters and John Hakkani-Tur and Benoit Dowding and Raquel Favre and Matthew Fernández and Mike Frampton and Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Llion Uszkoreit and Aidan N Jones and Łukasz Gomez and Kaiser Illia and Polosukhin. 2017 and Attention and S. Welleck and Jason Weston and Arthur Szlam},
  year={2022},
  url={https://www.semanticscholar.org/paper/4e37589fb896d1578ba4282f40c20708079ae8e5}
}

@article{ye2022complementaryexplanations,
  title={Complementary Explanations for Effective In-Context Learning},
  author={Xi Ye and Srini Iyer and Asli Celikyilmaz and Ves Stoyanov and Greg Durrett and Ramakanth Pasunuru},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2211.13892},
  url={https://www.semanticscholar.org/paper/097dc73d5d422b3c09286e72d16b2561ae5fb395},
  abstract={Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.},
  keywords={arxiv:2211.13892}
}

@article{fu2022complexitybasedprompting,
  title={Complexity-Based Prompting for Multi-Step Reasoning},
  author={Yao Fu and Hao-Chun Peng and Ashish Sabharwal and Peter Clark and Tushar Khot},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.00720},
  url={https://www.semanticscholar.org/paper/c88cafa3e980765a64febe369ceb7c2aa7261d2a},
  abstract={We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.},
  keywords={arxiv:2210.00720}
}

@article{li2022composingensembles,
  title={Composing Ensembles of Pre-trained Models via Iterative Consensus},
  author={Shuang Li and Yilun Du and J. Tenenbaum and A. Torralba and Igor Mordatch},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.11522},
  url={https://www.semanticscholar.org/paper/f3a13abf23afecf534c955954d70c3b0fc41d334},
  abstract={Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information, while vision models such as DALL-E can generate photorealistic photos but fail to understand complex language descriptions. In this work, we propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as"generators"or"scorers"and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5\%, without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation. Project page: https://energy-based-model.github.io/composing-pretrained-models.},
  keywords={arxiv:2210.11522}
}

@article{kuculo2022comprehensiveevent,
  title={Comprehensive Event Representations using Event Knowledge Graphs and Natural Language Processing},
  author={Tin Kuculo},
  year={2022},
  booktitle={The Web Conference},
  doi={10.1145/3487553.3524199},
  url={https://www.semanticscholar.org/paper/7fcb917cd4b6d0c77c41b4a1b1dc0c4d1965ba7b},
  abstract={Recent work has utilised knowledge-aware approaches to natural language understanding, question answering, recommendation systems, and other tasks. These approaches rely on well-constructed and large-scale knowledge graphs that can be useful for many downstream applications and empower knowledge-aware models with commonsense reasoning. Such knowledge graphs are constructed through knowledge acquisition tasks such as relation extraction and knowledge graph completion. This work seeks to utilise and build on the growing body of work that uses findings from the field of natural language processing (NLP) to extract knowledge from text and build knowledge graphs. The focus of this research project is on how we can use transformer-based approaches to extract and contextualise event information, matching it to existing ontologies, to build comprehensive knowledge graph-based event representations. Specifically, sub-event extraction is used as a way of creating sub-event-aware event representations. These event representations are then further enriched through fine-grained location extraction and contextualised through the alignment of historically relevant quotes.},
  keywords={arxiv:2303.04794}
}

@article{evtikhov2022computationalexperiment,
  title={Computational experiment – nondimensionalization of equations, computational stability and program testing},
  author={M. G. Evtikhov and V. G. Evtikhov},
  year={2022},
  booktitle={Radioelectronics Nanosystems Information Technologies},
  doi={10.17725/rensit.2022.14.331},
  url={https://www.semanticscholar.org/paper/094d38867c42533f3d61f76b92c0c3b82f54e3fb},
  abstract={From the stages of the computational experiment, the stage of non-dimensionalization of the initial equation (system of equations) of the problem is considered - the replacement of its variables by the product of the corresponding dimensionless quantities by their units of measurement with subsequent transformations. Such a transition from a physical model to a mathematical (dimensionless) one makes it possible to obtain software implementations for research. A critical evaluation of its complexity is carried out and possible errors in the results are evaluated. At the same time, new versions of software are formed. Object-oriented programming tools and version control systems (for example, git) allow you to create versions of software tools adapted to different conditions of their use and for different types of users. Parallelization of work on versions is carried out. At the same time, for further software implementation, the set-theoretic language of formulas with partially recursive functions is effective. To implement versions with large amounts of calculations and data, high-performance computing systems based on software and hardware acceleration, parallel information processing and cloud architectures are used. As a rule, a difference model of the problem and iterative methods for solving it are constructed for a program version. Computational stability conditions are usually stipulated in modern instructions for standard program libraries. For new algorithms, it is necessary to analyze the stability of difference schemes based on the refinement of their spectral properties and the use of functional analysis methods. For storage and subsequent application of the results of computational experiments, it is advisable to use modern databases. As a kind of computational experiment, testing of alpha and beta versions of programs and their releases is also considered.}
}

@article{khuralay2022computersimulation,
  title={Computer Simulation of Intelligent Control Systems for High-Precision Cruise Missiles},
  author={Moldamurat Khuralay and Akhmetov Kayrat Telektesovich and Otegen Alikhan Serikovich and Brimzhanova Saule Serikovna and Otyzbayeva Karlygash Zhalenovna and Zhiyenbek Arailym Oteulievna},
  year={2022},
  booktitle={2022 International Conference on Smart Information Systems and Technologies (SIST)},
  doi={10.1109/SIST54437.2022.9945703},
  url={https://www.semanticscholar.org/paper/84b3bd3fbcf52e2aa7c2595a6880586f12ce8f59}
}

@misc{unknown2022computerverifiedfoundations,
  title={Computer-Verified Foundations of Metaphysics and an Ontology of Natural Numbers in Isabelle/HOL},
  year={2022},
  url={https://www.semanticscholar.org/paper/02ad77812348e299794d5bc83a999090a7ee2139}
}

@article{smith2022constructvldatafree,
  title={ConStruct-VL: Data-Free Continual Structured VL Concepts Learning*},
  author={James Smith and Paola Cascante-Bonilla and Assaf Arbelle and Donghyun Kim and Rameswar Panda and David D. Cox and Diyi Yang and Z. Kira and R. Feris and Leonid Karlinsky},
  year={2022},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52729.2023.01440},
  url={https://www.semanticscholar.org/paper/6b3e939d93c82c269f552e7e2050524c3ad9b73b},
  abstract={Recently, large-scale pre-trained Vision-and-Language (VL) foundation models have demonstrated remarkable capabilities in many zero-shot downstream tasks, achieving competitive results for recognizing objects defined by as little as short text prompts. However, it has also been shown that VL models are still brittle in Structured VL Concept (SVLC) reasoning, such as the ability to recognize object attributes, states, and inter-object relations. This leads to reasoning mistakes, which need to be corrected as they occur by teaching VL models the missing SVLC skills; often this must be done using private data where the issue was found, which naturally leads to a data-free continual (no task-id) VL learning setting. In this work, we introduce the first Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark11Our code is publicly available at https://github.com/jamessealesmith/ConStruct-VL and show it is challenging for many existing data-free CL strategies. We, therefore, propose a data-free method comprised of a new approach of Adversarial Pseudo-Replay (APR) which generates adversarial reminders of past tasks from past task models. To use this method efficiently, we also propose a continual parameter-efficient Layered-LoRA (LaLo) neural architecture allowing no-memory-cost access to all past models at train time. We show this approach outperforms all data-free methods by as much as \~{} 7\% while even matching some levels of experience-replay (prohibitive for applications where data-privacy must be preserved).},
  keywords={arxiv:2211.09790}
}

@article{wagemaker2022concurrentnetkat,
  title={Concurrent NetKAT: Modeling and analyzing stateful, concurrent networks},
  author={J. Wagemaker and Nate Foster and Tobias Kapp'e and D. Kozen and J. Rot and Alexandra Silva},
  year={2022},
  booktitle={European Symposium on Programming},
  doi={10.1007/978-3-030-99336-8_21},
  url={https://www.semanticscholar.org/paper/eb5a72315f84c7234ca6697d327de571f96ffb28},
  abstract={We introduce Concurrent NetKAT (CNetKAT), an extension of NetKAT with operators for specifying and reasoning about concurrency in scenarios where multiple packets interact through state. We provide a model of the language based on partially-ordered multisets (pomsets), which are a well-established mathematical structure for defining the denotational semantics of concurrent languages. We provide a sound and complete axiomatization of this model, and we illustrate the use of CNetKAT through examples. More generally, CNetKAT can be understood as an algebraic framework for reasoning about programs with both local state (in packets) and global state (in a global store).},
  keywords={arxiv:2201.10485}
}

@article{hurst2022connectingsymbolic,
  title={Connecting symbolic fractions to their underlying proportions using iterative partitioning.},
  author={M. Hurst and Jacob R Butts and S. Levine},
  year={2022},
  booktitle={Developmental Psychology},
  doi={10.1037/dev0001384},
  url={https://www.semanticscholar.org/paper/60e87d430f117b616c456cf8f1955926036f128a},
  abstract={Fractions are a challenging mathematics topic for many elementary and middle school students, and even for adults. However, a growing body of developmental research suggests that young children can reason about visually presented proportions, well before fraction instruction, providing insight into how fractions might be introduced to improve learning. We designed a card game to teach first and second grade children (N = 195, including a racially and economically diverse sample from the United States) about fractions in one of three ways. In the Actively Divided condition we iteratively divided an area model into equal-sized units, in the Predivided condition we used an area model with the end-state of the Actively Divided condition, and in the Nondivided condition we used a continuous representation of the fraction magnitude that was not divided into unit-sized parts. Children in the actively divided condition demonstrated larger improvements matching symbolic fractions and visual fractions (i.e., pie charts) than children in the other two conditions. Posthoc analyses of children's gameplay revealed that the actively divided condition may have provided a more optimal level of difficulty for young children than the predivided condition, which was particularly difficult, and the nondivided condition, which was trivially easy. These differences in gameplay performance provide insights into possible mechanisms for our results. We discuss open research questions highlighted by this work and implications of these findings for both the development of proportional reasoning and fraction learning. (PsycInfo Database Record (c) 2022 APA, all rights reserved).}
}

@article{albilali2022constructingarabic,
  title={Constructing Arabic Reading Comprehension Datasets: Arabic WikiReading and KaifLematha},
  author={Eman Albilali and Nora Al-Twairesh and M. Hosny},
  year={2022},
  booktitle={Language Resources and Evaluation},
  doi={10.1007/s10579-022-09577-5},
  url={https://www.semanticscholar.org/paper/169f4557b4b9909c82eb1fb5e621c68763ddec2d}
}

@article{rozora2022constructiongoodnessoffit,
  title={Construction of goodness-of-fit criteria for the type of impulse response function},
  author={I. Rozora and A. Melnyk},
  year={2022},
  booktitle={Science Technology and Innovation},
  doi={10.35668/2520-6524-2022-2-07},
  url={https://www.semanticscholar.org/paper/d01bb60413ff14940ec6af9fdf0a1868d1d2acef},
  abstract={The article is devoted to the study of the impulse response function, its estimation and properties, square-Gaussian random variables and processes, the rate of convergence of the unknown impulse response function, testing the hypothesis about the type of impulse response function, building a simulation model. The study showed that the pulse response function is the output signal of the system during signal processing, when the input signal is a short pulse. In a more general form, the impulse response function describes the response or output of the system as a function of time. Also, the impulse response function is considered a property of linear displacement systems. During the study of the estimation of the impulse response function on orthonormal and trigonometric bases, two conditions A, B and remarks to them were formed, which are used in the future to find different coefficients. The study of square-Gaussian random variables and processes has shown the benefits of using them in relation to the impulse response function. A theorem was also presented, which estimated the probability of a large deviation of the square-Gaussian process in the norm of a continuous function. To study the rate of convergence of the unknown impulse response function in the space of continuous functions and in the space L2, a lemma was formed, as well as a theorem that directly showed the rate of convergence of the impulse response function in the space of continuous functions. Zero and alternative hypotheses were formed. The null hypothesis claimed that the impulse response function existed, and the alternative hypothesis suggested the opposite. To test the hypothesis about the form of the impulse response function, a theorem was used by which a criterion was formed. Visual Studio Community 2022 integrated development environment (C ++ programming language) and Wolfram Mathematica computer algebra system for analytical transformations and numerical calculations were used to build the simulation model, which allowed to make mathematical calculations quite accurately.}
}

@article{zamorski2022continuallearning,
  title={Continual learning on 3D point clouds with random compressed rehearsal},
  author={M. Zamorski and Michal Stypulkowski and Konrad Karanowski and Tomasz Trzciński and Maciej Ziȩba},
  year={2022},
  booktitle={Computer Vision and Image Understanding},
  doi={10.48550/arXiv.2205.08013},
  url={https://www.semanticscholar.org/paper/0fbb0c96dd9bf65e1af877a377bb4c63767906b8},
  abstract={Contemporary deep neural networks offer state-of-the-art results when applied to visual reasoning, e.g., in the context of 3D point cloud data. Point clouds are important datatype for precise modeling of three-dimensional environments, but effective processing of this type of data proves to be challenging. In the world of large, heavily-parameterized network architectures and continuously-streamed data, there is an increasing need for machine learning models that can be trained on additional data. Unfortunately, currently available models cannot fully leverage training on additional data without losing their past knowledge. Combating this phenomenon, called catastrophic forgetting, is one of the main objectives of continual learning. Continual learning for deep neural networks has been an active field of research, primarily in 2D computer vision, natural language processing, reinforcement learning, and robotics. However, in 3D computer vision, there are hardly any continual learning solutions specifically designed to take advantage of point cloud structure. This work proposes a novel neural network architecture capable of continual learning on 3D point cloud data. We utilize point cloud structure properties for preserving a heavily compressed set of past data. By using rehearsal and reconstruction as regularization methods of the learning process, our approach achieves a significant decrease of catastrophic forgetting compared to the existing solutions on several most popular point cloud datasets considering two continual learning settings: when a task is known beforehand, and in the challenging scenario of when task information is unknown to the model.},
  keywords={arxiv:2205.08013}
}

@article{pan2022contrastivelanguageimage,
  title={Contrastive Language-Image Pre-Training with Knowledge Graphs},
  author={Xuran Pan and Tianzhu Ye and Dongchen Han and S. Song and Gao Huang},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2210.08901},
  url={https://www.semanticscholar.org/paper/b3d8233b1d1368ccfe691f3a0cc80d5874439198},
  abstract={Recent years have witnessed the fast development of large-scale pre-training frameworks that can extract multi-modal representations in a unified form and achieve promising performances when transferred to downstream tasks. Nevertheless, existing approaches mainly focus on pre-training with simple image-text pairs, while neglecting the semantic connections between concepts from different modalities. In this paper, we propose a knowledge-based pre-training framework, dubbed Knowledge-CLIP, which injects semantic information into the widely used CLIP model. Through introducing knowledge-based objectives in the pre-training process and utilizing different types of knowledge graphs as training data, our model can semantically align the representations in vision and language with higher quality, and enhance the reasoning ability across scenarios and modalities. Extensive experiments on various vision-language downstream tasks demonstrate the effectiveness of Knowledge-CLIP compared with the original CLIP and competitive baselines.},
  keywords={arxiv:2210.08901}
}

@article{chen2022convfinqaexploring,
  title={ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering},
  author={Zhiyu Chen and SHIYANG LI and Charese Smiley and Zhiqiang Ma and Sameena Shah and William Yang Wang},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.03849},
  url={https://www.semanticscholar.org/paper/d96997265f8146e93b4c9350f19d55e46d1317f0},
  abstract={With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/ConvFinQA.},
  keywords={arxiv:2210.03849}
}

@article{ehberger2022correctionlanguage,
  title={Correction to: “The language of Dirac’s theory of radiation”: the inception and initial reception of a tool for the quantum field theorist},
  author={Markus Ehberger},
  year={2022},
  booktitle={Archive for History of Exact Sciences},
  doi={10.1007/s00407-022-00293-8},
  url={https://www.semanticscholar.org/paper/5bf8381ea5755c176bdd7e3d7cf576554ed7d697},
  abstract={In 1927, Paul Dirac first explicitly introduced the idea that electrodynamical processes can be evaluated by decomposing them into virtual (modern terminology), energy non-conserving subprocesses. This mode of reasoning structured a lot of the perturbative evaluations of quantum electrodynamics during the 1930s. Although the physical picture connected to Feynman diagrams is no longer based on energy non-conserving transitions but on off-shell particles, emission and absorption subprocesses still remain their fundamental constituents. This article will access the introduction and the initial reception of this picture of subsequent transitions (PST) by conceiving of concepts, models, and their representations as tools for the practitioners. I will argue for a multi-factorial explanation of Dirac’s initial, verbally explicit introduction: the mathematical representation he had developed was highly suggestive and already partly conceptualized; Dirac was philosophical flexible enough to talk about transitions when no actual transitions, according to the general interpretation of quantum mechanics of the time, occurred; and, importantly, Dirac eventually used the verbal exposition in the same paper in which he introduced it. The direct impact of PST on the conception of quantum electrodynamical processes will be exemplified by its reflection in diagrammatical representations. The study of the diverging ontological commitments towards PST immediately after its introduction opens up the prehistory of a philosophical debate that stretches out into the present: the dispute about the representational and ontological status of the physical picture connected to the evaluation of the perturbative series of QED and QFT.}
}

@misc{chen2022counterfactualdecoding,
  title={Counterfactual Decoding for Anti-Hallucination Knowledge-grounded Dialogue Generation},
  author={Anthony Chen and Chris DuBois and Sameer Singh},
  year={2022},
  url={https://www.semanticscholar.org/paper/708a9cfc47136377f192f996329d8ff3289280e4}
}

@article{li2022counterfactualreasoning,
  title={Counterfactual reasoning: Do language models need world knowledge for causal understanding?},
  author={Jiaxuan Li and Lang-Chi Yu and Allyson Ettinger},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.03278},
  url={https://www.semanticscholar.org/paper/91a82593721c03ecffdef1c72ea55c6d87c42473},
  abstract={Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.},
  keywords={arxiv:2212.03278}
}

@misc{ignacio2022courseguides,
  title={Course guides 270180 - DCS - Curve and Surface Design},
  author={Rodrigo Ignacio and Silveira Isoba and 10 SILVEIRAISOBA-},
  year={2022},
  url={https://www.semanticscholar.org/paper/ee2f4d01ddee42df906209ec07ae060971148ac0}
}

@article{jonsson2022creativemathematical,
  title={Creative Mathematical Reasoning: Does Need for Cognition Matter?},
  author={B. Jonsson and Julia Mossegård and Johan Lithner and Linnea Karlsson Wirebring},
  year={2022},
  booktitle={Frontiers in Psychology},
  doi={10.3389/fpsyg.2021.797807},
  url={https://www.semanticscholar.org/paper/9f6f01cba1158e6bcb17aaa43070ef3b64c59550},
  abstract={A large portion of mathematics education centers heavily around imitative reasoning and rote learning, raising concerns about students’ lack of deeper and conceptual understanding of mathematics. To address these concerns, there has been a growing focus on students learning and teachers teaching methods that aim to enhance conceptual understanding and problem-solving skills. One suggestion is allowing students to construct their own solution methods using creative mathematical reasoning (CMR), a method that in previous studies has been contrasted against algorithmic reasoning (AR) with positive effects on test tasks. Although previous studies have evaluated the effects of CMR, they have ignored if and to what extent intrinsic cognitive motivation play a role. This study investigated the effects of intrinsic cognitive motivation to engage in cognitive strenuous mathematical tasks, operationalized through Need for Cognition (NFC), and working memory capacity (WMC). Two independent groups, consisting of upper secondary students (N = 137, mean age 17.13, SD = 0.62, 63 boys and 74 girls), practiced non-routine mathematical problem solving with CMR and AR tasks and were tested 1 week later. An initial t-test confirmed that the CMR group outperformed the AR group. Structural equation modeling revealed that NFC was a significant predictor of math performance for the CMR group but not for the AR group. The results also showed that WMC was a strong predictor of math performance independent of group. These results are discussed in terms of allowing for time and opportunities for struggle with constructing own solution methods using CMR, thereby enhancing students conceptual understanding.}
}

@article{kumar2022criticalanalysis,
  title={Critical Analysis of Big Data Applications using Functional Linguistics and Diversified Integration},
  author={D. Kumar and Srinivasa Rao and R. Vijaya and Kumar Reddy and D. Kumar and D. Sai and Assoc.Professor},
  year={2022},
  booktitle={2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)},
  doi={10.1109/ICAAIC53929.2022.9792589},
  url={https://www.semanticscholar.org/paper/d0d2a1b809cf3a78f3439b89b39ce5f11e571664},
  abstract={In recent times, the top-down use of big information data analysis and the mechanical manipulation of man-made intellectual abilities has provided the center with specialized means to advance the practical coordination of semantic structure. Potentially related to the great information research of artificial reasoning (AI), the various embodiments of utilitarian phonetics have routinely experienced problems in applications. As a result, it created critical difficulties for the far-reaching improvement of useful etymology. In this unique situation, ideas related to the improvement of useful phonetics and artificial reasoning are explained. Starting from this premise, the useful etymologies of innovative union and internal incorporation are analyzed with respect to the scenario of large-scale information AI research. In addition, in order to stimulate the useful advancement of phonetics inside and outside, this study makes some proposals, among which are the rationalization of a dispersed group climate, the promotion of a practical phase of information about the language, the assumption of models, modalities of equality of information and the transmission of an equitable preparation of the brain network. These ideas constitute an initial phase of hypothetical etymological analyzes and capacity for improvement. In addition, some ideas were made to advance the internal and external improvement of the utilitarian etymology, such as structuring a useful linguistic information phase, updating an adequate group climate, preparing the circulating brain network, assuming equal information patterns and modalities to provide a reference to skills development and hypothetical phonetic exploration.}
}

@misc{wolf2022crosslingualspeaker,
  title={Cross-Lingual Speaker Identification from Weak Local Evidence},
  author={Thomas Wolf and Lysandre Debut and Julien Victor Sanh and Clement Chaumond and Anthony Delangue and Pier-339 Moi and Clara ric Cistac and Yacine Ma and Julien Jernite and Plu and Teven Xu and Sylvain Le Scao and Gugger and Mariama and Quentin Drame and M. LhoestAlexander and Rush and Michael Miller Yoder and Sopan Khosla and Qinlan Shen and Ben Zhou and Qiang Ning and Daniel Khashabi and Kyle Richardson and Tushar Khot},
  year={2022},
  url={https://www.semanticscholar.org/paper/385b71fb56b54b019b855ff0265bbdbb01ad01ea}
}

@article{yu2022crunchqasynthetic,
  title={CrunchQA: A Synthetic Dataset for Question Answering over Crunchbase Knowledge Graph},
  author={Lifan Yu and Nadya Abdel Madjid and D. Difallah},
  year={2022},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)},
  doi={10.1109/BigData55660.2022.10021012},
  url={https://www.semanticscholar.org/paper/dcb17d546ce8aec11174bafad2fb3d913e6b2e98},
  abstract={The digital transformation in the finance and enterprise sector has been driven by the advances made in big data and artificial intelligence technologies. For instance, data integration enables businesses to make better decisions by consolidating and mining heterogeneous data repositories. In particular, knowledge graphs (KGs) are used to facilitate the integration of disparate data sources and can be utilized to answer complex queries. This work proposes a new dataset for question-answering on knowledge graphs (KGQA) to reflect the challenges we identified in real-world applications which are not covered by existing benchmarks, namely, multi-hop constraints, numeric and literal embeddings, ranking, reification, and hyper-relations. To build the dataset, we create a new Knowledge Graph from the Crunchbase database using a lightweight schema to support high-quality entity embeddings in large graphs. Next, we create a Question Answering dataset based on natural language question generation using predefined multiple-hop templates and paraphrasing. Finally, we conduct extensive experiments with state-of-the-art KGQA models and compare their performance on CrunchQA. The results show that the existing models do not perform well, for example, on multi-hop constrained queries. Hence, CrunchQA can be used as a challenging benchmark dataset for future KGQA reasoning models. The dataset and scripts are available on the project repository. 1}
}

@article{chen2022curriculumbroadcoverage,
  title={Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding},
  author={Zeming Chen and Qiyue Gao},
  year={2022},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2204.06283},
  url={https://www.semanticscholar.org/paper/32c6607346e0bbe21844275f55fb368bbffd4699},
  abstract={In the age of large transformer language models, linguistic evaluation play an important role in diagnosing models’ abilities and limitations on natural language understanding. However, current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets. In this paper, we introduce Curriculum as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. Curriculum contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. We show that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality. In addition, our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models that may encourage future research on re-designing datasets, model architectures, and learning objectives.},
  keywords={arxiv:2204.06283}
}

@article{cho2022dallevalprobing,
  title={DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers},
  author={Jaemin Cho and Abhaysinh Zala and Mohit Bansal},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/804b27dc02becf7bbbd89ba949e1e07e8677c459}
}

@article{nuraina2022desainbahan,
  title={DESAIN BAHAN AJAR BERBASIS AKTIVITAS PENALARAN MATEMATIS MENGGUNAKAN MODEL MISSOURI MATHEMATIC PROJECT MATA KULIAH ANALISIS KOMPLEKS},
  author={Nuraina Nuraina and Muliana - Muliana and M. Mursalin and Mila Kartika Sari Bangun and U. Rahayu},
  year={2022},
  booktitle={Numeracy},
  doi={10.46244/numeracy.v9i2.1891},
  url={https://www.semanticscholar.org/paper/24f768f302b2a18082d523c11bc1c60a89fa76db},
  abstract={The failure of students in studying complex analysis courses is based on student learning habits that only focus on memorizing concepts from the material studied without understanding it properly, and lack of motivation to repeat the material that has been studied. The teaching materials used in this course are textbooks. However, the textbook used still does not contain activities for students' mathematical reasoning abilities, students cannot learn independently from the book, because the material presented is difficult to understand, even lecturers who teach this complex analysis course have to redesign the material, writing in written form. hand is then given to the student. One effort that could be to improve students' mathematical reasoning abilities is to facilitate learning resources with supporting teaching materials. In this study, teaching materials will be designed based on mathematical reasoning activities using the Missauri Mathematical Project (MMP) learning model. The research method used in this research is the Analysis, Design, Development, Implementation, and Evaluation (ADDIE) development model. The development of complex analysis textbooks based on reasoning activities using the Missauri Mathematical Project learning model is feasible to be developed with the percentage of assessment by media expert validators obtained an average score of 91.79\% in the "very valid" categories, and media expert validators obtained the average score. a score of 88.62\% with the "very valid" categories. The results of the small group validation obtained a score of 83.5\% with the "very valid" criteria. The results of the large group trial obtained a score of 85.5\% with the "very practical" criteria. 
Abstrak 
Salah satu kendala dalam mata kuliah analisis kompleks adalah kegagalan mahasiswa dalam mempelajari materi yang disebabkan oleh kebiasaan belajar mahasiswa yang hanya terfokus untuk menghafal konsep dari materi yang diajarkan tanpa memahaminya secara mendalam, serta kurangnya motivasi untuk mengkaji ulang materi yang telah dipelajari. Sumber utama yang digunakan sebagai bahan ajar pada mata kuliah ini yaitu buku paket. Namun, buku paket yang digunakan masih belum memuat  aktivitas kemampuan penalaran matematis mahasiswa, mahasiswa tidak bisa belajar secara mandiri dari buku tersebut, karena materi yang disajikan sulit untuk dipahami, bahkan dosen  yang  mengajarkan  mata  kuliah  analisis  kompleks  ini  harus  mendesain  ulang materinya,  menulis  dengan  tulisan  tangan  kemudian  diberikan  kepada  mahasiswa. Salah satu usaha yang bisa dilakukan untuk meningkatkan kemampuan penalaran matematis mahasiswa adalah dengan memfasilitasi sumber belajar dengan bahan ajar yang mendukung. Dalam penelitian pengembangan ini, bahan ajar akan disusun dengan berbasis aktivitas penalaran matematis menggunakan model pembelajaran missauri mathematic project. Metode penelitian yang dipakai dalam penelitian ini yaitu model pengembangan  analysis, design, development, implementation, and evaluation. Pengembangan buku ajar analisis kompleks berbasis aktivitas penlaran dengan menggunakan model pembelajaran missauri mathematic project layak dikembangkan dengan persentase penilaian oleh validator ahli media diperoleh skor rata-rata sebesar 91,79\% dengan kriteria “sangat valid”, dan validator ahli media didapat hasil skor rata-rata sebesar 88,62\% dengan kriteria “sangat valid”. Hasil validasi kelompok kecil didapat nilai sebesar 83,5\% dengan kriteria “sangat valid”. Hasil uji coba kelompok besar didapat nilai sebesar 85,5\%  dengan kriteria “sangat praktis”.}
}

@article{liu2022deplotoneshot,
  title={DePlot: One-shot visual language reasoning by plot-to-table translation},
  author={Fangyu Liu and Julian Martin Eisenschlos and Francesco Piccinno and Syrine Krichene and Chenxi Pang and Kenton Lee and Mandar Joshi and Wenhu Chen and Nigel Collier and Y. Altun},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10505},
  url={https://www.semanticscholar.org/paper/4d3a49d1439a0b8fbb0e9f588970ad0f1d70dec8},
  abstract={Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than>28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0\% improvement over finetuned SOTA on human-written queries from the task of chart QA.},
  keywords={arxiv:2212.10505}
}

@article{tian2022debiasingmodels,
  title={Debiasing NLU Models via Causal Intervention and Counterfactual Reasoning},
  author={Bing Tian and Yixin Cao and Yong Zhang and Chunxiao Xing},
  year={2022},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.1609/aaai.v36i10.21389},
  url={https://www.semanticscholar.org/paper/d1eb051c6b13eba8a9b333d5ee0a55250717195d},
  abstract={Recent studies have shown that strong Natural Language Understanding (NLU) models are prone to relying on annotation biases of the datasets as a shortcut, which goes against the underlying mechanisms of the task of interest. To reduce such biases, several recent works introduce debiasing methods to regularize the training process of targeted NLU models. In this paper, we provide a new perspective with causal inference to find out the bias. On one hand, we show that there is an unobserved confounder for the natural language utterances and their respective classes, leading to spurious correlations from training data. To remove such confounder, the backdoor adjustment with causal intervention is utilized to find the true causal effect, which makes the training process fundamentally different from the traditional likelihood estimation. On the other hand, in inference process, we formulate the bias as the direct causal effect and remove it by pursuing the indirect causal effect with counterfactual reasoning. We conduct experiments on large-scale natural language inference and fact verification benchmarks, evaluating on bias sensitive datasets that are specifically designed to assess the robustness of models against known biases in the training data. Experimental results show that our proposed debiasing framework outperforms previous state-of-the-art debiasing methods while maintaining the original in-distribution performance.}
}

@article{khot2022decomposedprompting,
  title={Decomposed Prompting: A Modular Approach for Solving Complex Tasks},
  author={Tushar Khot and H. Trivedi and Matthew Finlayson and Yao Fu and Kyle Richardson and Peter Clark and Ashish Sabharwal},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.02406},
  url={https://www.semanticscholar.org/paper/07955e96cbd778d0ae2a68f09d073b866dd84c2a},
  abstract={Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.},
  keywords={arxiv:2210.02406}
}

@article{rasal2022deepstructural,
  title={Deep Structural Causal Shape Models},
  author={Rajat Rasal and Daniel Coelho de Castro and Nick Pawlowski and Ben Glocker},
  year={2022},
  booktitle={ECCV Workshops},
  doi={10.48550/arXiv.2208.10950},
  url={https://www.semanticscholar.org/paper/8975f550eed321c203d3990692f82e3f7b112b8f},
  abstract={Causal reasoning provides a language to ask important interventional and counterfactual questions beyond purely statistical association. In medical imaging, for example, we may want to study the causal effect of genetic, environmental, or lifestyle factors on the normal and pathological variation of anatomical phenotypes. However, while anatomical shape models of 3D surface meshes, extracted from automated image segmentation, can be reliably constructed, there is a lack of computational tooling to enable causal reasoning about morphological variations. To tackle this problem, we propose deep structural causal shape models (CSMs), which utilise high-quality mesh generation techniques, from geometric deep learning, within the expressive framework of deep structural causal models. CSMs enable subject-specific prognoses through counterfactual mesh generation ("How would this patient's brain structure change if they were ten years older?"), which is in contrast to most current works on purely population-level statistical shape modelling. We demonstrate the capabilities of CSMs at all levels of Pearl's causal hierarchy through a number of qualitative and quantitative experiments leveraging a large dataset of 3D brain structures.},
  keywords={arxiv:2208.10950}
}

@article{hodge2022designplanning,
  title={Design and Planning of a Transdisciplinary Investigation into Farmland Pollinators: Rationale, Co-Design, and Lessons Learned},
  author={S. Hodge and O. Schweiger and A. Klein and S. Potts and Cecilia Costa and M. Albrecht and J. D. de Miranda and M. Mand and P. De la Rúa and M. Rundlöf and Eleanor Attridge and R. Dean and P. Bulet and D. Michez and R. Paxton and A. Babin and N. Cougoule and M. Laurent and Anne-Claire Martel and Laurianne Paris and M. Rivière and E. Dubois and M. Chauzat and K. Arafah and Dalel Askri and S. Voisin and T. Kiljanek and Irene Bottero and Christophe Dominik and Giovanni Tamburini and M. Pereira-Peixoto and Dimitry Wintermantel and T. Breeze and E. Cini and D. Senapathi and G. Di Prisco and P. Mędrzycki and S. Hagenbucher and A. Knauer and Janine M. Schwarz and Risto Raimets and Vicente Martínez-López and K. Ivarsson and C. Hartfield and P. Hunter and Mark Brown and J C Stout},
  year={2022},
  booktitle={Sustainability},
  doi={10.3390/su141710549},
  url={https://www.semanticscholar.org/paper/400a8763ed493d109f63a7ca7529811630839d8d},
  abstract={To provide a complete portrayal of the multiple factors negatively impacting insects in agricultural landscapes it is necessary to assess the concurrent incidence, magnitude, and interactions among multiple stressors over substantial biogeographical scales. Trans-national ecological field investigations with wide-ranging stakeholders typically encounter numerous challenges during the design planning stages, not least that the scientific soundness of a spatially replicated study design must account for the substantial geographic and climatic variation among distant sites. ‘PoshBee’ (Pan-European assessment, monitoring, and mitigation of Stressors on the Health of Bees) is a multi-partner transdisciplinary agroecological project established to investigate the suite of stressors typically encountered by pollinating insects in European agricultural landscapes. To do this, PoshBee established a network of 128 study sites across eight European countries and collected over 50 measurements and samples relating to the nutritional, toxicological, pathogenic, and landscape components of the bees’ environment. This paper describes the development process, rationale, and end-result of each aspect of the of the PoshBee field investigation. We describe the main issues and challenges encountered during the design stages and highlight a number of actions or processes that may benefit other multi-partner research consortia planning similar large-scale studies. It was soon identified that in a multi-component study design process, the development of interaction and communication networks involving all collaborators and stakeholders requires considerable time and resources. It was also necessary at each planning stage to be mindful of the needs and objectives of all stakeholders and partners, and further challenges inevitably arose when practical limitations, such as time restrictions and labour constraints, were superimposed upon prototype study designs. To promote clarity for all stakeholders, for each sub-component of the study, there should be a clear record of the rationale and reasoning that outlines how the final design transpired, what compromises were made, and how the requirements of different stakeholders were accomplished. Ultimately, multi-national agroecological field studies such as PoshBee benefit greatly from the involvement of diverse stakeholders and partners, ranging from field ecologists, project managers, policy legislators, mathematical modelers, and farmer organisations. While the execution of the study highlighted the advantages and benefits of large-scale transdisciplinary projects, the long planning period emphasized the need to formally describe a design framework that could facilitate the design process of future multi-partner collaborations.}
}

@article{li2022designsimulation,
  title={Design and Simulation Application of Fuzzy Controller Based on Granular Computing},
  author={Huiyue Li and Jianhua Yang and Wei Lu},
  year={2022},
  booktitle={International Conference on Computer and Information Application},
  doi={10.1109/iccia55271.2022.9828430},
  url={https://www.semanticscholar.org/paper/c54f2d59dca66c50ddc4e8f745f76ac659a1ac6f},
  abstract={Fuzzy control theory generates fuzzy rules based on expert experience and experimental data, so fuzzy control method can control complex and large-scale systems without precise mathematical models. But fuzzy control method still has problems: complex structure and rule explosion problem. Aiming at the above problems, this paper proposes a fuzzy control method based on granular computing. Firstly, design the fuzzy controller, the fuzzy rules are generated by the fuzzy space division method. Then, using the information granulation method, each fuzzy rule is granulated into an information granule. Taking points in the information granules to fit the realization function of the granular function, using that function control the object instead of the fuzzy controller. The fuzzy reasoning process is omitted, and the number of rules has nothing to do with the accuracy, it is only related to the number of granules. Therefore, the structure of the control system can be simplified under the condition of ensuring the accuracy of the system, and the problem of rule explosion can be avoided at the same time. The simulation experiment using the second-order inverted pendulum as the control object proves the feasibility and effectiveness of the fuzzy control method based on granular computing.}
}

@article{tsukanov2022designcircular,
  title={Design of circular air intakes for subsonic turbofans},
  author={Ruslan Tsukanov},
  year={2022},
  booktitle={Aerospace technic and technology},
  doi={10.32620/aktt.2022.4.01},
  url={https://www.semanticscholar.org/paper/e3b0d513d05ced7c6323462e74e858db318a2247},
  abstract={The subject matter of this article is the process of subsonic air intake shaping for high-bypass ratio turbofan at the airplane preliminarily designing stage. The goal was to improve a mathematical model of V. I. Polikovskii method of subsonic air intake shaping for high-bypass ratio turbofan. The tasks are to consider the presence of cant of inlet cross-section, required to perform effective operation at airplane cruising angle-of-attacks; to increase the radius of curvature of the air intake lip to provide air flow near it without flow separation, which was definitely determined and could not be increased in the existing method; to improve constant length velocity gradient law (used in this method) so that too large duct expansion angles near the air intake outlet cross-section can be avoided; to consider the engine inlet spinner presence. The methods used are analytical and digital mathematical methods, implemented in MathCAD and Microsoft Visual Studio systems. The following results were obtained: based on the proposed method, new calculation module for the Power Unit software version 11.8 has been developed (С-language Win32 UNICODE application) with a friendly user interface. Conclusions. The scientific novelty of the results obtained is as follows: 1) mathematical model (algorithm and its program implementation) for circular turbofan air intake shaping has been improved considering cant of the inlet cross-section, air intake lip rounding with two radiuses, presence of engine inlet spinner, and zero expansion angles in the diffuser outlet cross-section; 2) adequacy of calculation results using the improved mathematical model is shown using comparison with shapes of circular turbofan air intakes, developed by the leading aviation companies.}
}

@article{heru2022designsupplementary,
  title={Design of supplementary mathematics module for preparation of minimum competency assessment for fifth grade elementary school students},
  author={Heru Heru and R. E. Yuliani and Izah Zulpah},
  year={2022},
  booktitle={Jurnal Math Educator Nusantara Wahana Publikasi Karya Tulis Ilmiah di Bidang Pendidikan Matematika},
  doi={10.29407/jmen.v8i1.17682},
  url={https://www.semanticscholar.org/paper/11fa440d51f0257ebeb2b6e7e08d75b6d6c83530},
  abstract={Students' skills in solving numeracy questions in the AKM are still varied, especially for SD Negeri 3 Mendo Barat students. The implementation of AKM, especially numeracy skills, impacts the learning given to students. Students are not only required to understand mathematical concepts but must demonstrate other mathematical abilities such as reasoning and problem-solving abilities. Therefore, teaching materials are needed that can support the AKM-based learning process. At SD Negeri 3 Mendo Barat, there are still very few teaching materials owned by students, especially those based on AKM. This study aims to develop a companion mathematics module for preparing AKM for fifth-grade elementary school students that is valid, practical, and potentially affects learning outcomes. Development research using a 4-D model is used as a research technique. The research subjects were 22 students of class V SD Negeri 3 Mendo Barat, as many as 22 people. The results showed that the mathematics module was valid and practical—valid seen from the experts' assessment of the module. Experts consist of material, media, and language experts. Practicality is obtained from the results of student questionnaire analysis in a limited field test which shows a practicality percentage of 91.32 per cent. Based on the operational field test results, the average final score of students is 87.05, which indicates that student learning outcomes are in the very good category. The module can potentially affect student learning outcomes in the good category.}
}

@article{zoph2022designingeffective,
  title={Designing Effective Sparse Expert Models},
  author={Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and J. Dean and Noam M. Shazeer and W. Fedus},
  year={2022},
  booktitle={IEEE International Symposium on Parallel \& Distributed Processing, Workshops and Phd Forum},
  doi={10.1109/IPDPSW55747.2022.00171},
  url={https://www.semanticscholar.org/paper/e47da75675b9a3fe02ef1efadca39bc8cdfcdc17},
  abstract={Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).}
}

@article{albrecht2022despitesuperhuman,
  title={Despite "super-human" performance, current LLMs are unsuited for decisions about ethics and safety},
  author={Joshua Albrecht and Ellie Kitanidis and Abraham J. Fetterman},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.06295},
  url={https://www.semanticscholar.org/paper/7d5175db1b99552491063d2d9581b0b51e1d2932},
  abstract={Large language models (LLMs) have exploded in popularity in the past few years and have achieved undeniably impressive results on benchmarks as varied as question answering and text summarization. We provide a simple new prompting strategy that leads to yet another supposedly"super-human"result, this time outperforming humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset). Unfortunately, we find that relying on average performance to judge capabilities can be highly misleading. LLM errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label. We also observe signs of inverse scaling with model size on some examples, and show that prompting models to"explain their reasoning"often leads to alarming justifications of unethical actions. Our results highlight how human-like performance does not necessarily imply human-like understanding or reasoning.},
  keywords={arxiv:2212.06295}
}

@article{khokhlova2022developmentalgorithm,
  title={Development of an Algorithm to Analyze Vacancies in the Labor Market Based on Open-Source Data},
  author={O. Khokhlova and A. N. Khokhlova and A. Choyzhalsanova},
  year={2022},
  booktitle={Voprosy statistiki},
  doi={10.34023/2313-6383-2022-29-4-33-41},
  url={https://www.semanticscholar.org/paper/e58a983ea50412432b36dfe9c4c8b935e1d57c3d},
  abstract={In the introductory part of the article, the authors substantiate the relevance of developing methodological tools for analyzing job vacancies in the labor market in the context of the modern technological revolution, which significantly increases requirements for professional knowledge and experience of working personnel and changes the ratio between traditional and new professions.To assess the current situation on the labor market and the demand for currently existing professions, the main section of the published results of the study presents the algorithm for analyzing vacancies using large data arrays from open sources using mathematical and statistical tools and machine learning methods using the Python programming language and the IBM SPSS modeler analytical platform. The algorithm includes: parsing data on vacancies, analyzing vacancies by the main criteria, clustering vacancies by salary level and building a neural network model – a multilayer perceptron of the dependence of salary on a number of predictors. It should be noted that the developed algorithm is universal, because it can be used to analyze big data from any open source at a certain point in time.The results of the analysis will allow researchers and specialists of management structures to more realistically assess the current situation on the labor market, educational institutions will be able to adjust training programs in accordance with the modern requirements of employers, employers will make decisions on the development of competencies in their field of activity and conduct a comparative analysis of demanded vacancies in terms of quantitative and qualitative characteristics, and for the applicant it will be easier to see the demand for vacancies in the labor market and develop new skills.}
}

@article{tekin2022developmentattitude,
  title={Development of an Attitude Scale for Moral Literacy Skills: Validity and Reliability Study},
  author={İshak Tekin},
  year={2022},
  booktitle={Marife Dini Araştırmalar Dergisi},
  doi={10.33420/marife.1104203},
  url={https://www.semanticscholar.org/paper/eef5d97b3b2d750009d76c0b814a7bbb4cfe6964},
  abstract={In recent years, some changes have emerged in the general aims of education programs, and in this context, educational authorities have adopted new approaches aiming at nurturing skills. Especially in moral education, which is an indispensable part of citizenship education, the concept of moral literacy is one of the important isssue of educational discussions. In the literature, there are many studies asserting that schools should educate children for moral literacy as well as primitive language literacy, science and mathematics literacy, intercultural literacy. On the other hand, it should be noted that there are not enough studies dealing with the application of the subject in the field. In addition, it is seen that countries have not yet taken sufficient steps for a change in their educational systems and have not demonstrated a strong will. 
Moral literacy refers to a skill that includes thinking about one's own moral values, determining the possible consequences of various alternatives and their effects, making logical decisions about which option is compatible with one's values, acting in line with one's values, and taking responsibility for one's own actions. Moral literacy, conceptualized by Nancy Tuana, consists of three basic elements, namely moral moral sensitivity, moral reasoning skills, and moral imagination, and three sub-skills under them. These skills generally include the intellectual skills that lead the moral decision-making process. 
This study aimed to develop a valid and reliable attitude scale based on Tuana's theory. Within the scope of the research designed in the general survey model, the relevant literature was scanned and the studies on moral literacy were examined. With the clarification of the theoretical structure, the scale development process was started. The following stages were followed in the development of the moral literacy scale: i. Creating the item pool, ii. Content validaty, iii. Reviewing and finalizing the draft form, iv. Application of the scale, v. Item analysis, analysis of construct validity and reliability analysis, vi. Examining the correlation between subscales and the total score, vii. Putting into the final form of the scale and reporting. 
In the first stage, the draft form consisting of 27 items was sent to five field experts and a language expert, 1 item was removed from the scale as they were not compatible with the scope in line with the suggestions of the field experts, and 1 more item was added. The draft form, which was finalized with changes, was applied to 653 university students studying at Eskişehir Osmangazi University and Anadolu University in the fall semester of the 2018-2019 academic year. 88 data, which were not marked carefully and were not found reliable during the data entry to excel, were excluded from the analysis. In the item analysis of the scale, 3 items were excluded from the draft scale and the construct validity was passed. 
In the exploratory factor analysis, 7 more items with values less than .40 and overlapping were excluded from the analysis. As a result of the factor analysis performed after this process, a structure consisting of 5 factors and 20 items was obtained. As a result of the reliability analysis, it was seen that the whole scale had high reliability, while the sub-dimensions had medium and low reliability levels. It was also determined that the structure obtained by exploratory factor analysis was confirmed as a result of testing with confirmatory factor analysis. According to the path analysis, it can be said that the existing structure fits well. When the relationship between the sub-dimensions of the scale is examined, it can be stated that there is a significant positive relationship between the sub-dimensions and a total score can be obtained for the attitudes towards the moral literacy skill of the scale. As a result, it can be said that the scale obtained as a result of these processes can measure the attitudes of university students towards moral literacy skills in a reliable and valid way.}
}

@article{ziborov2022developmentselflearning,
  title={Development of self-learning intelligent decision support system to control of steel production technological processes},
  author={I. Ziborov and T. Zheldak},
  year={2022},
  booktitle={Systems and Technologies},
  doi={10.34185/1562-9945-3-140-2022-04},
  url={https://www.semanticscholar.org/paper/4c4ca4d83f47ba86df0706bd0271f82a1a879316},
  abstract={Taking to the consideration the current state of converter production and measuring equipment at Ukrainian enterprises, it follows that the smelting process is based on a complex dynamic non-deterministic system. The process is complicated by the large number of param-eters, the inability to accurately identify the state of the system at any time, as well as the dif-ficulty of forecasting system requirements. Preliminary analysis has shown that in the conditions of this production converter manufacturing efficiency increase can be reached at the expense of: - reducing the cost of raw materials, such as iron-containing additives, deoxidizers, non-metallic elements in steel; - reduction of melting time, especially blowing time; - reducing defects and improving product quality. It is proposed the architecture of integrated control DSS in converter steel production based on the principle of minimal interference in the production process. The primary aim of such a system is to predict the behavior of the production process, providing the recommen-dations for its impact in order to optimize the external criterion of efficiency. The source and amount of data required for the database formation and DSS knowledge base are substantiated. The mechanism of self-learning in the course of technological tasks is described. The structural scheme of self-learning DSS, self-learning algorithm, which is mainly featured with modularity, is offered in the paper. The approach allows testing of any number of existing algorithms for learning, forecasting and optimization in order to further select the most effective ones, modifies the system in the future and allows the parallel use of a number of com-peting algorithms. The operator has the opportunity to choose as a control solution one of the proposed systems, or the formation of its own, better by a certain external criterion of result quality. Based on the suggested software structure, a number of tasks are formulated that need to be performed to build a decision support system. It is also considered to apply the mathematical apparatus of fuzzy sets to describe certain pa-rameters of the technological process and quality criteria, fuzzy neural network for modeling reasoning processes and the choice of algorithm for its training.}
}

@article{li2022differentiablereasoning,
  title={Differentiable Reasoning over Long Stories - Assessing Systematic Generalisation in Neural Models},
  author={Wanshui Li and Pasquale Minervini},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2203.10620},
  url={https://www.semanticscholar.org/paper/59494dcb572cebb577a1bcb2d6f87dfca93d6591},
  abstract={Contemporary neural networks have achieved a series of developments and successes in many aspects; however, when exposed to data outside the training distribution, they may fail to predict correct answers. In this work, we were concerned about this generalisation issue and thus analysed a broad set of models systematically and robustly over long stories. Related experiments were conducted based on the CLUTRR, which is a diagnostic benchmark suite that can analyse generalisation of natural language understanding (NLU) systems by training over small story graphs and testing on larger ones. In order to handle the multi-relational story graph, we consider two classes of neural models:"E-GNN", the graph-based models that can process graph-structured data and consider the edge attributes simultaneously; and"L-Graph", the sequence-based models which can process linearized version of the graphs. We performed an extensive empirical evaluation, and we found that the modified recurrent neural network yield surprisingly accurate results across every systematic generalisation tasks which outperform the modified graph neural network, while the latter produced more robust models.},
  keywords={arxiv:2203.10620}
}

@misc{aralikatte2022discursivesocratic,
  title={Discursive Socratic Questioning: (Unsupervised) Interpreting Neural Language Models for Discourse Understanding},
  author={Rahul Aralikatte and Matthew Lamm and Daniel Hardt and Regina Barzilay and Mirella Lapata. 2008 and Modeling and Yonatan Belinkov and Sebastian Gehrmann and Ayal Klein and Eran Hirsch and Ron Eliav and Valentina Pyatkin and J. Mamou and Wei-Jen Ko and Cutter Dalton and Mark Simmons and Greg Fisher and Durrett Junyi and Jessy Li and Dis-737 and Yating Wu and Dananjay Srini-740 and Meichun Webber and Tat-Seng Liu and F. ChuaNancy and 746 and Wenqiang Lei and Yuanxin Xiang and Qian Yuwei Wang and Meichun Zhong and Liu Min-Yen and Kan and Lin-753 and Belinda Z. Li and Maxwell I. Nye and Hwee Ziheng Lin and Tou Ng and Min-Yen Kan and Au-766},
  year={2022},
  url={https://www.semanticscholar.org/paper/76e5069425547d4f53b5aa843a765a305b7fa470}
}

@article{shridhar2022distillingmultistep,
  title={Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions},
  author={Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.00193},
  url={https://www.semanticscholar.org/paper/72123a86eae2cb5c4eae8650f43524039d48875d}
}

@article{shridhar2022distillingreasoning,
  title={Distilling Reasoning Capabilities into Smaller Language Models},
  author={Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2023.findings-acl.441},
  url={https://www.semanticscholar.org/paper/8fd462f6248d5e3f1b6602697c09489086b5655f},
  abstract={Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70\% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM},
  keywords={arxiv:2212.00193}
}

@article{imberti2022divinginto,
  title={Diving into the Deep End: Machine Learning for the Chemist},
  author={S. Imberti},
  year={2022},
  booktitle={ACS Omega},
  doi={10.1021/acsomega.2c04373},
  url={https://www.semanticscholar.org/paper/7c838fcffb45a6c191130627911ef50c5795e9d3},
  abstract={I the past year, ACS Omega has seen a dramatic increase in the number of articles published with an Artificial Intelligence (AI) or Machine Learning, Deep Learning, Neural Networks theme. In 2021, 105 articles were published vs 45 articles published in the previous year, an increase of 133\%. This exceptional growth for a fully open access broad scope journal pairs with the growth seen at many other journals in the ACS portfolio. Interestingly, the growth registered in the past 2−3 years is not confined to the journals that specialize in chemical informatics: the Journal of Chemical Information and Modeling, the Journal of Chemical Theory and Computation, and, to some extent, the Journal of Physical Chemistry C. It also encompasses journals in the materials sciences, the physical sciences, measurement science, chemical engineering, and environmental science in the broader ACS portfolio (Figure 1). Looking at the wider publication landscape, the Directory of Open Access Journal lists 65 journal entries for scientific publications that pertain to the topic of AI. Eleven of these were added in the last year alone, and this includes only those journals queried in the computational science category. In addition to these, numerous other open access, broader scope journals also publish work without any perceived evaluation of immediate impact and where existing AI tools have been successfully applied to a variety of chemistry questions. Over the past 10 to 15 years, AI, especially deep learning, has effected dramatic technological progress and proven success in areas such as computer vision, speech recognition, natural language processing, common sense knowledge, strategic reasoning, and robotics. Exceptional results have also been reported in the medical sciences; for example, deep neural networks facilitated accurate diagnosis of skin cancer, and deep learning enabled extraction of new knowledge from old data, enabling accurate prediction of age, gender, smoking status, blood pressure, and heart attack propensity of individuals just by analyzing previously acquired retinal images. But, what about the status of AI and its perception in chemistry? In the ensuing text, as an entreé to the associated Virtual Issue, I will present a brief overview of the perceived usefulness of AI at this time in some fields of the chemical sciences and related areas, based on recently published reviews and perspectives by experts in the area, as well as other resources. A review by Baum et al. charted the growth and distribution of AI-related chemistry publications in the last two decades using the CAS Content Collection, which includes patents as well as research articles. In their paper, they refer to the “Hype Cycle of Emerging Technologies”, and from the data gathered, they determine that AI adoption in life sciences and analytical chemistry has navigated the so-called “peak of inflated expectations” and “trough of disillusionment” and successfully progressed to the “plateau of productivity”. It is common to overestimate the effect of a technology in the short run and underestimate it in the long run (anecdotally known as Amara’s law). For example, despite commercial interests and consequent investments being enormous, to this day, no new drug has yet been synthesized using AI. As another (counter) example, Peiretti and Brunel, in their Perspective published in 2018, ponder whether organic chemistry and in particular retrosynthesis might be the ideal next application of AI techniques. After all, it “f its perfectly” the definition of AI as a problem with “complex input−output relationships [that] are dif f icult or impractical to model procedurally”. However, Baum et al. firmly assess that “there are still areas of Chemistry like organic synthetic chemistry where AI is yet to make an impact”. Still, work is in progress, as standardized formats for reporting a chemical synthesis procedure are being developed and even classified (an essential step for scientific fields to exist) in the new taxonomy of digital chemistry or chemputation. At this stage in the discussion, it may be interesting to explore what the drivers are to greater or lesser success in applying AI methods to scientific problems. This point is examined in the Editorial by Jones et al., which accompanies an excellent JACS Au special collection on “Emerging Chemistry \& Machine Learning”. In their overview, three main reasons are indicated. Two of them are quite intrinsic to the method (algorithm development and theoretical derivation of descriptors), while one of them is, notably, not specific but resides in the availability of a collection of standardized, highquality data. A case in point, and unanimously defined as the most spectacular recent success of AI, is the recent prediction of a protein’s three-dimensional structure from its amino acid sequence via AlphaFold. The tool was trained on large publicly available databases, such as the RSCB Protein Data Bank, as well as protein sequences of unknown structure. In return, it is somewhat expected, although not guaranteed, that the new information generated is also made available to the public. The AlphaFold code is now publicly available. It can}
}

@article{mathur2022docinferdocumentlevel,
  title={DocInfer: Document-level Natural Language Inference using Optimal Evidence Selection},
  author={Puneet Mathur and Gautam Kunapuli and Riyaz Ahmad Bhat and Manish Shrivastava and Dinesh Manocha and M. Singh},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2022.emnlp-main.51},
  url={https://www.semanticscholar.org/paper/4430cb7ddb3c4a9860ddabf4f92568a8a03c2b18},
  abstract={We present DocInfer - a novel, end-to-end Document-level Natural Language Inference model that builds a hierarchical document graph enriched through inter-sentence relations (topical, entity-based, concept-based), performs paragraph pruning using the novel SubGraph Pooling layer, followed by optimal evidence selection based on REINFORCE algorithm to identify the most important context sentences for a given hypothesis. Our evidence selection mechanism allows it to transcend the input length limitation of modern BERT-like Transformer models while presenting the entire evidence together for inferential reasoning. We show this is an important property needed to reason on large documents where the evidence may be fragmented and located arbitrarily far from each other. Extensive experiments on popular corpora - DocNLI, ContractNLI, and ConTRoL datasets, and our new proposed dataset called CaseHoldNLI on the task of legal judicial reasoning, demonstrate significant performance gains of 8-12\% over SOTA methods. Our ablation studies validate the impact of our model. Performance improvement of 3-6\% on annotation-scarce downstream tasks of fact verification, multiple-choice QA, and contract clause retrieval demonstrates the usefulness of DocInfer beyond primary NLI tasks.}
}

@article{lewis2022doesclip,
  title={Does CLIP Bind Concepts? Probing Compositionality in Large Image Models},
  author={Martha Lewis and Nihal V. Nayak and Qinan Yu and Jack Merullo and Ellie Pavlick},
  year={2022},
  booktitle={Findings},
  doi={10.48550/arXiv.2212.10537},
  url={https://www.semanticscholar.org/paper/2de7790ed868510c8001a90c11737fe4e8a01930},
  abstract={Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ‘red cube’ by reasoning over the constituents ‘red’ and ‘cube’. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ‘cube behind sphere’ from ‘sphere behind cube’). To inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We benchmark them on three synthetic datasets – single-object, two-object, and relational – designed to test concept binding. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance drops dramatically. At the same time, CDSMs also perform poorly, with best performance at chance level.},
  keywords={arxiv:2212.10537}
}

@misc{elsaesser2022downloadfree,
  title={Download Free Film Theory An Introduction Through The Senses Thomas Elsaesser Pdf File Free},
  author={T. Elsaesser},
  year={2022},
  url={https://www.semanticscholar.org/paper/213b3e97d6cbf31ba582b4787c612507a2d545cf}
}

@article{jiang2022draftsketch,
  title={Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
  author={Albert Qiaochu Jiang and S. Welleck and J. Zhou and Wenda Li and Jiacheng Liu and M. Jamnik and Timothée Lacroix and Yuhuai Wu and Guillaume Lample},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.12283},
  url={https://www.semanticscholar.org/paper/7de36d6b14aadc8cdb6ad1340b9ca64b15375bca},
  abstract={The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9\% to 39.3\% on a collection of mathematical competition problems.},
  keywords={arxiv:2210.12283}
}

@article{lu2022dynamicprompt,
  title={Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Kai-Wei Chang and Y. Wu and Song-Chun Zhu and Tanmay Rajpurohit and Peter Clark and A. Kalyan},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2209.14610},
  url={https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4},
  abstract={Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31\% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.},
  keywords={arxiv:2209.14610}
}

@article{melnyk2022economicmathematical,
  title={ECONOMIC AND MATHEMATICAL TOOLS FOR PREDICTING THE CURRENCY EXCHANGE RATE},
  author={Ostap Melnyk and Oleksandr Novoseletskyy},
  year={2022},
  booktitle={Scientific opinion: Economics and Management},
  doi={10.32836/2521-666x/2022-78-24},
  url={https://www.semanticscholar.org/paper/46d121662cd6b020e9945b20c399559bd4c276ab},
  abstract={The article deals with the analysis of existing approaches to exchange rate forecasting. It also includes the review of Ukrainian and foreign scientists on this topic. The authors of this article have considered the main disadvantages and benefits of existing forecasting dimensions, as well as individual methods and models. They indicated ways to facilitate the implementation of currency exchange rate forecasting using neural networks with software libraries for various programming languages and individual software applications, as well. As a result, the authors have systematized knowledge about existing approaches used in the process of currency exchange rate forecasting. There are two dimensions of currency exchange rate forecasting, in particular, intuitive and formalized ones. The intuitive dimension is peculiar to short-term forecasting and is often used in trading. Its main advantages include the ability to consider structural changes in the economy that can significantly affect the exchange rate formation itself and the speed of forecasting. However, the disadvantage of intuitive methods is the inability to prove formally the quality of the obtained forecasts. The advantages of the formalized dimension of forecasting include the ability to prove the quality. Businesses and government agencies use it the most often. Extrapolation methods and machine learning methods are mainly used to predict the exchange rate using formalized methods. Moreover, the reviewed studies indicate that among the well-known extrapolation methods for predicting the exchange rate, autoregressive models (VAR, AR, ARMA, ARIMA, SARIMA, ARCH, GARCH, ARDL) and smoothing methods (floating averages, adaptive methods and models) are used the most frequently. Machine learning methods include neural networks. Trend models have proved to be ineffective for currency exchange rate forecasting. The reason for this appeared to be using large amounts of data for currency exchange rate forecasting, and each fluctuation there directly affects the whole phenomenon.}
}

@article{joshi2022ertestevaluating,
  title={ER-TEST Evaluating Explanation Regularization Methods for NLP Models},
  author={Brihi Joshi and Aaron Chan and Ziyi Liu and Shaoliang Nie and Maziar Sanjabi and Hamed Firooz and Xiang Ren},
  year={2022},
  booktitle={TRUSTNLP},
  doi={10.48550/arXiv.2205.12542},
  url={https://www.semanticscholar.org/paper/506f4614f2be3b02984a1b293553ce07d18b38bb}
}

@article{wang2022erecenhanced,
  title={EREC: Enhanced Language Representations with Event Chains},
  author={Huajie Wang and Yinglin Wang},
  year={2022},
  booktitle={Inf.},
  doi={10.3390/info13120582},
  url={https://www.semanticscholar.org/paper/a41c89afad3e5cfbcaa6dcd4acb02d0cd53a15b1},
  abstract={The natural language model BERT uses a large-scale unsupervised corpus to accumulate rich linguistic knowledge during its pretraining stage, and then, the information is fine-tuned for specific downstream tasks, which greatly improves the understanding capability of various natural language tasks. For some specific tasks, the capability of the model can be enhanced by introducing external knowledge. In fact, these methods, such as ERNIE, have been proposed for integrating knowledge graphs into BERT models, which significantly enhanced its capabilities in related tasks such as entity recognition. However, for two types of tasks, commonsense causal reasoning and predicting the ending of stories, few previous studies have combined model modification and process optimization to integrate external knowledge. Therefore, referring to ERNIE, in this paper, we propose enhanced language representation with event chains (EREC), which focuses on keywords in the text corpus and their implied relations. Event chains are integrated into EREC as external knowledge. Furthermore, various graph networks are used to generate embeddings and to associate keywords in the corpus. Finally, via multi-task training, external knowledge is integrated into the model generated in the pretraining stage so as to enhance the effect of the model in downstream tasks. The experimental process of the EREC model is carried out with a three-stage design, and the experimental results show that EREC has a deeper understanding of the causal relationship and event relationship contained in the text by integrating the event chains, and it achieved significant improvements on two specific tasks.}
}

@article{fredericks2022editorial,
  title={Editorial},
  author={B. Fredericks and M. Nakata and Katelyn Barney},
  year={2022},
  journal={The Australian Journal of Indigenous Education},
  doi={10.55146/ajie.v51i2.624},
  url={https://www.semanticscholar.org/paper/c5186d69aaa32a3f34d8b77217995114ded864db},
  abstract={Welcome to Volume 51.2 of The Australian Journal of Indigenous Education. This is our second volume since our shift to being an open access journal. We are very pleased that AJIE has recently been accepted into the Directory of Open Access Journals and was awarded the DOAJ Seal for best practice in open access. DOAJ is an extensive index of diverse open access journals internationally and their aim is to increase the visibility, accessibility, reputation, usage and impact of quality, peer-reviewed, open access scholarly research journals globally. We are also excited that since the journal became open access in August 2022 there has been over 20,000 views of whole articles and over 24,000 views of abstracts on our new open access website. 
This is a larger volume of AJIE than usual, and we thank the authors and reviewers for their contributions. You play a vital role in ensuring the quality of the journal. We would also like to thank Michelle James for her detailed and astute copyediting for the journal. Special thanks to Senior Publications Officer Sonia Nitchell for her continuing work on importing the large AJIE archive onto the new platform. 
The first suite of articles in this volume focuses on the early childhood context with articles by Locke and Webb providing us with insights into the inclusion of Indigenous knowledges and perspectives in early education and care settings in the first paper and how Aboriginal educators integrated their cultural knowledge and experiences to develop Aboriginal children’s skills in the second. In a South Saami context, Kroik explores preschool teachers’ identity as linguistic role models by means of analysing their own descriptions of language learning. In Canada, Schroeder et al. demonstrate the importance of making curricula relevant to Indigenous children by including content that is culturally relevant and developmentally appropriate. The interrelationships between language, identity and culture from Māori kaumātua (elders both male and female) and whānau (parents and extended family members) from Aotearoa (New Zealand) is explored by Berryman et al. 
The second suite of papers take us into the context of schools. Johnson and Flückiger explore the important role for Aboriginal Education Workers in remote Australian communities, while Goodall et al. draw on student and teacher memories of the early days of Indigenous-controlled adult education provider Tranby Aboriginal Co-operative Ltd. The paper by Guenther et al. analyses My School data for Very Remote Aboriginal schools, showing how the Remote School Attendance Strategy school attendance results compare with similar non-Remote School Attendance Strategy schools. Their findings raise ethical and accountability concerns about the Remote School Attendance Strategy, which they argue lacks evidence of attendance improvement, and which potentially causes harm. Whitau et al. also examine school attendance but in relation to Western Australian Aboriginal young women and the links between racism, teacher–student relationships, and peer connectedness, and how these were related to participant attendance and engagement at school. Moore et al. discuss the Whole of Community Engagement (WCE) initiative, which sought to identify barriers and enablers in Aboriginal students’ pathways to post-compulsory education in six remote communities in Arnhem Land and central Australia. They describe the features that led them to characterise the initiative and the remote community and school context as intercultural and complex. Also in relation to the Whole of Community Engagement initiative, Moore et al. propose an intercultural perspective as a refinement to the both-ways approach to remote education. Osborne et al. focus on aspirations of students, their families and communities at Nyangatjatjara College an independent Aboriginal school distributed across three campuses in the southern region of the Northern Territory. Macdonald and Gringart present a new measurement instrument, the Multi-Dimensional Student Perceptions of School Questionnaire (MSPSQ), validated with a moderate-sized sample of Indigenous and non-Indigenous secondary students in Western Australia. 
The next suite of papers has an international focus with papers from Canada, Aotearoa (New Zealand), Brazil, and Tonga. Stavrou and Murphy explore tensions surrounding Indigenising school mathematics in a Western Canadian prairie province conducted with three Cree elementary school teachers while Denston et al. examine teachers’ perceptions and experiences of a collaborative case study to adapt a literacy approach originally designed for an Aotearoa (New Zealand) English-medium context. Ioris et al. explore the main trends and pending gaps related to indigenous education in Brazil while Fonua et al. shares the stories of 26 successful Tongan science learners who participated in talanoa (open discussion without an agenda) about their engagement, enjoyment, and success in secondary and university science education in Aotearoa (New Zealand). 
The final papers in this volume shift to the university context with Hogarth exploring a small pilot study conducted at a Queensland university examining how academics perceive the inclusion of Indigenous Knowledges within institutional and professional contexts and initial teacher education programs. Forsyth et al. speak to the importance of employing Indigenous methodologies when conducting Indigenous research to improve dental and medical health outcomes for Indigenous peoples. Hook and Jessen reflect on the contentious nature of non-Indigenous academics teaching Indigenous Studies and draw on student survey data to illustrate the conflict between their pedagogic practices, student expectations and the structural impediments to their teaching aims. Smith et al. also provide a personal reflection on the higher education context by discussing the need to have institutional conversations about coloniality, institutional racism and white fragility within tertiary institutions. The final paper in this volume by Gibbs et al. explores the relationships between racism, cultural resilience, and educational engagement and academic outcomes for Aboriginal tertiary students. They highlight that cultural resilience and support is critical to Aboriginal student success within universities. Racism continues to be particularly important to address because, as the 2022 Australian Reconciliation Barometer recently highlighted, experiences of racial prejudice have increased for Aboriginal and Torres Strait Islander people over the last two years and certainly there is much work needed to improve relationships between Indigenous and non-Indigenous people. 
We hope you enjoy reading the articles in this volume and hope the articles lead to further dialogue and discussion about Indigenous educational success both in Australia and internationally. 
Bronwyn Fredericks, Martin Nakata, and Katelyn Barney}
}

@article{brabec2022editorialinvestigation,
  title={Editorial for “Investigation of the Inter‐ and Intra‐Scanner Reproducibility and Repeatability of Radiomics Features in Magnetic Resonance Imaging”},
  author={J. Brabec and F. Lennartsson},
  year={2022},
  journal={Journal of Magnetic Resonance Imaging},
  doi={10.1002/jmri.28190},
  url={https://www.semanticscholar.org/paper/26f5fc56ff98df31c9e54b1e760e976eb46028a2},
  abstract={Radiomics refers to a multistep process of feature extraction from medical images and their analysis by computer algorithms. The key idea is that the information contained in an image may not reveal itself by a naked eye but may become apparent through algorithms. This could lead to new imaging contrasts, perhaps because of the fundamentally different nature of how we, man and computer, arrive at image characterization—in the case of radiologists, the analysis relies on the wiring of a brain, whereas radiomics relies on simple but incremental mathematical operations. Furthermore, radiomics as a process can be divided into consecutive steps: acquisition of the medical images, identification of the volume of interest that relates to the pathology and its segmentation, extraction of relevant features describing the pathology, creating or sharing databases, and, finally, proposition with validation of predictive models. This is the main reason why radiomics holds the promise of discovering hidden correlations and bringing transparency into radiologists’ decisionmaking, respectively. One could summarize that radiomics is rather an approach than a specific method and that images are regarded as data rather than pictures. Radiomics is used to solve tasks by utilizing several families of features: these may be based on morphologic characteristics such as lesion size or volume, image intensity-based histograms or descriptors of the relationships between image voxels, such as gray-level co-occurrence, run-length, size-zone, or distance zone matrices. Features can also be extracted from images that are preprocessed by filters (eg, wavelet or Laplacian filters), as well as may stem from image fractal features. They can also be, however, based on machine learning techniques. One of such important tasks is a texture analysis—which is defined as a spatial distribution of an image that contains information on the local aspects of the tissue. To describe texture, we can use words in natural language such as coarseness, smoothness, or perhaps because it is difficult to grasp this concept in natural language a description by quantifiable features could be orthogonal and thus useful. MRI-based radiomics has already been applied in research, for example, to predict treatment response or outcome based on intensity histogram-based radiomic features. Radiomics could also bridge the gap between radiology and biology by correlating the features with underlying tumor genetics. Other applications include identification of malignant tissue, tumor classification, image guided radiotherapy, or distinguishing true progression from pseudo-progression. The results can be complementary to information obtained from histology, genotyping, laboratory results, clinical reports, or standard radiological reports or can be even corroborated in a decision support system. Although the possibilities of applications seem to be vast, and the theory is appealing, radiomics has not yet lived up to this promise and has not yet transitioned into the clinics because the key limitation of radiomics is its reproducibility. It is an issue even in the case when we use the same computational method and scan the participants on the same MR scanners! This is the question that is addressed in current issue of Journal of Magnetic Resonance in Imaging by the work titled “Investigation of the interand intra-scanner reproducibility and repeatability of radiomics features in magnetic resonance imaging” where the authors studied interand intra-scanner variability of radiomics variables in MRI trough multicenter validation. The authors found that, albeit few do, most of the radiomics features do not pass this minimal test! A way forward is to find a consensus on features that are reproducible, which is where this study contributes. However, one cannot be successful without a deeper understanding of the dependencies: the feature values depend both on parameter choices during calculations, but also on the parameters of the MR pulse-sequence. Based on this understanding, we can define features that are robust with respect to the dependencies. To further facilitate the data-driven approach, although challenging, it is crucial to create large and curated databases that also include a large span of the underlying dependencies. Because, if these are not at hand,}
}

@article{garcaros2022effectsselfregulated,
  title={Effects of self-regulated learning and procrastination on academic stress, subjective well-being, and academic achievement in secondary education},
  author={R. García-Ros and F. Pérez-Gónzalez and J. Tomás and P. Sancho},
  year={2022},
  booktitle={Current Psychology},
  doi={10.1007/s12144-022-03759-8},
  url={https://www.semanticscholar.org/paper/0e9087ca2feb8400039833999290ec0ef09d7bc0},
  abstract={The main objective of this study was to test a structural theoretical model of the effects of self-regulated learning on academic stress, subjective well-being, and academic achievement in Secondary Education, considering academic procrastination as a mediator. An additional aim was to explore whether these relationships were moderated by gender and educational level. Participants were 728 students in compulsory and post-compulsory secondary education in a large city in Eastern Spain. Path analysis results indicated that the proposed model showed satisfactory fit, with the three dimensions of self-regulated learning significantly predicting the educational outcomes considered, and that procrastination mediated these relationships. Overall, the model is able to predict 9.8\% of the variance of academic stress, 23.1\% of students wellbeing, and 14\% of academic achievement. Moreover, the multi-group routine revealed no moderation effects due to gender, but educational level moderated two relationships, between self-efficacy and academic achievement and between metacognitive strategies and procrastination. Additionally, supplementary models were tested for three specific subjects (Spanish Language, Foreign Language and Mathematics), which showed an improvement in explained variance, being respectively: 29\%, 28\% and 27\%. Results are discussed in light of previous research and in terms of their impact on educational practice.}
}

@article{shukor2022efficientvisionlanguage,
  title={Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment},
  author={Mustafa Shukor and Guillaume Couairon and M. Cord},
  year={2022},
  booktitle={British Machine Vision Conference},
  doi={10.48550/arXiv.2208.13628},
  url={https://www.semanticscholar.org/paper/966ccb741d4e8d92db931852f2d9480fb5c497a0},
  abstract={Vision and Language Pretraining has become the prevalent approach for tackling multimodal downstream tasks. The current trend is to move towards ever larger models and pretraining datasets. This computational headlong rush does not seem reasonable in the long term to move toward sustainable solutions, and de facto excludes academic laboratories with limited resources. In this work, we propose a new framework, dubbed ViCHA, that efficiently exploits the input data to boost the learning by: (a) a new hierarchical cross-modal alignment loss, (b) new self-supervised scheme based on masked image modeling, (c) leveraging image-level annotations, called Visual Concepts, obtained with existing foundation models such as CLIP to boost the performance of the image encoder. Although pretrained on four times less data, our ViCHA strategy outperforms other approaches on several downstream tasks such as Image-Text Retrieval, VQA, Visual Reasoning, Visual Entailment and Visual Grounding. The code will be made publicly available here: https://github.com/mshukor/ViCHA},
  keywords={arxiv:2208.13628}
}

@article{li2022eliteplmempirical,
  title={ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models},
  author={Junyi Li and Tianyi Tang and Zheng Gong and Lixin Yang and Zhuohao Yu and Z. Chen and Jingyuan Wang and Wayne Xin Zhao and Ji-rong Wen},
  year={2022},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2205.01523},
  url={https://www.semanticscholar.org/paper/7f84d56fb8feb4e50cd6c3da3e3fd4ff6c4772cf},
  abstract={Nowadays, pretrained language models (PLMs) have dominated the majority of NLP tasks. While, little research has been conducted on systematically evaluating the language abilities of PLMs. In this paper, we present a large-scale empirical study on general language ability evaluation of PLMs (ElitePLM). In our study, we design four evaluation dimensions, memory, comprehension, reasoning, and composition, to measure ten widely-used PLMs within five categories. Our empirical results demonstrate that: (1) PLMs with varying training objectives and strategies are good at different ability tests; (2) fine-tuning PLMs in downstream tasks is usually sensitive to the data size and distribution; (3) PLMs have excellent transferability between similar tasks. Moreover, the prediction results of PLMs in our experiments are released as an open resource for more deep and detailed analysis on the language abilities of PLMs. This paper can guide the future work to select, apply, and design PLMs for specific tasks. We have made all the details of experiments publicly available at https://github.com/RUCAIBox/ElitePLM.},
  keywords={arxiv:2205.01523}
}

@article{radke2022emergentbilingual,
  title={Emergent Bilingual Middle Schoolers’ Syncretic Reasoning in Statistical Modeling},
  author={Sarah C. Radke and Sara Vogel and Jasmine Y. Ma and C. Hoadley and Laura Ascenzi‐Moreno},
  year={2022},
  booktitle={Teachers College Record},
  doi={10.1177/01614681221104141},
  url={https://www.semanticscholar.org/paper/f51e959cc8a504de2549ef038da597f624b25ee8},
  abstract={Background/Context: Bi/multilingual students’ STEM learning is better supported when educators leverage their language and cultural practices as resources, but STEM subject divisions have been historically constructed based on oppressive, dominant values and exclude the ways of knowing of nondominant groups. Truly promoting equity requires expanding and transforming STEM disciplines. Purpose/Objective/Research Question/Focus of Study: This article contributes to efforts to illuminate emergent bi/multilingual students’ ways of knowing, languaging, and doing in STEM. We follow the development of syncretic literacies in relation to translanguaging practices, asking, How do knowledges and practices from different communities get combined and reorganized by students and teachers in service of new modeling practices? Setting and Participants: We focus on a seventh-grade science classroom, deliberately designed to support syncretic literacies and translanguaging practices, where computer science concepts were infused into the curriculum through modeling activities. The majority of the students in the bilingual program had arrived in the United States at most three years before enrolling, from the Caribbean and Central and South America. Research Design: We analyze one lesson that was part of a larger research–practice partnership focused on teaching computer science through leveraging translanguaging practices and syncretic literacies. The lesson was a modeling and computing activity codesigned by the teacher and two researchers about post–Hurricane María outmigration from Puerto Rico. Analysis used microethnographic methods to trace how students assembled translanguaging, social, and schooled practices to make sense of and construct models. Findings/Results: Findings show how students assembled representational forms from a variety of practices as part of accomplishing and negotiating both designed and emergent goals. These included sensemaking, constructing, explaining, justifying, and interpreting both the physical and computational models of migration. Conclusions/Recommendations: Implications support the development of theory and pedagogy that intentionally make space for students to engage in meaning-making through translanguaging and syncretic practices in order to provide new possibilities for lifting up STEM learning that may include, but is not constrained by, disciplinary learning. Additional implications for teacher education and student assessment practices call for reconceptualizing schooling beyond day-to-day curriculum as part of making an ontological shift away from prioritizing math, science, and CS disciplinary and language objectives as defined by and for schooling, and toward celebrating, supporting, and centering students’ diverse, syncretic knowledges and knowledge use.}
}

@article{webb2022emergentanalogical,
  title={Emergent analogical reasoning in large language models},
  author={Taylor W. Webb and K. Holyoak and Hongjing Lu},
  year={2022},
  booktitle={Nature Human Behaviour},
  doi={10.1038/s41562-023-01659-w},
  url={https://www.semanticscholar.org/paper/3cbffab9d7981da6662d474aaa056dcbd3c1701e},
  abstract={The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of Generative Pre-trained Transformer (GPT)-3) on a range of analogical tasks, including a non-visual matrix reasoning task based on the rule structure of Raven’s Standard Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT-4 indicated even better performance. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems. Webb et al. show that new artificial intelligence language models, such as Generative Pre-trained Transformer 3, are able to solve analogical reasoning problems at a human-like level of performance.},
  keywords={arxiv:2212.09196}
}

@article{dorrance2022energyefficient,
  title={Energy Efficient BNN Accelerator using CiM and a Time-Interleaved Hadamard Digital GRNG in 22nm CMOS},
  author={R. Dorrance and D. Dasalukunte and Hechen Wang and Renzhi Liu and B. Carlton},
  year={2022},
  booktitle={International Symposium on Security in Computing and Communications},
  doi={10.1109/A-SSCC56115.2022.9980539},
  url={https://www.semanticscholar.org/paper/49b7eeb8357c28527a05831c081785e1ddff9b5e},
  abstract={In recent years, Neural Networks (NNs) have achieved tremendous success in a variety of fields, such as computer vision, natural language processing, speech recognition, autonomous driving, and healthcare [1] –[4]. However, conventional NNs rely heavily on large, labeled training datasets, which can lead to overfitting and overconfident decision making (especially when faced with unfamiliar, out-of-distribution inputs) [1] –[4]. Unlike conventional NNs, the weights of Bayesian Neural Network (BNNs) are represented by probability distributions, providing a mathematical framework to quantify the uncertainties in a model’s final prediction [3]. These uncertainty estimates allow BNNs to mitigate overfitting issues, enable training with smaller datasets, and help increase overall model accuracy through the implicit use of stochastic rounding [5]. Figure 1 shows an example with a BNN version of LeNet-5 [6], where weights of the network are represented by Gaussian distributions. The ambiguous digit is classified as a ‘5’ and ‘3’ with probabilities of 80\% and 20\%, respectively. However, these uncertainty estimates come at great computational cost, as multiple forward inference passes are required to generate the necessary posterior distributions. As such, BNNs require not only efficient, high-performance multiply-accumulation (MAC), but an efficient Gaussian Random Number Generator (GRNG) with high-quality statistics as well.}
}

@article{adser2022englishproficiency,
  title={English Proficiency, Gender and the Occupations of Childhood Immigrants in the US},
  author={A. Adserà and Aditi Bhowmick},
  year={2022},
  journal={Journal of Labor Research},
  doi={10.1007/s12122-022-09339-w},
  url={https://www.semanticscholar.org/paper/aecf7b957e043ccdff84eeff7e520fbe94dc6c51}
}

@article{liu2022enhancingcommunication,
  title={Enhancing Communication Reliability from the Semantic Level under Low SNR},
  author={Yueling Liu and Yichi Zhang and Peng Luo and Shengteng Jiang and Kuo Cao and Haitao Zhao and Jibo Wei},
  year={2022},
  booktitle={Electronics},
  doi={10.3390/electronics11091358},
  url={https://www.semanticscholar.org/paper/d5b032831439dc252fee9fd6075ed7efd2d1e473},
  abstract={In the low signal-to-noise ratio region, a large number of bit errors occur, and it may exceed the channel error correction capability of the receiver. Traditional communication system may use the technology of automatic repeat-request to deal with this problem, which is time consuming and a waste of resources. To enhance the reliability of the communication system, we investigate reasoning and decoding at the semantic level instead of the grammar level. In particular, we propose a semantic communication model for text transmission, assisting the communication system to be more robust in terrible channel environments. Based on the traditional communication system, the language model BERT, part of speech tagging, and prior information concerning bit-flipping are introduced to enhance the semantic reasoning ability of the transceiver. Furthermore, this paper analyzes the effects of the sub-strategies on the performances of the improved communication model, such as the existence of a candidate set and language model. The numerical results show the effectiveness of our model in terms of improving the semantic accuracy measured by BLUE, the METEOR score, and the similarity score based on BERT between transmitted messages and recovered messages.}
}

@article{nararatwong2022enhancingfinancial,
  title={Enhancing Financial Table and Text Question Answering with Tabular Graph and Numerical Reasoning},
  author={Rungsiman Nararatwong and Natthawut Kertkeidkachorn and R. Ichise},
  year={2022},
  booktitle={AACL},
  doi={10.18653/v1/2022.aacl-main.72},
  url={https://www.semanticscholar.org/paper/e470b2cac11c9cbef93d395b7d778481b39a8735},
  abstract={Typical financial documents consist of tables, texts, and numbers. Given sufficient training data, large language models (LM) can learn the tabular structures and perform numerical reasoning well in question answering (QA). However, their performances fall significantly when data and computational resources are limited. This study improves this performance drop by infusing explicit tabular structures through a graph neural network (GNN). We proposed a model developed from the baseline of a financial QA dataset named TAT-QA. The baseline model, TagOp, consists of answer span (evidence) extraction and numerical reasoning modules. As our main contributions, we introduced two components to the model: a GNN-based evidence extraction module for tables and an improved numerical reasoning module. The latter provides a solution to TagOp’s arithmetic calculation problem specific to operations requiring number ordering, such as subtraction and division, which account for a large portion of numerical reasoning. Our evaluation shows that the graph module has the advantage in low-resource settings, while the improved numerical reasoning significantly outperforms the baseline model.}
}

@article{dorimana2022enhancingupper,
  title={Enhancing Upper Secondary Learners’ Problem-solving Abilities using Problem-based Learning in Mathematics},
  author={Aline Dorimana and Alphonse Uworwabayeho and Gabriel Nizeyimana},
  year={2022},
  journal={International Journal of Learning, Teaching and Educational Research},
  doi={10.26803/ijlter.21.8.14},
  url={https://www.semanticscholar.org/paper/12721d9e9e11b75e2e279d0d0e13995b89772004},
  abstract={Developing problem-solving abilities is a major objective of learning mathematics at school. However, learners’ problem-solving abilities are still critical. The main purpose of this study was to investigate how the problem-based learning model could enhance learners’ problem-solving abilities in mathematics. The study used quasi-experimental research with one group pre-test-post-test design. The population in this study consisted of fifty-four grade eleven learners (aged between 16 to 19 years old) from one school in Kayonza District in Rwanda. Data were collected using mathematical problem-solving tests and interviews and were analysed using paired t-tests for dependent sample means and descriptive analysis. The study results indicate that problem-based learning potentially impacts learners’ problem-solving abilities. It is shown from learners’ work in problem-solving that all indicators of problem-solving abilities, namely understanding the problem, planning ways to approach the problem, monitoring the progress while tackling the problem and reviewing the solution process, emerged as being fairly well improved. In addition, based on the interview results from some learners and their teachers, they like the PBL model because embedded tasks helped them to apply the knowledge that can improve their reasoning, creativity and thinking capability. The study recommends that schools encourage teachers to adopt PBL for enhancing learners’ problem-solving abilities. Additionally, researchers are urged to use findings from this study as a reference for further research. Furthermore, researchers could conduct similar research on a large scale using different methodologies.}
}

@article{lucasoliva2022equityparity,
  title={Equity and Parity in Primary Education: A Study on Performance in Language and Mathematics Using Hierarchical Linear Models},
  author={Inés Lucas-Oliva and Jesús García-Jiménez and J. Torres-Gordillo and Javier Rodríguez-Santero},
  year={2022},
  booktitle={Sustainability},
  doi={10.3390/su141912404},
  url={https://www.semanticscholar.org/paper/9267cdd12a9f958c7ce94e9ea04b656926055d0b},
  abstract={Education plays a crucial role in the development and consolidation of equality in society, which is reflected in the SDGs of the UN 2030 Agenda. Knowing the educational performance of schools is necessary to diagnose needs, evaluate proposals and undertake improvements in education policies. This study pursued a twofold objective: (1) to assess the equity and parity of Andalusian schools in relation to the competencies of mathematical reasoning and linguistic communication and (2) to study the relationship among educational performance, equity and parity in these competences. Hierarchical linear model research was designed and implemented in a population of 79,806 schoolchildren and 2092 schools. The results confirmed differences in equity and parity among schools. A relation was found between higher effectiveness and higher parity. Nonpublic schools are not more efficient than public schools; rather, it is the average economic and sociocultural status of schools that controls for their effectiveness. In conclusion, the educational system does not guarantee the same opportunities for all children; thus, the equity and parity of educational systems should be key criteria for their evaluation, ensuring that quality education reaches everyone equally. Further implications are also discussed.}
}

@misc{boschetty2022eruptingvolcano,
  title={Erupting Arc Volcano ( Villarrica , Chile ) from 2 Unsupervised Machine Learning Analysis of Mineral 3 Compositions},
  author={Felix O. Boschetty and D. Ferguson and J. Cortés and Eduardo and Morgado and S. Ebmeier and D. Morgan and J. E. Romero and C. S. Parejas},
  year={2022},
  url={https://www.semanticscholar.org/paper/e37d4137eba5cecac985e104aeed94209972d16a}
}

@article{spiliopoulou2022eventsrealm,
  title={EvEntS ReaLM: Event Reasoning of Entity States via Language Models},
  author={Evangelia Spiliopoulou and Artidoro Pagnoni and Yonatan Bisk and E. Hovy},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2211.05392},
  url={https://www.semanticscholar.org/paper/748a2700ec11f51560a69ec05c67ca9f97014be7},
  abstract={This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.},
  keywords={arxiv:2211.05392}
}

@article{peng2022evaluateconfidence,
  title={Evaluate Confidence Instead of Perplexity for Zero-shot Commonsense Reasoning},
  author={Letian Peng and Z. Li and Hai Zhao},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2208.11007},
  url={https://www.semanticscholar.org/paper/8e73435fba7d3e02fe5599524bfa62a12f9f63a8},
  abstract={Commonsense reasoning is an appealing topic in natural language processing (NLP) as it plays a fundamental role in supporting the human-like actions of NLP systems. With large-scale language models as the backbone, unsupervised pre-training on numerous corpora shows the potential to capture commonsense knowledge. Current pre-trained language model (PLM)-based reasoning follows the traditional practice using perplexity metric. However, commonsense reasoning is more than existing probability evaluation, which is biased by word frequency. This paper reconsiders the nature of commonsense reasoning and proposes a novel commonsense reasoning metric, Non-Replacement Confidence (NRC). In detail, it works on PLMs according to the Replaced Token Detection (RTD) pre-training objective in ELECTRA, in which the corruption detection objective reflects the confidence on contextual integrity that is more relevant to commonsense reasoning than existing probability. Our proposed novel method boosts zero-shot performance on two commonsense reasoning benchmark datasets and further seven commonsense question-answering datasets. Our analysis shows that pre-endowed commonsense knowledge, especially for RTD-based PLMs, is essential in downstream reasoning.},
  keywords={arxiv:2208.11007}
}

@article{li2022evaluatingbert,
  title={Evaluating BERT on cloud-edge time series forecasting and sentiment analysis via prompt learning},
  author={Qizhi Li and Xianyong Li and Yujia Song and Maolin Zhang and Longqi Chen and Gang Wang and Yajun Du},
  year={2022},
  booktitle={2022 IEEE 24th Int Conf on High Performance Computing \& Communications; 8th Int Conf on Data Science \& Systems; 20th Int Conf on Smart City; 8th Int Conf on Dependability in Sensor, Cloud \& Big Data Systems \& Application (HPCC/DSS/SmartCity/DependSys)},
  doi={10.1109/HPCC-DSS-SmartCity-DependSys57074.2022.00051},
  url={https://www.semanticscholar.org/paper/534909858fa7d4799d59bdcaf813f60036589f2c},
  abstract={Existing pre-trained language models (PTLMs), like BERT, have shown their powerful ca-pabilities in many natural language processing tasks. In sequence analysis, such as time series forecasting, anomaly detection, and sentiment analysis, PTLMs have also achieved new state-of-the-art results. However, does this mean that PTLMs know sequence analysis? This paper explores whether BERT pre-trained on a large amount of data contains knowledge of sequence analysis. Specifically, we adopt prompt learning to see whether BERT will achieve good results on cloud-edge time series forecasting and sentiment analysis tasks. For the cloud-edge time series forecasting task, we give BERT some regular cloud-edge data and let it predict the features of the next time step; For the sentiment analysis task, we give BERT some sentence with sentiment and ask it what sentiment these sen-tences carry. Our experimental results reveal that: (1) BERT performs not well on the cloud-edge time series forecasting task, which means the logical reasoning of BERT is not good; (2) for sentiment analysis task, BERT with the prompt template performs poorly on both English and Chinese datasets; and (3) for sentiment analysis task, BERT appears to be more likely to perceive the text as carrying positive sentiment.}
}

@article{liu2022evaluationjapanese,
  title={Evaluation of Japanese Teaching Quality Based on Deep Neural Network},
  author={Hailing Liu},
  year={2022},
  booktitle={Security and Communication Networks},
  doi={10.1155/2022/3466632},
  url={https://www.semanticscholar.org/paper/02433ab5edb91298be86431ea55e1087db8ce4c7},
  abstract={The 21st century is an era of rapid development of information and frequent international exchanges, and Japanese language teaching has received increasing attention. Because of this, colleges and universities are now focused on improving the quality of Japanese education, both now and in the future. We need to boost the whole management of teaching quality, notably the assessment of instructors’ teaching quality, in order to improve teaching quality. However, because a number of factors influence the quality of instruction, and each factor’s weight varies, the evaluation results are difficult to express in a mathematical analytical formula, resulting in a complex nonlinear classification problem that traditional classification methods cannot solve well. As a new technology, as a result of the artificial neural networks (ANNs) fundamental qualities, it has been extensively applied in different evaluation issues for pattern recognition, nonlinear classification, and other research. This subject introduces the optimized deep neural network theory into Japanese teaching quality evaluation and completes the following work: (1) the algorithm of discrete Hopfield neural network is introduced in detail, and the neural network theory is introduced into teaching evaluation. (2) Then, based on the evaluation data of teachers’ teaching quality in a school, a large number of simulation experiments and training were carried out, and a neural network model for evaluation of teachers’ teaching effect was constructed and designed. Experiments reveal that the neural network model proposed in this paper is a nonlinear mapping method, which increases the evaluation’s dependability and makes the outcomes more effective and objective.}
}

@article{b2022evolutiontechnology,
  title={Evolution of Technology in Artificial Intelligence (AI)},
  author={Mr. Nagesh U B and Vaishnavi PS and Varshith and Vshker Mayengbam},
  year={2022},
  journal={International Journal of Advanced Research in Science, Communication and Technology},
  doi={10.48175/ijarsct-5830},
  url={https://www.semanticscholar.org/paper/b0106f636afa8f8e6f42d13ddc0200365f5425bb},
  abstract={Artificial Intelligence (A.I.) is a multidisciplinary field whose objective is to mechanize exercises that by and by require human knowledge. Late accomplishments in A.I. incorporate mechanized clinical diagnosticians and frameworks that naturally redo equipment to specific client prerequisites. The serious pain points tended to in A.I. can be summed up as Perception, Manipulation, Reasoning, Communication, and Learning. Discernment is worried about building models of the actual world from tactile information (visual, sound, and so on) Control is worried about articulating extremities (e.g., mechanical arms, velocity gadgets) to affect an optimal state in the real world. Thinking is worried about more significant level mental capacities like preparation, reaching inferential determinations from a world model, diagnosing, planning, and so on Correspondence treats the issue comprehension and passing on data using language. At long last, Learning treats the issue of consequently further developing framework execution after some time in view of the framework's insight. Various huge particular thoughts have risen up out of A.I. that bind together these different trouble spots and that structure the underpinning of the logical discipline. By and large, A.I. frameworks work in view of a Knowledge Base of realities and decides that portray the framework's space of capability. The components of a Knowledge Base comprise of autonomously legitimate (or if nothing else conceivable) lumps of data. The framework should naturally sort out and use this data to tackle the particular issues that it experiences. This association cycle can be for the most part described as a Search coordinated toward explicit objectives. The pursuit is made complex in light of the need to decide the significance of data and in view of the incessant event of unsure and uncertain information. Heuristics give the A.I. framework with a component for centering its consideration and controlling its looking through processes. The fundamentally versatile association of A.I. frameworks yields the necessity for A.I. computational Architectures. All data utilized by the system ought to be tended to inside such a plan. The obtaining and encoding of true information into A.I. design contains the subfield of Knowledge Engineering}
}

@article{nishimura2022evolutionaryloss,
  title={Evolutionary loss of complexity in human vocal anatomy as an adaptation for speech},
  author={Takeshi Nishimura and Isao T. Tokuda and S. Miyachi and Jacob C. Dunn and C. Herbst and Kazuyoshi Ishimura and Akihisa Kaneko and Yuki Kinoshita and H. Koda and J. Saers and H. Imai and Tetsuya Matsuda and O. Larsen and U. Jürgens and Hideki Hirabayashi and S. Kojima and W. Fitch},
  year={2022},
  booktitle={Science},
  doi={10.1126/science.abm1574},
  url={https://www.semanticscholar.org/paper/1065c2a6c6c71d9d60bdef7b775395bfab2f1636},
  abstract={Human speech production obeys the same acoustic principles as vocal production in other animals but has distinctive features: A stable vocal source is filtered by rapidly changing formant frequencies. To understand speech evolution, we examined a wide range of primates, combining observations of phonation with mathematical modeling. We found that source stability relies upon simplifications in laryngeal anatomy, specifically the loss of air sacs and vocal membranes. We conclude that the evolutionary loss of vocal membranes allows human speech to mostly avoid the spontaneous nonlinear phenomena and acoustic chaos common in other primate vocalizations. This loss allows our larynx to produce stable, harmonic-rich phonation, ideally highlighting formant changes that convey most phonetic information. Paradoxically, the increased complexity of human spoken language thus followed simplification of our laryngeal anatomy. Description Complexity from simplification Human speech and language are highly complex, consisting of a large number of sounds. The human phonal apparatus, the larynx, has acquired the capability to create a wider array of sounds, even though previous work has revealed many similarities between our larynx and those in other primates. Looking across a large number of primates, Nishimura et al. used a combination of anatomical, phonal, and modeling approaches to characterize sound production in the larynx (see the Perspective by Gouzoules). They found that instead of the human larynx having increased complexity, it has actually simplified relative to other primates, allowing for clearer sound production with less aural chaos. —SNV The human larynx has undergone evolutionary simplification, facilitating the increased acoustic complexity of the spoken language.}
}

@article{li2022examininghomophily,
  title={Examining Homophily, Language Coordination, and Analytical Thinking in Web-Based Conversations About Vaccines on Reddit: Study Using Deep Neural Network Language Models and Computer-Assisted Conversational Analyses},
  author={Yue Li and W. Gee and Kun Jin and Robert M. Bond},
  year={2022},
  journal={Journal of Medical Internet Research},
  doi={10.2196/41882},
  url={https://www.semanticscholar.org/paper/e18680527f12dc9f1cbdcf241e0e6b01af4498c1},
  abstract={Background Vaccine hesitancy has been deemed one of the top 10 threats to global health. Antivaccine information on social media is a major barrier to addressing vaccine hesitancy. Understanding how vaccine proponents and opponents interact with each other on social media may help address vaccine hesitancy. Objective We aimed to examine conversations between vaccine proponents and opponents on Reddit to understand whether homophily in web-based conversations impedes opinion exchange, whether people are able to accommodate their languages to each other in web-based conversations, and whether engaging with opposing viewpoints stimulates higher levels of analytical thinking. Methods We analyzed large-scale conversational text data about human vaccines on Reddit from 2016 to 2018. Using deep neural network language models and computer-assisted conversational analyses, we obtained each Redditor’s stance on vaccines, each post’s stance on vaccines, each Redditor’s language coordination score, and each post or comment’s analytical thinking score. We then performed chi-square tests, 2-tailed t tests, and multilevel modeling to test 3 questions of interest. Results The results show that both provaccine and antivaccine Redditors are more likely to selectively respond to Redditors who indicate similar views on vaccines (P<.001). When Redditors interact with others who hold opposing views on vaccines, both provaccine and antivaccine Redditors accommodate their language to out-group members (provaccine Redditors: P=.044; antivaccine Redditors: P=.047) and show no difference in analytical thinking compared with interacting with congruent views (P=.63), suggesting that Redditors do not engage in motivated reasoning. Antivaccine Redditors, on average, showed higher analytical thinking in their posts and comments than provaccine Redditors (P<.001). Conclusions This study shows that although vaccine proponents and opponents selectively communicate with their in-group members on Reddit, they accommodate their language and do not engage in motivated reasoning when communicating with out-group members. These findings may have implications for the design of provaccine campaigns on social media.}
}

@article{jiang2022examiningcomputational,
  title={Examining computational thinking processes in modeling unstructured data},
  author={Shiyan Jiang and Yingxiao Qian and Hengtao Tang and Rabia Yalcinkaya and C. Rosé and J. Chao and W. Finzer},
  year={2022},
  journal={Education and Information Technologies : Official Journal of the IFIP technical committee on Education},
  doi={10.1007/s10639-022-11355-3},
  url={https://www.semanticscholar.org/paper/06f065d0938522794fba6ba07e89652a8f4817af}
}

@misc{singh2022explainingpatterns,
  title={Explaining Patterns in Data with Language Models via Interpretable Autoprompting},
  author={Chandan Singh and John X. Morris and J. Aneja and Alexander M. Rush and Jianfeng Gao},
  year={2022},
  url={https://www.semanticscholar.org/paper/224b8cd8c31cfa86c2a84bec3a65d9ba44f38280},
  abstract={Large language models (LLMs) have displayed an impressive ability to harness natural language to perform complex tasks. In this work, we explore whether we can leverage this learned ability to find and explain patterns in data. Specifically, given a pre-trained LLM and data examples, we introduce interpretable autoprompting (iPrompt), an algorithm that generates a natural-language string explaining the data. iPrompt iteratively alternates between generating explanations with an LLM and reranking them based on their performance when used as a prompt. Experiments on a wide range of datasets, from synthetic mathematics to natural-language understanding, show that iPrompt can yield meaningful insights by accurately finding groundtruth dataset descriptions. Moreover, the prompts produced by iPrompt are simultaneously human-interpretable and highly effective for generalization: on real-world sentiment classification datasets, iPrompt produces prompts that match or even improve upon human-written prompts for GPT-3. Finally, experiments with an fMRI dataset show the potential for iPrompt to aid in scientific discovery. All code for using the methods and data here is made available on Github.},
  keywords={arxiv:2210.01848}
}

@article{li2022explanationsfrom,
  title={Explanations from Large Language Models Make Small Reasoners Better},
  author={SHIYANG LI and Jianshu Chen and Yelong Shen and Zhiyu Chen and Xinlu Zhang and Zekun Li and Hong Wang and Jingu Qian and Baolin Peng and Yi Mao and Wenhu Chen and Xifeng Yan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.06726},
  url={https://www.semanticscholar.org/paper/7d29a84a589aa5655e5d3fed8d725ea472816599},
  abstract={Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5\% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.},
  keywords={arxiv:2210.06726}
}

@article{zhang2022explicitobject,
  title={Explicit Object Relation Alignment for Vision and Language Navigation},
  author={Yue Zhang and Parisa Kordjamshidi},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2022.acl-srw.24},
  url={https://www.semanticscholar.org/paper/82e0fd80ac234fbad07fd05058e4c2ab3256ca8a},
  abstract={In this paper, we investigate the problem of vision and language navigation. To solve this problem, grounding the landmarks and spatial relations in the textual instructions into visual modality is important. We propose a neural agent named Explicit Object Relation Alignment Agent (EXOR),to explicitly align the spatial information in both instruction and the visual environment, including landmarks and spatial relationships between the agent and landmarks.Empirically, our proposed method surpasses the baseline by a large margin on the R2R dataset. We provide a comprehensive analysis to show our model’s spatial reasoning ability and explainability.}
}

@article{anil2022exploringlength,
  title={Exploring Length Generalization in Large Language Models},
  author={Cem Anil and Yuhuai Wu and Anders Andreassen and Aitor Lewkowycz and Vedant Misra and V. Ramasesh and Ambrose Slone and Guy Gur-Ari and Ethan Dyer and Behnam Neyshabur},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2207.04901},
  url={https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a},
  abstract={The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.},
  keywords={arxiv:2207.04901}
}

@article{liu2022forgetfulcausal,
  title={FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners},
  author={Hao Liu and Xinyang Geng and Lisa Lee and Igor Mordatch and S. Levine and Sharan Narang and P. Abbeel},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.13432},
  url={https://www.semanticscholar.org/paper/75af74f4e371b371147fb1d1e81addd921449cdd}
}

@article{lopez2022flopsdiscriminant,
  title={FLOPs as a Discriminant for Dense Linear Algebra Algorithms},
  author={F. L'opez and L. Karlsson and P. Bientinesi},
  year={2022},
  booktitle={International Conference on Parallel Processing},
  doi={10.1145/3545008.3545072},
  url={https://www.semanticscholar.org/paper/cfd35d1dd27436fd850ee34b532e806eaa7b7b17},
  abstract={Expressions that involve matrices and vectors, known as linear algebra expressions, are commonly evaluated through a sequence of invocations to highly optimised kernels provided in libraries such as BLAS and LAPACK. A sequence of kernels represents an algorithm, and in general, because of associativity, algebraic identities, and multiple kernels, one expression can be evaluated via many different algorithms. These algorithms are all mathematically equivalent (i.e., in exact arithmetic, they all compute the same result), but often differ noticeably in terms of execution time. When faced with a decision, high-level languages, libraries, and tools such as Julia, Armadillo, and Linnea choose by selecting the algorithm that minimises the FLOP count. In this paper, we test the validity of the FLOP count as a discriminant for dense linear algebra algorithms, analysing ”anomalies”: problem instances for which the fastest algorithm does not perform the least number of FLOPs. To do so, we focused on relatively simple expressions and analysed when and why anomalies occurred. We found that anomalies exist and tend to cluster into large contiguous regions. For one expression anomalies were rare, whereas for the other they were abundant. We conclude that FLOPs is not a sufficiently dependable discriminant even when building algorithms with highly optimised kernels. Plus, most of the anomalies remained as such even after filtering out the inter-kernel cache effects. We conjecture that combining FLOP counts with kernel performance models will significantly improve our ability to choose optimal algorithms.},
  keywords={arxiv:2207.02070}
}

@article{han2022folionatural,
  title={FOLIO: Natural Language Reasoning with First-Order Logic},
  author={Simeng Han and Hailey Schoelkopf and Yilun Zhao and Zhenting Qi and Martin Riddell and Luke Benson and Lucy Sun and E. Zubova and Yujie Qiao and Matthew Burtell and David Peng and Jonathan Fan and Yixin Liu and Brian Wong and Malcolm Sailor and Ansong Ni and Linyong Nan and Jungo Kasai and Tao Yu and Rui Zhang and Shafiq R. Joty and Alexander R. Fabbri and Wojciech Kryscinski and Xi Victoria Lin and Caiming Xiong and Dragomir R. Radev},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2209.00840},
  url={https://www.semanticscholar.org/paper/5581bf85386737bd3378eec68189759a05280bea},
  abstract={Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.},
  keywords={arxiv:2209.00840}
}

@article{cockrell2022facilitatingautomated,
  title={Facilitating automated conversion of scientific knowledge into scientific simulation models with the Machine Assisted Generation, Calibration, and Comparison (MAGCC) Framework},
  author={Chase Cockrell and S. Christley and G. An},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2204.10382},
  url={https://www.semanticscholar.org/paper/85808a4a7199dbc7890152caf84e9ecd73ae7563},
  abstract={The Machine Assisted Generation, Comparison, and Calibration (MAGCC) framework provides machine assistance and automation of recurrent crucial steps and processes in the development, implementation, testing, and use of scientific simulation models. MAGCC bridges systems for knowledge extraction via natural language processing or extracted from existing mathematical models and provides a comprehensive workflow encompassing the composition of scientific models and artificial intelligence (AI)-assisted code generation. MAGCC accomplishes this through: 1) the development of a comprehensively expressive formal knowledge representation knowledgebase, the Structured Scientific Knowledge Representation (SSKR) that encompasses all the types of information needed to make any simulation model, 2) the use of an artificially-intelligent logic-reasoning system, the Computational Modeling Assistant (CMA), that takes information from the SSKR and generates, in a traceable fashion, model specifications across a range of simulation modeling methods, and 3) the use of the CMA to generate compliable/executable code for a simulation model from those model specifications. The current MAGCC framework can be customized any scientific domain’s specific knowledgebase and existing mathematical/computational models, and future work will involve expanding the types of computational model representation that can be generated and integrating newly-developed code generating AI systems.},
  keywords={arxiv:2204.10382}
}

@misc{imai2022facultymembers,
  title={Faculty Members and Labs in Department of Computer Science},
  author={Hiroshi Imai},
  year={2022},
  url={https://www.semanticscholar.org/paper/67f3fd328281e803e6fb2a6909f0ec02e3985b17}
}

@article{creswell2022faithfulreasoning,
  title={Faithful Reasoning Using Large Language Models},
  author={Antonia Creswell and M. Shanahan},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/f0a0e8b6e84207f50db4d24cc4016e40601214ef},
  abstract={Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.},
  keywords={arxiv:2208.14271}
}

@article{paltenghi2022followupattention,
  title={Follow-Up Attention: An Empirical Study of Developer and Neural Model Code Exploration},
  author={Matteo Paltenghi and Rahul Pandita and Austin Z. Henley and Albert Ziegler},
  year={2022},
  journal={IEEE Transactions on Software Engineering},
  doi={10.1109/TSE.2024.3445338},
  url={https://www.semanticscholar.org/paper/eff1130a44419b21971dc2228576066af36a20f8},
  abstract={Recent neural models of code, such as OpenAI Codex and AlphaCode, have demonstrated remarkable proficiency at code generation due to the underlying attention mechanism. However, it often remains unclear how the models actually process code, and to what extent their reasoning and the way their attention mechanism scans the code matches the patterns of developers. A poor understanding of the model reasoning process limits the way in which current neural models are leveraged today, so far mostly for their raw prediction. To fill this gap, this work studies how the processed attention signal of three open large language models - CodeGen, InCoder and GPT-J - agrees with how developers look at and explore code when each answers the same sensemaking questions about code. Furthermore, we contribute an open-source eye-tracking dataset comprising 92 manually-labeled sessions from 25 developers engaged in sensemaking tasks. We empirically evaluate five heuristics that do not use the attention and ten attention-based post-processing approaches of the attention signal of CodeGen against our ground truth of developers exploring code, including the novel concept of follow-up attention which exhibits the highest agreement between model and human attention. Our follow-up attention method can predict the next line a developer will look at with 47\% accuracy. This outperforms the baseline prediction accuracy of 42.3\%, which uses the session history of other developers to recommend the next line. These results demonstrate the potential of leveraging the attention signal of pre-trained models for effective code exploration.},
  keywords={arxiv:2210.05506}
}

@article{fiore2022formalmetatheory,
  title={Formal metatheory of second-order abstract syntax},
  author={M. Fiore and Dmitrij Szamozvancev},
  year={2022},
  booktitle={Proc. ACM Program. Lang.},
  doi={10.1145/3498715},
  url={https://www.semanticscholar.org/paper/12f42f44024d47a9fbbad04d7d2701b6398e6f44},
  abstract={Despite extensive research both on the theoretical and practical fronts, formalising, reasoning about, and implementing languages with variable binding is still a daunting endeavour – repetitive boilerplate and the overly complicated metatheory of capture-avoiding substitution often get in the way of progressing on to the actually interesting properties of a language. Existing developments offer some relief, however at the expense of inconvenient and error-prone term encodings and lack of formal foundations. We present a mathematically-inspired language-formalisation framework implemented in Agda. The system translates the description of a syntax signature with variable-binding operators into an intrinsically-encoded, inductive data type equipped with syntactic operations such as weakening and substitution, along with their correctness properties. The generated metatheory further incorporates metavariables and their associated operation of metasubstitution, which enables second-order equational/rewriting reasoning. The underlying mathematical foundation of the framework – initial algebra semantics – derives compositional interpretations of languages into their models satisfying the semantic substitution lemma by construction.},
  keywords={arxiv:2201.03504}
}

@article{yoon2022formalreasoning,
  title={Formal reasoning about layered monadic interpreters},
  author={Irene Yoon and Yannick Zakowski and Steve Zdancewic},
  year={2022},
  booktitle={Proc. ACM Program. Lang.},
  doi={10.1145/3547630},
  url={https://www.semanticscholar.org/paper/883f4010fcc3fd47068d0d17c838d19cad5fcbd7},
  abstract={Monadic computations built by interpreting, or handling, operations of a free monad are a compelling formalism for modeling language semantics and defining the behaviors of effectful systems. The resulting layered semantics offer the promise of modular reasoning principles based on the equational theory of the underlying monads. However, there are a number of obstacles to using such layered interpreters in practice. With more layers comes more boilerplate and glue code needed to define the monads and interpreters involved. That overhead is compounded by the need to define and justify the relational reasoning principles that characterize the equivalences at each layer. This paper addresses these problems by significantly extending the capabilities of the Coq interaction trees (ITrees) library, which supports layered monadic interpreters. We characterize a rich class of interpretable monads---obtained by applying monad transformers to ITrees---and show how to generically lift interpreters through them. We also introduce a corresponding framework for relational reasoning about "equivalence of monads up to a relation R". This collection of typeclasses, instances, new reasoning principles, and tactics greatly generalizes the existing theory of the ITree library, eliminating large amounts of unwieldy boilerplate code and dramatically simplifying proofs.}
}

@article{tran2022formalspecification,
  title={Formal specification and model checking of lattice-based key encapsulation mechanisms in Maude},
  author={Duong Dinh Tran and K. Ogata and Santiago Escobar and S. Akleylek and A. Otmani},
  year={2022},
  booktitle={FAVPQC@ICFEM},
  url={https://www.semanticscholar.org/paper/8ad95fba177e51c46192510578dfa0178b3d7ea4}
}

@article{drori2022fromhuman,
  title={From Human Days to Machine Seconds: Automatically Answering and Generating Machine Learning Final Exams},
  author={Iddo Drori and Sarah J. Zhang and Reece Shuttleworth and Sarah J. Zhang and Keith Tyser and Zad Chin and Pedro Lantigua and Saisamrit Surbehera and Gregory Hunter and Derek Austin and Leonard Tang and Yann Hicke and Sage Simhon and S. Karnik and Darnell Granberry and Madeleine Udell},
  year={2022},
  booktitle={Knowledge Discovery and Data Mining},
  doi={10.1145/3580305.3599827},
  url={https://www.semanticscholar.org/paper/a3dc36cb2ad9920f35746f980f003d423883f97c},
  abstract={A final exam in machine learning at a top institution such as MIT, Harvard, or Cornell typically takes faculty days to write, and students hours to solve. We demonstrate that large language models pass machine learning finals at a human level on finals available online and automatically generate new human-quality final exam questions in seconds. Previous work has developed program synthesis and few-shot learning methods to solve university-level problem set questions in mathematics and STEM courses. In this work, we develop and compare methods that solve final exams, which differ from problem sets in several ways: the questions are longer, have multiple parts, are more complicated, and span a broader set of topics. We curate a dataset and benchmark of questions from machine learning final exams available online and code for answering these questions and generating new questions. We show how to generate new questions from other questions and course notes. For reproducibility and future research on this final exam benchmark, we use automatic checkers for multiple-choice, numeric, and questions with expression answers. A student survey comparing the quality, appropriateness, and difficulty of machine-generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated questions and are suitable for final exams. We perform ablation studies comparing zero-shot learning with few-shot learning and chain-of-thought prompting using GPT-3, OPT, Codex, and ChatGPT across machine learning topics and find that few-shot learning methods perform best. We highlight the transformative potential of language models to streamline the writing and solution of large-scale assessments, significantly reducing the workload from human days to mere machine seconds. Our results suggest that rather than banning large language models such as ChatGPT in class, instructors should teach students to harness them by asking students meta-questions about correctness, completeness, and originality of the responses generated, encouraging critical thinking in academic studies.},
  keywords={arxiv:2206.05442}
}

@article{kohler2022frominference,
  title={From inference processes to situations of misunderstanding},
  author={Alaric Kohler and Teuta Mehmeti},
  year={2022},
  journal={Journal of Argumentation in Context},
  doi={10.1075/jaic.18010.koh},
  url={https://www.semanticscholar.org/paper/94fd6bf8a31a6856b7c037890d87f7c1b5b2d3b9},
  abstract={
In this paper, we describe inferences on a school task, which are reconstructed by the mean of two perspectives from argumentation theory: The pragma-dialectical model and Grize’s natural logic. Both analyses focus on the same item of mathematics, issued from a PISA survey, in order to discuss their specific contribution in elucidating the actual reasoning involved in both the student's answer and the evaluator’s expectations. The mismatch between these two points of view allow us to discuss the potentiality of a situation of misunderstanding.
Investigating how specific tasks in particular contexts are interpreted provides a contribution to methodological approaches treating thinking processes as situated and socially negotiated from a diversity of points of views, as for example Inhelder’s (1962) microgenetic approach. In order to extend such analysis to interpretations of discourse, an interdisciplinary approach combining argumentation theory and socio-cognitive psychology is needed.
Here, we observed for instance that students may provide the expected answers and still interpret the question or problem differently from the task’s designers (or “teacher”). The meaning of language and other signs, such as graphs or mathematical symbols, cannot be taken for granted when several interlocutors are involved. This issue chiefly concerns argumentation theory, since it raises the question of the integration of specific contexts and points of view in the analysis of argumentation. Therefore, argumentation should be analysed also as a process, and not only as a product; For more detail on this distinction, see for instance Grize (1996) and Kuhn \& Udell (2003, 2007).}
}

@article{dimitrov2022fusionbrainresearch,
  title={FusionBrain: Research Project in Multimodal and Multitask Learning},
  author={Dimitar I. Dimitrov and A. Kuznetsov and A. A. Mal’tseva and E. F. Goncharova},
  year={2022},
  booktitle={Doklady. Mathematics},
  doi={10.1134/S1064562422060242},
  url={https://www.semanticscholar.org/paper/91694fc5f0bae350157f4fc565d0207ae12f7eb9}
}

@article{yu2022fuzzytissuelike,
  title={Fuzzy tissue-like P systems with promoters and their application in power coordinated control of microgrid},
  author={Wenping Yu and Jieping Wu and Yufeng Chen and Yubo Wu},
  year={2022},
  journal={Journal of Membrane Computing},
  doi={10.1007/s41965-022-00109-2},
  url={https://www.semanticscholar.org/paper/b7440c855634668553dd5ceed5109667f4dd7f20}
}

@article{kashyap2022gptneocommonsense,
  title={GPT-Neo for commonsense reasoning-a theoretical and practical lens},
  author={Rohan Kashyap and Vivek Kashyap and Narendra C.P},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.15593},
  url={https://www.semanticscholar.org/paper/d7a9b3c750c7e5f0c9af3864a796b7fcdc07f030},
  abstract={Recent work has demonstrated substantial gains in pre-training large-language models (LLMs) followed by supervised fine-tuning on the downstream task. In this paper, we evaluate the performance of the GPT-neo model using \$6\$ commonsense reasoning benchmark tasks. We aim to examine the performance of smaller models using the GPT-neo models against several larger model baselines such as GPT-\$3\$, Llama-\$2\$, MPT and Falcon. Upon fine-tuning with the appropriate set of hyperparameters, our model achieves competitive accuracy on several tasks. We also investigate and substantiate our results using attention-head visualization to better understand the model performance. Finally, we conduct various robustness tests using various methods to gauge the model performance under numerous settings.},
  keywords={arxiv:2211.15593}
}

@article{taylor2022galacticalarge,
  title={Galactica: A Large Language Model for Science},
  author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and A. Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1},
  abstract={Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.},
  keywords={arxiv:2211.09085}
}

@article{mauec2022geodingeosciencebased,
  title={GeoDIN - Geoscience-Based Deep Interaction Networks for Predicting Flow Dynamics in Reservoir Simulation Models},
  author={M. Maučec and Ridwan Jalali},
  year={2022},
  journal={SPE Journal},
  doi={10.2118/203952-pa},
  url={https://www.semanticscholar.org/paper/e35997934ca17d4fffe1f35e410696279240d088},
  abstract={
 Network graphs represent a general language for describing complex systems and a framework for knowledge discovery. Graph learning is a new concept with applications emerging in biomedicine, pharmacology, smart mobility, and physical reasoning. When applied to petroleum systems, such as reservoir models, graphs provide unique differentiators for the abstraction of reservoir connectivity to facilitate “reservoir-centric” machine learning (ML) applications.
 In this paper, we demonstrate, for the first time, the application of geoscience-based deep interaction networks (GeoDIN) to learn complex physics relationships from 3D reservoir models for fast and accurate prediction of subsurface spatio-temporal flow dynamics. We build the network graph with embedded subsurface and physics representations and train the ML model to “act like the reservoir simulator.”
 We use a simulation benchmark model for two-phase incompressible flow, with approximately 1.1 million grid size, one central injector, and four corner producers. Static 3D grid properties include porosity and permeability. We use full-physics simulation output to construct the interaction network (IN) graph, where graph nodes objects (nodes) represent reservoir grid cells. We embed the feature vector combining pore, oil and water volumes, and pressure and relative permeability. The graph objects representing wells are connected with well completion factors. The producing wells have embedded oil and water production rates, while the objects representing injecting wells have embedded water injection rates. We represent graph relations (edges) with bidirectional transmissibility of the source cell. To preprocess the data for ML, we scale the graph object attributes using “min-max” normalization and we normalize the graph relation attributes using Box-Cox transformation.
 We train the GeoDIN framework to predict oil and water saturation dynamics in space and time. When benchmarked with full-physics simulation, the INs ran on two V100 graphics processing units and substantially accelerated the prediction phase compared to the physics-based simulator running on 70 Intel Xeon E5 CPU cores. On average, the error in GeoDIN predicted spatio-temporal distribution of oil saturation remains within 5\% of full-physics simulation for 90\% of model grid cells, while the error in water saturation remains within 2.5\% of full-physics simulation. The spatio-temporal propagation of pressure is more sensitive to local embeddings of INs, which communicate on node-to-node information transfer. This results in a larger prediction error of the GeoDIN model when benchmarked to full-physics simulation. On average, the error distribution suggests that the great majority (90 to 95\%) of grid cells fall within 10 to 30\% error bound relative to full-physics simulation.
 The presented GeoDIN approach to network learning carries a game-changing potential for the prediction of subsurface flow dynamics. As the way forward, we will investigate the implementation of graph neural networks with automated feature learning, generalization, and scaleup.}
}

@misc{shekkizhar2022geometrylearning,
  title={Geometry of learning: A data-driven understanding with neighborhood and graph methods},
  author={Sarath Shekkizhar},
  year={2022},
  url={https://www.semanticscholar.org/paper/59ec42d3f55d895a0f513ec406299dbf85d93f05}
}

@article{zhang2022greaselmgraph,
  title={GreaseLM: Graph REASoning Enhanced Language Models for Question Answering},
  author={Xikun Zhang and Antoine Bosselut and Michihiro Yasunaga and Hongyu Ren and Percy Liang and Christopher D. Manning and J. Leskovec},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/4ab41d9780f1d1ac34d39fa7e527e73652507fcc},
  abstract={Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.},
  keywords={arxiv:2201.08860}
}

@article{min2022groundingvisual,
  title={Grounding Visual Representations with Texts for Domain Generalization},
  author={Seonwoo Min and Nokyung Park and Siwon Kim and Seunghyun Park and Jinkyu Kim},
  year={2022},
  booktitle={European Conference on Computer Vision},
  doi={10.48550/arXiv.2207.10285},
  url={https://www.semanticscholar.org/paper/08fe439561157188b076b5c4b5c45e7e72b38741},
  abstract={Reducing the representational discrepancy between source and target domains is a key component to maximize the model generalization. In this work, we advocate for leveraging natural language supervision for the domain generalization task. We introduce two modules to ground visual representations with texts containing typical reasoning of humans: (1) Visual and Textual Joint Embedder and (2) Textual Explanation Generator. The former learns the image-text joint embedding space where we can ground high-level class-discriminative information into the model. The latter leverages an explainable model and generates explanations justifying the rationale behind its decision. To the best of our knowledge, this is the first work to leverage the vision-and-language cross-modality approach for the domain generalization task. Our experiments with a newly created CUB-DG benchmark dataset demonstrate that cross-modality supervision can be successfully used to ground domain-invariant visual representations and improve the model generalization. Furthermore, in the large-scale DomainBed benchmark, our proposed method achieves state-of-the-art results and ranks 1st in average performance for five multi-domain datasets. The dataset and codes are available at https://github.com/mswzeus/GVRT.},
  keywords={arxiv:2207.10285}
}

@misc{lv2022guesteditorial,
  title={Guest Editorial Preface},
  author={Jianhui Lv},
  year={2022},
  url={https://www.semanticscholar.org/paper/b216a212533213429aef2e29ded888d53b05df2d}
}

@article{snchez2022hiddenschema,
  title={Hidden Schema Networks},
  author={Ramsés J. Sánchez and L. Conrads and Pascal Welke and K. Cvejoski and C. Ojeda},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2207.03777},
  url={https://www.semanticscholar.org/paper/bcf185005b4741d6b57fb017c9620a7a704db1c1},
  abstract={Large, pretrained language models infer powerful representations that encode rich semantic and syntactic content, albeit implicitly. In this work we introduce a novel neural language model that enforces, via inductive biases, explicit relational structures which allow for compositionality onto the output representations of pretrained language models. Specifically, the model encodes sentences into sequences of symbols (composed representations), which correspond to the nodes visited by biased random walkers on a global latent graph, and infers the posterior distribution of the latter. We first demonstrate that the model is able to uncover ground-truth graphs from artificially generated datasets of random token sequences. Next, we leverage pretrained BERT and GPT-2 language models as encoder and decoder, respectively, to infer networks of symbols (schemata) from natural language datasets. Our experiments show that (i) the inferred symbols can be interpreted as encoding different aspects of language, as e.g. topics or sentiments, and that (ii) GPT-2-like models can effectively be conditioned on symbolic representations. Finally, we explore training autoregressive, random walk “reasoning” models on schema networks inferred from commonsense knowledge databases, and using the sampled paths to enhance the performance of pretrained language models on commonsense If-Then reasoning tasks.},
  keywords={arxiv:2207.03777}
}

@article{gao2022hierarchicalannotation,
  title={Hierarchical Annotation for Building A Suite of Clinical Natural Language Processing Tasks: Progress Note Understanding},
  author={Yanjun Gao and Dmitriy Dligach and Timothy Miller and S. Tesch and Ryan Laffin and M. Churpek and M. Afshar},
  year={2022},
  booktitle={International Conference on Language Resources and Evaluation},
  doi={10.48550/arXiv.2204.03035},
  url={https://www.semanticscholar.org/paper/6a3a75561c627c118778e4d056c080cd70056d21},
  abstract={Applying methods in natural language processing on electronic health records (EHR) data has attracted rising interests. Existing corpus and annotation focus on modeling textual features and relation prediction. However, there are a paucity of annotated corpus built to model clinical diagnostic thinking, a processing involving text understanding, domain knowledge abstraction and reasoning. In this work, we introduce a hierarchical annotation schema with three stages to address clinical text understanding, clinical reasoning and summarization. We create an annotated corpus based on a large collection of publicly available daily progress notes, a type of EHR that is time-sensitive, problem-oriented, and well-documented by the format of Subjective, Objective, Assessment and Plan (SOAP). We also define a new suite of tasks, Progress Note Understanding, with three tasks utilizing the three annotation stages. This new suite aims at training and evaluating future NLP models for clinical text understanding, clinical knowledge representation, inference and summarization.},
  keywords={arxiv:2204.03035}
}

@article{liang2022holisticevaluation,
  title={Holistic Evaluation of Language Models},
  author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher Ré and Diana Acosta-Navas and Drew A. Hudson and E. Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel J. Orr and Lucia Zheng and Mert Yüksekgönül and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri S. Chatterji and O. Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.09110},
  url={https://www.semanticscholar.org/paper/29abcf865613287c661385c39401424f709a3fda},
  abstract={Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
  keywords={arxiv:2211.09110}
}

@article{kant2022housekeeptidying,
  title={Housekeep: Tidying Virtual Households using Commonsense Reasoning},
  author={Yash Kant and Arun Ramachandran and Sriram Yenamandra and Igor Gilitschenski and Dhruv Batra and Andrew Szot and Harsh Agrawal},
  year={2022},
  booktitle={European Conference on Computer Vision},
  doi={10.48550/arXiv.2205.10712},
  url={https://www.semanticscholar.org/paper/7890ece03cfb88e0620f8e791105569bd7128c76},
  abstract={We introduce Housekeep, a benchmark to evaluate commonsense reasoning in the home for embodied AI. In Housekeep, an embodied agent must tidy a house by rearranging misplaced objects without explicit instructions specifying which objects need to be rearranged. Instead, the agent must learn from and is evaluated against human preferences of which objects belong where in a tidy house. Specifically, we collect a dataset of where humans typically place objects in tidy and untidy houses constituting 1799 objects, 268 object categories, 585 placements, and 105 rooms. Next, we propose a modular baseline approach for Housekeep that integrates planning, exploration, and navigation. It leverages a fine-tuned large language model (LLM) trained on an internet text corpus for effective planning. We show that our baseline agent generalizes to rearranging unseen objects in unknown environments. See our webpage for more details: https://yashkant.github.io/housekeep/},
  keywords={arxiv:2205.10712}
}

@article{gilson2022welldoes,
  title={How Well Does ChatGPT Do When Taking the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment},
  author={A. Gilson and C. Safranek and Ting Huang and V. Socrates and L. Chi and R. A. Taylor and David Chartash},
  year={2022},
  booktitle={medRxiv},
  doi={10.1101/2022.12.23.22283901},
  url={https://www.semanticscholar.org/paper/7d4867e28b02059eef4cb25bfcd304b2071b30a9},
  abstract={Background: ChatGPT is a 175 billion parameter natural language processing model which can generate conversation style responses to user input. Objective: To evaluate the performance of ChatGPT on questions within the scope of United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as analyze responses for user interpretability. Methods: We used two novel sets of multiple choice questions to evaluate ChatGPT's performance, each with questions pertaining to Step 1 and Step 2. The first was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the userbase. The second, was the National Board of Medical Examiners (NBME) Free 120-question exams. After prompting ChatGPT with each question, ChatGPT's selected answer was recorded, and the text output evaluated across three qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results: On the four datasets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free- Step2, ChatGPT achieved accuracies of 44\%, 42\%, 64.4\%, and 57.8\%. The model demonstrated a significant decrease in performance as question difficulty increased (P=.012) within the AMBOSS- Step1 dataset. We found logical justification for ChatGPT's answer selection was present in 100\% of outputs. Internal information to the question was present in >90\% of all questions. The presence of information external to the question was respectively 54.5\% and 27\% lower for incorrect relative to correct answers on the NBME-Free-Step1 and NBME-Free-Step2 datasets (P<=.001). Conclusion: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at greater than 60\% threshold on the NBME-Free- Step-1 dataset we show that the model is comparable to a third year medical student. Additionally, due to the dialogic nature of the response to questions, we demonstrate ChatGPT's ability to provide reasoning and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as a medical education tool.}
}

@article{muratet2022assistdesigners,
  title={How to assist designers to model learning games with Petri nets?},
  author={Mathieu Muratet and T. Carron and Amel Yessad},
  year={2022},
  booktitle={International Conference on Foundations of Digital Games},
  doi={10.1145/3555858.3555937},
  url={https://www.semanticscholar.org/paper/beee1f469926964e4560ec2c666385c313af5569},
  abstract={In previous research, we presented a methodological framework that provides players with adaptive feedback. The core of this framework relies on modeling the learning game with a Petri net. However, this modeling is a challenging task. Indeed, Petri nets are well adapted to model dynamic and complex systems but require a mastery of the underlying mathematical formalism to build them manually. In particular, when the learning game is characterized by a large freedom of action. In this paper, we present an authoring tool and its domain-specific language to assist designers to model learning games with Petri nets. We carried out a case study where our contribution was implemented. Results show that our contribution helps designers to build the Petri net in combination with classical Petri net editors which are still useful to visualize, to check and to validate the Petri nets built.}
}

@article{manning2022humanlanguage,
  title={Human Language Understanding \& Reasoning},
  author={Christopher D. Manning},
  year={2022},
  booktitle={Daedalus},
  doi={10.1162/daed_a_01905},
  url={https://www.semanticscholar.org/paper/a0c87ee1b0903c1c9ac72809caf75b6b6997baa0},
  abstract={Abstract The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.}
}

@article{hagendorff2022humanlikeintuitive,
  title={Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT},
  author={Thilo Hagendorff and Sarah Fabi and Michal Kosinski},
  year={2022},
  booktitle={Nature Computational Science},
  doi={10.1038/s43588-023-00527-x},
  url={https://www.semanticscholar.org/paper/0bfc05adcddd4fe5d1335d96cc313c41526d4558},
  abstract={We design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics. The reasoning capabilities of OpenAI’s generative pre-trained transformer family were tested using semantic illusions and cognitive reflection tests that are typically used in human studies. While early models were prone to human-like cognitive errors, ChatGPT decisively outperformed humans, avoiding the cognitive traps embedded in the tasks.},
  keywords={arxiv:2306.07622}
}

@article{prihar2022identifyingexplanations,
  title={Identifying Explanations Within Student-Tutor Chat Logs},
  author={Ethan Prihar},
  year={2022},
  booktitle={Educational Data Mining},
  url={https://www.semanticscholar.org/paper/53ef2dccbd6ff2b521bff18702231d67de77749f}
}

@article{mamyrbayev2022identifyinginfluence,
  title={Identifying the influence of transfer learning method in developing an end-to-end automatic speech recognition system with a low data level},
  author={O. Mamyrbayev and K. Alimhan and Dina Oralbekova and A. Bekarystankyzy and B. Zhumazhanov},
  year={2022},
  journal={Eastern-European Journal of Enterprise Technologies},
  doi={10.15587/1729-4061.2022.252801},
  url={https://www.semanticscholar.org/paper/a58a42aefa22c59b188421752b9b3305e4ec1c27},
  abstract={Ensuring the best quality and performance of modern speech technologies, today, is possible based on the widespread use of machine learning methods. The idea of this project is to study and implement an end-to-end system of automatic speech recognition using machine learning methods, as well as to develop new mathematical models and algorithms for solving the problem of automatic speech recognition for agglutinative (Turkic) languages.
Many research papers have shown that deep learning methods make it easier to train automatic speech recognition systems that use an end-to-end approach. This method can also train an automatic speech recognition system directly, that is, without manual work with raw signals. Despite the good recognition quality, this model has some drawbacks. These disadvantages are based on the need for a large amount of data for training. This is a serious problem for low-data languages, especially Turkic languages such as Kazakh and Azerbaijani. To solve this problem, various methods are needed to apply. Some methods are used for end-to-end speech recognition of languages belonging to the group of languages of the same family (agglutinative languages). Method for low-resource languages is transfer learning, and for large resources – multi-task learning. To increase efficiency and quickly solve the problem associated with a limited resource, transfer learning was used for the end-to-end model. The transfer learning method helped to fit a model trained on the Kazakh dataset to the Azerbaijani dataset. Thereby, two language corpora were trained simultaneously. Conducted experiments with two corpora show that transfer learning can reduce the symbol error rate, phoneme error rate (PER), by 14.23 \% compared to baseline models (DNN+HMM, WaveNet, and CNC+LM). Therefore, the realized model with the transfer method can be used to recognize other low-resource languages.}
}

@article{ingraham2022illuminatingprotein,
  title={Illuminating protein space with a programmable generative model},
  author={John Ingraham and Max Baranov and Zak Costello and Vincent Frappier and Ahmed Ismail and Shan Tie and Wujie Wang and Vincent Xue and F. Obermeyer and Andrew L. Beam and G. Grigoryan},
  year={2022},
  booktitle={bioRxiv},
  doi={10.1038/s41586-023-06728-8},
  url={https://www.semanticscholar.org/paper/2cfc26b4af99f195b433fa7aa00f221b111c7cd4},
  abstract={Three billion years of evolution has produced a tremendous diversity of protein molecules\^{} 1 , but the full potential of proteins is likely to be much greater. Accessing this potential has been challenging for both computation and experiments because the space of possible protein molecules is much larger than the space of those likely to have functions. Here we introduce Chroma, a generative model for proteins and protein complexes that can directly sample novel protein structures and sequences, and that can be conditioned to steer the generative process towards desired properties and functions. To enable this, we introduce a diffusion process that respects the conformational statistics of polymer ensembles, an efficient neural architecture for molecular systems that enables long-range reasoning with sub-quadratic scaling, layers for efficiently synthesizing three-dimensional structures of proteins from predicted inter-residue geometries and a general low-temperature sampling algorithm for diffusion models. Chroma achieves protein design as Bayesian inference under external constraints, which can involve symmetries, substructure, shape, semantics and even natural-language prompts. The experimental characterization of 310 proteins shows that sampling from Chroma results in proteins that are highly expressed, fold and have favourable biophysical properties. The crystal structures of two designed proteins exhibit atomistic agreement with Chroma samples (a backbone root-mean-square deviation of around 1.0 Å). With this unified approach to protein design, we hope to accelerate the programming of protein matter to benefit human health, materials science and synthetic biology. Evolution has produced a range of diverse proteins, and now a generative model called Chroma can expand that set by allowing the user to design new proteins and protein complexes with desired properties and functions.}
}

@article{vilnis2022impaktdataset,
  title={ImPaKT: A Dataset for Open-Schema Knowledge Base Construction},
  author={L. Vilnis and Zachary Kenneth Fisher and Bhargav Kanagal and Patrick C. Murray and Sumit K. Sanghai},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.10770},
  url={https://www.semanticscholar.org/paper/18e1dd6604f98ebcdf3f281803a5c92763e3ffef},
  abstract={Large language models have ushered in a golden age of semantic parsing. The seq2seq paradigm allows for open-schema and abstractive attribute and relation extraction given only small amounts of finetuning data. Language model pretraining has simultaneously enabled great strides in natural language inference, reasoning about entailment and implication in free text. These advances motivate us to construct ImPaKT, a dataset for open-schema information extraction, consisting of around 2500 text snippets from the C4 corpus, in the shopping domain (product buying guides), professionally annotated with extracted attributes, types, attribute summaries (attribute schema discovery from idiosyncratic text), many-to-one relations between compound and atomic attributes, and implication relations. We release this data in hope that it will be useful in fine tuning semantic parsers for information extraction and knowledge base construction across a variety of domains. We evaluate the power of this approach by fine-tuning the open source UL2 language model on a subset of the dataset, extracting a set of implication relations from a corpus of product buying guides, and conducting human evaluations of the resulting predictions.},
  keywords={arxiv:2212.10770}
}

@misc{sap2022imaginedversus,
  title={Imagined versus Remembered Stories: Quantifying Differences in Narrative Flow},
  author={Maarten Sap and A. Jafarpour and Yejin Choi and Noah A. Smith and J. Pennebaker and E. Horvitz},
  year={2022},
  url={https://www.semanticscholar.org/paper/5b49f4a3b4cd6c3bed3441a927eaa721fd8d36dd},
  abstract={Lifelong experiences and learned knowledge lead to shared expectations about how common situations tend to unfold. Such knowledge of narrative event flow enables people to weave together a story. However, comparable computational tools to evaluate the flow of events in narratives are limited. We quantify the differences between autobiographical and imagined stories by introducing sequentiality , a measure of narrative flow of events, drawing probabilistic inferences from a cutting-edge large language model (GPT-3). Sequentiality captures the flow of a narrative by comparing the probability of a sentence with and without its preceding story context. We applied our measure to study thousands of diary-like stories, collected from crowdworkers about either a recent remembered experience or an imagined story on the same topic. The results show that imagined stories have higher sequentiality than autobiographical stories and that the sequentiality of autobiographical stories increases when the memories are retold several months later. In pur-suit of deeper understandings of how sequentiality measures the flow of narratives, we explore proportions of major and minor events in story sentences, as annotated by crowdworkers. We find that lower sequentiality is associated with higher proportions of major events. The methods and results highlight opportunities to use cutting-edge computational analyses, such as sequentiality, on large corpora of matched imagined and autobiographical stories to investigate the influences of memory and reasoning on language generation processes.},
  keywords={arxiv:2201.02662}
}

@article{nadiah2022implementationdecision,
  title={Implementation of Decision Tree Algorithm Machine Learning in Detecting Covid-19 Virus Patients Using Public Datasets},
  author={Nadiah Nadiah and Sopian Soim and Sholihin Sholihin},
  year={2022},
  journal={Indonesian Journal of Artificial Intelligence and Data Mining},
  doi={10.24014/ijaidm.v5i1.17054},
  url={https://www.semanticscholar.org/paper/f83d0e2fda2682b6e724c42771c73766e0bd7c62},
  abstract={The advancement of AI (Artificial Intelligence) technology has been widely implemented in numerous sectors of daily life. Machine Learning is one of the subfields of Artificial Intelligence. Using statistics, mathematics, and data mining, machine learning is developed so that machines may learn by assessing data without being reprogrammed. At this time the world is on alert for the spread of a popular virus, the corona virus. Coronaviruses are part of a family of viruses caused by diseases ranging from the flu. The disease caused by the coronavirus is known as Covid-19. Therefore, to help identify whether a somebody has coronavirus disease based on certain symptoms, a model is created that can classify people with the covid-19 virus using machine learning. The classification methods utilized in this study are decision trees and large-scale machine learning projects. The study employed Python 3.7 as its programming language and PyCharm as its Integrated Development Environment (IDE). Based on the results, the accuracy rate as expected after conducting various trials is 99\%.}
}

@article{ye2022improvingcommonsense,
  title={Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles},
  author={Shuquan Ye and Yujia Xie and Dongdong Chen and Yichong Xu and Lu Yuan and Chenguang Zhu and Jing Liao},
  year={2022},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52729.2023.00259},
  url={https://www.semanticscholar.org/paper/111dadc41f7b0337235fb526bf0cd3a4ac23b98d},
  abstract={This paper focuses on analyzing and improving the commonsense ability of recent popular vision-language (VL) models. Despite the great success, we observe that existing VL-models still lack commonsense knowledge/reasoning ability (e.g., “Lemons are sour”), which is a vital component towards artificial general intelligence. Through our analysis, we find one important reason is that existing large-scale VL datasets do not contain much commonsense knowledge, which motivates us to improve the commonsense of VL-models from the data perspective. Rather than collecting a new VL training dataset, we propose a more scalable strategy, i.e., “Data Augmentation with kNowledge graph linearization for CommonsensE capability” (DANCE). It can be viewed as one type of data augmentation technique, which can inject commonsense knowledge into existing VL datasets on the fly during training. More specifically, we leverage the commonsense knowledge graph (e.g., ConceptNet) and create variants of text description in VL datasets via bidirectional sub-graph sequentialization. For better commonsense evaluation, we further propose the first retrieval-based commonsense diagnostic benchmark. By conducting extensive experiments on some representative VL-models, we demonstrate that our DANCE technique is able to significantly improve the commonsense ability while maintaining the performance on vanilla retrieval tasks. The code and data are available at https://github.com/pleaseconnectwifi/DANCE.},
  keywords={arxiv:2211.16504}
}

@article{shevchenko2022improvingradioelectronic,
  title={Improving the Radioelectronic Device Simulation Quality by Using a Step Recovery Diode},
  author={Gleb M. Shevchenko and E. Semyonov},
  year={2022},
  booktitle={International Siberian Conference on Control and Communications},
  doi={10.1109/SIBCON56144.2022.10003001},
  url={https://www.semanticscholar.org/paper/c82c398ccec82924bb71a8a7e059cdf73fe186f0},
  abstract={Computer design of radio electronic facilities and systems is currently the main tool for their creation by radio engineers. Diodes are widespread among the electronic component base used in the design. Modern physical layer models of diodes describe their operation with good accuracy, however, equivalent circuits models are used in radio engineering computer-aided design systems. In the vast majority of cases, these are simplified quasi-static models developed in the 70s of the last century. So, the dynamics of the diode operation is observed with a large error, and some aspects of transient processes are not modeled at all. In this paper we consider a refined non-quasi-static model of a diode with the dependence of the lifetime of nonequilibrium charge carriers on the forward current, the mathematical apparatus of which is expressed in the language of equivalent circuits. Therefore, it can be implemented directly by engineers. Using the dependence between the lifetime of nonequilibrium charge carriers and the forward current at a high level of injection in the non-quasistatic diode model, the modeling error of the output voltage of the push-pull pulse sharper does no more than 5\%. The standard quasi-static model gives a significantly larger modeling error for both waveform and position. It is shown that the delay between the experimental and model curve is reduced by a factor of half.}
}

@article{patel2022innetworkfractional,
  title={In-network fractional calculations using P4 for scientific computing workloads},
  author={Shivam Patel and Rigden Atsatsang and K. Tichauer and M. Wang and J. Kowalkowski and Nik Sultana},
  year={2022},
  booktitle={EuroP4@CoNEXT},
  doi={10.1145/3565475.3569083},
  url={https://www.semanticscholar.org/paper/d2eced12f2499332e1e1ee800b362ff183653b1a},
  abstract={Recent P4 research has motivated the need for in-network fractional calculations to support functions in Networking (for calculations related to active queue management and load balancing) and in Machine Learning. The P4 language and ASICs do not natively support fractional types (e.g., float). Existing P4 techniques provide incomplete emulations of the IEEE-754 standard, which was designed as a generic approach that can benefit from dedicated hardware acceleration, but whose features are difficult to fully support in P4. This paper re-thinks the foundation of in-network fractional calculation and proposes a new approach that is more resource conscious and is straightforward to encode in P4. Instead of floating-point, it uses a fixed-point encoding of numerals; and instead of sampling functions into tables it uses Taylor Approximation to reduce data-plane calculations to simple arithmetic over pre-calculated coefficients, requiring constant space and linear time. The paper describes and evaluates a P4 code synthesis algorithm that allows users to trade-off switch resources for accuracy, grounded on an application of a well-understood mathematical theory. It describes how to encode π and various functions including cos, log and exp. This technique is being developed to support Scientific Computing (SC) applications which typically make heavy use of fractional approximations of Real numbers. The paper applies this technique in a novel P4 program that is being open-sourced: in-network Monte Carlo simulation of photon propagation that models the analysis that is carried out in a class of cancer treatments. This technique is also being used in ongoing work on another SC application: online event detection in a large-scale neutrino detection experiment.}
}

@article{alamin2022incrementalaccurate,
  title={Incremental and accurate computation of machine learning models with smart data summarization},
  author={Sikder Tahsin Al-Amin and Carlos Ordonez},
  year={2022},
  journal={Journal of Intelligence and Information Systems},
  doi={10.1007/s10844-021-00690-5},
  url={https://www.semanticscholar.org/paper/83477adf53eb65408cc2938172111ecb89515551}
}

@article{tyshchuk2022informationsystem,
  title={Information system for converting audio in Ukrainian language into its textual representation using nlp methods and machine learning},
  author={Yurii Tyshchuk and V. Vysotska and Olha Vlasenko},
  year={2022},
  booktitle={Vìsnik Nacìonalʹnogo unìversitetu "Lʹvìvsʹka polìtehnìka". Serìâ Ìnformacìjnì sistemi ta merežì},
  doi={10.23939/sisn2022.12.023},
  url={https://www.semanticscholar.org/paper/f3bc69f32e76a5936ab4f833e0d912b333d057a0},
  abstract={Speech recognition involves various models, methods and algorithms for analysing and processing the user’s recorded voice. This allows people to control different systems that support one type of speech recognition. A speech-to-text conversion system is a type of speech recognition that uses spoken data for further processing. It also provides several stages for processing an audio file, which uses electroacoustic means, filtering algorithms in the audio file to isolate relevant sounds, electronic data arrays for the selected language, as well as mathematical models that make up the most likely words from phonemes. Thanks to the conversion of speech to text, people whose professions are closely related to typing a large amount of text on the keyboard, significantly speed up and facilitate the work process, as well as reduce the amount of stress. In addition, such systems help businesses, because the concept of remote work is becoming more and more popular, and therefore companies need tools to record and systematize meetings in the form of written text. The object of the research is the process of converting the Ukrainian-language text into a written one based on NLP and machine learning methods. The subject of the research is file processing algorithms for extracting relevant sounds and recognizing phonemes, as well as mathematical models for recognizing an array of phonemes as specific words. The purpose of the work is to design and develop an information system for converting audio Ukrainian-language text into written text based on the Ukrainian Speech-to-text Web application, which is a technology for accurate and easy analysis of Ukrainian-language audio files and their subsequent transcription into text. The application supports downloading files from the file system and recording using the microphone, as well as saving the analysed data. The article also describes the stages of design and the general typical architecture of the corresponding system for converting audio Ukrainian-language text into written text. According to the results of the experimental testing of the developed system, it was found that the number of words does not affect the accuracy of the conversion algorithm, and the decrease in percentage is not large and occurred due to the complexity of the words and the low quality of the microphone, and therefore the recorded file.}
}

@article{huang2022innermonologue,
  title={Inner Monologue: Embodied Reasoning through Planning with Language Models},
  author={Wenlong Huang and F. Xia and Ted Xiao and Harris Chan and Jacky Liang and Peter R. Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and P. Sermanet and Noah Brown and Tomas Jackson and Linda Luu and S. Levine and Karol Hausman and Brian Ichter},
  year={2022},
  booktitle={Conference on Robot Learning},
  doi={10.48550/arXiv.2207.05608},
  url={https://www.semanticscholar.org/paper/f3cf71c51b882fe3111d71c4bf104297d38197f8},
  abstract={Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
  keywords={arxiv:2207.05608}
}

@article{liu2022instancesequencereasoning,
  title={Instance-sequence reasoning for video question answering},
  author={R. Liu and Yahong Han},
  year={2022},
  booktitle={Frontiers of Computer Science},
  doi={10.1007/s11704-021-1248-1},
  url={https://www.semanticscholar.org/paper/fc7398788a8e14ea0b0286bf8e66e3ce9ad3b07d}
}

@article{trivedi2022interleavingretrieval,
  title={Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions},
  author={H. Trivedi and Niranjan Balasubramanian and Tushar Khot and Ashish Sabharwal},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10509},
  url={https://www.semanticscholar.org/paper/f208ea909fa7f54fea82def9a92fd81dfc758c39},
  abstract={Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.},
  keywords={arxiv:2212.10509}
}

@article{xie2022thererole,
  title={Is There a Role for Thromboprophylaxis in Selected Outpatients With COVID-19?-Reply.},
  author={Junqing Xie and D. Prieto-Alhambra},
  year={2022},
  booktitle={JAMA Internal Medicine},
  doi={10.1001/jamainternmed.2022.5881},
  url={https://www.semanticscholar.org/paper/ee20d54e70f9e9e3bcba2a060049dd55de5b443d},
  abstract={In Reply We appreciate the opportunity to respond to the letter about our study1 on maternal prescriptions filled for antipsychotics during pregnancy and offsprings’ school performance. Dr Jingjing and colleagues suggest the implementation of propensity score matching over multivariable regression to balance the covariables. Propensity score matching is commonly used for control of confounding in pharmacoepidemiologic studies, especially when the outcomes are rare, many confounders are present, and there are systematic differences in the distribution of characteristics between groups.2 In the present study,1 outcomes were standardized test scores in language and mathematics (continuous variables), and our sample was sufficiently large to allow for the inclusion of all a priori identified confounders without overfitting the models. In this scenario, the propensity score approach would yield similar or slightly weaker associations to the traditional multivariable regression3 and would reduce precision and statistical power due to nonmatched observations being discarded. Furthermore, our conclusions across all 7 sensitivity analyses were consistent, including analyses of better-balanced populations (eg, sibling comparisons and children of mothers with antipsychotic prescription fills before vs during pregnancy).1 In addition, Dr Jingjing and colleagues remarked on the potential confounding by prepregnancy body mass index (BMI). Our thoughts are the following. Individuals who are overweight and obese are at higher risk of developing mental illnesses.4 Therefore, prepregnancy BMI could potentially be associated with the likelihood of receiving antipsychotic treatment during pregnancy. Our study1 was based on the linkages of Danish nationwide registers, and information on prepregnancy BMI was not included in the Danish Medical Birth Registry until 2003.5 We acknowledged potential residual confounding from any unmeasured factors in our discussion.1 However, we considered other covariates, eg, maternal age, smoking, and education, which would partly control for prepregnancy BMI, and we expect that further adjustment for prepregnancy BMI would not change the results substantially. To test this assumption, we conducted a subgroup analysis restricted to 296 306 children born from 2004 to 2009. In this additional analysis of 565 671 language tests and 329 328 mathematics tests, we found that maternal prescription fill for antipsychotics was not associated with test score performance in language (adjusted difference, 0.5 [95\% CI, −1.3 to 2.2], and further adjustment for prepregnancy BMI, 0.7 [95\% CI, −1.0 to 2.4]) or in mathematics (adjusted difference, 0.7 [95\% CI, −1.4 to 2.7] and further adjustment for prepregnancy BMI, 0.9 [95\% CI, −1.2 to 2.9]). These results completely align with our findings in the primary analyses in the full population (children born in 1997–2009, adjusted difference, 0.5 [95\% CI, −0.8 to 1.7] for language and 0.4; [95\% CI, −1.0 to 1.8] for mathematics). Therefore, further adjustment for prepregnancy BMI did not change the overall conclusion of the study; that is, maternal antipsychotic prescription during pregnancy did not appear to be associated with offspring standardized test scores.1}
}

@article{patel2022questiondecomposition,
  title={Is a Question Decomposition Unit All We Need?},
  author={Pruthvi H. Patel and Swaroop Mishra and Mihir Parmar and Chitta Baral},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2205.12538},
  url={https://www.semanticscholar.org/paper/a80f2102e5de3ead1b9689b440503f49383ddc94},
  abstract={Large Language Models (LMs) have achieved state-of-the-art performance on many Natural Language Processing (NLP) benchmarks. With the growing number of new benchmarks, we build bigger and more complex LMs. However, building new LMs may not be an ideal option owing to the cost, time and environmental impact associated with it. We explore an alternative route: can we modify data by expressing it in terms of the model’s strengths, so that a question becomes easier for models to answer? We investigate if humans can decompose a hard question into a set of simpler questions that are relatively easier for models to solve. We analyze a range of datasets involving various forms of reasoning and find that it is indeed possible to significantly improve model performance (24\% for GPT3 and 29\% for RoBERTa-SQuAD along with a symbolic calculator) via decomposition. Our approach provides a viable option to involve people in NLP research in a meaningful way. Our findings indicate that Human-in-the-loop Question Decomposition (HQD) can potentially provide an alternate path to building large LMs.},
  keywords={arxiv:2205.12538}
}

@article{zheng2022jarvisneurosymbolic,
  title={JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents},
  author={Kai Zheng and KAI-QING Zhou and Jing Gu and Yue Fan and Jialu Wang and Zong-xiao Li and Xuehai He and X. Wang},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2208.13266},
  url={https://www.semanticscholar.org/paper/961a1772f3b90d9dffd2b571c6996007a1d0ccd1},
  abstract={Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1\textbackslash\{\}\% to 15.8\textbackslash\{\}\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.},
  keywords={arxiv:2208.13266}
}

@article{yu2022jecccommonsense,
  title={JECC: Commonsense Reasoning Tasks Derived from Interactive Fictions},
  author={Mo Yu and Xiaoxiao Guo and Yufei Feng and Yi Gu and Xiao-Dan Zhu and M. Greenspan and Murray Campbell and Chuang Gan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.15456},
  url={https://www.semanticscholar.org/paper/868f9bb603dfa8f6951787040fc6d62c909a15c2},
  abstract={Commonsense reasoning simulates the human ability to make presumptions about our physical world, and it is an essential cornerstone in building general AI systems. We propose a new commonsense reasoning dataset based on human's Interactive Fiction (IF) gameplay walkthroughs as human players demonstrate plentiful and diverse commonsense reasoning. The new dataset provides a natural mixture of various reasoning types and requires multi-hop reasoning. Moreover, the IF game-based construction procedure requires much less human interventions than previous ones. Different from existing benchmarks, our dataset focuses on the assessment of functional commonsense knowledge rules rather than factual knowledge. Hence, in order to achieve higher performance on our tasks, models need to effectively utilize such functional knowledge to infer the outcomes of actions, rather than relying solely on memorizing facts. Experiments show that the introduced dataset is challenging to previous machine reading models as well as the new large language models with a significant 20\% performance gap compared to human experts.},
  keywords={arxiv:2210.15456}
}

@article{hong2022karamlintegrating,
  title={KARaML: Integrating Knowledge-Based and Machine Learning Approaches to Solve the Winograd Schema Challenge},
  author={S. Hong and B. Bennett and Judith Clymo and Lucía Gómez Álvarez},
  year={2022},
  booktitle={Make},
  url={https://www.semanticscholar.org/paper/5394454e442f0b2d0b8e4f93dea69d458511d17e}
}

@article{gao2022kmirbenchmark,
  title={KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models},
  author={Daniel Gao and Yantao Jia and Lei Li and Chengzhen Fu and Zhicheng Dou and Hao Jiang and Xinyu Zhang and Lei Chen and Zhao Cao},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/718343008a6cfca9e86ab6160caba353c52c17cf},
  abstract={Previous works show the great potential of pre-trained language models (PLMs) for storing a large amount of factual knowledge. However, to figure out whether PLMs can be reliable knowledge sources and used as alternative knowledge bases (KBs), we need to further explore some critical features of PLMs. Firstly, knowledge memorization and identification abilities: traditional KBs can store various types of entities and relationships; do PLMs have a high knowledge capacity to store different types of knowledge? Secondly, reasoning ability: a qualified knowledge source should not only provide a collection of facts, but support a symbolic reasoner. Can PLMs derive new knowledge based on the correlations between facts? To evaluate these features of PLMs, we propose a benchmark, named Knowledge Memorization, Identification, and Reasoning test (KMIR). KMIR covers 3 types of knowledge, including general knowledge, domain-specific knowledge, and commonsense, and provides 184,348 well-designed questions. Preliminary experiments with various representative pre-training language models on KMIR reveal many interesting phenomenons: 1) The memorization ability of PLMs depends more on the number of parameters than training schemes. 2) Current PLMs are struggling to robustly remember the facts. 3) Model compression technology retains the amount of knowledge well, but hurts the identification and reasoning abilities. We hope KMIR can facilitate the design of PLMs as better knowledge sources.},
  keywords={arxiv:2202.13529}
}

@misc{chan2022knifedistilling,
  title={KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales},
  author={Aaron Chan and Zhiyuan Zeng and Wyatt Lake and Brihi Joshi and Hanjie Chen and Xiang Ren},
  year={2022},
  url={https://www.semanticscholar.org/paper/c6a47dec94ded708fe759ca0b71ad57ab72fec07},
  abstract={Language models (LMs) have yielded impressive results on many language reasoning tasks, but their unexpected errors raise doubts about their reasoning abilities. In light of this, there is growing interest in finetuning/prompting LMs with both task instances and their associated free-text rationales (FTRs), which explain the correct reasoning process for predicting the correct task output (i.e., how to be"right for the right reasons"). However, existing finetuning methods fail to improve LM performance, while prompting needs prohibitively large (i.e.,>50B) LMs to work well. We propose KNIFE, which shows that reasoning knowledge can be effectively distilled from FTRs into a small (i.e.,<1B) LM and improve the LM's performance. First, KNIFE finetunes a teacher LM (given task input and FTR) to predict the task output, transferring reasoning knowledge from the FTRs to the teacher's hidden states. Second, KNIFE finetunes a student LM (given task input only) such that its hidden states are aligned with the teacher's. Thus, the student is endowed with reasoning knowledge but can be used for inference without direct FTR input. On two question-answering datasets, KNIFE outperforms various finetuning and prompting baselines in fully-supervised and low-resource settings. Also, we observe that FTR quality is crucial to KNIFE's performance.},
  keywords={arxiv:2212.09721}
}

@article{s2022korelasiantara,
  title={KORELASI ANTARA KEMAMPUAN BELAJAR MATEMATIKA DENGAN HASIL BELAJAR FISIKA SECARA ONLINE SISWA SMUN 1 TAKENGON},
  author={Richasanty Septima S and Yenni Tirtasari},
  year={2022},
  booktitle={JURNAL RISET RUMPUN MATEMATIKA DAN ILMU PENGETAHUAN ALAM},
  doi={10.55606/jurrimipa.v1i2.509},
  url={https://www.semanticscholar.org/paper/bf2a23af7bc9b66f89b718922a837de1dcafa257},
  abstract={This study aims to see and find out whether there is a relationship between the ability to learn mathematics and the results of learning physics online. Physics is a branch of science that studies the behavior of nature through experimental observations and quantitative measurements. In studying nature, Physics uses the language of mathematics to model natural phenomena in mathematical equations. In studying Physics, it is necessary to be able to The research method used is descriptive correlation analysis. The sample of this study was 60 students for three different classes. The research instrument is a report on student learning outcomes or a list of odd semester report cards for the 2020/2021 school year. The results of data analysis show that there is a significant relationship between mathematics learning ability and online physics learning outcomes with a large correlation coefficient (r) of 0.787 For class X-1, the value of the correlation coefficient is r = 0.734 for class X-2 and r = 0.661 For class X-3. This shows that if the ability to learn mathematics is high, the learning outcomes of physics will be high as well.}
}

@misc{chen2022kritknowledgereasoning,
  title={KRIT: Knowledge-Reasoning Intelligence in vision-language Transformer},
  author={Kezhen Chen and Qiuyuan Huang and Daniel J. McDuff and Yonatan Bisk and Jianfeng Gao},
  year={2022},
  url={https://www.semanticscholar.org/paper/5d47143c13591def061e203bf6dd6d97fd110631}
}

@article{li2022kafspknowledgeaware,
  title={KaFSP: Knowledge-Aware Fuzzy Semantic Parsing for Conversational Question Answering over a Large-Scale Knowledge Base},
  author={Junzhuo Li and Deyi Xiong},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2022.acl-long.35},
  url={https://www.semanticscholar.org/paper/98932fb1bd273b01bfe31339224398c2fd45c674},
  abstract={In this paper, we study two issues of semantic parsing approaches to conversational question answering over a large-scale knowledge base: (1) The actions defined in grammar are not sufficient to handle uncertain reasoning common in real-world scenarios. (2) Knowledge base information is not well exploited and incorporated into semantic parsing. To mitigate the two issues, we propose a knowledge-aware fuzzy semantic parsing framework (KaFSP). It defines fuzzy comparison operations in the grammar system for uncertain reasoning based on the fuzzy set theory. In order to enhance the interaction between semantic parsing and knowledge base, we incorporate entity triples from the knowledge base into a knowledge-aware entity disambiguation module. Additionally, we propose a multi-label classification framework to not only capture correlations between entity types and relations but also detect knowledge base information relevant to the current utterance. Both enhancements are based on pre-trained language models. Experiments on a large-scale conversational question answering benchmark demonstrate that the proposed KaFSP achieves significant improvements over previous state-of-the-art models, setting new SOTA results on 8 out of 10 question types, gaining improvements of over 10\% F1 or accuracy on 3 question types, and improving overall F1 from 83.01\% to 85.33\%. The source code of KaFSP is available at https://github.com/tjunlp-lab/KaFSP.}
}

@article{groth2022knowledgegraphs,
  title={Knowledge Graphs and their Role in the Knowledge Engineering of the 21st Century (Dagstuhl Seminar 22372)},
  author={Paul Groth and E. Simperl and M. Erp and Denny Vrandečić},
  year={2022},
  booktitle={Dagstuhl Reports},
  doi={10.4230/DagRep.12.9.60},
  url={https://www.semanticscholar.org/paper/6de46ecb3ea78ea7ec88bf03a3b70bb8a791784f}
}

@article{huang2022knowledgegraph,
  title={Knowledge graph representation learning and graph neural networks for language understanding},
  author={Jing Huang},
  year={2022},
  booktitle={GRADES-NDA@SIGMOD},
  doi={10.1145/3534540.3534710},
  url={https://www.semanticscholar.org/paper/1ec4ac0318daf526fa77cd710b03e602449a1620},
  abstract={As AI technologies become mature in natural language processing, speech recognition and computer vision, "intelligent" user interfaces emerge to handle complex and diverse tasks that require human-like knowledge and reasoning capability. In Part 1, I will present our recent work on knowledge graph representation learning using Graph Neural Networks (GNNs): the first approach is called orthogonal transform embedding (OTE), which integrates graph context into the embedding distance scoring function and improves prediction accuracy on complex relations such as the difficult N-to-1, 1-to-N and N-to-N cases; the second approach is called multi-hop attention GNN (MAGNA), a principled way to incorporate multi-hop context information into every layer of attention computation. MAGNA uses a diffusion prior on attention values, to efficiently account for all paths between the pair of disconnected nodes. Experimental results on knowledge graph completion as well as node classification benchmarks show that MAGNA achieves state-of-the-art results. In Part 2, I will present how we take advantage of GNNs for language understanding and reasoning tasks. We show that combined with large pre-trained language models and knowledge graph embeddings, GNNs are proven effective in multi-hop reading comprehension across documents, improving time sensitivity for question answering over temporal knowledge graphs, and constructing robust syntactic information for aspect-level sentiment analysis.}
}

@article{choi2022knowledgepower,
  title={Knowledge is Power: Symbolic Knowledge Distillation, Commonsense Morality, \& Multimodal Script Knowledge},
  author={Yejin Choi},
  year={2022},
  booktitle={Web Search and Data Mining},
  doi={10.1145/3488560.3500242},
  url={https://www.semanticscholar.org/paper/8703985fd545ba7498a75fcbcc27845a3b549833}
}

@article{din2022lagcsemantics,
  title={LAGC Semantics of Concurrent Programming Languages},
  author={Crystal Chang Din and Reiner Hähnle and L. Henrio and E. Johnsen and Ka I Pun and S. L. T. Tarifa},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/770d32ad8346db10f2b96d312fd9b688be688622},
  abstract={Formal, mathematically rigorous programming language semantics are the essential prerequisite for the design of logics and calculi that permit automated reasoning about concurrent programs. We propose a novel modular semantics designed to align smoothly with program logics used in deductive verification and formal specification of concurrent programs. Our semantics separates local evaluation of expressions and statements performed in an abstract, symbolic environment from their composition into global computations, at which point they are concretised. This makes incremental addition of new language concepts possible, without the need to revise the framework. The basis is a generalisation of the notion of a program trace as a sequence of evolving states that we enrich with event descriptors and trailing continuation markers. This allows to postpone scheduling constraints from the level of local evaluation to the global composition stage, where well-formedness predicates over the event structure declaratively characterise a wide range of concurrency models. We also illustrate how a sound program logic and calculus can be defined for this semantics.},
  keywords={arxiv:2202.12195}
}

@article{kazemi2022lambadabackward,
  title={LAMBADA: Backward Chaining for Automated Reasoning in Natural Language},
  author={Seyed Mehran Kazemi and Najoung Kim and Deepti Bhatia and Xinyuan Xu and Deepak Ramachandran},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.13894},
  url={https://www.semanticscholar.org/paper/03fb95e6be583ca954c3d00812a9e9a40f118e51},
  abstract={Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.},
  keywords={arxiv:2212.13894}
}

@article{khalifa2022lepuspromptbased,
  title={LEPUS: Prompt-based Unsupervised Multi-hop Reranking for Open-domain QA},
  author={Muhammad Khalifa and Lajanugen Logeswaran and Moontae Lee and Honglak Lee and Lu Wang},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2205.12650},
  url={https://www.semanticscholar.org/paper/6650e44f00f74c596b3981d01493b5f75cfa6f63}
}

@article{choi2022lmpriorspretrained,
  title={LMPriors: Pre-Trained Language Models as Task-Specific Priors},
  author={Kristy Choi and Chris Cundy and Sanjari Srivastava and Stefano Ermon},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.12530},
  url={https://www.semanticscholar.org/paper/f663c1b574d0a56c2e1c8e7aee3d8caf90b52808},
  abstract={Particularly in low-data regimes, an outstanding challenge in machine learning is developing principled techniques for augmenting our models with suitable priors. This is to encourage them to learn in ways that are compatible with our understanding of the world. But in contrast to generic priors such as shrinkage or sparsity, we draw inspiration from the recent successes of large-scale language models (LMs) to construct task-specific priors distilled from the rich knowledge of LMs. Our method, Language Model Priors (LMPriors), incorporates auxiliary natural language metadata about the task—such as variable names and descriptions—to encourage downstream model outputs to be consistent with the LM’s common-sense reasoning based on the metadata. Empirically, we demonstrate that LMPriors improve model performance in settings where such natural language descriptions are available, and perform well on several tasks that benefit from such prior knowledge, such as feature selection, causal inference, and safe reinforcement learning.},
  keywords={arxiv:2210.12530}
}

@article{han2022lunalanguage,
  title={LUNA: Language Understanding with Number Augmentations on Transformers via Number Plugins and Pre-training},
  author={Hongwei Han and Jialiang Xu and Mengyuan Zhou and Yijia Shao and Shi Han and Dongmei Zhang},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.02691},
  url={https://www.semanticscholar.org/paper/0e97b13624bc15e7f30c82402252eca4d1a8eeba},
  abstract={Transformers are widely used in NLP tasks. However, current approaches to leveraging transformers to understand language expose one weak spot: Number understanding. In some scenarios, numbers frequently occur, especially in semi-structured data like tables. But current approaches to rich-number tasks with transformer-based language models abandon or lose some of the numeracy information - e.g., breaking numbers into sub-word tokens - which leads to many number-related errors. In this paper, we propose the LUNA framework which improves the numerical reasoning and calculation capabilities of transformer-based language models. With the number plugin of NumTok and NumBed, LUNA represents each number as a whole to model input. With number pre-training, including regression loss and model distillation, LUNA bridges the gap between number and vocabulary embeddings. To the best of our knowledge, this is the first work that explicitly injects numeracy capability into language models using Number Plugins. Besides evaluating toy models on toy tasks, we evaluate LUNA on three large-scale transformer models (RoBERTa, BERT, TabBERT) over three different downstream tasks (TATQA, TabFact, CrediTrans), and observe the performances of language models are constantly improved by LUNA. The augmented models also improve the official baseline of TAT-QA (EM: 50.15 ->59.58) and achieve SOTA performance on CrediTrans (F1 = 86.17).},
  keywords={arxiv:2212.02691}
}

@article{kadavath2022languagemodels,
  title={Language Models (Mostly) Know What They Know},
  author={Saurav Kadavath and Tom Conerly and Amanda Askell and T. Henighan and Dawn Drain and Ethan Perez and Nicholas Schiefer and Z. Dodds and Nova Dassarma and Eli Tran-Johnson and Scott Johnston and S. El-Showk and Andy Jones and Nelson Elhage and Tristan Hume and Anna Chen and Yuntao Bai and Sam Bowman and Stanislav Fort and Deep Ganguli and Danny Hernandez and Josh Jacobson and John Kernion and Shauna Kravec and Liane Lovitt and Kamal Ndousse and Catherine Olsson and Sam Ringer and Dario Amodei and Tom B. Brown and Jack Clark and Nicholas Joseph and Benjamin Mann and Sam McCandlish and Chris Olah and Jared Kaplan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2207.05221},
  url={https://www.semanticscholar.org/paper/142ebbf4760145f591166bde2564ac70c001e927},
  abstract={We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability"P(True)"that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict"P(IK)", the probability that"I know"the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
  keywords={arxiv:2207.05221}
}

@article{saparov2022languagemodels,
  title={Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},
  author={Abulhair Saparov and He He},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.01240},
  url={https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a},
  abstract={Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.},
  keywords={arxiv:2210.01240}
}

@article{shi2022languagemodels,
  title={Language Models are Multilingual Chain-of-Thought Reasoners},
  author={Freda Shi and Mirac Suzgun and Markus Freitag and Xuezhi Wang and Suraj Srivats and Soroush Vosoughi and Hyung Won Chung and Yi Tay and Sebastian Ruder and Denny Zhou and Dipanjan Das and Jason Wei},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.03057},
  url={https://www.semanticscholar.org/paper/62f0db3a5ad5c795ec18fc7a6e7b01836809df57},
  abstract={We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.},
  keywords={arxiv:2210.03057}
}

@article{madaan2022languagemodels,
  title={Language Models of Code are Few-Shot Commonsense Learners},
  author={Aman Madaan and Shuyan Zhou and Uri Alon and Yiming Yang and Graham Neubig},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.07128},
  url={https://www.semanticscholar.org/paper/39e40821b7207125e54e6ed7112e55cd38c6f0c3},
  abstract={We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph.To employ large language models (LMs) for this task, existing approaches ‘serialize’ the output graph as a flat list of nodes and edges.Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all.We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (codex) outperforms natural-LMs that are fine-tuned on the target task (T5) and other strong LMs such as GPT-3 in the few-shot setting.},
  keywords={arxiv:2210.07128}
}

@article{dasgupta2022languagemodels,
  title={Language models show human-like content effects on reasoning},
  author={Ishita Dasgupta and Andrew Kyle Lampinen and Stephanie C. Y. Chan and Antonia Creswell and D. Kumaran and James L. McClelland and Felix Hill},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2207.07051},
  url={https://www.semanticscholar.org/paper/290732e9fb08a29af8892a7c1f73c9d2a1b9d7db},
  abstract={Reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable"content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models \$\textbackslash\{\}unicode\{x2014\}\$ whose prior expectations capture some aspects of human knowledge \$\textbackslash\{\}unicode\{x2014\}\$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large language models, as well as humans, and find that the language models reflect many of the same patterns observed in humans across these tasks \$\textbackslash\{\}unicode\{x2014\}\$ like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected both in answer patterns, and in lower-level features like the relationship between model answer distributions and human response times. Our findings have implications for understanding both these cognitive effects in humans, and the factors that contribute to language model performance.},
  keywords={arxiv:2207.07051}
}

@article{huang2022largelanguage,
  title={Large Language Models Can Self-Improve},
  author={Jiaxin Huang and S. Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.11610},
  url={https://www.semanticscholar.org/paper/3fa70115248377c3d1517c9f978791a296fbc1dd},
  abstract={Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate"high-confidence"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4\%->82.1\% on GSM8K, 78.2\%->83.0\% on DROP, 90.0\%->94.4\% on OpenBookQA, and 63.4\%->67.9\% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.},
  keywords={arxiv:2210.11610}
}

@article{taesiri2022largelanguage,
  title={Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors},
  author={Mohammad Reza Taesiri and Finlay Macklon and Yihe Wang and Hengshuo Shen and C. Bezemer},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.02506},
  url={https://www.semanticscholar.org/paper/55e3fe05598be7c3dd357d51166869f6571b824f},
  abstract={Video game testing requires game-specific knowledge as well as common sense reasoning about the events in the game. While AI-driven agents can satisfy the first requirement, it is not yet possible to meet the second requirement automatically. Therefore, video game testing often still relies on manual testing, and human testers are required to play the game thoroughly to detect bugs. As a result, it is challenging to fully automate game testing. In this study, we explore the possibility of leveraging the zero-shot capabilities of large language models for video game bug detection. By formulating the bug detection problem as a question-answering task, we show that large language models can identify which event is buggy in a sequence of textual descriptions of events from a game. To this end, we introduce the GameBugDescriptions benchmark dataset, which consists of 167 buggy gameplay videos and a total of 334 question-answer pairs across 8 games. We extensively evaluate the performance of six models across the OPT and InstructGPT large language model families on our benchmark dataset. Our results show promising results for employing language models to detect video game bugs. With the proper prompting technique, we could achieve an accuracy of 70.66\%, and on some video games, up to 78.94\%. Our code, evaluation data and the benchmark can be found on https://asgaardlab.github.io/LLMxBugs},
  keywords={arxiv:2210.02506}
}

@article{singhal2022largelanguage,
  title={Large language models encode clinical knowledge},
  author={K. Singhal and Shekoofeh Azizi and T. Tu and S. Mahdavi and Jason Wei and Hyung Won Chung and Nathan Scales and A. Tanwani and H. Cole-Lewis and S. Pfohl and P. Payne and Martin G. Seneviratne and P. Gamble and C. Kelly and Nathaneal Scharli and A. Chowdhery and P. A. Mansfield and B. A. Y. Arcas and D. Webster and Greg S. Corrado and Yossi Matias and K. Chou and Juraj Gottweis and Nenad Tomašev and Yun Liu and A. Rajkomar and J. Barral and Christopher Semturs and A. Karthikesalingam and Vivek Natarajan},
  year={2022},
  booktitle={Nature},
  doi={10.1038/s41586-023-06291-2},
  url={https://www.semanticscholar.org/paper/6052486bc9144dc1730c12bf35323af3792a1fd0},
  abstract={Med-PaLM, a state-of-the-art large language model for medicine, is introduced and evaluated across several medical question answering tasks, demonstrating the promise of these models in this domain. Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model\^{} 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM\^{} 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA\^{} 3 , MedMCQA\^{} 4 , PubMedQA\^{} 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics\^{} 6 ), including 67.6\% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17\%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.},
  keywords={arxiv:2212.13138}
}

@article{saulite2022latviannational,
  title={Latvian National Corpora Collection – Korpuss.lv},
  author={Baiba Saulite and Roberts Darģis and Normunds Gruzitis and I. Auzina and K. Levane-Petrova and L. Pretkalnina and Laura Rituma and Peteris Paikens and Arturs Znotins and Laine Strankale and Kristīne Pokratniece and Ilmars Poikans and Guntis Barzdins and I. Skadina and Anda Baklāne and Valdis Saulespurēns and Jānis Ziediņš},
  year={2022},
  booktitle={International Conference on Language Resources and Evaluation},
  url={https://www.semanticscholar.org/paper/ed3efcf1864e1580a81cfaa34f93f9f9505d80c6}
}

@article{lu2022learnexplain,
  title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
  author={Pan Lu and Swaroop Mishra and Tony Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and A. Kalyan},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2209.09513},
  url={https://www.semanticscholar.org/paper/d3135733aa39dec20ce72aa138589dda27c8406d},
  abstract={When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of \~{}21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20\% in few-shot GPT-3 and 3.99\% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96\%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40\% of the data. The data and code are available at https://scienceqa.github.io.},
  keywords={arxiv:2209.09513}
}

@misc{rakovan2022learningcomplex,
  title={Learning Complex Natural Language Inferences with Relational Neural Models},
  author={Boris Rakovan},
  year={2022},
  url={https://www.semanticscholar.org/paper/d3712e0fb82e281f48cbdeed580ee950f71e16c0}
}

@article{sahu2022learninganswer,
  title={Learning to Answer Semantic Queries over Code},
  author={Surya Prakash Sahu and Madhurima Mandal and Shikhar Bharadwaj and Aditya Kanade and Petros Maniatis and S. Shevade},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2209.08372},
  url={https://www.semanticscholar.org/paper/ff5b5a98571e54b1c948e6e954aa2f7c05772736}
}

@misc{lanchantin2022learningreason,
  title={Learning to Reason and Memorize with Self-Questioning},
  author={Jack Lanchantin and Shubham Toshniwal and J. Weston and Arthur Szlam and Sainbayar Sukhbaatar},
  year={2022},
  url={https://www.semanticscholar.org/paper/d21364775c881b9bb6c99885652c81df29d755e5}
}

@misc{wang2022learningreason,
  title={Learning to Reason with a Scalable Probabilistic Logic},
  author={William Yang Wang},
  year={2022},
  doi={10.1184/r1/21652148.v1},
  url={https://www.semanticscholar.org/paper/2e17c3b883f0cdfe72e4877cdd88d3cc98e9836c}
}

@article{zhou2022leasttomostprompting,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Denny Zhou and Nathanael Scharli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and D. Schuurmans and O. Bousquet and Quoc Le and Ed H. Chi},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2205.10625},
  url={https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321},
  abstract={Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
  keywords={arxiv:2205.10625}
}

@article{yu2022legalprompting,
  title={Legal Prompting: Teaching a Language Model to Think Like a Lawyer},
  author={Fang Yu and Lee Quartey and Frank Schilder},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.01326},
  url={https://www.semanticscholar.org/paper/cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3},
  abstract={Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought (CoT) prompts can improve arithmetic or common sense tasks significantly. We explore how such approaches fare with legal reasoning tasks and take the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning approaches. Our findings show that while CoT prompting and fine-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from specific legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.},
  keywords={arxiv:2212.01326}
}

@misc{mishra2022lilaunified,
  title={Lila: A Unified Benchmark for Mathematical Reasoning},
  author={Swaroop Mishra and Pan Lu and A. Kalyan},
  year={2022},
  url={https://www.semanticscholar.org/paper/a630c70aed27b52f6d04d1e772b153c5a7b6f6fe},
  abstract={Mathematical reasoning skills are essential for general-purpose intelligent systems to perform tasks from grocery shopping to climate modeling. Towards evaluating and improving AI systems in this domain, we propose LILA, a unified mathematical reasoning benchmark consisting of 23 diverse tasks along four dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs, thereby obtaining explainable solutions in addition to the correct answer. We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation. Finally, we introduce BHASKARA, a general-purpose mathematical reasoning model trained on LILA. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83\% F1 score vs. single-task models), while the best performing model only obtains 60.40\%, indicating the room for improvement in general mathematical reasoning and understanding.},
  keywords={arxiv:2210.17517}
}

@article{qian2022limitationslanguage,
  title={Limitations of Language Models in Arithmetic and Symbolic Induction},
  author={Jingu Qian and Hong Wang and Zekun Li and SHIYANG LI and Xifeng Yan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2208.05051},
  url={https://www.semanticscholar.org/paper/2a7ae3e98357569c41424dacd60c62d3df78a0db},
  abstract={Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models. However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs. Experimental results show that none of these techniques can solve the simplest addition induction problem completely. In the end, we introduce LMs with tutor, which demonstrates every single step of teaching. LMs with tutor is able to deliver 100\% accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction.},
  keywords={arxiv:2208.05051}
}

@article{razumovskaia2022littleriding,
  title={Little Red Riding Hood Goes around the Globe: Crosslingual Story Planning and Generation with Large Language Models},
  author={E. Razumovskaia and Joshua Maynez and Annie Louis and Mirella Lapata and Shashi Narayan},
  year={2022},
  booktitle={International Conference on Language Resources and Evaluation},
  doi={10.48550/arXiv.2212.10471},
  url={https://www.semanticscholar.org/paper/30cc7ae95583ade1f05226c08c6f6609777aeedd},
  abstract={Previous work has demonstrated the effectiveness of planning for story generation exclusively in a monolingual setting focusing primarily on English. We consider whether planning brings advantages to automatic story generation across languages. We propose a new task of crosslingual story generation with planning and present a new dataset for this task. We conduct a comprehensive study of different plans and generate stories in several languages, by leveraging the creative and reasoning capabilities of large pretrained language models. Our results demonstrate that plans which structure stories into three acts lead to more coherent and interesting narratives, while allowing to explicitly control their content and structure.},
  keywords={arxiv:2212.10471}
}

@article{pi2022logiganlearning,
  title={LogiGAN: Learning Logical Reasoning via Adversarial Pre-training},
  author={Xinyu Pi and Wanjun Zhong and Yan Gao and Nan Duan and Jian-Guang Lou},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2205.08794},
  url={https://www.semanticscholar.org/paper/8b78827faf49277b8f9f4510a766cba30e5fbe20},
  abstract={We present LogiGAN, an unsupervised adversarial pre-training framework for improving logical reasoning abilities of language models. Upon automatic identifying logical reasoning phenomena in massive text corpus via detection heuristics, we train language models to predict the masked-out logical statements. Inspired by the facilitation effect of reflective thinking in human learning, we analogically simulate the learning-thinking process with an adversarial Generator-Verifier architecture to assist logic learning. LogiGAN implements a novel sequential GAN approach that (a) circumvents the non-differentiable challenge of the sequential GAN by leveraging the Generator as a sentence-level generative likelihood scorer with a learning objective of reaching scoring consensus with the Verifier; (b) is computationally feasible for large-scale pre-training with arbitrary target length. Both base and large size language models pre-trained with LogiGAN demonstrate obvious performance improvement on 12 datasets requiring general reasoning abilities, revealing the fundamental role of logic in broad reasoning, as well as the effectiveness of LogiGAN. Ablation studies on LogiGAN components reveal the relative orthogonality between linguistic and logic abilities and suggest that reflective thinking's facilitation effect might also generalize to machine learning.},
  keywords={arxiv:2205.08794}
}

@article{tessler2022logicprobability,
  title={Logic, Probability, and Pragmatics in Syllogistic Reasoning},
  author={Michael Henry Tessler and J. Tenenbaum and Noah D. Goodman},
  year={2022},
  booktitle={Topics in Cognitive Science},
  doi={10.1111/tops.12593},
  url={https://www.semanticscholar.org/paper/781d57e426c2e4e2e33e8d1d95edd0591b5c7137},
  abstract={Syllogistic reasoning lies at the intriguing intersection of natural and formal reasoning of language and logic. Syllogisms comprise a formal system of reasoning yet make use of natural language quantifiers (e.g., all, some) and invite natural language conclusions. The conclusions people tend to draw from syllogisms, however, deviate substantially from the purely logical system. Are principles of natural language understanding to blame? We introduce a probabilistic pragmatic perspective on syllogistic reasoning: We decompose reasoning with natural language arguments into two subproblems: language comprehension and language production. We formalize models of these processes within the Rational Speech Act framework and explore the pressures that pragmatic reasoning places on the production of conclusions. We test our models on a recent, large data set of syllogistic reasoning and find that the selection process of conclusions from syllogisms are best modeled as a pragmatic speaker who has the goal of aligning the beliefs of a naive listener with those of their own. We compare our model to previously published models that implement two alternative theories-Mental Models and Probability Heuristics-finding that our model quantitatively predicts the full distributions of responses as well as or better than previous accounts, but with far fewer parameters. Our results suggest that human syllogistic reasoning may be best understood not as a poor approximation to ideal logical reasoning, but rather as rational probabilistic inference in support of natural communication.}
}

@misc{akbik2022logicalfallacy,
  title={Logical Fallacy Detection},
  author={A. Akbik and Tanja Bergmann and Duncan Blythe and Kashif and Stefan Rasul and Schweter Roland and Vollgraf and Tom B. Brown and Benjamin Mann and N. Ryder and Jared D Subbiah and Prafulla Kaplan and A. Dhariwal and P. Neelakantan and Girish Shyam and Amanda Sastry and Sandhini Askell and Ariel Agarwal and Herbert-Voss and Gretchen Krueger and T. Henighan and R. Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Chris Hesse and Mark Chen and M. Sigler and Scott Litwin and Benjamin Gray and Chess and Alec Radford and I. Sutskever and Kevin Clark and Minh-Thang Luong and Quoc V. Le and Giovanni Da and San Martino and Alberto Barrón-Cedeño and Simona C Kaplan and A. Morrison and Thomas M Goldin and Richard G Olino and Heimberg and Lev Konstantinovskiy and Oliver Price and Mevan Babakar and Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut and Yuhao Peng Qi and Yuhui Zhang and Jason Zhang and Bolton and D. Luan},
  year={2022},
  url={https://www.semanticscholar.org/paper/c21f978f8be7253bd12c254cd7f05064b0db4526}
}

@article{fujisawa2022logicaltasks,
  title={Logical Tasks for Measuring Extrapolation and Rule Comprehension},
  author={Ippei Fujisawa and R. Kanai},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.07727},
  url={https://www.semanticscholar.org/paper/965e409a3e7b5670d609837fac9823b160d6639c},
  abstract={Logical reasoning is essential in a variety of human activities. A representative example of a logical task is mathematics. Recent large-scale models trained on large datasets have been successful in various fields, but their reasoning ability in arithmetic tasks is limited, which we reproduce experimentally. Here, we recast this limitation as not unique to mathematics but common to tasks that require logical operations. We then propose a new set of tasks, termed logical tasks, which will be the next challenge to address. This higher point of view helps the development of inductive biases that have broad impact beyond the solution of individual tasks. We define and characterize logical tasks and discuss system requirements for their solution. Furthermore, we discuss the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias. Finally, we provide directions for solving logical tasks.},
  keywords={arxiv:2211.07727}
}

@article{osborne2022lostsupermarket,
  title={Lost in the Supermarket? A Commentary on Gries, Müller, and Jost},
  author={D. Osborne and Nicole Satherley and C. Sibley},
  year={2022},
  booktitle={Psychological Inquiry},
  doi={10.1080/1047840X.2022.2065132},
  url={https://www.semanticscholar.org/paper/6f4169c24369c49ce4c7f12c27e75d790ba54388},
  abstract={Scholars have long-debated how citizens come to adopt a political ideology. Whereas some suggest that material needs and/or self-interest motivate citizens to endorse the issue positions and ideological stances that maximize utility (see Chong, 2000; Chong \& Mullinix, 2022; Sniderman, Glaser, \& Griffin, 1991; Weeden \& Kurzban, 2017), others argue that less rational—and even irrational—forces are at play and instead focus on the psychological needs met by (Jost, 2020, 2021; Jost, Glaser, Kruglanski, \& Sulloway, 2003b), as well as symbolic attachments to (Jardina, 2019; Reny \& Sears, 2020; Sears, 1993; Sears \& Henry, 2005), specific ideologies. It seems that the extant literature is at an impasse over the antecedents to belief systems. Are citizens rational? Or are they not? Gries, M€ uller, and Jost (this issue) reconcile these contrasting perspectives by asserting that both rational and irrational processes motivate people’s ideological preferences. To these ends, the authors develop a comprehensive model of ideological choice that incorporates both (a) psychological and (b) consumption needs which are weighted by the importance assigned to them by the individual. On the other end of the production chain, ideological entrepreneurs supply ideologies that differentially reconcile these demands and disseminate them within a larger marketplace of beliefs. Although a formal mathematical model is used to identify the ideologies available within the frontier of options that best reconcile these dual needs, Gries et al. assert that, given the informational costs associated with becoming perfectly informed, most citizens simply “try out” different ideologies until they find one that satisfices their psychological and consumption needs. In seeking to resolve the perennial quandary over the determinants of ideology, Gries et al. (this issue) make multiple important contributions to the literature. First, in our view, much of the debate over mass belief systems entails discussions where both parties talk past one another. Those in the ideological purists camp (generally comprised of political scientists) define ideology in rigid terms focused on the presence of a stable and coherent belief system as articulated by Converse (1964) and others, whereas those in the ideological minimalists camp (often comprised of psychologists) have resuscitated the competence of the average voter by treating ideology as a self-defined/identity-based concept present in the vast majority of people (Jost, 2006, 2021). Gries et al. bridge this divide by acknowledging that ideologies are comprised of a “network of attitudes and beliefs... [that are] linked together logically and/or psychologically” (p. 65). Such a compromise brings both sides of this seemingly intractable conflict together and provides the foundations for a promising resolution to one of the most enduring debates in political psychology. In a similar manner, Gries et al. (this issue) help to reconcile the debate over rational and irrational approaches to political ideology by recognizing that both play a role in shaping people’s issue positions. Whereas there is a longstanding tradition of scholars pitting symbolic and self-interested approaches against each other when explaining political attitudes (e.g., Sears, Hensler, \& Speer, 1979; Sears, Lau, Tyler, \& Allen, 1980; Weeden \& Kurzban, 2017), Gries et al. develop a sophisticated model that acknowledges that psychological and consumption needs collectively motivate people to adopt an ideology that best-satisfies these needs. Specifically, the weighting factor within their model recognizes that people will assign different levels of importance to reconciling these distinct needs. For some, an ideology that partially satisfies a highly valued consumption need will be more appealing than a competing ideology that fully satisfies epistemic needs for certainty. Conversely, others will choose an ideology that fulfills their need to belong even if it conflicts with their consumption needs. In this sense, Gries et al.’s model help explains how both psychological and consumption needs motivate people to adopt a given ideology from the larger marketplace of ideas. Gries et al.’s (this issue) mathematical model of ideological choice also helps to reduce the ambiguity inherent in variables measured within the behavioral sciences (but see our discussion on the falsifiability of the model below). Indeed, as noted by the authors, “ordinary language is inherently ambiguous” (p. 70). Nebulous concepts like “self-interest,” “rationality,” and “epistemic needs” belie direct measurement and render explicit hypothesis testing difficult or near impossible. By explicitly quantifying, a priori, the relationships that psychological and consumption needs have with ideologies and their respective (perceived) abilities to resolve these needs, the authors provide a useful tool for evaluating the rationality of citizens. That is, the rationality of the public can be assessed within Gries et al.’s model by calculating the multivariate distance between people’s ideological choice and the array of (weighted) psychological and consumption needs that motivate their beliefs—the further}
}

@misc{boratko2022easureheoretic,
  title={M EASURE -T HEORETIC S ET R EPRESENTATION L EARNING},
  author={Michael Boratko and Shib Dhruvesh Patel and Sankar Dasgupta and Andrew McCallum},
  year={2022},
  url={https://www.semanticscholar.org/paper/bd4caa6a5210e7b4fe24735dc7b5169c5284670b}
}

@article{he2022mapskbmillionscale,
  title={MAPS-KB: A Million-scale Probabilistic Simile Knowledge Base},
  author={Qi He and Xintao Wang and Jiaqing Liang and Yanghua Xiao},
  year={2022},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2212.05254},
  url={https://www.semanticscholar.org/paper/83ab9f2fdb2d7445f8b72b732fe17f9e21603082},
  abstract={The ability to understand and generate similes is an imperative step to realize human-level AI. However, there is still a considerable gap between machine intelligence and human cognition in similes, since deep models based on statistical distribution tend to favour high-frequency similes. Hence, a large-scale symbolic knowledge base of similes is required, as it contributes to the modeling of diverse yet unpopular similes while facilitating additional evaluation and reasoning. To bridge the gap, we propose a novel framework for large-scale simile knowledge base construction, as well as two probabilistic metrics which enable an improved understanding of simile phenomena in natural language. Overall, we construct MAPS-KB, a million-scale probabilistic simile knowledge base, covering 4.3 million triplets over 0.4 million terms from 70 GB corpora. We conduct sufficient experiments to justify the effectiveness and necessity of the methods of our framework. We also apply MAPS-KB on three downstream tasks to achieve state-of-the-art performance, further demonstrating the value of MAPS-KB. Resources of MAPS-KB are publicly available at https://github.com/Abbey4799/MAPS-KB.},
  keywords={arxiv:2212.05254}
}

@article{jong2022mentionmemory,
  title={MENTION MEMORY : INCORPORATING TEXTUAL KNOWLEDGE INTO TRANSFORMERS THROUGH ENTITY MENTION ATTENTION},
  author={Michiel de Jong and Yury Zemlyanskiy and Nicholas FitzGerald and Fei Sha and W. Cohen},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7},
  abstract={Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Specifically, our method represents knowledge with `mention memory', a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim verification benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining.},
  keywords={arxiv:2110.06176}
}

@article{karpas2022mrklsystems,
  title={MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning},
  author={Ehud Karpas and Omri Abend and Yonatan Belinkov and Barak Lenz and Opher Lieber and Nir Ratner and Y. Shoham and Hofit Bata and Yoav Levine and Kevin Leyton-Brown and Dor Muhlgay and N. Rozen and Erez Schwartz and Gal Shachaf and Shai Shalev-Shwartz and A. Shashua and Moshe Tenenholtz},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2205.00445},
  url={https://www.semanticscholar.org/paper/1bcde55995a957b3e8a595d536b816cb8989cf1d},
  abstract={Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced"miracle") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.},
  keywords={arxiv:2205.00445}
}

@article{saha2022murmurmodular,
  title={MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation},
  author={Swarnadeep Saha and Xinyan Velocity Yu and Mohit Bansal and Ramakanth Pasunuru and Asli Celikyilmaz},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.08607},
  url={https://www.semanticscholar.org/paper/5791c2b41dd23310c53d6738a4c0d587107c2dc8},
  abstract={Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR, a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26\% more logically consistent summaries on LogicNLG, compared to direct prompting.},
  keywords={arxiv:2212.08607}
}

@article{bardin2022machinelearning,
  title={Machine Learning and Logical Reasoning: The New Frontier (Dagstuhl Seminar 22291)},
  author={Sébastien Bardin and S. Jha and Vijay Ganesh},
  year={2022},
  booktitle={Dagstuhl Reports},
  doi={10.4230/DagRep.12.7.80},
  url={https://www.semanticscholar.org/paper/642f92924f88ad5d91a30f293783442d86f87bb7}
}

@article{choudhury2022machinereading,
  title={Machine Reading, Fast and Slow: When Do Models “Understand” Language?},
  author={Sagnik Ray Choudhury and Anna Rogers and Isabelle Augenstein},
  year={2022},
  booktitle={International Conference on Computational Linguistics},
  doi={10.48550/arXiv.2209.07430},
  url={https://www.semanticscholar.org/paper/2a215364e3fbe5a4c45b61dc5bd869399fa82661},
  abstract={Two of the most fundamental issues in Natural Language Understanding (NLU) at present are: (a) how it can established whether deep learning-based models score highly on NLU benchmarks for the ”right” reasons; and (b) what those reasons would even be. We investigate the behavior of reading comprehension models with respect to two linguistic ”skills”: coreference resolution and comparison. We propose a definition for the reasoning steps expected from a system that would be ”reading slowly”, and compare that with the behavior of five models of the BERT family of various sizes, observed through saliency scores and counterfactual explanations. We find that for comparison (but not coreference) the systems based on larger encoders are more likely to rely on the ”right” information, but even they struggle with generalization, suggesting that they still learn specific lexical patterns rather than the general principles of comparison.},
  keywords={arxiv:2209.07430}
}

@article{li2022makinglanguage,
  title={Making Language Models Better Reasoners with Step-Aware Verifier},
  author={Yifei Li and Zeqi Lin and Shizhuo Zhang and Qiang Fu and B. Chen and Jian-Guang Lou and Weizhu Chen},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2023.acl-long.291},
  url={https://www.semanticscholar.org/paper/e826ac71dad8c4ce36d82fb7add43e3d306bb7e1},
  abstract={Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9\% to 58.1\% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4\% to 83.2\%).},
  keywords={arxiv:2206.02336}
}

@article{lin2022mathclmmathematical,
  title={MathCLM: Mathematical Cognitive Learning Model Based on the Evolution of Knowledge Graph},
  author={Gongqi Lin and Xiuqin Zhong and Hongguang Fu},
  year={2022},
  booktitle={International Conference on Control, Automation, Robotics and Vision},
  doi={10.1109/ICARCV57592.2022.10004260},
  url={https://www.semanticscholar.org/paper/cdc45b158dea0d58c47fead1105356d6bb174937},
  abstract={In real-world applications, the effective integration of learning and reasoning in a cognitive agent model is a challenging mission. However, such integration may lead to a better understanding, practice, and construction of more realistic models, especially for mathematical learning. Unfortunately, existing models are either oversimplified or require much processing time, which is unsuitable for online learning and education. Therefore, we propose a novel cognitive learning model, called Mathematical Cognitive Learning Model (MathCLM) based on the evolution of knowledge graph, for online mathematical learning that seeks to effectively represent, learn, and reason in online learning environments. The model's architecture combines cognitive learning with symbolic knowledge representation based on natural language processing (NLP). We introduce the mathematical instance concept to build the strategies by mathematical knowledge, such as theorems, axioms, etc., and infer new custom instances based on the learning knowledge. Furthermore, it can deal with uncertainty and errors from instances recommendation using a graph matching model and displays the inference progressing with different combinations of instances. We build a platform to promote and validate our model. The validation of the model on the real-world platform and the results presented here indicate the promise of the approach when performing online learning and reasoning in real-world scenarios, with possible applications in various areas.}
}

@misc{arthur2022mathematicallanguage,
  title={Mathematical language shapes how we understand the economy},
  author={W. Arthur},
  year={2022},
  url={https://www.semanticscholar.org/paper/51aae5840a02bafe5368753e31b4168eeacdf841}
}

@article{munozrubke2022mathematicaltools,
  title={Mathematical tools for making sense of a global pandemic},
  author={Felipe Munoz-Rubke and Felipe Almuna and Jaclyn Duemler and Eloísa Velásquez},
  year={2022},
  journal={International Journal of Science Education, Part B},
  doi={10.1080/21548455.2022.2100941},
  url={https://www.semanticscholar.org/paper/8ee28890f6cddb1f0ecd255af33e7b1968c0e445},
  abstract={ABSTRACT
 The COVID-19 pandemic revealed that many countries have failed to provide the general population with the cognitive tools to thoroughly understand and cope with a global health crisis. While scientists and leaders worldwide have struggled to discover ways to contain the spread of the virus, this difficult task has become overwhelming due to the limited ability of many citizens to grasp the urgency of the situation. Although in today’s digitized world we have endless access to data and more ways to represent information and statistics than ever before, numerous incidents have demonstrated that the frequent misapprehension of data can cause confusion rather than clarity. This opinion paper examines how issues such as the misunderstanding of large quantities, fractions, probabilities, and mathematical modeling may be affecting the way people view the current pandemic. Finally, we also discuss how numeracy can act as a protective factor against motivated reasoning, which often affects how we consume information related to the pandemic.}
}

@article{chen2022measurementevaluation,
  title={Measurement, Evaluation, and Model Construction of Mathematical Literacy Based on IoT and PISA},
  author={Yunfeng Chen},
  year={2022},
  booktitle={Mathematical Problems in Engineering},
  doi={10.1155/2022/3278401},
  url={https://www.semanticscholar.org/paper/da5c08f5175374114940aec48ddc6dcba494dab3},
  abstract={“Mathematics Curriculum Standard for Ordinary Senior High School” points out that mathematical modelling literacy is the literacy of abstracting real problems mathematically, expressing problems with mathematical language, and building models with mathematical methods to solve problems. It assesses the amount to which students who are about to finish compulsory schooling have the knowledge and aptitude to deal with future life issues, based on the notion of lifelong learning. The usage of a local integrated development environment requires a number of complex processes, including installation and setup, as well as the procurement of necessary hardware. In reality, in order to nurture students, the core literacy of mathematics is to completely execute quality education, further identify the training and development direction of talents, and infiltrate the core literacy material into junior high school mathematics classroom instruction. For such a large-scale international education measurement project, the research and development of test questions is very important. Therefore, detailed technical consideration has been carried out and test volumes have been designed that are relatively suitable for different countries. The test questions are closely related to life, while there are few questions related to real situational problems in the test, which are mathematically processed first. Due to the international background of evaluation, it is a great challenge for researchers to realize the localization of evaluation on the basis of adapting to the domestic situation. At the same time, there is no scientific and perfect mathematical modelling literacy evaluation system in China’s basic education research. How can middle school front-line teachers better evaluate students’ mathematical modelling literacy? As a result, it is critical to develop a scientific and quantitative mathematical literacy measurement model that will aid in the teaching and research of mathematical modelling by middle school front-line teachers, as well as contributing theoretically to the evaluation and research of mathematical modelling literacy in China. Therefore, based on mathematics literacy evaluation, this paper studies the hierarchy of the IoT and the measurement and evaluation model of mathematics literacy based on the IoT, as well as its enlightenment to the compilation of mathematics academic test questions in China.}
}

@article{heidler2022mechanizationlagc,
  title={Mechanization of LAGC Semantics in Isabelle},
  author={Niklas Heidler},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/6da3c8e830a4f4398da15ddd16afca0fae962b76},
  abstract={Formal programming language semantics are imperative when trying to verify properties of programs in an automated manner. Using a new approach, Din et al. strengthen the ability of reasoning about concurrent programs by proposing a modular trace semantics, which can flexibly adapt to the most prominent imperative programming language paradigms. These semantics decouple the evaluation in the local environments from the evaluation in the global environment by generating abstract, symbolic traces for the individual, local systems. The traces are then composed and concretized, resulting in global traces for the global system. Hence, these semantics are called Locally Abstract, Globally Concrete (LAGC). In this work, we present a formalization of the LAGC semantics in the popular theorem proving environment Isabelle/HOL. The given model is based on the prior work on the theory of LAGC semantics by Din et al. and includes formalizations of the basic theorems, the LAGC semantics for the While Language (WL), as well as the LAGC semantics for an extended version of the While Language (WLEXT). We furthermore use our Isabelle model in order to provide formal proofs for several advanced properties of the LAGC semantics, which have not been analyzed in the original paper. Whilst the main goal of the work was to formalize the LAGC semantics in a mathematically rigorous manner, we also achieve a high level of proof automatization and manage to contribute an efficient code-generation for the computation of program traces. As the formalization of the semantics is highly modular, the given theories could in the future be extended with even more sophisticated programming language paradigms.},
  keywords={arxiv:2202.08017}
}

@article{hugues2022mechanizationlarge,
  title={Mechanization of a Large DSML: An Experiment with AADL and Coq},
  author={J. Hugues and L. Wrage and J. Hatcliff and D. Stewart},
  year={2022},
  booktitle={International Conference on Formal Methods and Models for Co-Design},
  doi={10.1109/MEMOCODE57689.2022.9954589},
  url={https://www.semanticscholar.org/paper/b02e3df037c35e15bff81e1cd80a545b5dbbcfc3},
  abstract={Domain-Specific Modeling Languages (DSMLs) rely on model-based techniques to deliver tailored languages to meet specific needs, such as system modeling, formal verification, and code generation. A DSML has specific static and dynamic behavior rules that must be properly assessed before processing the model. The definition of these rules remains a challenge. Meta-modeling techniques usually lack the foundational elements required to fully express behavioral semantics. In this context, using an interactive theorem prover provides a mathematical foundation with which the semantics of a DSML can be defined. This includes an abstract syntax tree, typing rules, and derivation of an executable simulator. In this paper, we report on an ongoing effort to capture the SAE AADL language using Coq along with specific analysis capabilities. Our contribution provides an unambiguous semantics for a large set of the language and can be used as a foundation to build rich analysis capabilities.}
}

@article{bereczky2022mechanizingmatching,
  title={Mechanizing Matching Logic in Coq},
  author={Péter Bereczky and Xiaohong Chen and D'aniel Horp'acsi and Tam'as B'alint Mizsei and Lucas Peña and Jan Tusil},
  year={2022},
  booktitle={FROM},
  doi={10.4204/EPTCS.369.2},
  url={https://www.semanticscholar.org/paper/70f0ea217682d704f618fb2b09d55cbe9cd66701},
  abstract={Matching logic is a formalism for specifying, and reasoning about, mathematical structures, using patterns and pattern matching. Growing in popularity, it has been used to define many logical systems such as separation logic with recursive definitions and linear temporal logic. In addition, it serves as the logical foundation of the K semantic framework, which was used to build practical verifiers for a number of real-world languages. Despite being a fundamental formal system accommodating substantial theories, matching logic lacks a general-purpose, machine-checked formalization. Hence, we formalize matching logic using the Coq proof assistant. Specifically, we create a new representation of matching logic that uses a locally nameless encoding, and we formalize the syntax, semantics, and proof system of this representation in the Coq proof assistant. Crucially, we prove the soundness of the formalized proof system and provide a means to carry out interactive matching logic reasoning in Coq. We believe this work provides a previously unexplored avenue for reasoning about matching logic, its models, and the proof system.},
  keywords={arxiv:2201.05716}
}

@article{pal2022medmcqalargescale,
  title={MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering},
  author={Ankit Pal and Logesh Kumar Umapathi and Malaikannan Sankarasubbu},
  year={2022},
  booktitle={ACM Conference on Health, Inference, and Learning},
  doi={10.48550/arXiv.2203.14371},
  url={https://www.semanticscholar.org/paper/741776172685b9717159a9fcd21841461bb33b14},
  abstract={This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS \textbackslash\{\}\&NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects \textbackslash\{\}\&topics. A detailed explanation of the solution, along with the above information, is provided in this study.},
  keywords={arxiv:2203.14371}
}

@article{comsa2022miqabenchmark,
  title={MiQA: A Benchmark for Inference on Metaphorical Questions},
  author={I. Comsa and Julian Martin Eisenschlos and S. Narayanan},
  year={2022},
  booktitle={AACL},
  doi={10.48550/arXiv.2210.07993},
  url={https://www.semanticscholar.org/paper/777683db4795ff691533c2c4be3244fabd826842},
  abstract={We propose a benchmark to assess the capability of large language models to reason with conventional metaphors. Our benchmark combines the previously isolated topics of metaphor detection and commonsense reasoning into a single task that requires a model to make inferences by accurately selecting between the literal and metaphorical register. We examine the performance of state-of-the-art pre-trained models on binary-choice tasks and find a large discrepancy between the performance of small and very large models, going from chance to near-human level. We also analyse the largest model in a generative setting and find that although human performance is approached, careful multiple-shot prompting is required.},
  keywords={arxiv:2210.07993}
}

@article{liu2022mindsgrounded,
  title={Mind's Eye: Grounded Language Model Reasoning through Simulation},
  author={Ruibo Liu and Jason Wei and S. Gu and Te-Yen Wu and Soroush Vosoughi and Claire Cui and Denny Zhou and Andrew M. Dai},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.05359},
  url={https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8},
  abstract={Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9\% zero-shot, and 46.0\% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.},
  keywords={arxiv:2210.05359}
}

@misc{abdulredha2022miningdistributed,
  title={Mining Distributed Data using Vertical Federated Learning Review},
  author={Manaaf Abdulredha and DR Yassen and Lamia AbedNoor and Muhammed},
  year={2022},
  url={https://www.semanticscholar.org/paper/7d76acadc9712d229687c91cbd752ccb86d6c2f4}
}

@article{jung2022modalspecificpseudo,
  title={Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval},
  author={Minjoon Jung and Seongho Choi and Joo-Kyung Kim and Jin-Hwa Kim and Byoung-Tak Zhang},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.12617},
  url={https://www.semanticscholar.org/paper/42d0c43d016b1cc1b8bf0fb01cbad17bdbb16400},
  abstract={Video corpus moment retrieval (VCMR) is the task to retrieve the most relevant video moment from a large video corpus using a natural language query.For narrative videos, e.g., drama or movies, the holistic understanding of temporal dynamics and multimodal reasoning are crucial.Previous works have shown promising results; however, they relied on the expensive query annotations for the VCMR, i.e., the corresponding moment intervals.To overcome this problem, we propose a self-supervised learning framework: Modal-specific Pseudo Query Generation Network (MPGN).First, MPGN selects candidate temporal moments via subtitle-based moment sampling.Then, it generates pseudo queries exploiting both visualand textual information from the selected temporal moments.Through the multimodal information in the pseudo queries, we show that MPGN successfully learns to localize the video corpus moment without any explicit annotation.We validate the effectiveness of MPGN on TVR dataset, showing the competitive results compared with both supervised models and unsupervised setting models.},
  keywords={arxiv:2210.12617}
}

@article{bellomarini2022modelindependentdesign,
  title={Model-Independent Design of Knowledge Graphs - Lessons Learnt From Complex Financial Graphs},
  author={Luigi Bellomarini and Andrea Gentili and Eleonora Laurenza and Emanuel Sallinger},
  year={2022},
  booktitle={International Conference on Extending Database Technology},
  doi={10.48786/edbt.2022.46},
  url={https://www.semanticscholar.org/paper/c3e145773ae5ef9ffa3107f21d0e5f5636f8734a}
}

@article{mascaro2022modelingcovid19,
  title={Modeling COVID-19 disease processes by remote elicitation of causal Bayesian networks from medical experts},
  author={S. Mascaro and Yue Wu and Owen Woodberry and Erik P. Nyberg and Ross Pearson and J. Ramsay and A. Mace and David Foley and T. Snelling and A. Nicholson},
  year={2022},
  booktitle={BMC Medical Research Methodology},
  doi={10.1186/s12874-023-01856-1},
  url={https://www.semanticscholar.org/paper/1df6c7e235460d5beeed23fbd45f350dd739645e},
  abstract={Background COVID-19 is a new multi-organ disease causing considerable worldwide morbidity and mortality. While many recognized pathophysiological mechanisms are involved, their exact causal relationships remain opaque. Better understanding is needed for predicting their progression, targeting therapeutic approaches, and improving patient outcomes. While many mathematical causal models describe COVID-19 epidemiology, none have described its pathophysiology. Methods In early 2020, we began developing such causal models. The SARS-CoV-2 virus’s rapid and extensive spread made this particularly difficult: no large patient datasets were publicly available; the medical literature was flooded with sometimes conflicting pre-review reports; and clinicians in many countries had little time for academic consultations. We used Bayesian network (BN) models, which provide powerful calculation tools and directed acyclic graphs (DAGs) as comprehensible causal maps. Hence, they can incorporate both expert opinion and numerical data, and produce explainable, updatable results. To obtain the DAGs, we used extensive expert elicitation (exploiting Australia’s exceptionally low COVID-19 burden) in structured online sessions. Groups of clinical and other specialists were enlisted to filter, interpret and discuss the literature and develop a current consensus. We encouraged inclusion of theoretically salient latent (unobservable) variables, likely mechanisms by extrapolation from other diseases, and documented supporting literature while noting controversies. Our method was iterative and incremental: systematically refining and validating the group output using one-on-one follow-up meetings with original and new experts. 35 experts contributed 126 hours face-to-face, and could review our products. Results We present two key models, for the initial infection of the respiratory tract and the possible progression to complications, as causal DAGs and BNs with corresponding verbal descriptions, dictionaries and sources. These are the first published causal models of COVID-19 pathophysiology. Conclusions Our method demonstrates an improved procedure for developing BNs via expert elicitation, which other teams can implement to model emergent complex phenomena. Our results have three anticipated applications: (i) freely disseminating updatable expert knowledge; (ii) guiding design and analysis of observational and clinical studies; (iii) developing and validating automated tools for causal reasoning and decision support. We are developing such tools for the initial diagnosis, resource management, and prognosis of COVID-19, parameterized using the ISARIC and LEOSS databases.}
}

@article{liaskos2022modelingreasoning,
  title={Modeling and reasoning about uncertainty in goal models: a decision-theoretic approach},
  author={S. Liaskos and Shakil M. Khan and M. Soutchanski and John and Mylopoulos},
  year={2022},
  journal={Journal of Software and Systems Modeling},
  doi={10.1007/s10270-021-00968-w},
  url={https://www.semanticscholar.org/paper/3a28969d4d4c6c59da63d36f26368966cbf33386}
}

@article{palmgren2022modellingroles,
  title={Modelling Roles of Mathematics in Physics},
  author={E. Palmgren and Tapio Rasa},
  year={2022},
  booktitle={Science Education},
  doi={10.1007/s11191-022-00393-5},
  url={https://www.semanticscholar.org/paper/2de4ebee2bcbe09f66f6d3367830b18460f50130},
  abstract={Modelling roles of mathematics in physics has proved to be a difficult task, with previous models of the interplay between the two disciplines mainly focusing on mathematical modelling and problem solving. However, to convey a realistic view of physics as a field of science to our students, we need to do more than train them to become fluent in modelling and problem solving. In this article, we present a new characterisation of the roles mathematics plays in physics and physics education, taking as a premise that mathematics serves as a constitutive structure in physics analogous to language. In doing so, we aim to highlight how mathematics affects the way we conceptualise physical phenomena. To contextualise our characterisation, we examine some of the existing models and discuss aspects of the interplay between physics and mathematics that are missing in them. We then show how these aspects are incorporated in our characterisation in which mathematics serves as a foundation upon which physical theories are built, and on which we may build mathematical representations of physical information that in turn serve as a basis for further reasoning and modifications. Through reasoning processes mathematics also aids in generating new information and explanations. We have elucidated each of these roles with an example from the historical development of quantum physics. To conclude, we discuss how our new characterisation may aid the development of physics education and physics education research.}
}

@article{ikman2022modellingfuzzy,
  title={Modelling of Fuzzy Expert System for an Assessment of Security Information Management System UIS (University Information System)},
  author={Ljilja Šikman and T. Latinovic},
  year={2022},
  booktitle={Tehnički Vjesnik},
  doi={10.17559/tv-20200721154801},
  url={https://www.semanticscholar.org/paper/5350b60583818e81d02c19fa4613fdb0b3ffea6e},
  abstract={: Several methodologies based on the international standard ISO/IEC 27001 have been developed for modelling information security management systems within higher education. This paper transformed the ISO/IEC 27001 standard into a questionnaire, which was sent digitally to about 100 universities in Bosnia and Herzegovina, and to the EU, Norway and the USA. The questions are arranged by levels, and the levels have their numerical weights, derived from individual questions in the levels themselves. Otherwise, the questions are asked with Yes or No and thus are reduced to binary variables. The rules necessary for the functioning of the system have been calculated. The fuzzy logic method represents a new approach to the problems of managing complex systems, which is very difficult to describe with a certain mathematical model, as well as in systems with a large number of inputs and outputs where there are unclear interactions. Risk assessment is a major part of the ISMS process. Traditional risk calculation models are based on the application of probability and classical set theory. Here, we have converted the risk assessment into a system rating of 5 to 10 numerically or from five to ten descriptively. We perform fuzzy optimization by finding the values of the input parameters of a complex simulated system, which results in the desired output. We use the fuzzy logic controller to execute fuzzy inference rules from the fuzzy rule database in determining congestion parameters, obtaining warning information and appropriate action. Simulating the situation of an advanced system that evaluates the protection quality of such a system with fuzzy logic, we use MATLAB. The paper combines the original Visual Basic programming language and MATLAB's Fuzzy Toolbox, to solve the complex problem of assessing compliance with the ISO/IEC 27001 standard, as one of the main standards for information systems security modelling. University information systems were used, but it is also applicable to all other information systems. The evaluation has been done for several universities and it has been proven that the system evaluates correctly, but these universities must not be publicly named. There was no such approach in the use of fuzzy logic and on such systems, and that is the originality of this work.}
}

@article{nepomiastchy2022modulecosoftware,
  title={Moduleco, software for macroeconomic modelization},
  author={P. Nepomiastchy},
  year={2022},
  booktitle={Mausam},
  doi={10.54302/mausam.v36i2.1834},
  url={https://www.semanticscholar.org/paper/ccf4c20249a44483c49be8f94901d225bead8aa6},
  abstract={The paper describes the Moduleco system which is designed to facilitate the construction and the use of large scale (1000 equations or more) dynamic and non-linear macroecenomic models. 
The Moduleco system will include a software for the management of the time-series data base, a special modeling language for the model equations Input, a special common language to active the tasks, several tools of formal computation and an interactive language for easy data input-output and for easy scenario generation. 
The paper describes also the mathematical algorithms which are to be included in the Moduleco system. Indeed, we have noticed that most of the macroeconomic models can be put in a quasi triangular form: possibly after renumbering of the variables and equations, there exist a small set of variables, called loop variables, such as for given values of them, the remaining model is triangular and can be solved directly. As we have shown that quasi-triangular models can be simulated and optimized. much faster than general ones, the Moduleco system will include methods for automatic renumbering of variables and equations in order to minimize the number of loop variables. 
The simulation and optimization algorithms will then be adapted to take into account this quasi triangularity. Experiments made on 4 concrete macroeconomic models have shown the efficiency of the proposed methods. 
Moreover, the adjoint variable technique, well known in optimal control theory, has been adapted to the structure of macroeconomic models. On the example of the French STAR model (139 equations), it is shown that this technique is 106 times faster to compute the gradient than the finite difference technique generally used by economists.}
}

@article{chen2022muragmultimodal,
  title={MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text},
  author={Wenhu Chen and Hexiang Hu and Xi Chen and Pat Verga and William W. Cohen},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.02928},
  url={https://www.semanticscholar.org/paper/38b0803b59e4973f09018ce942164b02be4b8bc9},
  abstract={While language Models store a massive amount of world knowledge implicitly in their parameters, even very large models often fail to encode information about rare entities and events, while incurring huge computational costs. Recently, retrieval-augmented models, such as REALM, RAG, and RETRO, have incorporated world knowledge into language generation by leveraging an external non-parametric index and have demonstrated impressive performance with constrained model sizes. However, these methods are restricted to retrieving only textual knowledge, neglecting the ubiquitous amount of knowledge in other modalities like images – much of which contains information not covered by any text. To address this limitation, we propose the first Multimodal Retrieval-Augmented Transformer (MuRAG), which accesses an external non-parametric multimodal memory to augment language generation. MuRAG is pre-trained with a mixture of large-scale image-text and text-only corpora using a joint contrastive and generative loss. We perform experiments on two different datasets that require retrieving and reasoning over both images and text to answer a given query: WebQA, and MultimodalQA. Our results show that MuRAG achieves state-of-the-art accuracy, outperforming existing models by 10-20\% absolute on both datasets and under both distractor and full-wiki settings.},
  keywords={arxiv:2210.02928}
}

@article{bao2022multistepdeductive,
  title={Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation},
  author={Qiming Bao and A. Peng and Tim Hartill and N. Tan and Zhenyun Deng and M. Witbrock and Jiamou Liu},
  year={2022},
  booktitle={International Workshop on Neural-Symbolic Learning and Reasoning},
  doi={10.48550/arXiv.2207.14000},
  url={https://www.semanticscholar.org/paper/4a52399e66da3fb1406132ecedf274925b5f7972},
  abstract={Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gated attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datasets, we develop PARARULE-Plus, a large dataset with more examples that require deeper reasoning steps. Experimental results show that the addition of PARARULE-Plus can increase the model's performance on examples requiring deeper reasoning depths. The source code and data are available at https://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.},
  keywords={arxiv:2207.14000}
}

@article{mavi2022multihopquestion,
  title={Multi-hop Question Answering},
  author={Vaibhav Mavi and Anubhav Jangra and A. Jatowt},
  year={2022},
  booktitle={Foundations and Trends in Information Retrieval},
  doi={10.1561/1500000102},
  url={https://www.semanticscholar.org/paper/63e939b0606207941673def2b69c6240d549d198},
  abstract={The task of Question Answering (QA) has attracted significant research interest for long. Its relevance to language understanding and knowledge retrieval tasks, along with the simple setting makes the task of QA crucial for strong AI systems. Recent success on simple QA tasks has shifted the focus to more complex settings. Among these, Multi-Hop QA (MHQA) is one of the most researched tasks over the recent years. In broad terms, MHQA is the task of answering natural language questions that involve extracting and combining multiple pieces of information and doing multiple steps of reasoning. An example of a multi-hop question would be"The Argentine PGA Championship record holder has won how many tournaments worldwide?". Answering the question would need two pieces of information:"Who is the record holder for Argentine PGA Championship tournaments?"and"How many tournaments did [Answer of Sub Q1] win?". The ability to answer multi-hop questions and perform multi step reasoning can significantly improve the utility of NLP systems. Consequently, the field has seen a surge with high quality datasets, models and evaluation strategies. The notion of 'multiple hops' is somewhat abstract which results in a large variety of tasks that require multi-hop reasoning. This leads to different datasets and models that differ significantly from each other and makes the field challenging to generalize and survey. We aim to provide a general and formal definition of the MHQA task, and organize and summarize existing MHQA frameworks. We also outline some best practices for building MHQA datasets. This book provides a systematic and thorough introduction as well as the structuring of the existing attempts to this highly interesting, yet quite challenging task.},
  keywords={arxiv:2204.09140}
}

@article{pratapa2022multilingualevent,
  title={Multilingual Event Linking to Wikidata},
  author={Adithya Pratapa and Rishubh Gupta and T. Mitamura},
  year={2022},
  booktitle={MIA},
  doi={10.18653/v1/2022.mia-1.5},
  url={https://www.semanticscholar.org/paper/e20531a7084a6b2c2bfee7c8cf911752841e5910},
  abstract={We present a task of multilingual linking of events to a knowledge base. We automatically compile a large-scale dataset for this task, comprising of 1.8M mentions across 44 languages referring to over 10.9K events from Wikidata. We propose two variants of the event linking task: 1) multilingual, where event descriptions are from the same language as the mention, and 2) crosslingual, where all event descriptions are in English. On the two proposed tasks, we compare multiple event linking systems including BM25+ (Lv and Zhai, 2011) and multilingual adaptations of the biencoder and crossencoder architectures from BLINK (Wu et al., 2020). In our experiments on the two task variants, we find both biencoder and crossencoder models significantly outperform the BM25+ baseline. Our results also indicate that the crosslingual task is in general more challenging than the multilingual task. To test the out-of-domain generalization of the proposed linking systems, we additionally create a Wikinews-based evaluation set. We present qualitative analysis highlighting various aspects captured by the proposed dataset, including the need for temporal reasoning over context and tackling diverse event descriptions across languages.},
  keywords={arxiv:2204.06535}
}

@article{buehler2022multiscalemodeling,
  title={Multiscale Modeling at the Interface of Molecular Mechanics and Natural Language through Attention Neural Networks.},
  author={M. Buehler},
  year={2022},
  booktitle={Accounts of Chemical Research},
  doi={10.1021/acs.accounts.2c00330},
  url={https://www.semanticscholar.org/paper/832529dd12d1e5ec6db62291280fca231d6d17f4},
  abstract={ConspectusHumans are continually bombarded with massive amounts of data. To deal with this influx of information, we use the concept of attention in order to perceive the most relevant input from vision, hearing, touch, and others. Thereby, the complex ensemble of signals is used to generate output by querying the processed data in appropriate ways. Attention is also the hallmark of the development of scientific theories, where we elucidate which parts of a problem are critical, often expressed through differential equations. In this Account we review the emergence of attention-based neural networks as a class of approaches that offer many opportunities to describe materials across scales and modalities, including how universal building blocks interact to yield a set of material properties. In fact, the self-assembly of hierarchical, structurally complex, and multifunctional biomaterials remains a grand challenge in modeling, theory, and experiment. Expanding from the process by which material building blocks physically interact to form a type of material, in this Account we view self-assembly as both the functional emergence of properties from interacting building blocks as well as the physical process by which elementary building blocks interact and yield structure and, thereby, functions. This perspective, integrated through the theory of materiomics, allows us to solve multiscale problems with a first-principles-based computational approach based on attention-based neural networks that transform information to feature to property while providing a flexible modeling approach that can integrate theory, simulation, and experiment. Since these models are based on a natural language framework, they offer various benefits including incorporation of general domain knowledge via general-purpose pretraining, which can be accomplished without labeled data or large amounts of lower-quality data. Pretrained models then offer a general-purpose platform that can be fine-tuned to adapt these models to make specific predictions, often with relatively little labeled data. The transferrable power of the language-based modeling approach realizes a neural olog description, where mathematical categorization is learned by multiheaded attention, without domain knowledge in its formulation. It can hence be applied to a range of complex modeling tasks─such as physical field predictions, molecular properties, or structure predictions, all using an identical formulation. This offers a complementary modeling approach that is already finding numerous applications, with great potential to solve complex assembly problems, enabling us to learn, build, and utilize functional categorization of how building blocks yield a range of material functions. In this Account, we demonstrate the approach in various application areas, including protein secondary structure prediction and prediction of normal-mode frequencies as well as predicting mechanical fields near cracks. Unifying these diverse problem areas is the building block approach, where the models are based on a universally applicable platform that offers benefits ranging from transferability, interpretability, and cross-domain pollination of knowledge as exemplified through a transformer model applied to predict how musical compositions infer de novo protein structures. We discuss future potentialities of this approach for a variety of material phenomena across scales, including the use in multiparadigm modeling schemes.}
}

@article{sammani2022nlxgptmodel,
  title={NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks},
  author={Fawaz Sammani and Tanmoy Mukherjee and Nikos Deligiannis},
  year={2022},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52688.2022.00814},
  url={https://www.semanticscholar.org/paper/bc64190d42d9dc34077b6a096d9053bb88deaa3a},
  abstract={Natural language explanation (NLE) models aim at explaining the decision-making process of a black box system via generating natural language sentences which are human-friendly, high-level and fine-grained. Current NLE models11Throughout this paper, we refer to NLE models as Natural Language Explanation models aimed for vision and vision-language tasks. explain the decision-making process of a vision or vision-language model (a.k.a., task model), e.g., a VQA model, via a language model (a.k.a., explanation model), e.g., GPT. Other than the additional memory resources and inference time required by the task model, the task and explanation models are completely independent, which disassociates the explanation from the reasoning process made to predict the answer. We introduce NLX-GPT, a general, compact and faithful language model that can simultaneously predict an answer and explain it. We first conduct pre-training on large scale data of image-caption pairs for general understanding of images, and then formulate the answer as a text prediction task along with the explanation. Without region proposals nor a task model, our resulting overall framework attains better evaluation scores, contains much less parameters and is 15× faster than the current SoA model. We then address the problem of evaluating the explanations which can be in many times generic, data-biased and can come in several forms. We therefore design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based attack, a selfevaluation framework that requires no labels. Code is at: https://github.com/fawazsammani/nlxgpt.},
  keywords={arxiv:2203.05081}
}

@article{salimeh2022naturallanguage,
  title={Natural Language Processing and Parallel Computing for Information Retrieval from Electronic Health Records},
  author={Ali Abu Salimeh and Najah Al-shanableh and M. Alzyoud},
  year={2022},
  booktitle={ITM Web of Conferences},
  doi={10.1051/itmconf/20224201013},
  url={https://www.semanticscholar.org/paper/6bb946c85a31de264306bf565bd11e88f89e312a},
  abstract={In this paper, we review the literature to find suitable information retrieval techniques for EHealth. Also discussed NLP techniques that have been proved their capability to extract valuable information in unstructured data from EHR. One of the best NLP techniques used for searching free text is LSI, due to its capability of finding semantic terms and in rich the search results by finding the hidden relations between terms. LSI uses a mathematical model called SVD, which is not scalable for large amounts of data due to its complexity and exhausts the memory, and a review for recent applications of LSI was discussed.}
}

@article{welleck2022naturalprovergrounded,
  title={NaturalProver: Grounded Mathematical Proof Generation with Language Models},
  author={S. Welleck and Jiacheng Liu and Ximing Lu and Hannaneh Hajishirzi and Yejin Choi},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2205.12910},
  url={https://www.semanticscholar.org/paper/d593b9b8d63426f0d6a795dd7f2294619bc03610},
  abstract={Theorem proving in natural mathematical language - the mixture of symbolic and natural language used by humans - plays a central role in mathematical advances and education, and tests aspects of reasoning that are core to intelligence. Yet it has remained underexplored with modern generative models. We study large-scale language models on two new generation tasks: suggesting the next step in a mathematical proof, and full proof generation. We develop NaturalProver, a language model that generates proofs by conditioning on background references (e.g. theorems and definitions that are either retrieved or human-provided), and optionally enforces their presence with constrained decoding. On theorems from the NaturalProofs benchmark, NaturalProver improves the quality of next-step suggestions and generated proofs over fine-tuned GPT-3, according to human evaluations from university-level mathematics students. NaturalProver is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40\% of the time, which is to our knowledge the first demonstration of these capabilities using neural language models.},
  keywords={arxiv:2205.12910}
}

@article{ahmed2022neuralcollaborative,
  title={Neural Collaborative Filtering with Ontologies for Integrated Recommendation Systems},
  author={Rana Alaa El-deen Ahmed and M. Fernández-Veiga and M. Gawich},
  year={2022},
  booktitle={Italian National Conference on Sensors},
  doi={10.3390/s22020700},
  url={https://www.semanticscholar.org/paper/c56ab563455e1571f4653ed738244041d5998a5e},
  abstract={Machine learning (ML) and especially deep learning (DL) with neural networks have demonstrated an amazing success in all sorts of AI problems, from computer vision to game playing, from natural language processing to speech and image recognition. In many ways, the approach of ML toward solving a class of problems is fundamentally different than the one followed in classical engineering, or with ontologies. While the latter rely on detailed domain knowledge and almost exhaustive search by means of static inference rules, ML adopts the view of collecting large datasets and processes this massive information through a generic learning algorithm that builds up tentative solutions. Combining the capabilities of ontology-based recommendation and ML-based techniques in a hybrid system is thus a natural and promising method to enhance semantic knowledge with statistical models. This merge could alleviate the burden of creating large, narrowly focused ontologies for complicated domains, by using probabilistic or generative models to enhance the predictions without attempting to provide a semantic support for them. In this paper, we present a novel hybrid recommendation system that blends a single architecture of classical knowledge-driven recommendations arising from a tailored ontology with recommendations generated by a data-driven approach, specifically with classifiers and a neural collaborative filtering. We show that bringing together these knowledge-driven and data-driven worlds provides some measurable improvement, enabling the transfer of semantic information to ML and, in the opposite direction, statistical knowledge to the ontology. Moreover, the novel proposed system enables the extraction of the reasoning recommendation results after updating the standard ontology with the new products and user behaviors, thus capturing the dynamic behavior of the environment of our interest.}
}

@article{lu2022neurosymboliccausal,
  title={Neuro-Symbolic Causal Language Planning with Commonsense Prompting},
  author={Yujie Lu and Weixi Feng and Wanrong Zhu and Wenda Xu and X. Wang and Miguel P. Eckstein and William Yang Wang},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2206.02928},
  url={https://www.semanticscholar.org/paper/71fd336f1ca337a638dffb236b432c29cdd19f3d}
}

@article{lanzinger2022perspectivesfuzzy,
  title={New Perspectives for Fuzzy Datalog (Extended Abstract)},
  author={Matthias Lanzinger and Stefano Sferrazza and G. Gottlob},
  year={2022},
  booktitle={Datalog},
  url={https://www.semanticscholar.org/paper/937753fb585974890f923444d7197e4f997aeeb5}
}

@article{valls2022reasoningmodels,
  title={New Reasoning Models: Improving Optimisation and Decision Support with the Management of Uncertainty and Constraints},
  author={A. Valls and César Fernández and Mateu Villaret},
  year={2022},
  journal={International Journal of Computational Intelligence Systems},
  doi={10.1007/s44196-022-00151-z},
  url={https://www.semanticscholar.org/paper/3c6cc5252a1ec483f95229ae255e02c561bd9cc0},
  abstract={• Management of Uncertainty In this area, we have four papers that cover the following topics: The paper by Ojeda-Hernández et al. extends the standard Formal Concept Analysis model (based on lattices) by incorporating mixed attributes that model positive and negative information in the same framework. It has been applied to a Mathematical course to define learning paths and study the knowledge space of the students. Alsinet et al. propose the use of graph isomorphism networks to approximately compute the polarization degree of arguments in conversational systems. Experiments are done in the Reddit debate tool. Pascual-Fontanilles et al. present a method for using incoming sets of new examples to adapt a classification model, which is based on fuzzy logic. It is tested in the problem of diagnosis of diabetic retinopathy. The paper of Zhu et al. proposes a linguistic multiple criteria decision aiding model for large groups of decision makers, to be used in evaluation of opinions in complex livelihood projects. They use uncertain linguistic values based on intervals and define different aggregation and consensus operations for this kind of information. • Constraints Satisfaction In this category, we have six papers. The one by Akbay et al. provides an adaptative version of the hybrid combinatorial optimization generic algorithm Construct, merge, solve and adapt. This version is able to selfadapt, avoiding an intensive parameter tuning process while still being state-of-the-art in the minimum positive influence dominating set problem. The one by Zhou et al. addresses the Budgeted Maximum Coverage Problem with a variable depth local search 1 algorithm that is further improved with neighbour structures. Their proposed method outperforms the best existing heuristics and the exact solver CPLEX.The other four papers deal with SAT and its optimization version, MaxSAT. The paper by Almagro-Blanco and Giráldez-Cru analyzes the accuracy of several machine learning techniques to estimate the hardness of realistic pseudo-industrial SAT instances, generated with the Popularity-Similarity SAT model. Their experimental results show that ensemble methods (e.g., Random Forest) achieve the best performance, remaining robust to perturbations in the training phase. The paper by Bofill et al. explores the impact of using implied constraints in a MaxSAT model for the problem of scheduling business-to-business meetings. The experimental results clearly show that variable selection by the SAT solver is significantly affected depending on the implied constraints used in the encoding. Nurcahyadi et al. describe an ant colony optimization solver with negative learning for solving MaxSAT. The experimental results indicate that the proposed approach can be used to improve the results of existing solvers by internally using them to solve smaller sub-instances. Finally, Li et al. tackle the problem of reducing non-clausal MaxSAT and MinSAT to clausal MaxSAT and MinSAT. They define three cost-preserving transformations and report on an empirical comparison that provides evidence that nonclausal MaxSAT and MinSAT can be effectively solved * Aida Valls aida.valls@urv.cat}
}

@misc{afrin2022newseditsdataset,
  title={NewsEdits : A Dataset of News Article Revision Histories and a Novel Document-Level Reasoning Challenge},
  author={T. Afrin and Elaine Lin Wang and Diane Litman and Lind-659 say and Clare Matsumura and Richard Correnti and Norm Goldstein and Irshad Bhat and Talita Anthonio and D. Blei and Andrew Y. Ng and Ryan L Boyd and Kate G Blackburn and James W Pen-685 and Felipe Bravo-Marquez and Manuel Manriquez and Lynn Carlson and Daniel Marcu and Mary Ellen and Sarah Cohen and James T Hamilton and Roman Grundkiewicz and Marcin Junczys-Dowmunt and Kathleen A. Hansen and Jean Ward and J. L. Conners and Tatsunori Hashimoto and Kelvin Guu and Yonatan Oren and Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Erik W Johnson and Jonathan P. Schreiner and C. Leacock and M. Chodorow and Michael Gamon and Joel Tetreault. 2010 and N. Mostafazadeh and Michael Roth and Annie Louis and Nanyun Peng and Marjan Ghazvininejad and Jonathan May and J. Pustejovsky and Patrick Hanks and R. Saurí and R. Gaizauskas and Dragomir Andrea Setzer and F. M. Zanzotto and Fan Zhang and H. Hashemi and Rebecca Hwa},
  year={2022},
  url={https://www.semanticscholar.org/paper/fdfa68ef25d640a76640ef3162404c47b23f47a4}
}

@article{spangher2022newseditsnews,
  title={NewsEdits: A News Article Revision Dataset and a Novel Document-Level Reasoning Challenge},
  author={Alexander Spangher and Xiang Ren and Jonathan May and Nanyun Peng},
  year={2022},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2206.07106},
  url={https://www.semanticscholar.org/paper/4ef0484cf6898a74a6d17bc5f2efa4e287722471},
  abstract={News article revision histories provide clues to narrative and factual evolution in news articles. To facilitate analysis of this evolution, we present the first publicly available dataset of news revision histories, NewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million articles with 4.6 million versions from over 22 English- and French-language newspaper sources based in three countries, spanning 15 years of coverage (2006-2021).We define article-level edit actions: Addition, Deletion, Edit and Refactor, and develop a high-accuracy extraction algorithm to identify these actions. To underscore the factual nature of many edit actions, we conduct analyses showing that added and deleted sentences are more likely to contain updating events, main content and quotes than unchanged sentences. Finally, to explore whether edit actions are predictable, we introduce three novel tasks aimed at predicting actions performed during version updates. We show that these tasks are possible for expert humans but are challenging for large NLP models. We hope this can spur research in narrative framing and help provide predictive tools for journalists chasing breaking news.},
  keywords={arxiv:2206.07106}
}

@article{kontorovich2022straightforwardappears,
  title={Not as Straightforward as It Appears: Undergraduates Leverage Areas to Find Definite Integrals},
  author={I. Kontorovich and Tianqing Li},
  year={2022},
  journal={International Journal of Science and Mathematics Education},
  doi={10.1007/s10763-022-10339-6},
  url={https://www.semanticscholar.org/paper/bbed03b5026e35cfc10c0639ba170854d823c0df}
}

@article{dizaji2022novelcomputational,
  title={Novel computational mathematical algorithms for structural optimization using graph-theoretical methods},
  author={Farzad Shafiei Dizaji and Mehrdad Shafiei Dizaji},
  year={2022},
  booktitle={Engineering computations},
  doi={10.1108/ec-09-2021-0547},
  url={https://www.semanticscholar.org/paper/ea5ecd4af90ac120cf121ddf50e8d751ccbb13ba},
  abstract={PurposeThe purpose is to reduce round-off errors in numerical simulations. In the numerical simulation, different kinds of errors may be created during analysis. Round-off error is one of the sources of errors. In numerical analysis, sometimes handling numerical errors is challenging. However, by applying appropriate algorithms, these errors are manageable and can be reduced. In this study, five novel topological algorithms were proposed in setting up a structural flexibility matrix, and five different examples were used in applying the proposed algorithms. In doing so round-off errors were reduced remarkably.Design/methodology/approachFive new algorithms were proposed in order to optimize the conditioning of structural matrices. Along with decreasing the size and duration of analyses, minimizing analytical errors is a critical factor in the optimal computer analysis of skeletal structures. Appropriate matrices with a greater number of zeros (sparse), a well structure and a well condition are advantageous for this objective. As a result, a problem of optimization with various goals will be addressed. This study seeks to minimize analytical errors such as rounding errors in skeletal structural flexibility matrixes via the use of more consistent and appropriate mathematical methods. These errors become more pronounced in particular designs with ill-suited flexibility matrixes; structures with varying stiffness are a frequent example of this. Due to the usage of weak elements, the flexibility matrix has a large number of non-diagonal terms, resulting in analytical errors. In numerical analysis, the ill-condition of a matrix may be resolved by moving or substituting rows; this study examined the definition and execution of these modifications prior to creating the flexibility matrix. Simple topological and algebraic features have been mostly utilized in this study to find fundamental cycle bases with particular characteristics. In conclusion, appropriately conditioned flexibility matrices are obtained, and analytical errors are reduced accordingly.Findings(1) Five new algorithms were proposed in order to optimize the conditioning of structural flexibility matrices. (2) A JAVA programming language was written for all five algorithms and a friendly GUI software tool is developed to visualize sub-optimal cycle bases. (3) Topological and algebraic features of the structures were utilized in this study.Research limitations/implicationsThis is a multi-objective optimization problem which means that sparsity and well conditioning of a matrix cannot be optimized simultaneously. In conclusion, well-conditioned flexibility matrices are obtained, and analytical errors are reduced accordingly.Practical implicationsEngineers always finding mathematical modeling of real-world problems and make them as simple as possible. In doing so, lots of errors will be created and these errors could cause the mathematical models useless. Applying decent algorithms could make the mathematical model as precise as possible.Social implicationsErrors in numerical simulations should reduce due to the fact that they are toxic for real-world applications and problems.Originality/valueThis is an original research. This paper proposes five novel topological mathematical algorithms in order to optimize the structural flexibility matrix.}
}

@article{mishra2022numgluesuite,
  title={NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks},
  author={Swaroop Mishra and Arindam Mitra and Neeraj Varshney and Bhavdeep Singh Sachdeva and Peter Clark and Chitta Baral and A. Kalyan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2204.05660},
  url={https://www.semanticscholar.org/paper/39238a92de090c104936a4f78375b95600e42ce5},
  abstract={Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 \%). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 \% on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.},
  keywords={arxiv:2204.05660}
}

@article{spokoyny2022numericalcorrelation,
  title={Numerical Correlation in Text},
  author={Daniel M. Spokoyny and Chien-Sheng Wu and Caiming Xiong},
  year={2022},
  booktitle={MATHNLP},
  doi={10.18653/v1/2022.mathnlp-1.5},
  url={https://www.semanticscholar.org/paper/ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9},
  abstract={Evaluation of quantitative reasoning of large language models is an important step towards understanding their current capabilities and limitations. We propose a new task, Numerical Correlation in Text, which requires models to identify the correlation between two numbers in a sentence. To this end, we introduce a new dataset, which contains over 2,000 Wikipedia sentences with two numbers and their correlation labels. Using this dataset we are able to show that recent numerically aware pretraining methods for language models do not help generalization on this task posing a challenge for future work in this area.}
}

@article{huang2022numericalsimulation,
  title={Numerical Simulation of Well Type Optimization in Tridimensional Development of Multi-Layer Shale Gas Reservoir},
  author={Tao Huang and Xin-Yi Liao and Zhaoqin Huang and F. Song and Renyi Wang},
  year={2022},
  booktitle={Energies},
  doi={10.3390/en15186529},
  url={https://www.semanticscholar.org/paper/86e51d3deabb7a6bfb685741f1d3754908249d45},
  abstract={Aimed at the development of shale gas reservoirs with large reservoir thickness and multiple layers, this paper carried out a numerical simulation study on the optimization of three different well types: horizontal well, deviated well, and vertical well. To make the model more in line with the characteristics of shale gas reservoirs, a two-phase gas–water seepage mathematical model of shale gas reservoirs was established, considering the adsorption and desorption of shale gas, Knudsen diffusion effect, and stress sensitivity effect. The embedded discrete fracture model was used to describe hydraulic fracture and natural fracture. Based on Fortran language, a numerical simulator for multi-layer development of shale gas reservoirs was compiled, and the calculation results were compared with the actual production data of Barnett shale gas reservoirs to verify the reliability of the numerical simulator. The spread range of hydraulic fractures in the reservoir with different natural fracture densities is calculated by the simulation to determine well spacing and fracture spacing. The orthogonal experimental design method is then used to optimize the best combination of well spacing and fracture spacing for different well types. The results show that the well productivity of the high-density (0.012 m/m2) natural fractures reservoir > the well productivity of the medium-density (0.006 m/m2) natural fractures reservoir > the well productivity of the low-density (0.001 m/m2) natural fractures reservoir. According to the design of the orthogonal test, it can be seen that the most significant factor affecting the productivity of horizontal wells is the fracture spacing in the Y direction. For deviated wells and vertical wells, the X-direction well spacing has the greatest impact on its productivity.}
}

@article{fatahillah2022numericalanalysis,
  title={Numerical analysis of blood flow in abdominal aortic aneurysm using finite volume method},
  author={A. Fatahillah and Azza Liarista Anggraini and S. Setiawani},
  year={2022},
  booktitle={Desimal: Jurnal Matematika},
  doi={10.24042/djm.v5i2.9928},
  url={https://www.semanticscholar.org/paper/6391f7520b13664d268596b3e91fba6019a1a7bb},
  abstract={There is a deadly cardiovascular disease that can cause swelling of the Abdominal Aorta. This disease is known as Abdominal Aortic Aneurysm (AAA). AAA is believed to be a degenerative process caused by genetic factors, gender, body weight, and age. Changes in collagen and elastin in the aortic wall are the cause of the degeneration process. Therefore, it will cause dilatation of the aortic wall. Swelling of the aortic blood vessels will affect the blood flow velocity in the aortic blood vessels. This research aims to analyze the velocity of blood flow in the Abdominal Aortic Aneurysm based on swelling diameter, proximal neck length, and aneurysm channel length using Computational Fluids Dynamics (CFD). The blood flow velocity was modeled using mathematical language based on mass continuity equations and momentum equations.  Then the finite volume method was one method to solve the mathematical model. MATLAB and ANSYS FLUENT software were used to simulate the velocity of blood flow analysis. The results of the research were shown that the larger the diameter and swelling channel length, the smaller the velocity of blood flow produced. Then, the greater the length of the proximal neck, the faster the resulting blood flow will be.}
}

@article{magossi2022conceitomedida,
  title={O conceito de medida, o continuum e o discreto},
  author={J. Magossi and Vania Rosa Izidoro},
  year={2022},
  booktitle={Revista Professor de Matemática On line},
  doi={10.21711/2319023x2022/pmo1028},
  url={https://www.semanticscholar.org/paper/53c3ab853327a016bee163da3fc67850a9ccb03b},
  abstract={The word “measure” has been used throughout the history of humanity in almost every sector of human activity. It is not surprising that any changes in the way things are measured, also cause impacts in the developent of science and technologies. The refinements in the measurement criteria, in a broad sense, occur thanks to existing technologies, or they emerge from some well-defined rule, written in some language, with some scientific or practical purpose. Whereas, in a remote past, the diameter of the Earth was measured based on similarities of triangles, in modern times technologies made these measures much more precise. Even so a consensus is not yet reached, given that the mathematical continuum imposes restrictions on measurement reality. For example, there is no way to use in its fullness in laboratories, since approximations are necessary, taking into account that it is an irrational number with infinite decimal places. This characterizes a seesaw, in which, on one hand, there are the practical measures in the reality we live in, and, on the other, the theoretical measures. The goal in this article is to expose that, on one hand, from the perspective of mathematics, some examples characterize the relation between continuum and the discrete, in the measured aspect. On the other hand, we show that this relationship can indicate contradictions, in a stage of interactions between the practical and the theoretical world, if no careful reading happens. Apart from a historical digression with examples, it is shown that something similar occurs with the concept of measure when it is seen as an amount of information, called entropy by C. E. Shannon. There is also care to be taken regarding entropy, seen from the point of view of discrete models, and their extension to continuous models, differential entropy. While on the discrete side the amount of information is positive, the differential entropy, on the continuous side, can be negative, positive or arbitrarily large.}
}

@article{kumar2022olgaontology,
  title={OLGA : An Ontology and LSTM-based approach for generating Arithmetic Word Problems (AWPs) of transfer type},
  author={Suresh Kumar and P. S. Kumar},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.12164},
  url={https://www.semanticscholar.org/paper/94d5aa5c39fef504e0eddf2464cb2285459fc744},
  abstract={Machine generation of Arithmetic Word Problems (AWPs) is challenging as they express quantities and mathematical relationships and need to be consistent. ML-solvers require a large annotated training set of consistent problems with language variations. Exploiting domain-knowledge is needed for consistency checking whereas LSTM-based approaches are good for producing text with language variations. Combining these we propose a system, OLGA, to generate consistent word problems of TC (Transfer-Case) type, involving object transfers among agents. Though we provide a dataset of consistent 2-agent TC-problems for training, only about 36\% of the outputs of an LSTM-based generator are found consistent. We use an extension of TC-Ontology, proposed by us previously, to determine the consistency of problems. Among the remaining 64\%, about 40\% have minor errors which we repair using the same ontology. To check consistency and for the repair process, we construct an instance-specific representation (ABox) of an auto-generated problem. We use a sentence classifier and BERT models for this task. The training set for these LMs is problem-texts where sentence-parts are annotated with ontology class-names. As three-agent problems are longer, the percentage of consistent problems generated by an LSTM-based approach drops further. Hence, we propose an ontology-based method that extends consistent 2-agent problems into consistent 3-agent problems. Overall, our approach generates a large number of consistent TC-type AWPs involving 2 or 3 agents. As ABox has all the information of a problem, any annotations can also be generated. Adopting the proposed approach to generate other types of AWPs is interesting future work.},
  keywords={arxiv:2211.12164}
}

@article{iyer2022optimlscaling,
  title={OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
  author={S. Iyer and Xi Victoria Lin and Ramakanth Pasunuru and Todor Mihaylov and Daniel Simig and Ping Yu and Kurt Shuster and Tianlu Wang and Qing Liu and Punit Singh Koura and Xian Li and Brian O'Horo and Gabriel Pereyra and Jeff Wang and Christopher Dewan and Asli Celikyilmaz and Luke S. Zettlemoyer and Veselin Stoyanov},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/e965e93e76a9e6c4e4863d145b5c007b540d575d},
  abstract={Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.},
  keywords={arxiv:2212.12017}
}

@article{collier2022realitylimits,
  title={On Reality and the Limits of Language Data},
  author={Nigel Collier and Fangyu Liu and Ehsan Shareghi},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2208.11981},
  url={https://www.semanticscholar.org/paper/4217467e747182b9ad8035e8a2d657d2ce80af07}
}

@article{shaikh2022secondthought,
  title={On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning},
  author={Omar Shaikh and Hongxin Zhang and William B. Held and Michael Bernstein and Diyi Yang},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.08061},
  url={https://www.semanticscholar.org/paper/b1b8c3e47f44158d22fb70bb453d2494ed013b70},
  abstract={Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model’s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.},
  keywords={arxiv:2212.08061}
}

@article{li2022advancemaking,
  title={On the Advance of Making Language Models Better Reasoners},
  author={Yifei Li and Zeqi Lin and Shizhuo Zhang and Qiang Fu and Bei Chen and Jian-Guang Lou and Weizhu Chen},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2206.02336},
  url={https://www.semanticscholar.org/paper/760561c57f68044e2f1d089088df1da6c627b09a}
}

@article{unknown2022openingremarks,
  title={Opening Remarks of the AI Journey Team},
  year={2022},
  booktitle={Doklady. Mathematics},
  doi={10.1134/S1064562422060254},
  url={https://www.semanticscholar.org/paper/294a22d68a7caa844e1f5c5278972c4f7cb2ac51}
}

@article{hiratani2022optimalquadratic,
  title={Optimal Quadratic Binding for Relational Reasoning in Vector Symbolic Neural Architectures},
  author={Naoki Hiratani and H. Sompolinsky},
  year={2022},
  booktitle={Neural Computation},
  doi={10.1162/neco_a_01558},
  url={https://www.semanticscholar.org/paper/606512e75059aa0bde761748bf8b4f349539ef19},
  abstract={Abstract Binding operation is fundamental to many cognitive processes, such as cognitive map formation, relational reasoning, and language comprehension. In these processes, two different modalities, such as location and objects, events and their contextual cues, and words and their roles, need to be bound together, but little is known about the underlying neural mechanisms. Previous work has introduced a binding model based on quadratic functions of bound pairs, followed by vector summation of multiple pairs. Based on this framework, we address the following questions: Which classes of quadratic matrices are optimal for decoding relational structures? And what is the resultant accuracy? We introduce a new class of binding matrices based on a matrix representation of octonion algebra, an eight-dimensional extension of complex numbers. We show that these matrices enable a more accurate unbinding than previously known methods when a small number of pairs are present. Moreover, numerical optimization of a binding operator converges to this octonion binding. We also show that when there are a large number of bound pairs, however, a random quadratic binding performs, as well as the octonion and previously proposed binding methods. This study thus provides new insight into potential neural mechanisms of binding operations in the brain.},
  keywords={arxiv:2204.07186}
}

@article{binh2022optimalcentroids,
  title={Optimal centroids model approach for many-feature data structure prediction},
  author={Le Thi Cam Binh and Pham Van Nha},
  year={2022},
  booktitle={Evolutionary Intelligence},
  doi={10.1007/s12065-022-00747-6},
  url={https://www.semanticscholar.org/paper/43a6bb7e8d8915168647f309c5de189402a58b2f}
}

@misc{unknown2022optimizationlogistics,
  title={Optimization Of Logistics},
  year={2022},
  url={https://www.semanticscholar.org/paper/2661f6ad22389c7980232f05af4c520ff01613e2}
}

@article{thorburn2022optimizinglanguage,
  title={Optimizing Language Models for Argumentative Reasoning},
  author={Luke Thorburn and Ariel Kruger},
  year={2022},
  booktitle={ArgML@COMMA},
  url={https://www.semanticscholar.org/paper/ae3a6bbe22ea136280e2927807775b3ac8356440}
}

@article{nam2022outofdistributiongeneralization,
  title={Out-of-Distribution Generalization in Algorithmic Reasoning Through Curriculum Learning},
  author={A. Nam and Mustafa Abdool and Trevor Maxfield and James L. McClelland},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.03275},
  url={https://www.semanticscholar.org/paper/a6874229b18c793baa94f72ae65bd750dcceb4a6}
}

@article{sharma2022overcomingbarriers,
  title={Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic},
  author={Mandar Sharma and N. Muralidhar and Naren Ramakrishnan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.02098},
  url={https://www.semanticscholar.org/paper/1ec4ae64ded99846bcd8a1a3b16fe2875f55c7c1},
  abstract={Through their transfer learning abilities, highly-parameterized large pre-trained language models have dominated the NLP landscape for a multitude of downstream language tasks. Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning. However, as we illustrate in this paper, building a general purpose language model that also happens to be proficient in mathematical reasoning is not as straight-forward as training it on a numeric dataset. In this work, we develop a novel framework that enables language models to be mathematically proficient while retaining their linguistic prowess. Specifically, we offer information-theoretic interventions to overcome the catastrophic forgetting of linguistic skills that occurs while injecting non-linguistic skills into language models.},
  keywords={arxiv:2211.02098}
}

@article{chetioui2022problemmagnolia,
  title={P3 problem and Magnolia language: Specializing array computations for emerging architectures},
  author={Benjamin Chetioui and Marius Kleppe Larnøy and Jaakko Järvi and M. Haveraaen and L. Mullin},
  year={2022},
  booktitle={Frontiers of Computer Science},
  doi={10.3389/fcomp.2022.931312},
  url={https://www.semanticscholar.org/paper/50ac3b6bb980016189c142f67e35406a2b623b08},
  abstract={The problem of producing portable high-performance computing (HPC) software that is cheap to develop and maintain is called the P3 (performance, portability, productivity) problem. Good solutions to the P3 problem have been achieved when the performance profiles of the target machines have been similar. The variety of HPC architectures is, however, large and can be expected to grow larger. Software for HPC therefore needs to be highly adaptable, and there is a pressing need to provide developers with tools to produce software that can target machines with vastly different profiles. Multi-dimensional array manipulation constitutes a core component of numerous numerical methods, such as finite difference solvers of Partial Differential Equations (PDEs). The efficiency of these computations is tightly connected to traversing and distributing array data in a hardware-friendly way. The Mathematics of Arrays (MoA) allows for formally reasoning about array computations and enables systematic transformations of array-based programs, e.g., to use data layouts that fit to a specific architecture. This paper presents a programming methodology aimed for tackling the P3 problem in domains that are well-explored using Magnolia, a language designed to embody generic programming. The Magnolia programmer can restrict the semantic properties of abstract generic types and operations by defining so-called axioms. Axioms can be used to produce tests for concrete implementations of specifications, for formal verification, or to perform semantics-preserving program transformations. We leverage Magnolia's semantic specification facilities to extend the Magnolia compiler with a term rewriting system. We implement MoA's transformation rules in Magnolia, and demonstrate through a case study on a finite difference solver of PDEs how our rewriting system allows exploring the space of possible optimizations.}
}

@article{gao2022programaidedlanguage,
  title={PAL: Program-aided Language Models},
  author={Luyu Gao and Aman Madaan and Shuyan Zhou and Uri Alon and Pengfei Liu and Yiming Yang and Jamie Callan and Graham Neubig},
  year={2022},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2211.10435},
  url={https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7},
  abstract={Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15\% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .},
  keywords={arxiv:2211.10435}
}

@article{ghosh2022pastadataset,
  title={PASTA: A Dataset for Modeling PArticipant STAtes in Narratives},
  author={Sayontan Ghosh and Mahnaz Koupaee and Isabella Chen and Francis Ferraro and Nathanael Chambers and Niranjan Balasubramanian},
  year={2022},
  journal={Transactions of the Association for Computational Linguistics},
  doi={10.1162/tacl_a_00600},
  url={https://www.semanticscholar.org/paper/e894fb15054d3bc9659060406a12dfd1055ae32e},
  abstract={Abstract The events in a narrative are understood as a coherent whole via the underlying states of their participants. Often, these participant states are not explicitly mentioned, instead left to be inferred by the reader. A model that understands narratives should likewise infer these implicit states, and even reason about the impact of changes to these states on the narrative. To facilitate this goal, we introduce a new crowdsourced English-language, Participant States dataset, PASTA. This dataset contains inferable participant states; a counterfactual perturbation to each state; and the changes to the story that would be necessary if the counterfactual were true. We introduce three state-based reasoning tasks that test for the ability to infer when a state is entailed by a story, to revise a story conditioned on a counterfactual state, and to explain the most likely state change given a revised story. Experiments show that today’s LLMs can reason about states to some degree, but there is large room for improvement, especially in problems requiring access and ability to reason with diverse types of knowledge (e.g., physical, numerical, factual).1},
  keywords={arxiv:2208.00329}
}

@article{kurnia2022pengembanganbahan,
  title={PENGEMBANGAN BAHAN AJAR MATEMATIKA BERBASIS MULTIMODAL PADA MATERI BARISAN DAN DERET},
  author={Nurul Kurnia and Subhan Ajiz Awalludin},
  year={2022},
  booktitle={EduMatSains : Jurnal Pendidikan, Matematika dan Sains},
  doi={10.33541/edumatsains.v7i1.3934},
  url={https://www.semanticscholar.org/paper/5af2401442c0936f54daeddf04e0ea734458d4ce},
  abstract={The purpose of this study is to develop multimodal-based mathematics teaching materials on row and series materials using assistance media in the form of Power Point, iSpring Suite 10 and Website 2 apk builder. The method used in this study is the Waterfall model. A tool used to analyze compilers using Unified Modeling Language (UML). The result of this research is an android application from the development of mathematics teaching materials on multimodal-based lineups and series materials. Multimodal is a way of learning by combining several techniques, be it sound, visual and also writing. Testing this application uses a black box test, which is testing an application, which can be tested on anyone and the android media used in this test is the Samsung Galaxy A51. Based on the test results, all the buttons in the application work as intended. Then it can be concluded that the application of mathematics to the material of rows and series can be used to train students' mathematical reasoning skills, especially in row and series materials. For this application, it is hoped that it can be developed again in various ways, be it animation, design, features, practice questions, various and more varied quiz questions and more effective programming so that it becomes more interesting.}
}

@article{tondo2022pengembanganlembar,
  title={PENGEMBANGAN LEMBAR KERJA SISWA DENGAN PENDEKATAN SAINTIFIK PADA MATERI BILANGAN KELAS VII MTs MUHAMMADIYAH 1 MALANG},
  author={Abner Alosius Ama Tondo and N. Irianti and R. Setiawan},
  year={2022},
  booktitle={PRISMATIKA Jurnal Pendidikan dan Riset Matematika},
  doi={10.33503/prismatika.v5i1.1885},
  url={https://www.semanticscholar.org/paper/fc900df9d33b59fd31907fd89636643ea05f2269},
  abstract={The development of LKS can help teaching and learning activities in the classroom especially during the covid-19 pandemic. Worksheets that are made in an attractive and systematic manner can help students to learn to be more active in learning mathematics. Choice of approach and model Learning is also something that plays a very important role in supporting development Interesting and systematic worksheets. The purpose of this study was to determine the feasibility of developing LKS with a scientific approach on the number material for class VII MTs Muhammadiyah 1 Malang. This research was a development research using the ADDIE development model which consists of five main stages, namely Analysis, Design, Development, Implementation, and Evaluation. The LKS developed was validated by three validators, namely two mathematics education lecturers and a seventh grade mathematics teacher. The quality of the LKS with a scientific approach that was developed was assessed from the aspects of media, language and material with an average validation result of 83.12\%. The subject of the experiment was carried out on class VII A students of MTs Muhammadiyah 1 Malang with a total of 23 students. Data collection techniques were carried out by interviews, validation, tests and questionnaires. Based on the results of the small group trial, an average percentage score of 80.45\% showed that the LKS was very valid, in the large group the LKS that the researchers developed got a very interesting response with an average percentage score of 81.13\%. So that the LKS compiled with a scientific approach is feasible to use.}
}

@article{kusumaningrum2022pengembanganmodul,
  title={PENGEMBANGAN MODUL MATEMATIKA DENGAN MODEL CONSTRUCTIVIST LEARNING DESIGN PADA MATERI RELASI DAN FUNGSI UNTUK KELAS VIII SMP},
  author={Silvia Kusumaningrum},
  year={2022},
  booktitle={SCIENCE : Jurnal Inovasi Pendidikan Matematika dan IPA},
  doi={10.51878/science.v2i2.1267},
  url={https://www.semanticscholar.org/paper/d47e8f98efb4423f13d1ddc21ea15c53eaf9d193},
  abstract={This research aims to develop teaching materials in the form of mathematics modules for class VIII junior high schools. Based on the needs analysis, the material developed in this module is relation and functions using the Constructivist Learning Design (CLD) model that has six stages, namely situations, groupings, bridges, questions, exhibits, and reflections. The method used in this research is research and development. This research and development procedure consists of five stages, namely conducting needs analysis, initial product development, expert validation, small-scale field trials, and large-scale field trials and module feasibility trials for teachers. The student's math modules are developed according to the Constructivist Learning Design (CLD) model, and the language used in the modules is by with Ejaan Yang Disempurnakan (EYD). Based on the validation results of media experts, the average percentage of the entire questionnaire was 89.79\%, so the category was obtained very well. The presentation of the module and the graphic design of the module are proportional. Based on the results of small-scale field trials, the average percentage of the entire questionnaire was 81.11\%, so the category was obtained very well. Based on the results of large-scale field trials, the average percentage of the entire questionnaire was 82.39\%, so the category was obtained very well. Students feel the benefits of modules and are interested in the use of modules. Based on the results of the module feasibility trial for teachers, the average percentage of the entire questionnaire was 93.88\%, so an excellent category was obtained. The developed modules can be used and understood easily by students. Therefore, it can be concluded that the mathematics module developed meets the feasibility of being used in the learning of relation and function materials.
ABSTRAKPenelitian ini bertujuan untuk mengembangkan bahan ajar berupa modul matematika untuk kelas VIII SMP. Berdasarkan analisis kebutuhan, materi yang dikembangkan dalam modul ini adalah relasi dan fungsi dengan menggunakan model Constructivist Learning Design (CLD) yang mempunyai enam tahapan yaitu situations, groupings, bridge, questions, exhibit, dan reflections. Keenam tahapan tersebut terdapat pada bagian-bagian di dalam modul. Metode yang digunakan pada penelitian ini adalah penelitian dan pengembangan (research and development). Prosedur penelitian dan pengembangan ini terdiri dari lima tahap, yaitu melakukan analisis kebutuhan, pengembangan produk awal, validasi ahli, uji coba lapangan skala kecil, dan uji coba lapangan skala besar serta uji coba kelayakan modul kepada guru. Berdasarkan hasil validasi ahli materi dan bahasa, persentase rata-rata keseluruhan angket sebesar 89,46\% maka diperoleh kategori sangat baik. Modul yang dikembangkan sesuai dengan model Constructivist Learning Design (CLD), dan bahasa yang digunakan dalam modul sesuai dengan kaidah Ejaan Yang Disempurnakan (EYD). Berdasarkan hasil validasi ahli media, persentase rata-rata keseluruhan angket sebesar 89,79\% maka diperoleh kategori sangat baik. Penyajian modul dan desain grafis modul sudah proporsional. Berdasarkan hasil uji coba lapangan skala kecil, persentase rata-rata keseluruhan angket sebesar 81,11\% maka diperoleh kategori sangat baik. Berdasarkan hasil uji coba lapangan skala besar, persentase rata-rata keseluruhan angket sebesar 82,39\% maka diperoleh kategori sangat baik. Siswa merasakan adanya manfaat modul dan tertarik dengan adanya penggunaan modul. Berdasarkan hasil uji coba kelayakan modul kepada guru, persentase rata-rata keseluruhan angket sebesar 93,88\% maka diperoleh kategori sangat baik. Modul yang dikembangkan dapat digunakan dan dipahami dengan mudah oleh siswa. Dengan demikian, dapat disimpulkan bahwa modul matematika yang dikembangkan memenuhi kelayakan untuk digunakan pada pembelajaran materi relasi dan fungsi.}
}

@article{anisa2022pengembanganmodul,
  title={PENGEMBANGAN MODUL PEMBELAJARAN ILMU PENGETAHUAN ALAM BERBASIS STEM (SCIENCE, TECHNOLOGY, ENGINEERING, AND MATHEMATICS) UNTUK MENINGKATKAN BERPIKIR KRITIS SISWA SD/MI},
  author={I. Anisa and Retno Triwoelandari and Yono Yono},
  year={2022},
  booktitle={Refleksi Edukatika},
  doi={10.24176/re.v12i2.6840},
  url={https://www.semanticscholar.org/paper/5ec98e8c54046704edddc47d3016eb48e8069ecf},
  abstract={The purpose of this study is to determine the effectiveness of developing natural science learning modules based on STEM (Science, Technology, Engineering, and Mathematics) to improve critical thinking and suitable for use for learning natural sciences in grade IV SD/MI. This research method uses research and development or is called Research and Development (RD) which refers to the ASSURE development model. The subject of this research is class IV SDIT Khoiru Ummah. This learning module goes through the stages of expert validation. The result show based on the results of the validation of the learning module, it was declared feasible to use, seen from the results of design validation, which obtained 76.31\%, language validation 87.5\% and material validation 71.5\%. In addition, the increase in students' critical thinking was declared effective. Based on the results of the large group which was divided into 2, namely the experimental class got greater results than the control class. From the results presented, the STEM-based natural science learning module (Science, Technology, Engineering, and Mathematics) is suitable for use by fourth graders and is effective in improving critical thinking of fourth grade elementary/MI students.}
}

@article{astuti2022pengembangansoal,
  title={PENGEMBANGAN SOAL KEMAMPUAN PENALARAN MATEMATIS UNTUK SISWA SMA},
  author={Y. Astuti and Ristontowi},
  year={2022},
  booktitle={Jurnal Math-UMB.EDU},
  doi={10.36085/mathumbedu.v9i2.2508},
  url={https://www.semanticscholar.org/paper/a974957e5465e8364b7ca150ecbc5b3eaeab15df},
  abstract={Abstrak
Tujuan penelitian ini mengembangkan soal-soal kemampuan penalaran matematis yang valid dan praktis. Metode penelitian yang digunakan research and development dengan model Tessmer (modifikasi Zulkardi, 2006) yang terdiri dari tahap preliminary, self evaluation, expert review dan one-to-one, small group dan field test. Penelitian ini hanya sampai  pada tahap small group. Subjek penelitian ini adalah 30 orang siswa kelas X. Penelitian ini menggunakan instrumen berupa dokumen, lembar validasi, lembar komentar/saran dan prototype. Kevalidan soal dilihat dari hasil analisis penilaian validator pada lembar validasi yang menyatakan soal-soal dikembangkan baik berdasarkan konstruk, konten dan bahasa. Keterbacaan soal dilihat dari tahap one-to-one. Setelah selesai pada tahap one-to-one dilanjutkan pada tahap small group yaitu uji coba prototype II kepada 30 orang siswa SMA kelas X. Penelitian  ini  menghasilkan soal-soal  penalaran  matematis yang  valid  dan  praktis.  
Kata kunci: Pengembangan Soal Kemampuan Penalaran Matematis.
 
Abstract
The purpose of this study is to develop problems of mathematical reasoning skills that are valid and practical. Research methods used research and development with tessmer model (modification Zulkardi, 2006) consisting of preliminary stages, self evaluation, expert review and one-to-one, small group and field test. This research only reached the small group stage. The subjects of this study were 30 students of class X. This study used instruments in the form of documents, validation sheets, comment sheets / suggestions and prototypes. The validity of the problem is seen from the results of the validator assessment analysis on the validation sheet that states the problems are developed both based on construct, content and language. The readability of the question is viewed from the one-to-onestage. After completion in the one-to-one stage continued in the small group stage, namely the prototype II trial to 30 students of high school class X. This research produces valid and practical mathematical reasoning problems.
Keywords: Development of Mathematical Reasoning Skills.}
}

@article{naresh2022pentatronpersonalized,
  title={PENTATRON: PErsonalized coNText-Aware Transformer for Retrieval-based cOnversational uNderstanding},
  author={Niranjan Uma Naresh and Ziyan Jiang and Ankit and Sungjin Lee and Jie Hao and Xing Fan and Chenlei Guo},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.12308},
  url={https://www.semanticscholar.org/paper/ecdee4c3e7c6a5ce0c25c4d24bbfa363e1bbb5aa},
  abstract={Conversational understanding is an integral part of modern intelligent devices. In a large fraction of the global traffic from customers using smart digital assistants, frictions in dialogues may be attributed to incorrect understanding of the entities in a customer's query due to factors including ambiguous mentions, mispronunciation, background noise and faulty on-device signal processing. Such errors are compounded by two common deficiencies from intelligent devices namely, (1) the device not being tailored to individual customers, and (2) the device responses being unaware of the context in the conversation session. Viewing this problem via the lens of retrieval-based search engines, we build and evaluate a scalable entity correction system, PENTATRON. The system leverages a parametric transformer-based language model to learn patterns from in-session customer-device interactions coupled with a non-parametric personalized entity index to compute the correct query, which aids downstream components in reasoning about the best response. In addition to establishing baselines and demonstrating the value of personalized and context-aware systems, we use multitasking to learn the domain of the correct entity. We also investigate the utility of language model prompts. Through extensive experiments, we show a significant upward movement of the key metric (Exact Match) by up to 500.97\% (relative to the baseline).},
  keywords={arxiv:2210.12308}
}

@article{diwimuri2022perancangangame,
  title={PERANCANGAN GAME EDUKASI “THINKING MATH” UNTUK MELATIH KEMAMPUAN PENALARAN MATEMATIS},
  author={Alaya Diwimuri and Joko Soebagyo},
  year={2022},
  booktitle={EduMatSains : Jurnal Pendidikan, Matematika dan Sains},
  doi={10.33541/edumatsains.v7i1.3916},
  url={https://www.semanticscholar.org/paper/664918791769ba61f9ece018cac3b5867d5a9cef},
  abstract={The purpose of this study is to design and build an educational game based on android thinking math using the Kodular web-based tool with block programming language. This educational game is needed as an effort to train one's mathematical reasoning skills, and eliminate boredom and fear for those who want to learn mathematics. The results of five studies related to mathematics educational games, generally stated that mathematics educational games could increase one's interest in learning mathematics which were used effectively and interestingly. The method used is the Waterfall educational game development method. The tool used to analyze the compiler uses the Unified Modeling Language (UML). The result of the study is an Android-based math learning game. Based on the test results, it can be concluded that the implementation of the thinking math educational game to train mathematical reasoning skills was successfully carried out. For this educational game itself, it is hoped that it can be further developed in terms of animation, design, features, music, various quiz questions with more varied subject matter and more effective programming so that it becomes more interesting.}
}

@article{esser2022pp88bayesian,
  title={PP88 Bayesian Joint Models For Cost-Effectiveness Analyses Based On Clustered Participant Data, With Implementation In Stan},
  author={Jonas Esser and Anita Varga and M. E. Alili and J. V. van Dongen and J. Bosmans},
  year={2022},
  journal={International Journal of Technology Assessment in Health Care},
  doi={10.1017/S026646232200215X},
  url={https://www.semanticscholar.org/paper/e041df5638268762afd2ace56130263b631b7020},
  abstract={Introduction Cost-effectiveness analyses of empirical participant data are frequently complicated by irregularly distributed and correlated observations, which are not well approximated by normal distributions. Things get even more difficult when observations are clustered within higher level units (for example, hospitals) or the participant (that is, multiple measurements at different timepoints). Therefore, we developed a flexible Bayesian approach to jointly model costs and effects of two competing interventions with a multilevel structure. Methods Our new model is presented in mathematical form and discussed in detail. We model costs and Quality-Adjusted Life-Years effects through Gamma and Beta distributions, and account for the dependency between costs and effects by adding the effects as a predictor for the costs. We further include hurdle models to account for costs of for the presence of zero costs and perfect health scores. The full model is implemented in the probabilistic programming language Stan. To compare the performance of our Bayesian model to a frequentist approach (linear mixed model combined with bootstrapping), we simulate 1000 datasets consisting of 400 participants and 20 clusters. Performance of both models is assessed in terms of variance, bias and coverage probability with respect to the costs and effects defined in the simulation. Results We ran a preliminary simulation with high intraclass correlation, strong negative correlation for patient-level costs and effects, and positive correlation of cluster effects on both outcomes. The analysis shows that the Bayesian model exhibits a slightly larger bias for estimated costs, but smaller errors and higher coverage probability compared to the frequentist alternative. We will explore different scenarios where we vary the parameters of the simulations and assess whether the results are robust to change. Conclusions It is very important that economic evaluations in health care produce precise and reliable results. Our Bayesian approach is able to handle multiple statistical complexities at once and performs better than a comparable frequentist model. Whether this conclusion holds for different simulation scenarios will be explored in further stages of this study.}
}

@article{chowdhery2022palmscaling,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={A. Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and P. Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and M. Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and S. Ghemawat and Sunipa Dev and H. Michalewski and Xavier García and Vedant Misra and Kevin Robinson and L. Fedus and Denny Zhou and Daphne Ippolito and D. Luan and Hyeontaek Lim and Barret Zoph and A. Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and R. Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Díaz and Orhan Firat and Michele Catasta and Jason Wei and K. Meier-Hellstern and D. Eck and J. Dean and Slav Petrov and Noah Fiedel},
  year={2022},
  journal={Journal of machine learning research},
  url={https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb},
  abstract={Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  keywords={arxiv:2204.02311}
}

@article{zelikman2022parselunified,
  title={Parsel: A Unified Natural Language Framework for Algorithmic Reasoning},
  author={E. Zelikman and Qian Huang and Gabriel Poesia and Noah D. Goodman and Nick Haber},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.10561},
  url={https://www.semanticscholar.org/paper/239b5649b12f28fd610de036afba41b9246db6c9}
}

@article{zelikman2022parselalgorithmic,
  title={Parsel🦆: Algorithmic Reasoning with Language Models by Composing Decompositions},
  author={E. Zelikman and Qian Huang and Gabriel Poesia and Noah D. Goodman and Nick Haber},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/e325fe41c8c1d547ccd102ac82be3ec8b23960f2},
  abstract={Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\textbackslash\{\}\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\textbackslash\{\}\% to 85\textbackslash\{\}\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel},
  keywords={arxiv:2212.10561}
}

@article{geng2022pathlanguage,
  title={Path Language Modeling over Knowledge Graphsfor Explainable Recommendation},
  author={Shijie Geng and Zuohui Fu and Juntao Tan and Yingqiang Ge and Gerard de Melo and Yongfeng Zhang},
  year={2022},
  booktitle={The Web Conference},
  doi={10.1145/3485447.3511937},
  url={https://www.semanticscholar.org/paper/75f4423820a6d2de93535fda5f80e17ae051dc47},
  abstract={To facilitate human decisions with credible suggestions, personalized recommender systems should have the ability to generate corresponding explanations while making recommendations. Knowledge graphs (KG), which contain comprehensive information about users and products, are widely used to enable this. By reasoning over a KG in a node-by-node manner, existing explainable models provide a KG-grounded path for each user-recommended item. Such paths serve as an explanation and reflect the historical behavior pattern of the user. However, not all items can be reached following the connections within the constructed KG under finite hops. Hence, previous approaches are constrained by a recall bias in terms of existing connectivity of KG structures. To overcome this, we propose a novel Path Language Modeling Recommendation (PLM-Rec) framework, learning a language model over KG paths consisting of entities and edges. Through path sequence decoding, PLM-Rec unifies recommendation and explanation in a single step and fulfills them simultaneously. As a result, PLM-Rec not only captures the user behaviors but also eliminates the restriction to pre-existing KG connections, thereby alleviating the aforementioned recall bias. Moreover, the proposed technique makes it possible to conduct explainable recommendation even when the KG is sparse or possesses a large number of relations. Experiments and extensive ablation studies on three Amazon e-commerce datasets demonstrate the effectiveness and explainability of the PLM-Rec framework.}
}

@article{wahyuni2022pengaruhketerampilan,
  title={Pengaruh Keterampilan Bahasa Guru terhadap Penalaran Siswa},
  author={W. Wahyuni},
  year={2022},
  booktitle={Jurnal Ilmiah Pendidikan Matematika Al Qalasadi},
  doi={10.32505/qalasadi.v6i2.4970},
  url={https://www.semanticscholar.org/paper/a94e6b475cbd2d110cc65e76283cbcf7b1e20812},
  abstract={Understanding the vocabulary that is part of the body of language contributes significantly to overall understanding in many areas of content, such as mathematical reasoning. Teachers are expected to be able to pay attention to the way children communicate their reasoning in order to respond appropriately to improve children's reasoning and communication in their mathematical thinking. This study explores specifically the effects on teachers' language skills in demonstration lessons while teaching mathematics in the classroom and the learning objectives of improving students' reasoning ability. The study involved teachers from three different schools in Langsa City. The large number of teachers does not seem to pay special attention in the course of demonstrations of the broader pattern of justification-based discourse being formed. Only with the introduction and understanding of these specific difficulties can teachers then begin to address the instructional needs of their students from a language perspective. Designing and delivering vocabulary instructions effectively is a necessary action. Failure to design, convey, and understand children's language will have an effect on the growth of their mathematical alignment. In an effort to improve the overall mathematical performance of students, teachers need to recognize the importance of language to children's mathematical reasoning.}
}

@article{polln2022perspectiveschallenges,
  title={Perspectives and Challenges of AI Techniques in the Field of Social Sciences and Communication},
  author={Raúl Ramos Pollán},
  year={2022},
  journal={Journal of Autonomous Intelligence},
  doi={10.32629/jai.v5i1.504},
  url={https://www.semanticscholar.org/paper/ef3721038c3bf4b3924f0ad2f5b4722523f82f85},
  abstract={In the past decade, the methods and technologies of artificial intelligence (AI) have made great progress. In many cases, they have become part of the usual landscape of solving new or old problems in different fields of human knowledge. In this progress, there are several aspects, especially three aspects: the availability and universality of data in many fields of human activities; a deeper understanding of the mathematics of the basic control algorithm; and the availability and capability of hardware and computing which allows a wide range and a large number of data experiments. Considering these aspects, the key challenge for each problem and application area is to understand how to use these technologies, to what extent they may reach, and what constraints need to be overcome in order to obtain beneficial results (in terms of production cost, value, etc.). This challenge includes identifying data sources and their integration and recovery requirements, the necessity and cost of acquiring or constructing tag data sets, volume data required for measurement, verifying its feasibility, technical method of data analysis task and its consistency with the final application goal, and social and communication sciences are no exception. The knowledge in these fields is related to artificial intelligence, but they do have particularities that define the most appropriate type of artificial intelligence technology and method (i.e. natural language processing). The successful use of AI technology in these disciplines involves not only technical knowledge, but also the establishment of a viable application environment, including the availability of data, the appropriate complexity of tasks to be performed, and verification procedures with experts in the field. This paper introduces the methodology of generating artificial intelligence model, summarizes the artificial intelligence methods and services most likely to be used in social and communication sciences, and finally gives some application examples to illustrate the practical and technical considerations in this regard.}
}

@article{valmeekam2022planbenchextensible,
  title={PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change},
  author={Karthik Valmeekam and Alberto Olmo and S. Sreedharan and Subbarao Kambhampati},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc},
  abstract={Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.},
  keywords={arxiv:2206.10498}
}

@article{pallagani2022plansformergenerating,
  title={Plansformer: Generating Symbolic Plans using Transformers},
  author={Vishal Pallagani and Bharath Muppasani and K. Murugesan and F. Rossi and L. Horesh and Biplav Srivastava and F. Fabiano and A. Loreggia},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.08681},
  url={https://www.semanticscholar.org/paper/ae441f7305dc2cd58c708528b3ecee3501cc5c46},
  abstract={Large Language Models (LLMs) have been the subject of active research, significantly advancing the field of Natural Language Processing (NLP). From BERT to BLOOM, LLMs have surpassed state-of-the-art results in various natural language tasks such as question answering, summarization, and text generation. Many ongoing efforts focus on understanding LLMs' capabilities, including their knowledge of the world, syntax, and semantics. However, extending the textual prowess of LLMs to symbolic reasoning has been slow and predominantly focused on tackling problems related to the mathematical field. In this paper, we explore the use of LLMs for automated planning - a branch of AI concerned with the realization of action sequences (plans) to achieve a goal, typically executed by intelligent agents, autonomous robots, and unmanned vehicles. We introduce Plansformer; an LLM fine-tuned on planning problems and capable of generating plans with favorable behavior in terms of correctness and length with reduced knowledge-engineering efforts. We also demonstrate the adaptability of Plansformer in solving different planning domains with varying complexities, owing to the transfer learning abilities of LLMs. For one configuration of Plansformer, we achieve \~{}97\% valid plans, out of which \~{}95\% are optimal for Towers of Hanoi - a puzzle-solving domain.},
  keywords={arxiv:2212.08681}
}

@article{tiong2022plugandplayzeroshot,
  title={Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training},
  author={A. Tiong and Junnan Li and Boyang Albert Li and S. Savarese and S. Hoi},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.08773},
  url={https://www.semanticscholar.org/paper/26fd105d0b5a458979c012cddb3ba2de943388c4},
  abstract={Visual question answering (VQA) is a hallmark of vision and language reasoning and a challenging task under the zero-shot setting. We propose Plug-and-Play VQA (PNP-VQA), a modular framework for zero-shot VQA. In contrast to most existing works, which require substantial adaptation of pretrained language models (PLMs) for the vision modality, PNP-VQA requires no additional training of the PLMs. Instead, we propose to use natural language and network interpretation as an intermediate representation that glues pretrained models together. We first generate question-guided informative image captions, and pass the captions to a PLM as context for question answering. Surpassing end-to-end trained baselines, PNP-VQA achieves state-of-the-art results on zero-shot VQAv2 and GQA. With 11B parameters, it outperforms the 80B-parameter Flamingo model by 8.5\% on VQAv2. With 738M PLM parameters, PNP-VQA achieves an improvement of 9.1\% on GQA over FewVLM with 740M PLM parameters. Code is released at https://github.com/salesforce/LAVIS/tree/main/projects/pnp-vqa},
  keywords={arxiv:2210.08773}
}

@article{xie2022powersystem,
  title={Power system terminal continuous trust evaluation model based on fine-grained data flow analysis},
  author={Ming Xie},
  year={2022},
  booktitle={Other Conferences},
  doi={10.1117/12.2626908},
  url={https://www.semanticscholar.org/paper/20947d6808a8b14b3fa8cf25f8efd366c07fe081},
  abstract={With the wide application of new power services, the continuous strengthening of plant network coordination and interaction, resulting in a large extension of data network, network security protection is more difficult. We propose a power system terminal continuous trust evaluation model based on fine-grained data flow analysis, which effectively solves the problem of weak anti-jamming of traditional trust evaluation and unstable trust evaluation results through the analysis of the context behavior of the access subject, evidence reasoning, and identification of intent of confidence propagation. Innovative application of natural language processing (NLP) technology to Web application traffic intrusion detection, multi-level, multi-grained traffic depth analysis, dynamic intelligent correlation and drilling analysis for network traffic data, reduce the traditional feature-based and reputation detection technology leakage rate, the location, tracking and traceability of abnormal traffic, the accuracy of the detection results reached 96.59 percent.}
}

@misc{unknown2022practicalmath,
  title={Practical Math Third Edition A Answers},
  year={2022},
  url={https://www.semanticscholar.org/paper/e6effd2e3f6a98e35bf8b3d2e2a14df0437d5218}
}

@misc{unknown2022predictivemaintenance,
  title={Predictive Maintenance Beyond Prediction Of Failures},
  year={2022},
  url={https://www.semanticscholar.org/paper/cbd373cb11d8a7549f1e100f2ce910013cf1177e}
}

@article{boyd2022predictivequerying,
  title={Predictive Querying for Autoregressive Neural Sequence Models},
  author={Alex Boyd and Samuel Showalter and S. Mandt and Padhraic Smyth},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2210.06464},
  url={https://www.semanticscholar.org/paper/de7d334a543d077f4162ebcd8da7eee843b7b10a},
  abstract={In reasoning about sequential events it is natural to pose probabilistic queries such as"when will event A occur next"or"what is the probability of A occurring before B", with applications in areas such as user modeling, medicine, and finance. However, with machine learning shifting towards neural autoregressive models such as RNNs and transformers, probabilistic querying has been largely restricted to simple cases such as next-event prediction. This is in part due to the fact that future querying involves marginalization over large path spaces, which is not straightforward to do efficiently in such models. In this paper we introduce a general typology for predictive queries in neural autoregressive sequence models and show that such queries can be systematically represented by sets of elementary building blocks. We leverage this typology to develop new query estimation methods based on beam search, importance sampling, and hybrids. Across four large-scale sequence datasets from different application domains, as well as for the GPT-2 language model, we demonstrate the ability to make query answering tractable for arbitrary queries in exponentially-large predictive path-spaces, and find clear differences in cost-accuracy tradeoffs between search and sampling methods.},
  keywords={arxiv:2210.06464}
}

@misc{nguyen2022principledapproaches,
  title={Principled Approaches Applications Interpretability Robustness Efficiency Deep Learning Systems Natural Language Processing Computer Vision Mathematical Modeling},
  author={T. Nguyen},
  year={2022},
  url={https://www.semanticscholar.org/paper/56f32a568a3be4122c3399d79803c4976246d45b}
}

@article{zhong2022proqastructural,
  title={ProQA: Structural Prompt-based Pre-training for Unified Question Answering},
  author={Wanjun Zhong and Yifan Gao and Ning Ding and Yujia Qin and Zhiyuan Liu and Ming Zhou and Jiahai Wang and Jian Yin and Nan Duan},
  year={2022},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2205.04040},
  url={https://www.semanticscholar.org/paper/c963c505ffc4cc8b33315eb967784d0a466b3910},
  abstract={Question Answering (QA) is a longstanding challenge in natural language processing. Existing QA works mostly focus on specific question types, knowledge domains, or reasoning skills. The specialty in QA research hinders systems from modeling commonalities between tasks and generalization for wider applications. To address this issue, we present ProQA, a unified QA paradigm that solves various tasks through a single model. ProQA takes a unified structural prompt as the bridge and improves the QA-centric ability by structural prompt-based pre-training. Through a structurally designed prompt-based input schema, ProQA concurrently models the knowledge generalization for all QA tasks while keeping the knowledge customization for every specific QA task. Furthermore, ProQA is pre-trained with structural prompt-formatted large-scale synthesized corpus, which empowers the model with the commonly-required QA ability. Experimental results on 11 QA benchmarks demonstrate that ProQA consistently boosts performance on both full data fine-tuning, few-shot learning, and zero-shot testing scenarios. Furthermore, ProQA exhibits strong ability in both continual learning and transfer learning by taking the advantages of the structural prompt.},
  keywords={arxiv:2205.04040}
}

@article{loureiro2022probingcommonsense,
  title={Probing Commonsense Knowledge in Pre-trained Language Models with Sense-level Precision and Expanded Vocabulary},
  author={Daniel Loureiro and A. Jorge},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.06376},
  url={https://www.semanticscholar.org/paper/3cd9e5de457662f5e3c268f75341a93a16254e55},
  abstract={Progress on commonsense reasoning is usually measured from performance improvements on Question Answering tasks designed to require commonsense knowledge. However, fine-tuning large Language Models (LMs) on these specific tasks does not directly evaluate commonsense learned during pre-training. The most direct assessments of commonsense knowledge in pre-trained LMs are arguably cloze-style tasks targeting commonsense assertions (e.g., A pen is used for [MASK].). However, this approach is restricted by the LM's vocabulary available for masked predictions, and its precision is subject to the context provided by the assertion. In this work, we present a method for enriching LMs with a grounded sense inventory (i.e., WordNet) available at the vocabulary level, without further training. This modification augments the prediction space of cloze-style prompts to the size of a large ontology while enabling finer-grained (sense-level) queries and predictions. In order to evaluate LMs with higher precision, we propose SenseLAMA, a cloze-style task featuring verbalized relations from disambiguated triples sourced from WordNet, WikiData, and ConceptNet. Applying our method to BERT, producing a WordNet-enriched version named SynBERT, we find that LMs can learn non-trivial commonsense knowledge from self-supervision, covering numerous relations, and more effectively than comparable similarity-based approaches.},
  keywords={arxiv:2210.06376}
}

@article{vlasov2022problematicissues,
  title={Problematic issues of management of the Federal fire service and other types of fire protection},
  author={K. Vlasov},
  year={2022},
  booktitle={Technology of technosphere safety},
  doi={10.25257/tts.2022.3.97.131-143},
  url={https://www.semanticscholar.org/paper/8695da5df3ef8c9452cbc98ee31092d3b98eef0f},
  abstract={Introduction. The activities of fire and rescue units can be conditionally divided into extinguishing a large number of ordinary fires and single large fires. Considering these categories of fires from the point of view of economic efficiency and ensuring the level of combat readiness of fire departments, a decision tree model is proposed to identify the most promising ways to develop the organization. The functioning of fire and rescue units within the garrison is studied from the standpoint of assessing the long-term cyclical development of the organization, taking into account various external and internal factors. The tasks of the research are to identify problematic issues of management of fire and rescue units of the Federal Fire Service and fire protection units of other types stationed on the territory of the fire and rescue garrison, taking into account the scale of fires and evaluating the effectiveness of the organization of activities. Methods. Big Data technologies based on the methods of mathematical statistics of the software modules Panda and NumPy of the high-level programming language Python 3 are applied. The "Decision Tree" method implemented by means of the sklearn Python module and the packages CHAID and rpart of the programming language for statistical data processing R is used. Results and discussion. The frequency of occurrence of large fires, when the involvement of almost all available forces and means of the fire and rescue garrison is required, is about one case for an interval of 4-5 years. The number of ordinary fires during the same time is approximately 100-130 thousand cases. Such a ratio of ordinary and large fires is a source of significant contradiction and leads to a number of problematic issues related to the organization of management of Federal Fire Service units, as well as units of other types of fire protection. To study the proposed issues in the context of comparing the categories of economic efficiency and the level of combat readiness of units, the "decision tree" method was applied. Conclusions. The analysis of the results of the functioning of the fire and rescue garrison at the limit of permissible possibilities arising in the process of eliminating a large fire shows that all the management structures of the garrison are affected and, based on the results of the study of the actions of the garrison units, objective prerequisites for a leap (qualitative) reform of the management system can be formed. Keywords: large fire, operational activity, busy time, mobile fire and rescue equipment, fire extinguishing devices, histogram, decision tree.}
}

@misc{jin2022programworkshop,
  title={Program YES Workshop 2022 Optimal Transport, Statistics, Machine Learning and moving in between},
  author={G. Jin and Valentina Masarotto},
  year={2022},
  url={https://www.semanticscholar.org/paper/8cde92023b267ed8490ce125795a3979cdfd624f}
}

@article{astriani2022projectbasedlearning,
  title={Project-Based Learning: Rewards and Challenges},
  author={Linda Astriani and Sasnia Akmalia},
  year={2022},
  booktitle={Book of Proceedings 2022},
  doi={10.23918/vesal2022a4},
  url={https://www.semanticscholar.org/paper/bb619e58a3c425497bfed0172bf7d23d6e386606},
  abstract={The purpose of this study is to evaluate the accuracy and usefulness of the project-based learning-based spatial and statistical modules. In this study, the five steps of the ADDIE development model-Analysis, Design, Development, Implementation, and Evaluation are used. The study's participants were 5th graders at SD Negeri 1 Cisantana in the Kuningan Regency's Cigugur District during the academic year 2021–2022. Analyses of descriptive, qualitative, and quantitative data were used in data gathering approaches. The study's findings show that 1) the language validity test is very valid, scoring an average of 96\%; 2) the material validity test is very valid, scoring an average of 96\%; 3) the media validity test is very valid, scoring an average of 98\%; and 4) the practicality test is very valid, scoring an average of 98\%. 3) The media validity test has a very high average score of 98\%, however the practicality test has a lower average score. 4) The teacher's results of the practicality test received a score of 96\% in the very practical area, 5) the small group's results of the practicality test received a score of 96\% in the the practical category, and 6) the large group's results of the practicality test received a score of 97\% in the very practical category. As a result, it can be said that the Spatial Structure and Statistics Module, which was created utilizing the ADDIE model, is legitimate and useful for use in the even semester 5th grade elementary school mathematics curriculum.}
}

@article{suzgun2022promptandrerankmethod,
  title={Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models},
  author={Mirac Suzgun and Luke Melas-Kyriazi and Dan Jurafsky},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2205.11503},
  url={https://www.semanticscholar.org/paper/0d6bb585493e34975f0437faa3179db3a02f6ae8},
  abstract={We propose a method for arbitrary textual style transfer (TST)—the task of transforming a text into any given style—utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent components: textual similarity, target style strength, and fluency. Our method uses zero-shot or few-shot prompting to obtain a set of candidate generations in the target style, and then re-ranks them according to the three components. Our method enables small pre-trained language models to perform on par with state-of-the-art large-scale models while using two orders of magnitude less compute and memory. We also investigate the effect of model size and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on style transfer quality across seven diverse textual style transfer datasets, finding, among other things, that delimiter-pair choice has a large impact on performance, and that models have biases on the direction of style transfer.},
  keywords={arxiv:2205.11503}
}

@article{deng2022promptbasedconservation,
  title={Prompt-based Conservation Learning for Multi-hop Question Answering},
  author={Zhenyun Deng and Yonghua Zhu and Yang Chen and Qianqian Qi and M. Witbrock and P. Riddle},
  year={2022},
  booktitle={International Conference on Computational Linguistics},
  doi={10.48550/arXiv.2209.06923},
  url={https://www.semanticscholar.org/paper/432b1611029cb9c8ff7a632bcef0f47f0b879004},
  abstract={Multi-hop question answering (QA) requires reasoning over multiple documents to answer a complex question and provide interpretable supporting evidence. However, providing supporting evidence is not enough to demonstrate that a model has performed the desired reasoning to reach the correct answer. Most existing multi-hop QA methods fail to answer a large fraction of sub-questions, even if their parent questions are answered correctly. In this paper, we propose the Prompt-based Conservation Learning (PCL) framework for multi-hop QA, which acquires new knowledge from multi-hop QA tasks while conserving old knowledge learned on single-hop QA tasks, mitigating forgetting. Specifically, we first train a model on existing single-hop QA tasks, and then freeze this model and expand it by allocating additional sub-networks for the multi-hop QA task. Moreover, to condition pre-trained language models to stimulate the kind of reasoning required for specific multi-hop questions, we learn soft prompts for the novel sub-networks to perform type-specific reasoning. Experimental results on the HotpotQA benchmark show that PCL is competitive for multi-hop QA and retains good performance on the corresponding single-hop sub-questions, demonstrating the efficacy of PCL in mitigating knowledge loss by forgetting.},
  keywords={arxiv:2209.06923}
}

@article{hu2022promptcappromptguided,
  title={PromptCap: Prompt-Guided Task-Aware Image Captioning},
  author={Yushi Hu and Hang Hua and Zhengyuan Yang and Weijia Shi and Noah A. Smith and Jiebo Luo},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.09699},
  url={https://www.semanticscholar.org/paper/a5cb8f26acb71edd77ff9a143d3ddaab2367eb40},
  abstract={Knowledge-based visual question answering (VQA) involves questions that require world knowledge beyond the image to yield the correct answer. Large language models (LMs) like GPT-3 are particularly helpful for this task because of their strong knowledge retrieval and reasoning capabilities. To enable LM to understand images, prior work uses a captioning model to convert images into text. However, when summarizing an image in a single caption sentence, which visual entities to describe are often underspecified. Generic image captions often miss visual details essential for the LM to answer visual questions correctly. To address this challenge, we propose PromptCap (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs. Different from generic captions, PromptCap takes a natural-language prompt to control the visual entities to describe in the generated caption. The prompt contains a question that the caption should aid in answering. To avoid extra annotation, PromptCap is trained by examples synthesized with GPT-3 and existing datasets. We demonstrate PromptCap's effectiveness on an existing pipeline in which GPT-3 is prompted with image captions to carry out VQA. PromptCap outperforms generic captions by a large margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks (60.4\% on OK-VQA and 59.6\% on A-OKVQA). Zero-shot results on WebQA show that PromptCap generalizes well to unseen domains.},
  keywords={arxiv:2211.09699}
}

@article{si2022promptinggpt3,
  title={Prompting GPT-3 To Be Reliable},
  author={Chenglei Si and Zhe Gan and Zhengyuan Yang and Shuohang Wang and Jianfeng Wang and Jordan L. Boyd-Graber and Lijuan Wang},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.09150},
  url={https://www.semanticscholar.org/paper/c8d594f09413b1555970f43e68847c211235d60f},
  abstract={Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.},
  keywords={arxiv:2210.09150}
}

@misc{azerbayev2022proofnetbenchmark,
  title={ProofNet: A Benchmark for Autoformalizing and Formally Proving Undergraduate-Level Mathematics Problems},
  author={Zhangir Azerbayev and Bartosz Piotrowski and J. Avigad},
  year={2022},
  url={https://www.semanticscholar.org/paper/d9c05c32b7935dc8f7a048f79c2ce2f45558ddc8}
}

@article{romero2022propositionalreasoning,
  title={Propositional Reasoning via Neural Transformer Language Models},
  author={Oscar J. Romero and A. Tomasic and A. Steinfeld and John Zimmerman},
  year={2022},
  booktitle={International Workshop on Neural-Symbolic Learning and Reasoning},
  url={https://www.semanticscholar.org/paper/ca68b7b6a6f062da58453a48898e1f14b4200a27}
}

@article{guo2022quantengineering,
  title={Quant 4.0: engineering quantitative investment with automated, explainable, and knowledge-driven artificial intelligence},
  author={Jian Guo and Sai Wang and L. Ni and H. Shum},
  year={2022},
  booktitle={Frontiers of Information Technology \& Electronic Engineering},
  doi={10.1631/FITEE.2300720},
  url={https://www.semanticscholar.org/paper/497a1accfd0be6cad1be4f2b6fa88078dae7414a},
  abstract={Quantitative investment (abbreviated as “quant” in this paper) is an interdisciplinary field combining financial engineering, computer science, mathematics, statistics, etc. Quant has become one of the mainstream investment methodologies over the past decades, and has experienced three generations: quant 1.0, trading by mathematical modeling to discover mis-priced assets in markets; quant 2.0, shifting the quant research pipeline from small “strategy workshops” to large “alpha factories”; quant 3.0, applying deep learning techniques to discover complex nonlinear pricing rules. Despite its advantage in prediction, deep learning relies on extremely large data volume and labor-intensive tuning of “black-box” neural network models. To address these limitations, in this paper, we introduce quant 4.0 and provide an engineering perspective for next-generation quant. Quant 4.0 has three key differentiating components. First, automated artificial intelligence (AI) changes the quant pipeline from traditional hand-crafted modeling to state-of-the-art automated modeling and employs the philosophy of “algorithm produces algorithm, model builds model, and eventually AI creates AI.” Second, explainable AI develops new techniques to better understand and interpret investment decisions made by machine learning black boxes, and explains complicated and hidden risk exposures. Third, knowledge-driven AI supplements data-driven AI such as deep learning and incorporates prior knowledge into modeling to improve investment decisions, in particular for quantitative value investing. Putting all these together, we discuss how to build a system that practices the quant 4.0 concept. We also discuss the application of large language models in quantitative finance. Finally, we propose 10 challenging research problems for quant technology, and discuss potential solutions, research directions, and future trends.},
  keywords={arxiv:2301.04020}
}

@article{weng2022quantitativetrading,
  title={Quantitative Trading Method based on Neural Network Machine Learning},
  author={W.-S. Weng},
  year={2022},
  booktitle={2022 Asia Conference on Algorithms, Computing and Machine Learning (CACML)},
  doi={10.1109/CACML55074.2022.00107},
  url={https://www.semanticscholar.org/paper/bbf0ed0223e9faa16789a2ca0f1439517145d339},
  abstract={Quantitative trading plays an essential role in the investment field with its advanced mathematical models for computer-aided trading of investment strategies. The artificial neural network algorithm is the trading algorithm with the largest amount of funds managed in the world. Due to the short history of quantitative trading research in China, large-scale funds have not been reported to be managed by the neural network algorithm. The results of tests on financial derivatives using neural networks with different structures demonstrate that the neural network strategies all have positive expected return. Within a considerable range of changes in structure. In this paper, the python language is majorly used to design a model implementation plan for a quantitative trading system reading currently widely recognized stock technical indicators, such as MA, MACD, KDJ, and BOLL. Additionally, position management strategies are optimized. Furthermore, a quantitative trading method based on neural network machine learning is constructed and verified with examples.}
}

@article{gavazzo2022quantitativemetric,
  title={Quantitative and Metric Rewriting: Abstract, Non-Expansive, and Graded Systems},
  author={Francesco Gavazzo and Cecilia Di Florio},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2206.13610},
  url={https://www.semanticscholar.org/paper/aac084f99fcc73b4afe9f80d20c3da8fd1ddfd0d},
  abstract={Modern mathematics begins with symbolic manipulation. The central role of signs and symbols per se is one of the main achievement of the Medieval culture [88] leading, among others, to the development of elementary or symbolic algebra. Starting from the latter, the syntactic manipulation of symbols more or less independently of their meaning — i.e. to what symbols stand for — has become an essential part of mathematical reasoning, not to say of reasoning in general. Today, symbolic manipulation is not just a pillar of mathematics, but it is at the very hearth of computation. Indeed, the symbolic manipulations of elementary algebra carry a computational content and, vice versa, computational processes can be fully described symbolically. Rewriting theory [94, 25] is the discipline that studies (the computational content of) symbolic manipulation in general. As such, rewriting has its origin both in symbolic algebra as the study of the algorithmic properties of equational reasoning, and in computability and programming language theory, where rewriting systems have been used to define symbolic models of computation — such as the \_-calculus [17] and combinatory logic [34, 74] — as well as the (operational) semantics and implementation of programming languages [7]. In both cases, rewriting is motivated by the need to define operational notions of equality revealing the computational content of equational deductions. Remarkably, operationality is ultimately achieved by making equality asymmetric, so that the aforementioned computational content can be fully uncovered by orienting equations. Nowadays, these oriented equations (and the evolution thereof) are known as rewriting — or reduction — relations. All of that highlights a crucial trait of rewriting theory, namely its deep connection with equational reasoning. In fact, rewriting does not actually focus on arbitrary symbolic transformations, but with equality-preserving ones: a rewriting relation refines equality by making the latter operational, and it is thus contained in it.},
  keywords={arxiv:2206.13610}
}

@article{gao2022rarrresearching,
  title={RARR: Researching and Revising What Language Models Say, Using Language Models},
  author={Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Zhao and N. Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2023.acl-long.910},
  url={https://www.semanticscholar.org/paper/66242baf48b0f6b828e7547ac39ffaa5e1b2cb3e},
  abstract={Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search.},
  keywords={arxiv:2210.08726}
}

@article{sevillanogarca2022revelframework,
  title={REVEL Framework to measure Local Linear Explanations for black-box models: Deep Learning Image Classification case of study},
  author={Iván Sevillano-García and Juli'an Luengo-Mart'in and Francisco Herrera},
  year={2022},
  journal={International Journal of Intelligent Systems},
  doi={10.1155/2023/8068569},
  url={https://www.semanticscholar.org/paper/f1a48aa95cdbe3e54e381e3a20bed981061fcc86},
  abstract={Explainable artificial intelligence is proposed to provide explanations for reasoning performed by artificial intelligence. There is no consensus on how to evaluate the quality of these explanations, since even the definition of explanation itself is not clear in the literature. In particular, for the widely known local linear explanations, there are qualitative proposals for the evaluation of explanations, although they suffer from theoretical inconsistencies. The case of image is even more problematic, where a visual explanation seems to explain a decision while detecting edges is what it really does. There are a large number of metrics in the literature specialized in quantitatively measuring different qualitative aspects, so we should be able to develop metrics capable of measuring in a robust and correct way the desirable aspects of the explanations. Some previous papers have attempted to develop new measures for this purpose. However, these measures suffer from lack of objectivity or lack of mathematical consistency, such as saturation or lack of smoothness. In this paper, we propose a procedure called REVEL to evaluate different aspects concerning the quality of explanations with a theoretically coherent development which do not have the problems of the previous measures. This procedure has several advances in the state of the art: it standardizes the concepts of explanation and develops a series of metrics not only to be able to compare between them but also to obtain absolute information regarding the explanation itself. The experiments have been carried out on four image datasets as benchmark where we show REVEL’s descriptive and analytical power.},
  keywords={arxiv:2211.06154}
}

@article{yu2022rgfgmlxmertanimprove,
  title={RGFGM-LXMERT-An Improve Architecture Based On LXMERT},
  author={Renjie Yu},
  year={2022},
  booktitle={International Conferences on Computing and Pattern Recognition},
  doi={10.1145/3581807.3581879},
  url={https://www.semanticscholar.org/paper/5209fef6918535a2f243ed47653c22cc6077879d},
  abstract={LXMERT (Learning Cross-Modality Encoder Representations from Transformers) is a two-stream cross-modality pre-trained model that performs well in different downstream tasks which contain two visual question answering datasets and a challenging visual-reasoning task (i.e., VQA, GQA, and NLVR). But the large-scale model still has a lot of room for progress. That is, the model accuracy is very low, the generalization ability is weak, and it is easy to be attacked by adversarial attacks. Furthermore, training the LXMERT model takes a lot of time and money, so there is an urgent need to improve. Thus, I try to improve the training speed, generalization ability, and accuracy of the model by enhancing both the training method and the model structure. In the training method, FGM (Fast Gradient Method) adversarial training is introduced in the finetune phase of the model by adding the disturbances in both the language embedding layer's and visual feature linear layer's weights, which effectively improves the model accuracy and generalization ability. In the model structure, a residual block with weight is used to improve the training speed by 1.6\% in the pre-training phase of this model without losing the model performance. Next, t the most important structure, the Encoder, is redesigned to make the model more convergent. The Encoder's FFN (Feed-Forward Neural Network) is replaced by GLU (Gated Linear Unit), which also improves the ability of model fitting and model performance. The improved model performs better on the VQA task than the benchmark (i.e., LXMERT). In the end, detailed ablation studies prove that my enhancement strategies are effective for LXMERT and observe the effectiveness of different measures on the model.}
}

@article{ji2022reducinghallucination,
  title={RHO (\$ρ\$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding},
  author={Ziwei Ji and Zihan Liu and Nayeon Lee and Tiezheng Yu and Bryan Wilie and Mini Zeng and Pascale Fung},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.01588},
  url={https://www.semanticscholar.org/paper/4e53b481beabba42aac027e5a8c69fed26ab4062},
  abstract={Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO (\$\textbackslash\{\}rho\$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54\% in FeQA).},
  keywords={arxiv:2212.01588}
}

@article{golovneva2022roscoesuite,
  title={ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning},
  author={O. Yu. Golovneva and Moya Chen and Spencer Poff and Martin Corredor and Luke Zettlemoyer and Maryam Fazel-Zarandi and Asli Celikyilmaz},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.07919},
  url={https://www.semanticscholar.org/paper/391246ce9c59d61c94cca3f8bef56c95542a4708},
  abstract={Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics.},
  keywords={arxiv:2212.07919}
}

@article{vlag2022ratemlcode,
  title={RateML: A Code Generation Tool for Brain Network Models},
  author={Michiel van der Vlag and M. Woodman and J. Fousek and Sandra Diaz-Pier and Aarón Pérez Martín and Viktor Jirsa  and A. Morrison},
  year={2022},
  booktitle={Frontiers in Network Physiology},
  doi={10.3389/fnetp.2022.826345},
  url={https://www.semanticscholar.org/paper/0cadee0c50f3992f4cb595feaf2effce2896c33f},
  abstract={Whole brain network models are now an established tool in scientific and clinical research, however their use in a larger workflow still adds significant informatics complexity. We propose a tool, RateML, that enables users to generate such models from a succinct declarative description, in which the mathematics of the model are described without specifying how their simulation should be implemented. RateML builds on NeuroML’s Low Entropy Model Specification (LEMS), an XML based language for specifying models of dynamical systems, allowing descriptions of neural mass and discretized neural field models, as implemented by the Virtual Brain (TVB) simulator: the end user describes their model’s mathematics once and generates and runs code for different languages, targeting both CPUs for fast single simulations and GPUs for parallel ensemble simulations. High performance parallel simulations are crucial for tuning many parameters of a model to empirical data such as functional magnetic resonance imaging (fMRI), with reasonable execution times on small or modest hardware resources. Specifically, while RateML can generate Python model code, it enables generation of Compute Unified Device Architecture C++ code for NVIDIA GPUs. When a CUDA implementation of a model is generated, a tailored model driver class is produced, enabling the user to tweak the driver by hand and perform the parameter sweep. The model and driver can be executed on any compute capable NVIDIA GPU with a high degree of parallelization, either locally or in a compute cluster environment. The results reported in this manuscript show that with the CUDA code generated by RateML, it is possible to explore thousands of parameter combinations with a single Graphics Processing Unit for different models, substantially reducing parameter exploration times and resource usage for the brain network models, in turn accelerating the research workflow itself. This provides a new tool to create efficient and broader parameter fitting workflows, support studies on larger cohorts, and derive more robust and statistically relevant conclusions about brain dynamics.}
}

@article{yao2022reactsynergizing,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d},
  abstract={While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  keywords={arxiv:2210.03629}
}

@article{kulshreshtha2022reasoningcircuits,
  title={Reasoning Circuits: Few-shot Multi-hop Question Generation with Structured Rationales},
  author={Saurabh Kulshreshtha and Anna Rumshisky},
  year={2022},
  booktitle={NLRSE},
  doi={10.48550/arXiv.2211.08466},
  url={https://www.semanticscholar.org/paper/2a0953e6aa8a8c4b88928957338e93f8636ebe84},
  abstract={Multi-hop Question Generation is the task of generating questions which require the reader to reason over and combine information spread across multiple passages employing several reasoning steps. Chain-of-thought rationale generation has been shown to improve performance on multi-step reasoning tasks and make model predictions more interpretable. However, few-shot performance gains from including rationales have been largely observed only in +100B language models, and otherwise require large-scale manual rationale annotation. In this paper, we introduce a new framework for applying chain-of-thought inspired structured rationale generation to multi-hop question generation under a very low supervision regime (8- to 128-shot). We propose to annotate a small number of examples following our proposed multi-step rationale schema, treating each reasoning step as a separate task to be performed by a generative language model. We show that our framework leads to improved control over the difficulty of the generated questions and better performance compared to baselines trained without rationales, both on automatic evaluation metrics and in human evaluation. Importantly, we show that this is achievable with a modest model size.},
  keywords={arxiv:2211.08466}
}

@article{wang2022reconstructingactionconditioned,
  title={Reconstructing Action-Conditioned Human-Object Interactions Using Commonsense Knowledge Priors},
  author={Xi Wang and Gengyan Li and Yen-Ling Kuo and Muhammed Kocabas and Emre Aksan and Otmar Hilliges},
  year={2022},
  booktitle={International Conference on 3D Vision},
  doi={10.1109/3DV57658.2022.00047},
  url={https://www.semanticscholar.org/paper/bb26db1a4af5b3199d4b9a4767fa12c23507b40f},
  abstract={We present a method for inferring diverse 3D models of human-object interactions from images. Reasoning about how humans interact with objects in complex scenes from a single 2D image is a challenging task given ambiguities arising from the loss of information through projection. In addition, modeling 3D interactions requires the generalization ability towards diverse object categories and interaction types. We propose an action-conditioned modeling of interactions that allows us to infer diverse 3D arrangements of humans and objects without supervision on contact regions or 3D scene geometry. Our method extracts high-level commonsense knowledge from large language models (such as GPT-3), and applies them to perform 3D reasoning of human-object interactions. Our key insight is priors extracted from large language models can help in reasoning about human-object contacts from textural prompts only. We quantitatively evaluate the inferred 3D models on a large human-object interaction dataset and show how our method leads to better 3D reconstructions. We further qualitatively evaluate the effectiveness of our method on real images and demonstrate its generalizability towards interaction types and object categories.},
  keywords={arxiv:2209.02485}
}

@misc{saxon2022relationleakage,
  title={Relation Leakage in Elicited Natural Language Inference Datasets},
  author={Michael Stephen Saxon and Xinyi Wang and Wenda Xu and William Yang Wang},
  year={2022},
  url={https://www.semanticscholar.org/paper/5a6f6f44a2e05709d81245526786f8dc8f8ab263}
}

@article{li2022researchfrontiers,
  title={Research frontiers of pre-training mathematical models based on BERT},
  author={Guang Li and Wennan Wang and Liukai Zhu and Jun Peng and Xujia Li and Ruijie Luo},
  year={2022},
  booktitle={2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)},
  doi={10.1109/EEBDA53927.2022.9744791},
  url={https://www.semanticscholar.org/paper/b55b85c3868060ed5de3b403d4691d84fd4229f4},
  abstract={Natural language processing (NLP) is a popular technology after the rise of big data and machine learning in recent years. With the development of deep learning, the field of natural language processing has also undergone a landmark transformation, including the emergence of the BERT large-scale language training model. The emergence of this model makes text mining a qualitative leap, meets more practical needs, and solves the related problems of feature vectorization of unstructured data. This article will sort out the connotation, task application, and main optimization and improvement methods of the BERT pre-training model released by Google, and provide a reference for subsequent related research and development based on BERT.}
}

@article{liu2022researchemotional,
  title={Research on emotional content recognition of music video based on support vector machine},
  author={Xi Liu},
  year={2022},
  booktitle={International Symposium on Parameterized and Exact Computation},
  doi={10.1145/3544109.3544350},
  url={https://www.semanticscholar.org/paper/caec30c92ef52b10130e65768e1e04d314966dd9},
  abstract={As one of the main ways of mass entertainment, music video itself also has rich emotional connotation and strong emotional regulation function to meet the emotional needs of the audience. At present, the feelings of music videos are mostly based on the evaluation of music videos by listeners or experts. Its workload is quite large, and people with different cognitive levels have different evaluations of the same music video. The emotional content of music video is the emotional intensity and emotional type that users are expected to be induced in the process of watching music video. At present, many scholars are engaged in the research of emotional content of music video, mainly through pattern classifier, rule reasoning and other methods to establish the mapping relationship between low-level feature space and basic emotion type space, and identify the emotion types of music video according to this relationship. However, emotion belongs to the category of psychology, which has typical uncertainty and unknowness. Different audiences have great differences in their recognition of the emotional content of the same music video. Other similar studies focus on the identification of emotional content, but do not consider the uncertainty of emotion. Based on the theory of unascertained mathematics, a new emotion content recognition algorithm based on unascertained measure is proposed to identify the emotion types in music video. Based on the statistics of support vector machine model, it is found that music video emotion is often expressed and rendered in a specific form. It can be concluded that there must be an internal relationship between emotional content and some low-level features of music video. We choose scene brightness, scene rhythm and color energy as three low-level feature indexes to analyze the emotional content of music video. Music video scene segments are selected as the basic unit to study emotional content. The mirror head segmentation algorithm is used to segment the shots of music video clips, and the support vector machine algorithm is used to segment and extract music video scenes. This algorithm opens up a new perspective for the study of music video emotion type recognition, and believes that with the in-depth research and system integration of music video feature emotion attributes, the performance of music video emotion content recognition based on deterministic measure will be improved in the future.}
}

@misc{jha2022responsiblereasoning,
  title={Responsible Reasoning with Large Language Models and the Impact of Proper Nouns},
  author={Sumit Kumar Jha},
  year={2022},
  url={https://www.semanticscholar.org/paper/a70fcd82d8d26bf4c8b0bdde88e96b6ce2c80ecf}
}

@article{he2022rethinkingwith,
  title={Rethinking with Retrieval: Faithful Large Language Model Inference},
  author={Hangfeng He and Hongming Zhang and D. Roth},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2301.00303},
  url={https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1},
  abstract={Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.},
  keywords={arxiv:2301.00303}
}

@misc{liu2022retrievalaugmentedknowledgegrounded,
  title={Retrieval-Augmented and Knowledge-Grounded Language Models for Faithful Clinical Medicine},
  author={Fenglin Liu and Bang-ju Yang and Chenyu You and Xian Wu and Shen Ge and Zhangdaihong Liu and Xunhu Sun and Yang Yang and D. Clifton},
  year={2022},
  url={https://www.semanticscholar.org/paper/6bff251e4503607cd439295770be1907eebb6700},
  abstract={Language models (LMs), including large language models (such as ChatGPT), have the potential to assist clinicians in generating various clinical notes. However, LMs are prone to produce ``hallucinations'', i.e., generated content that is not aligned with facts and knowledge. In this paper, we propose the Re\$\^{}3\$Writer method with retrieval-augmented generation and knowledge-grounded reasoning to enable LMs to generate faithful clinical texts. We demonstrate the effectiveness of our method in generating patient discharge instructions. It requires the LMs not to only understand the patients' long clinical documents, i.e., the health records during hospitalization, but also to generate critical instructional information provided both to carers and to the patient at the time of discharge. The proposed Re\$\^{}3\$Writer imitates the working patterns of physicians to first \textbackslash\{\}textbf\{re\}trieve related working experience from historical instructions written by physicians, then \textbackslash\{\}textbf\{re\}ason related medical knowledge. Finally, it \textbackslash\{\}textbf\{re\}fines the retrieved working experience and reasoned medical knowledge to extract useful information, which is used to generate the discharge instructions for previously-unseen patients. Our experiments show that, using our method, the performance of five representative LMs can be substantially boosted across all metrics. Meanwhile, we show results from human evaluations to measure the effectiveness in terms of fluency, faithfulness, and comprehensiveness.},
  keywords={arxiv:2210.12777}
}

@article{zhong2022romqabenchmark,
  title={RoMQA: A Benchmark for Robust, Multi-evidence, Multi-answer Question Answering},
  author={Victor Zhong and Weijia Shi and Wen-tau Yih and Luke Zettlemoyer},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.14353},
  url={https://www.semanticscholar.org/paper/b09a0e0398023683da479afc31df31440abb8f3e},
  abstract={We introduce RoMQA, the first benchmark for robust, multi-evidence, multi-answer question answering (QA). RoMQA contains clusters of questions that are derived from related constraints mined from the Wikidata knowledge graph. RoMQA evaluates robustness of QA models to varying constraints by measuring worst-case performance within each question cluster. Compared to prior QA datasets, RoMQA has more human-written questions that require reasoning over more evidence text and have, on average, many more correct answers. In addition, human annotators rate RoMQA questions as more natural or likely to be asked by people. We evaluate state-of-the-art large language models in zero-shot, few-shot, and fine-tuning settings, and find that RoMQA is challenging: zero-shot and few-shot models perform similarly to naive baselines, while supervised retrieval methods perform well below gold evidence upper bounds. Moreover, existing models are not robust to variations in question constraints, but can be made more robust by tuning on clusters of related questions. Our results show that RoMQA is a challenging benchmark for large language models, and provides a quantifiable test to build more robust QA methods.},
  keywords={arxiv:2210.14353}
}

@article{manas2022robusttraffic,
  title={Robust Traffic Rules and Knowledge Representation for Conflict Resolution in Autonomous Driving},
  author={K. Manas and Stefan Zwicklbauer and A. Paschke},
  year={2022},
  booktitle={RuleML+RR},
  url={https://www.semanticscholar.org/paper/ac18b4cf109c57018d0bfeb93077a98daf5f66a5}
}

@article{bugajskajaszczot2022rozwizywaniezada,
  title={Rozwiązywanie zadań tekstowych przez studentów – przyszłych nauczycieli edukacji wczesnoszkolnej},
  author={Beata Bugajska-Jaszczołt and M. Czajkowska},
  year={2022},
  booktitle={Problemy Wczesnej Edukacji},
  doi={10.26881/pwe.2022.55.09},
  url={https://www.semanticscholar.org/paper/83fe695fd3c0295508ff40139ecfcab223b12a57},
  abstract={The results of the research aimed at diagnosing the ability to solve textual tasks by the students – candidates for lower primary teachers are presented in the article. The research was carried out between the years 2017 and 2019. It comprised 392 students of pedagogy with teaching specialization. The main research method was a competency test, and the technique was document analysis. The research shows that the students have well-mastered calculational algorithms and techniques but they have difficulties in presenting their reasoning in the language of mathematics. A large group of students strive to write down the solution in the form of a ready mathematical formula, without representing the situation on a drawing/diagram or manipulating on concretes to help them solve the problem.}
}

@article{blinov2022rumedbenchrussian,
  title={RuMedBench: A Russian Medical Language Understanding Benchmark},
  author={Pavel Blinov and A. Reshetnikova and A. Nesterov and Galina Zubkova and V. Kokh},
  year={2022},
  booktitle={Conference on Artificial Intelligence in Medicine in Europe},
  doi={10.1007/978-3-031-09342-5_38},
  url={https://www.semanticscholar.org/paper/11a348120b115ddeb4bcee18c876a60a07852355},
  abstract={The paper describes the open Russian medical language understanding benchmark covering several task types (classification, question answering, natural language inference, named entity recognition) on a number of novel text sets. Given the sensitive nature of the data in healthcare, such a benchmark partially closes the problem of Russian medical dataset absence. We prepare the unified format labeling, data split, and evaluation metrics for new tasks. The remaining tasks are from existing datasets with a few modifications. A single-number metric expresses a model's ability to cope with the benchmark. Moreover, we implement several baseline models, from simple ones to neural networks with transformer architecture, and release the code. Expectedly, the more advanced models yield better performance, but even a simple model is enough for a decent result in some tasks. Furthermore, for all tasks, we provide a human evaluation. Interestingly the models outperform humans in the large-scale classification tasks. However, the advantage of natural intelligence remains in the tasks requiring more knowledge and reasoning.},
  keywords={arxiv:2201.06499}
}

@article{budiarti2022soalkemampuan,
  title={SOAL KEMAMPUAN PENALARAN MATEMATIS MATERI BANGUN RUANG SISI DATAR BERKONTEKS BENGKULU},
  author={Etika Budiarti and Nyayu Masyita Ariani and Adi Asmara},
  year={2022},
  booktitle={Jurnal Math-UMB.EDU},
  doi={10.36085/mathumbedu.v9i3.3582},
  url={https://www.semanticscholar.org/paper/63e1ef7f20c25220bdc890c8d221b91126a11e9e},
  abstract={Tujuan penelitian ini untuk menghasilkan soal-soal kemampuan penalaran matematis materi bangun ruang sisi datar berkonteks Bengkulu yang terstandar: yang valid, memiliki keterbacaan baik,  memiliki tingkat kesukaran dan indeks daya beda baik. Penelitian ini yang dilaksanakan pada tahun ajaran 2020/2021 ini mengikuti alur  Tessmer. Subjek ujicoba adalah siswa kelas VIII SMP di Bengkulu. Sebelum prototipe soal dibuat, dilakukan analisis siswa, kurikulum, materi dan konteks. Telaah dan revisi soal dilakukan pada tahap experts review. Soal ditelaah oleh 3 orang validator dari sisi konten, konstruk, dan bahasa. Komentar dan saran validator pada proses validasi menjadi acuan untuk revisi soal. Hasil analisis validasi akhir para validator menunjukkan bahwa soal telah valid. Proses one to one pada 3 orang siswa kelas VIII yang  masing-masing berkemampuan tinggi, sedang, dan rendah, Tergambar dari proses one to one bahwa maksud soal dapat terbaca oleh siswa. Bukti empirik mengenai tingkat kesukaran dan indeks daya beda butir soal diperoleh pada tahap small group, dengan cara mengujicobakan soal yang telah valid dan memiliki keterbacaan yang baik kepada 31 siswa kelas VIII SMP di Bengkulu.  Skor siswa terhadap soal tersebut dianalisis secara kuantitatif. Hasil analisis menunjukkan bahwa 8 soal dikembangkan dikategorikan terstandar. Soal-soal ini dapat digunakan untuk melatih kemampuan penalaran matematis siswa SMP.
Kata Kunci: Soal matematika, kemampuan penalaran matematis, konteks Bengkulu, bangun ruang sisi datar
 Abstract
The purpose of this research was to produce standardized items on mathematical reasoning abilities in the Bengkulu context of flat-sided geometry: those that were valid, had good clarity appeal, had a difficulty level and a good discrimination index. This research, which was conducted in the 2020/2021 academic year, follows the Tessmer’ model. The test subjects were students of class VIII SMP in Bengkulu. Before the items prototype was made, an analysis of students, curriculum, material and context was carried out. The review and revision of the items were carried out at the experts review stage. The items were reviewed by 3 validators in terms of content, construct, and language. The validator's comments and suggestions in the validation process become a reference for revision of the questions. The results of the final validation analysis of the validators, showed that the questions were valid. One to one process for 3 students of class VIII, each of which has high, medium, and low abilities. It is illustrated from the one to one process that the meaning of the questions can be read by students. Empirical evidence regarding the level of difficulty and discrimination index of items was obtained at the small group stage, by testing items that were valid and had good readability to 31 grade VIII SMP students in Bengkulu. Student scores on these items were analyzed quantitatively. The results of the analysis showed that the 8 items developed were categorized as standardized. These items can be used to exercise the mathematical reasoning skills of junior high school students.
Keywords: Item test of mathematics, mathematical reasoning ability, Bengkulu context, flat-sided geometry}
}

@misc{zoph2022stmoedesigning,
  title={ST-MoE: Designing Stable and Transferable Sparse Expert Models},
  author={Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and J. Dean and Noam M. Shazeer and W. Fedus},
  year={2022},
  url={https://www.semanticscholar.org/paper/1bc9865ebf52b59abac7f5ee4456ff2ac37fcff3},
  abstract={Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).},
  keywords={arxiv:2202.08906}
}

@article{liu2022studyinfluence,
  title={STUDY ON THE INFLUENCE OF NETWORK FACTORS ON THE LOYALTY AND EMOTIONAL BEHAVIOR OF CHINESE PROFESSIONAL FOOTBALL CLUB FANS},
  author={Shaoyong Liu and Wenlang Huang},
  year={2022},
  journal={International Journal of Neuropsychopharmacology},
  doi={10.1093/ijnp/pyac032.107},
  url={https://www.semanticscholar.org/paper/0f7b40c92fa6410cad7c1b30f78bf8e1b353589e},
  abstract={Abstract Background China Professional Football League has been in operation for nearly 30 years and has attracted more and more attention from the public and society. China Professional Football League has promoted the professional and commercial development of Chinese football and attracted a large number of fans. At present, Chinese professional football clubs are struggling, but they still get the support of many fans. The development of any club is inseparable from the support of loyal fans, who are the basis of the development of the club. Only with many loyal fans can the club have long-term and strong vitality. Only by making outstanding achievements, excavating the economic benefits of the club, establishing and maintaining the club culture and cultivating a large number of loyal fans, can the club bring profound heritage and solid economic benefits to the club. Football club has a large number of loyal fans, which is the cornerstone of the sustainable and vigorous development of professional football league. However, due to many factors, the loyalty of fans has decreased, which is related to the evaluation of pursuit by the Internet. Subjects and Methods Based on this, this paper studies the loyalty of Chinese football fans, takes the fans of China Henan sslm football club as the research object, and analyzes the current situation of China Henan sslm football club and its fans. Through the research, it is found that with the continuous development of the Internet, public emotional behavior will have an important impact on football public opinion. There are two directions: one is the top-down impact, the other is the bottom-up impact; It is mainly manifested in two typical ways: social mobilization and emotional social struggle in football public opinion. The main expressions of their emotional behavior are: weakness, anger, sadness and anger, etc. On the basis of combing the collective behavior and emotional struggle, the research finds that the communication framework of emotional behavior mainly includes the communication paths of discourse co meaning, identity co meaning and emotional co meaning; Functional analysis includes target function, attribution function and ideographic function. From the tendency of public sentiment, football public opinion shows criticism, populism, nationalism, pragmatism, patriotism and justice. The social expression of public sentiment in microblog public opinion includes the spiral phenomenon of silence, butterfly effect, herd effect, resentment and so on. From the perspective of psychology, the public emotions in football public opinion are fear, anxiety, anger and sadness, while in terms of expression, the public express their feelings through direct expression and folk language. Results Using the methods of literature review, questionnaire survey and mathematical statistics, this paper analyzes the dimension of fan loyalty and constructs the fan loyalty model of sslm football club in Henan Province. Conclusion According to the research, fan participation is divided into three dimensions: pleasure participation, symbolic participation and central participation; Product attributes related to the club, such as team performance, star players, head coach and team management, are positively correlated with club fan loyalty; Logo design, stadium, game quality and team tradition are also positively correlated with club fan loyalty; There is a significant positive correlation between fans' attitude loyalty and fans' behavior loyalty. The author believes that club brand communication is an important factor for fans to participate in the development of football clubs. It is necessary for football clubs to strengthen the publicity of fans, let more fans know the real situation of the club, increase the exposure of football clubs and improve the satisfaction of fans. Secondly, the importance of the performance of the club team in the correlation of product related attributes is self-evident. The club needs to constantly improve the performance of the team in the game and win more attention from the outside world. Therefore, the club must actively train star players, select excellent coaches and lead the team to achieve better results. The club needs to strengthen the development of Lenovo products again, which have attributes unrelated to products. The design of club logo, the construction and maintenance of competition venues, the quality of competition and the tradition of the team are the basis of influencing the loyalty of fans. The club needs to continuously strengthen the product research and development of Lenovo's non product related attributes. Work can not only bring economic benefits to the club, but also shape the club's fan culture and further improve the club's fan loyalty. Finally, the club needs to provide a space for fans to communicate: there is a positive correlation between fans' escape, recognition, acceptance, nostalgia, regional glory and fans' loyalty. Let fans find suitable community organizations, form common interests and help improve the loyalty of club fans. Acknowledgements This research was supported by the general project of philosophy and Social Sciences Planning of Zhejiang Province (20NDJC184YB), and the Fundamental Research Funds for Zhejiang Provincial Universities and Research Institutes (2021R005).}
}

@misc{zelikman2022starbootstrapping,
  title={STaR: Bootstrapping Reasoning With Reasoning},
  author={E. Zelikman and Yuhuai Wu and Noah D. Goodman},
  year={2022},
  url={https://www.semanticscholar.org/paper/23dd78e424d32f6a48660dcd67ce994b8a7db8be},
  abstract={Generating step-by-step"chain-of-thought"rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the"Self-Taught Reasoner"(STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30\$\textbackslash\{\}times\$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.},
  keywords={arxiv:2203.14465}
}

@article{levy2022safetextbenchmark,
  title={SafeText: A Benchmark for Exploring Physical Safety in Language Models},
  author={Sharon Levy and Emily Allaway and Melanie Subbiah and Lydia B. Chilton and D. Patton and K. McKeown and William Yang Wang},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.10045},
  url={https://www.semanticscholar.org/paper/2b6291eb76e2ff885238e94704bb795046d7d530},
  abstract={Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and requires additional commonsense knowledge to comprehend that it leads to physical harm. We create the first benchmark dataset, SafeText, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. We utilize SafeText to empirically study commonsense physical safety across various models designed for text generation and commonsense reasoning tasks. We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice. As a result, we argue for further studies of safety and the assessment of commonsense physical safety in models before release.},
  keywords={arxiv:2210.10045}
}

@article{czinczoll2022scientificcreative,
  title={Scientific and Creative Analogies in Pretrained Language Models},
  author={Tamara Czinczoll and H. Yannakoudakis and Pushkar Mishra and Ekaterina Shutova},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2211.15268},
  url={https://www.semanticscholar.org/paper/933f60dda5847f208d9d3fd65e9b0df9cfed2403},
  abstract={This paper examines the encoding of analogy in large-scale pretrained language models, such as BERT and GPT-2. Existing analogy datasets typically focus on a limited set of analogical relations, with a high similarity of the two domains between which the analogy holds. As a more realistic setup, we introduce the Scientific and Creative Analogy dataset (SCAN), a novel analogy dataset containing systematic mappings of multiple attributes and relational structures across dissimilar domains. Using this dataset, we test the analogical reasoning capabilities of several widely-used pretrained language models (LMs). We find that state-of-the-art LMs achieve low performance on these complex analogy tasks, highlighting the challenges still posed by analogy understanding.},
  keywords={arxiv:2211.15268}
}

@article{creswell2022selectioninferenceexploiting,
  title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
  author={Antonia Creswell and M. Shanahan and I. Higgins},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd},
  abstract={Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100\% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.},
  keywords={arxiv:2205.09712}
}

@article{su2022selectiveannotation,
  title={Selective Annotation Makes Language Models Better Few-Shot Learners},
  author={Hongjin Su and Jungo Kasai and Chen Henry Wu and Weijia Shi and Tianlu Wang and Jiayi Xin and Rui Zhang and Mari Ostendorf and Luke Zettlemoyer and Noah A. Smith and Tao Yu},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2209.01975},
  url={https://www.semanticscholar.org/paper/86d0d3855f94105e25d81cab9f3d269c6062a9c4},
  abstract={Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9\%/11.4\% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks. Our code is available at https://github.com/HKUNLP/icl-selective-annotation.},
  keywords={arxiv:2209.01975}
}

@article{wang2022selfconsistencyimproves,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Xuezhi Wang and Jason Wei and D. Schuurmans and Quoc Le and Ed H. Chi and Denny Zhou},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2},
  abstract={Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  keywords={arxiv:2203.11171}
}

@article{miao2022selfpacedmultigrained,
  title={Self-Paced Multi-Grained Cross-Modal Interaction Modeling for Referring Expression Comprehension},
  author={Peihan Miao and Wei Su and Gaoang Wang and Xuewei Li and Xi Li},
  year={2022},
  journal={IEEE Transactions on Image Processing},
  doi={10.1109/TIP.2023.3334099},
  url={https://www.semanticscholar.org/paper/09b85c61b99083a292b561b51a25b8a931dcd1b1},
  abstract={As an important and challenging problem in vision-language tasks, referring expression comprehension (REC) generally requires a large amount of multi-grained information of visual and linguistic modalities to realize accurate reasoning. In addition, due to the diversity of visual scenes and the variation of linguistic expressions, some hard examples have much more abundant multi-grained information than others. How to aggregate multi-grained information from different modalities and extract abundant knowledge from hard examples is crucial in the REC task. To address aforementioned challenges, in this paper, we propose a Self-paced Multi-grained Cross-modal Interaction Modeling framework, which improves the language-to-vision localization ability through innovations in network structure and learning mechanism. Concretely, we design a transformer-based multi-grained cross-modal attention, which effectively utilizes the inherent multi-grained information in visual and linguistic encoders. Furthermore, considering the large variance of samples, we propose a self-paced sample informativeness learning to adaptively enhance the network learning for samples containing abundant multi-grained information. The proposed framework significantly outperforms state-of-the-art methods on widely used datasets, such as RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame datasets, demonstrating the effectiveness of our method.},
  keywords={arxiv:2204.09957}
}

@article{chou2022semisupervisedgrounding,
  title={Semi-supervised Grounding Alignment for Multi-modal Feature Learning},
  author={Shih-Han Chou and Zicong Fan and J. Little and L. Sigal},
  year={2022},
  booktitle={Canadian Conference on Computer and Robot Vision},
  doi={10.1109/CRV55824.2022.00015},
  url={https://www.semanticscholar.org/paper/3735b7ac2bc306a7345a678a215fdd158d3947d6},
  abstract={Self-supervised transformer-based architectures, such as ViLBERT [1] and others, have recently emerged as dominant paradigms for multi-modal feature learning. Such architectures leverage large-scale datasets (e.g., Conceptual Captions [2]) and, typically, image-sentence pairings, for self-supervision. However, conventional multi-modal feature learning requires huge datasets and computing for both pre-training and fine-tuning to the target task. In this paper, we illustrate that more granular semi-supervised alignment at a region-phrase level is an additional useful cue and can further improve the performance of such representations. To this end, we propose a novel semi-supervised grounding alignment loss, which leverages an off-the-shelf pre-trained phrase grounding model for pseudo-supervision (by producing region-phrase alignments). This semi-supervised formulation enables better feature learning in the absence of any additional human annotations on the large-scale (Conceptual Captions) dataset. Further, it shows an even larger margin of improvement on smaller data splits, leading to effective data-efficient feature learning. We illustrate the superiority of the learned features by fine-tuning the resulting models to multiple vision-language downstream tasks: visual question answering (VQA), visual commonsense reasoning (VCR), and visual grounding. Experiments on the VQA, VCR, and grounding benchmarks demonstrate the improvement of up to 1.3\% in accuracy (in visual grounding) with large-scale training; up to 5.9\% (in VQA) with 1/8 of the data for pre-training and fine-tuning11We will release the code and all pre-trained models upon acceptance..}
}

@article{jurewicz2022interdependencetransformer,
  title={Set Interdependence Transformer: Set-to-Sequence Neural Networks for Permutation Learning and Structure Prediction},
  author={Mateusz Jurewicz and Leon Derczynski},
  year={2022},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2206.03720},
  url={https://www.semanticscholar.org/paper/d4740b0cdaf0e855fbc1f41364f9e707a76cc25e},
  abstract={The task of learning to map an input set onto a permuted sequence of its elements is challenging for neural networks. Set-to-sequence problems occur in natural language processing, computer vision and structure prediction, where interactions between elements of large sets define the optimal output. Models must exhibit relational reasoning, handle varying cardinalities and manage combinatorial complexity. Previous attention-based methods require n layers of their set transformations to explicitly represent n-th order relations. Our aim is to enhance their ability to efficiently model higher-order interactions through an additional interdependence component. We propose a novel neural set encoding method called the Set Interdependence Transformer, capable of relating the set's permutation invariant representation to its elements within sets of any cardinality. We combine it with a permutation learning module into a complete, 3-part set-to-sequence model and demonstrate its state-of-the-art performance on a number of tasks. These range from combinatorial optimization problems, through permutation learning challenges on both synthetic and established NLP datasets for sentence ordering, to a novel domain of product catalog structure prediction. Additionally, the network's ability to generalize to unseen sequence lengths is investigated and a comparative empirical analysis of the existing methods' ability to learn higher-order interactions is provided.},
  keywords={arxiv:2206.03720}
}

@article{khan2022singlestreammultilevel,
  title={Single-Stream Multi-Level Alignment for Vision-Language Pretraining},
  author={Zaid Khan and B. Vijaykumar and Xiang Yu and S. Schulter and Manmohan Chandraker and Y. Fu},
  year={2022},
  booktitle={European Conference on Computer Vision},
  doi={10.48550/arXiv.2203.14395},
  url={https://www.semanticscholar.org/paper/c10370810c8c5ccf12ae5a604b0f23601f90c4b2},
  abstract={Self-supervised vision-language pretraining from pure images and text with a contrastive loss is effective, but ignores fine-grained alignment due to a dual-stream architecture that aligns image and text representations only on a global level. Earlier, supervised, non-contrastive methods were capable of finer-grained alignment, but required dense annotations that were not scalable. We propose a single stream architecture that aligns images and language at multiple levels: global, fine-grained patch-token, and conceptual/semantic, using two novel tasks: symmetric cross-modality reconstruction (XMM) and a pseudo-labeled key word prediction (PSL). In XMM, we mask input tokens from one modality and use cross-modal information to reconstruct the masked token, thus improving fine-grained alignment between the two modalities. In PSL, we use attention to select keywords in a caption, use a momentum encoder to recommend other important keywords that are missing from the caption but represented in the image, and then train the visual encoder to predict the presence of those keywords, helping it learn semantic concepts that are essential for grounding a textual token to an image region. We demonstrate competitive performance and improved data efficiency on image-text retrieval, grounding, visual question answering/reasoning against larger models and models trained on more data. Code and models available at zaidkhan.me/SIMLA.},
  keywords={arxiv:2203.14395}
}

@article{badruddoja2022smartercontracts,
  title={Smarter Contracts to Predict using Deep-Learning Algorithms},
  author={Syed Badruddoja and R. Dantu and Yanyan He and Mark A. Thompson and Abiola Salau and Kritagya Upadhyay},
  year={2022},
  booktitle={International Conference on Blockchain Computing and Applications},
  doi={10.1109/BCCA55292.2022.9922240},
  url={https://www.semanticscholar.org/paper/5fdc9c5a5fa205dfb97da54a7e3cb0ca0b6c877e},
  abstract={Deep learning techniques can predict cognitive intelligence from large datasets involving complex computations with activation functions. However, the prediction output needs verification for trust and reliability. Moreover, these algorithms suffer from the model's provenance to keep track of model updates and developments. Blockchain smart contracts provide a trustable ledger with consensus-based decisions that assure integrity and verifiability. In addition, the immutability feature of blockchain also supports the provenance of data that can help deep learning algorithms. Nevertheless, smart contract languages cannot predict due to the absence of floating-point operations required by activation functions of neural networks. In this paper, we derive a novel method using the Taylor series expansion to compute the floating-point equivalent output for activation functions. We train the deep learning model off-chain using a standard Python programming language. Moreover, we store models and predict on-chain with blockchain smart contracts to produce a trusted forecast. Our experiment and analysis achieved an accuracy (99\%) similar to popular Keras Python library models for the MNIST dataset. Furthermore, any blockchain platform can reproduce the activation function using our derived method. Last but not least, other deep learning algorithms can reuse the mathematical model to predict on-chain.}
}

@article{xiao2022smoothquantaccurate,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Guangxuan Xiao and Ji Lin and Mickael Seznec and Julien Demouth and Song Han},
  year={2022},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2211.10438},
  url={https://www.semanticscholar.org/paper/2c994fadbb84fb960d8306ee138dbeef41a5b323},
  abstract={Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.},
  keywords={arxiv:2211.10438}
}

@article{razeghi2022snoopyonline,
  title={Snoopy: An Online Interface for Exploring the Effect of Pretraining Term Frequencies on Few-Shot LM Performance},
  author={Yasaman Razeghi and R. Mekala and IV RobertL.Logan and Matt Gardner and Sameer Singh},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2022.emnlp-demos.39},
  url={https://www.semanticscholar.org/paper/9715be0a94c9b05bafe299cbfb4f846453bfd2ab},
  abstract={Current evaluation schemes for large language models often fail to consider the impact of the overlap between pretraining corpus and test data on model performance statistics. Snoopy is an online interface that allows researchers to study this impact in few-shot learning settings. Our demo provides term frequency statistics for the Pile, which is an 800 GB corpus, ac-companied by the precomputed performance of EleutherAI/GPT models on more than 20 NLP benchmarks, including numerical, commonsense reasoning, natural language understanding, and question-answering tasks. Snoopy allows a user to interactively align specific terms in test instances with their frequency in the Pile, enabling exploratory analysis of how term frequency is related to the accuracy of the models, which are hard to discover through au-tomated means. A user can look at correla-tions over various model sizes and numbers of in-context examples and visualize the re-sult across multiple (potentially aggregated) datasets. Using Snoopy , we show that a re-searcher can quickly replicate prior analyses for numerical tasks, while simultaneously allowing for much more expansive exploration that was previously challenging. Snoopy is available at https://nlp.ics.uci.edu/snoopy .}
}

@article{zeng2022socraticmodels,
  title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  author={Andy Zeng and Adrian S. Wong and Stefan Welker and K. Choromanski and F. Tombari and Aveek Purohit and M. Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Peter R. Florence},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/ada81a4de88a6ce474df2e2446ad11fea480616e},
  abstract={Large pretrained (e.g.,"foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.},
  keywords={arxiv:2204.00598}
}

@article{zhu2022solvingmath,
  title={Solving Math Word Problem via Cooperative Reasoning induced Language Models},
  author={Xinyu Zhu and Junjie Wang and Lin Zhang and Yuxiang Zhang and Ruyi Gan and Jiaxing Zhang and Yujiu Yang},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.16257},
  url={https://www.semanticscholar.org/paper/01f7bb1f9c611b5e849558e445fdccb98a3a3040}
}

@article{lewkowycz2022solvingquantitative,
  title={Solving Quantitative Reasoning Problems with Language Models},
  author={Aitor Lewkowycz and Anders Andreassen and David Dohan and Ethan Dyer and H. Michalewski and V. Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2206.14858},
  url={https://www.semanticscholar.org/paper/ab0e3d3e4d42369de5933a3b4c237780b41c0d77},
  abstract={Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.},
  keywords={arxiv:2206.14858}
}

@article{kumar2022solvingtctype,
  title={Solving TC-type AWPs using external knowledge \& learning},
  author={Suresh Kumar and P. S. Kumar},
  year={2022},
  booktitle={COMAD/CODS},
  doi={10.1145/3493700.3493744},
  url={https://www.semanticscholar.org/paper/5f34bd0d2206d1a55645706b88e5429f27be729a},
  abstract={Arithmetic Word Problems(AWPs) are mathematical numerical problems expressed in natural language like English, and they provide a natural representation for many quantitative reasoning situations, such as understanding epidemic facts, finance \& sports news, etc. Transfer-Case(TC) problems are specific AWPs involving the transfer of objects from one agent to another. In the current modeling, we assume that TC-type AWPs involve a single transfer of a quantity. A TC-type AWP consists of one or more sentences and they essentially contain the information of four types: beforetransfer(BT), transfer(TR), after-transfer(AT), question(QS). The state-of-the-art approaches adopt either statistical, treebased, or template-based modelings. Tree-based approaches[1, 2, 4, 7] leverage the idea of transforming an arithmetic expression to an equivalent tree-structure whereas template-based approach[6] focuses on predicting the appropriate template for the given AWP.We try solving the TC-type AWPs using existing approaches and experiment with number-of-sentences, question-template, complexity-ofthe-sentences, etc; and observe the followings: previous approaches are less efficient in processing the compound or longer sentences of the problem-text, and are less robust to even small template variations. Additionally, they rely on extensive manually-given annotations. Moreover, existing approaches are missing relevant background knowledge(humans solve AWPs efficiently, as they have required knowledge). Knowledge-based systems are considered imperative for building intelligent AI systems. They have a wide variety of applications in various domains like program analysis, natural language understanding, decision making, search \& analysis, and knowledge-infused learning, etc. Therefore, in the proposed work1, we focus on solving TC-type AWPs using external knowledge and learning. In Figure 1, we present an example TC-type AWP. SystemOverview(Figure 2): Broadly, the proposed framework has three components; classifier, TC-Ontology, and SWRL module.}
}

@misc{macdonald2022speakershello,
  title={Speakers at the Hello Session},
  author={Anson Macdonald and Brock D Sherlock and Dilshan Wijesena and Hongzhi Liao and Josef I. Bisits and J. Connor and Joshua Graham and Kevin Pan and Ryan Seelig and Samuel Mason and Stuart-James M. Burney and Yerlan Nessipbayev},
  year={2022},
  url={https://www.semanticscholar.org/paper/a1d089feab0eaa0f1dcfe6e839e768428a99c2e7}
}

@article{huang2022specialissue,
  title={Special issue on AI-based web information processing},
  author={Chuanchao Huang and Shuren Zhou},
  year={2022},
  booktitle={Neural computing \& applications (Print)},
  doi={10.1007/s00521-022-07342-x},
  url={https://www.semanticscholar.org/paper/d53f097c6255c542345cf958fbcb99eb67f2526e}
}

@misc{unknown2022specicationtopdown,
  title={Speciﬁcation and top-down design of distributed systems},
  year={2022},
  url={https://www.semanticscholar.org/paper/65c11dceacaa587c9fd0fc3f0b0b3160ee9fa445}
}

@article{lee2022statisticalrelational,
  title={Statistical Relational Extension of Answer Set Programming},
  author={Joohyung Lee and Zhun Yang},
  year={2022},
  booktitle={RW},
  doi={10.1007/978-3-031-31414-8_4},
  url={https://www.semanticscholar.org/paper/a7cf30fd9ba609fb8db4575a04dd90b009f3dd52}
}

@article{collins2022structuredflexible,
  title={Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks},
  author={K. M. Collins and Catherine Wong and Jiahai Feng and Megan Wei and J. Tenenbaum},
  year={2022},
  booktitle={Annual Meeting of the Cognitive Science Society},
  doi={10.48550/arXiv.2205.05718},
  url={https://www.semanticscholar.org/paper/7ef9aafc68511afab5b287e62b754576ea37b4ce},
  abstract={Human language offers a powerful window into our thoughts -- we tell stories, give explanations, and express our beliefs and goals through words. Abundant evidence also suggests that language plays a developmental role in structuring our learning. Here, we ask: how much of human-like thinking can be captured by learning statistical patterns in language alone? We first contribute a new challenge benchmark for comparing humans and distributional large language models (LLMs). Our benchmark contains two problem-solving domains (planning and explanation generation) and is designed to require generalization to new, out-of-distribution problems expressed in language. We find that humans are far more robust than LLMs on this benchmark. Next, we propose a hybrid Parse-and-Solve model, which augments distributional LLMs with a structured symbolic reasoning module. We find that this model shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.},
  keywords={arxiv:2205.05718}
}

@article{sameth2022studentscritical,
  title={Students Critical Thinking Skills Through Project-Based Learning Models in Solving Pythagoras Theorem Problems},
  author={Liliyani Sameth and Djaffar Lessy},
  year={2022},
  journal={Indo-MathEdu Intellectuals Journal},
  doi={10.54373/imeij.v3i1.35},
  url={https://www.semanticscholar.org/paper/6b017a47f524b2eaf635551e3aaac3a9b78dda84},
  abstract={Mathematics is a symbolic language of its characteristics is to use deductive reasoning and inductive reasoning methods. In the teaching of mathematics each subject is a unit so that the mastery of one subject is a support to learn the next subject. Similarly, the ability of students in carrying out calculating operations in mathematics lessons in developing students' critical thinking skills. To perform calculating operations on the right learning requires the right learning model and teaching materials. Therefore, the right learning model in cultivating students' critical thinking skills is a project-based learning model. This research aims to find out the ability to think critically through project-based learning models on the material of the theorem theorem phytagoras of students in class VIII of State Junior High School 5 West Leihitu District. The type of research used in this study is qualitative descriptive. The process of taking subjects is based on the highest value of group work on LKS. From the results of the group's work, 3 students were taken from the total number of students, namely 30 people. The instruments used in this study were researchers, tests and interviews. The data analysis techniques used follow concepts developed by Miles and Huberman, namely data reduction, data presentation, and inference. The results of the data analysis obtained that the critical thinking skills possessed by each student vary. From the results of interviews with 3 subjects, it is known that sl subjects meet 4 indicators of critical thinking skills used in this study, namely providing simple explanations, building basic abilities, making conclusions, and providing advanced explanations. The AH subject meets two indicators: providing simple explanations and building basic skills. While the subject of RK only met one indicator used in this study, namely building basic capabilities in the problem-solving process.}
}

@article{domenico2022subordinationalgebras,
  title={Subordination Algebras as Semantic Environment of Input/Output Logic},
  author={Andrea De Domenico and A. Farjami and Krishna Manoorkar and A. Palmigiano and Mattia Panettiere and Xiaolong Wang},
  year={2022},
  booktitle={Workshop on Logic, Language, Information and Computation},
  doi={10.48550/arXiv.2205.13903},
  url={https://www.semanticscholar.org/paper/4aa1e5936529d87829247415b1569d7f7e224e20},
  abstract={We establish a novel connection between two research areas in non-classical logics which have been developed independently of each other so far: on the one hand, input/output logic, introduced within a research program developing logical formalizations of normative reasoning in philosophical logic and AI; on the other hand, subordination algebras, investigated in the context of a research program integrating topological, algebraic, and duality-theoretic techniques in the study of the semantics of modal logic. Specifically, we propose that the basic framework of input/output logic, as well as its extensions, can be given formal semantics on (slight generalizations of) subordination algebras. The existence of this interpretation brings benefits to both research areas: on the one hand, this connection allows for a novel conceptual understanding of subordination algebras as mathematical models of the properties and behaviour of norms; on the other hand, thanks to the well developed connection between subordination algebras and modal logic, the output operators in input/output logic can be given a new formal representation as modal operators, whose properties can be explicitly axiomatised in a suitable language, and be systematically studied by means of mathematically established and powerful tools.},
  keywords={arxiv:2205.13903}
}

@article{dua2022successiveprompting,
  title={Successive Prompting for Decomposing Complex Questions},
  author={Dheeru Dua and Shivanshu Gupta and Sameer Singh and Matt Gardner},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2212.04092},
  url={https://www.semanticscholar.org/paper/c90151f00b1ac4abf1cc353849b453aa21cc2df3},
  abstract={Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce “Successive Prompting” where, we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate synthetic dataset which can be used to bootstrap model’s ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement in F1 of \~{}5\% when compared with a state-of-the-art model with synthetic augmentations and few-shot version of the DROP dataset.},
  keywords={arxiv:2212.04092}
}

@misc{li2022symbolicdata,
  title={Symbolic Data Augmentation for Assisted Neural Reasoning},
  author={Muhan Li},
  year={2022},
  url={https://www.semanticscholar.org/paper/7865ae1c4d7ba0e3ba48aef54270c3fdf61ec11d}
}

@article{gaur2022symbolicmath,
  title={Symbolic Math Reasoning with Language Models},
  author={Vedant Gaur and Nikunj Saunshi},
  year={2022},
  booktitle={2022 IEEE MIT Undergraduate Research Technology Conference (URTC)},
  doi={10.1109/URTC56832.2022.10002218},
  url={https://www.semanticscholar.org/paper/f557f3a32d309373e7d31bb93ca1b80b4a6e39e7},
  abstract={The emergence of large language models (LLMs) such as OpenAI’s GPT-3, Google’s LaMDA, Meta’s OPT [2, 3, 7, 10] etc. have revolutionized the field of natural language processing (NLP). These models with upwards of hundreds of billions of parameters are trained on large unlabeled text corpora and can subsequently solve downstream tasks with little to no labeled data. While these models are increasingly versatile in their abilities, e.g., solving math word problems, the larger question of their ability to reason remains. Using and modifying the SVAMP dataset, we find that GPT-3’s davinci-002 model, in addition to having good performance on numerical math word problems, also performs well on the potentially harder symbolic version of the same problems. Furthermore, adopting a two-step approach (solve symbolically and then substitute numerical values) leads to better accuracy on the numerical test set in the zero-shot regime. Additionally, we find that the use of specific prompting techniques pushes the model, in many cases, to actively describe its thought process and aid in the final answer output when faced with a complex, multi-step problem, aligning with recent observations.}
}

@article{wachowiak2022systematicanalysis,
  title={Systematic Analysis of Image Schemas in Natural Language through Explainable Multilingual Neural Language Processing},
  author={Lennart Wachowiak and Dagmar Gromann},
  year={2022},
  booktitle={International Conference on Computational Linguistics},
  url={https://www.semanticscholar.org/paper/964b613686e1c533c8994c78a952ffe823101578}
}

@article{geng2022systematictransformation,
  title={Systematic Transformation Method from UML to Event-B},
  author={Xue Geng and Sheng-rong Zou and Junpeng Yao},
  year={2022},
  booktitle={IEEE International Conference on Software Quality, Reliability and Security Companion},
  doi={10.1109/QRS-C57518.2022.00127},
  url={https://www.semanticscholar.org/paper/028323a772a952e38bab6b028b8c096ab5a52456},
  abstract={In object-oriented software development, UML has become a de facto modeling standard. However, although UML is easy to understand and apply, it has inaccurate semantics, and UML is a semi-formal modeling language, which cannot be formally verified. Event-B is a formal method based on a large number of mathematical predicate logic, which is precise but difficult to understand and apply. Therefore, how to combine the advantages of UML diagram and Event-B method is the focus of the research. The previous transformation methods are based on the transformation from UML scatter diagram to Event-B, which is prone to conflict and inconsistency. Therefore, we propose a systematic transformation method that can realize the corresponding unification of elements in UML and those in Event-B. The general software system is a medium-sized system. We believe that the medium-sized system can be clearly expressed by using use case diagram, class diagram, state diagram and sequence diagram. In this paper, the transformation methods from these four diagrams to Event-B are given respectively. The transformation method of the system is applied to the elevator control system which requires high safety and reliability. The system transformation method from UML to Event-B not only improves the accuracy of UML and is easy for software practitioners to use, but also enhances the comprehensibility of formal methods and is conducive to the promotion and application of formal methods.}
}

@article{li2022systematicitygpt3s,
  title={Systematicity in GPT-3's Interpretation of Novel English Noun Compounds},
  author={Siyan Li and Riley Carlson and Christopher Potts},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.09492},
  url={https://www.semanticscholar.org/paper/74be37384adc9b643b0c0a2d3b26c1361c5d779b},
  abstract={Levin et al. (2019) show experimentally that the interpretations of novel English noun compounds (e.g., stew skillet), while not fully compositional, are highly predictable based on whether the modifier and head refer to artifacts or natural kinds. Is the large language model GPT-3 governed by the same interpretive principles? To address this question, we first compare Levin et al.'s experimental data with GPT-3 generations, finding a high degree of similarity. However, this evidence is consistent with GPT3 reasoning only about specific lexical items rather than the more abstract conceptual categories of Levin et al.'s theory. To probe more deeply, we construct prompts that require the relevant kind of conceptual reasoning. Here, we fail to find convincing evidence that GPT-3 is reasoning about more than just individual lexical items. These results highlight the importance of controlling for low-level distributional regularities when assessing whether a large language model latently encodes a deeper theory.},
  keywords={arxiv:2210.09492}
}

@article{shiri2022tcgeventeffective,
  title={TCG-Event: Effective Task Conditioning for Generation-based Event Extraction},
  author={Fatemeh Shiri and Tongtong Wu and Yuan-Fang Li and Gholamreza Haffari},
  year={2022},
  booktitle={Australasian Language Technology Association Workshop},
  url={https://www.semanticscholar.org/paper/cd23ff769057416e1d1a702210ff019db8d4763f}
}

@article{ge2022tgealargescale,
  title={TGEA 2.0: A Large-Scale Diagnostically Annotated Dataset with Benchmark Tasks for Text Generation of Pretrained Language Models},
  author={Huibin Ge and Xiaohu Zhao and Chuang Liu and Yulong Zeng and Qun Liu and Deyi Xiong},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/44eca0cf8397b536fcf82c2e249eb05ed19b0ce4}
}

@article{zhou2022tacubeprecomputing,
  title={TaCube: Pre-computing Data Cubes for Answering Numerical-Reasoning Questions over Tabular Data},
  author={Fan Zhou and Mengkang Hu and Haoyu Dong and Zhoujun Cheng and Shi Han and Dongmei Zhang},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2205.12682},
  url={https://www.semanticscholar.org/paper/52b3087525b262f6f467453e22fdfa843353d40c},
  abstract={Existing auto-regressive pre-trained language models (PLMs) like T5 and BART, have been well applied to table question answering by UNIFIEDSKG and TAPEX, respectively, and demonstrated state-of-the-art results on multiple benchmarks. However, auto-regressive PLMs are challenged by recent emerging numerical reasoning datasets, such as TAT-QA, due to the error-prone implicit calculation. In this paper, we present TaCube, to pre-compute aggregation/arithmetic results for the table in advance, so that they are handy and readily available for PLMs to answer numerical reasoning questions. TaCube systematically and comprehensively covers a collection of computational operations over table segments. By simply concatenating TaCube to the input sequence of PLMs, it shows significant experimental effectiveness. TaCube promotes the F1 score from 49.6\% to 66.2\% on TAT-QA and achieves new state-of-the-art results on WikiTQ (59.6\% denotation accuracy). TaCube’s improvements on numerical reasoning cases are even more notable: on TAT-QA, TaCube promotes the exact match accuracy of BART-large by 39.6\% on sum, 52.5\% on average, 36.6\% on substraction, and 22.2\% on division. We believe that TaCube is a general and portable pre-computation solution that can be potentially integrated to various numerical reasoning frameworks},
  keywords={arxiv:2205.12682}
}

@article{li2022tacklingmath,
  title={Tackling Math Word Problems with Fine-to-Coarse Abstracting and Reasoning},
  author={Ai Li and Xueyao Jiang and Bang Liu and Jiaqing Liang and Yanghua Xiao},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2205.08274},
  url={https://www.semanticscholar.org/paper/a4901628fc09ab1edbeaee7ebe771f442c41d006},
  abstract={Math Word Problems (MWP) is an important task that requires the ability of understanding and reasoning over mathematical text. Existing approaches mostly formalize it as a generation task by adopting Seq2Seq or Seq2Tree models to encode an input math problem in natural language as a global representation and generate the output mathematical expression. Such approaches only learn shallow heuristics and fail to capture fine-grained variations in inputs. In this paper, we propose to model a math word problem in a fine-to-coarse manner to capture both the local fine-grained information and the global logical structure of it. Instead of generating a complete equation sequence or expression tree from the global features, we iteratively combine low-level operands to predict a higher-level operator, abstracting the problem and reasoning about the solving operators from bottom to up. Our model is naturally more sensitive to local variations and can better generalize to unseen problem types. Extensive evaluations on Math23k and SVAMP datasets demonstrate the accuracy and robustness of our method.},
  keywords={arxiv:2205.08274}
}

@article{hunter2022takingassetbased,
  title={Taking an Asset-Based Approach in the Use of a Culturally Located Task to Construct Functional Reasoning},
  author={R. Hunter and Jodie Hunter and Bronwyn Gibbs},
  year={2022},
  booktitle={Teachers College Record},
  doi={10.1177/01614681221103958},
  url={https://www.semanticscholar.org/paper/2a1341e7d5001aeeabdf87ab9fbbf55a2d60a847},
  abstract={Background: Algebra has traditionally been seen as a site of inequity and a gatekeeper for marginalized learners within both national and international studies. However, considerable research has shown that taking an asset-based approach in mathematics teaching improves learning for marginalized students, including Māori and Pāsifika learners. Purpose: The purpose of this research was to explore how a culturally located task set with a familiar Pāsifika context and within an algebraic frame was mathematized to support rich understandings. The focus was placed on how Pāsifika and Māori students drew on multimodal forms of communication (gesture, drawings, language, and symbols) to collectively make sense of a culturally located growing pattern task designed to develop functional reasoning. Research Design: The 12 student participants in the study were aged between 11 and 12 years and were of Māori and Pāsifika nations ethnicity, as were their teacher and the two researchers. The lesson reported on in this study was one of eight lessons and was representative of all lessons as the teacher and students drew on a variety of multimodal means of communication to construct functional reasoning. The design drew on both qualitative case study and design research. Data collection tools included interviews, video-recorded classroom observations, field notes, and photographs of work samples. The research design and analysis were informed through use of the Ula model as appropriate to all participants’ Pāsifika ethnicity. Similarly, storying was used, given that it is a traditional form of sharing used by indigenous peoples. Findings: Clearly evident in the results was how use of a culturally located task within a known context and the teacher’s asset-based approach supported students to engage in a challenging algebraic task. Multimodal forms of representation as reasoning and communication tools supported them to access more sophisticated forms of algebraic understandings as they began to generalize recursive, covarying, and correspondence relationships in increasingly sophisticated ways. Use of body language and gesturing was central to their communication, as was use of their first language and natural language. Conclusions/Recommendations: To change the gatekeeping role of algebra, teachers need to take a strength-based perspective and draw on what students bring to school as valued knowledge. For all students to access powerful ways of reasoning algebraically, the key role of many multimodal forms of communication needs to be recognized and affirmed.}
}

@article{zhang2022taskcompass,
  title={Task Compass: Scaling Multi-task Pre-training with Task Prefix},
  author={Zhuosheng Zhang and Shuo Wang and Yichong Xu and Yuwei Fang and W. Yu and Yang Liu and H. Zhao and Chenguang Zhu and Michael Zeng},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.06277},
  url={https://www.semanticscholar.org/paper/0979695b5d74016e97ab8f306f632114e98bd6d9},
  abstract={Leveraging task-aware annotated data as supervised signals to assist with self-supervised learning on large-scale unlabeled data has become a new trend in pre-training language models. Existing studies show that multi-task learning with large-scale supervised tasks suffers from negative effects across tasks. To tackle the challenge, we propose a task prefix guided multi-task pre-training framework to explore the relationships among tasks. We conduct extensive experiments on 40 datasets, which show that our model can not only serve as the strong foundation backbone for a wide range of tasks but also be feasible as a probing tool for analyzing task relationships. The task relationships reflected by the prefixes align transfer learning performance between tasks. They also suggest directions for data augmentation with complementary tasks, which help our model achieve human-parity results on commonsense reasoning leaderboards. Code is available at https://github.com/cooelf/CompassMTL},
  keywords={arxiv:2210.06277}
}

@article{zhou2022teachingalgorithmic,
  title={Teaching Algorithmic Reasoning via In-context Learning},
  author={Hattie Zhou and Azade Nova and H. Larochelle and Aaron C. Courville and Behnam Neyshabur and Hanie Sedghi},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.09066},
  url={https://www.semanticscholar.org/paper/4d17732d90440682b0500f4e209c6cc4fac20e0e},
  abstract={Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.},
  keywords={arxiv:2211.09066}
}

@article{magister2022teachingsmall,
  title={Teaching Small Language Models to Reason},
  author={Lucie Charlotte Magister and Jonathan Mallinson and Jakub Adamek and Eric Malmi and A. Severyn},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.08410},
  url={https://www.semanticscholar.org/paper/126a4776ff8315fd506766cb8f3c722cf746ad9e},
  abstract={Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11\% to 21.99\% and 18.42\% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.},
  keywords={arxiv:2212.08410}
}

@article{fogarty2022simplerules,
  title={Ten simple rules for principled simulation modelling},
  author={L. Fogarty and Madeleine Ammar and Thomas Holding and Adam Powell and A. Kandler},
  year={2022},
  booktitle={PLoS Comput. Biol.},
  doi={10.1371/journal.pcbi.1009917},
  url={https://www.semanticscholar.org/paper/4c3d680f2431316ea290de3b6376ed9da6d66c5f},
  abstract={ed counterpart (Fig 2 in [13]). However, it is an updated version of this second map that is given to millions of tourists travelling across London every year. The inclusion of only important information, abstraction, and omission of extraneous geographical detail has made it one of the most famously useful graphics in history. Models of complex systems should be the same—as detailed as needed but no more detailed than that. So, if you shouldn’t model the system you know in complete detail, what should you model instead? This depends to a great extent on the type of model that you have decided to make (Rule 1). Assuming that some mechanistic insight is the aim, based on knowledge of the system and precise formulation of the research question, a good strategy is to identify putative interactions or mechanisms that could underlie the behaviour or phenomenon under investigation and model those mechanisms. In other words, it is sensible to propose an engine that could be driving the phenomenon of interest and then make characterising and investigating that engine the central focus of the model. Anything in the system irrelevant to that engine is irrelevant to the model. Part of the art of modelling is identifying putative mechanisms and ensuring that the logical thread between mechanism and behaviour is unbroken. Rule 3: Be rigorous, and allow the time to be rigorous Perhaps this is an obvious rule—cience and rigour usually go hand in hand. Successful modelling requires considerable attention to mathematical and computational detail. It is vital that all concepts and all code used in a simulation model are clear to the author and rigorously tested—from their construction to their downstream implications (see Rule 6). Using convenient mathematical or statistical concepts with little or no knowledge about their inner workings will likely lead to their misuse and maybe to serious fundamental errors. Similarly, it can be dangerous to use, for example, software packages or useful-looking chunks of code as untested black boxes (but see Rule 6). The assumptions of the methods implemented in the code, and the implementation itself, must be clear and consistent with the assumptions and aims of your model. Alongside the mathematical and technical details of each part of the model, careful consideration should be given to the order in which these parts or processes are executed and what effect that ordering (or scheduling) has on the output. All of this is to say that modelling decisions need careful thought and, as with empirical data analysis, designing a model that describes your system in adequate (but not excessive) detail, and deriving robust inferences from it, can take significant time. This is time which needs to be budgeted for in any simulation modelling project from the outset. Rule 4: Have a plan for analysis The aim of this rule is to help you to avoid a surprisingly common and deeply frustrating situation: creating a monster simulation with so many moving parts that it just can’t be analysed meaningfully—a situation where you could say to yourself something like “I’ve finished coding this simulation, but how on earth can I analyse so many interactions and so many outputs?” Reaching this point probably means that something has gone wrong in the implementation of Rules 1 or 2—or that the model has been coded without a good plan for its analysis. So, perhaps you know the question you want to answer, you know or have guessed the relevant aspects of the system, but you didn’t plan how the model can answer the question. In general, it is important to have an idea of the analysis steps necessary to answer aspects of the central question. These plans are flexible and develop throughout a project, of course, but an initial plan is crucial. Your exact plan for analysis will depend on your aim and the kind of model you have constructed. For example, if you are interested in unravelling mechanistic relationships, you might use simulated data to explore the workings of the model and to analyse exactly how PLOS COMPUTATIONAL BIOLOGY PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1009917 March 31, 2022 3 / 8 different parameters or processes affect the outcome. To do this successfully, the number of parameters and processes should be manageable (see Rule 2), and parameter explorations should be principled (see Rule 8). Or, if you are interested in fitting your model to observed data, you may need to consider exactly how the model output maps to existing and future data and identify appropriate statistical techniques to robustly test between models and estimate parameters. Here, it will be important to be aware of the limitations and assumptions of the statistical techniques in addition to those of the model itself (see Rule 3). Rule 5: Be kind to your future self (and your readers), and minimise opportunities for errors You will need to revisit your code, sometimes many months after the analysis has been done and the paper has been written. This can either be a difficult exercise for your future self or a simple matter of glancing at your structured, documented, and organised code. Of course, it is best to aim for the latter. This means that code needs to be well structured and well commented so that you are able to easily reconstruct what has been done. To this end, this rule advises a number of good coding practices that are particularly important for simulation models, which can become large and unwieldy projects very quickly. All of these practices will benefit the readers of your model as much as they benefit your future self. First, the names of variables, functions, and parameters should be intuitive, consistent, and immediately intelligible. This makes the code more readable and minimises unnecessary comments. Similarly, adopting consistent formatting conventions (such as indentation and other whitespace), much like punctuation in a sentence, improves code readability. Commenting should be done with both your future self and your future readers in mind. For larger-scale multiresearcher and multiyear projects, more detailed documentation should be created and developed in tandem with the code. As a project grows, it becomes difficult to keep track of the interactions and dependencies between the different parts of the code. This complexity can be made more manageable by giving your code a well-defined and modular structure, meaning that sections of code should be divided into logically related units and the interfaces between these units of code should be well defined. Different programming languages offer different tools to achieve this, and the following advice applies equally to all of them. We’ll use the concept of functions as an example because this is the most fundamental and most commonly used. The aim here is to identify units of self-contained logic and create functions that encapsulate this logic. Functions should have a single clear purpose so, if you find you’re writing a function which performs multiple conceptual actions, consider breaking it up into 2 or more functions. Encapsulating logic in this way creates higher-order building blocks that can be reused and avoid code duplication. This reduces the opportunity for errors, not only because there will be less code to begin with, but also because each logical unit can be easily tested individually. In fact, it is good practice to test functions as you write them—this helps to avoid bugs in “low-level” functions that often manifest as difficult to diagnose problems downstream. The ubiquitously useful coding mantra “fail early and fail loudly” posits that it is better to raise an error as soon as it could feasibly be detected. So, for example, it is best to have functions check that their inputs are valid rather than allowing them to blindly compute and pass nonsensical output downstream. Despite taking these steps to manage the complexity of your project, you will write bugs. Everyone does and it is unavoidable. But, if you have written modular, well-structured code, it will be much easier to test the code, find the bugs, and squash them. PLOS COMPUTATIONAL BIOLOGY PLOS Computational Biology | https://doi.org/10.1371/journal.pcbi.1009917 March 31, 2022 4 / 8 With a similar aim in mind, it is now common practice and explicitly required by some journals that model code is published alongside the paper it generates. This has a number of important benefits. It facilitates open and reproducible science. It also benefits you as a modeller. Bugs and coding issues can be identified by reviewers and readers who go through the code. This can be an important source of error checking. Finally, but equally importantly, you will need to test that the (now hopefully bug free) code actually does what it is supposed to in terms of the model’s dynamics. The full code, and each constituent part, should be put through its paces using simple test cases for which you have either good intuitions or solid analytical expectations to which the model outcomes should conform. Rule 6: Write practical code Practical code doesn’t have to be perfect, fully optimised, or even beautifully written, but it is fit for purpose, well tested, and clearly documented so that others (and you) can have confidence in it (see Rule 5). Practical coding also means using your coding time efficiently. Investing some time in learning to use the debugging tools available to your programming language will quickly pay for itself in time saved identifying and fixing bugs more quickly. There are a great many well-tested and already optimised scientific libraries/packages, and there is usually nothing to be gained by reimplementing functionality that already exists. Making use of good quality libraries will speed up development and reduce your opportunities to write bugs (see Rule 5), although it is critical to ensure that the methods used are appropriate for your context (see the note abou}
}

@article{bertolini2022testinglarge,
  title={Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment},
  author={Lorenzo Bertolini and Julie Weeds and David Weir},
  year={2022},
  booktitle={International Conference on Computational Linguistics},
  url={https://www.semanticscholar.org/paper/1606793daaa20d4a4a78e859c2fd6b4f7535680c}
}

@article{valentino2022textgraphs2022,
  title={TextGraphs 2022 Shared Task on Natural Language Premise Selection},
  author={Marco Valentino and Deborah Ferreira and Mokanarangan Thayaparan and André Freitas and Dmitry Ustalov},
  year={2022},
  booktitle={Workshop on Graph-based Methods for Natural Language Processing},
  url={https://www.semanticscholar.org/paper/43845c54af0246baab17d572953e671487e1dcd1}
}

@article{jimnezparra2022effectsactive,
  title={The Effects of the ACTIVE VALUES Program on Psychosocial Aspects and Executive Functions},
  author={José Francisco Jiménez-Parra and Noelia Belando-Pedreño and A. Valero-valenzuela},
  year={2022},
  journal={International Journal of Environmental Research and Public Health},
  doi={10.3390/ijerph20010595},
  url={https://www.semanticscholar.org/paper/41e975b03d0f10c24340702e03191483ace7f27e},
  abstract={The main objective of this study was to implement an educational program named ACTIVE VALUES and to analyse the psychosocial and cognitive effects of its application. It is a quasi-experimental repeated measures research with a non-randomised experimental group (EG) and a control group (CG). The sample consisted of 102 students in the 6th grade of primary school, aged between 11 and 13 years (M = 11.59; SD = 0.60), and 4 teachers aged between 27 and 52 years (M = 38.5). The intervention program lasted 4 months, in which the EG implemented a teaching methodology based on the incorporation of classroom-based physical activity (CB-PA) in the structure of the Teaching for Personal and Social Responsibility (TPSR) model to develop personal and social values in students, as well as to reduce children’s sedentary behaviour in the classroom in different educational areas (e.g., mathematics, Spanish language, social sciences and natural sciences), while the CG used a conventional methodology based on direct instruction. The main results found show significant improvements in intrinsic motivation variables (including intrinsic motivation for achievement, stimulating experiences and knowledge), self-determination index, autonomy, relatedness, psychological mediators index, personal and social responsibility, teacher climate, intention to be physically active and executive functions in the EG, while amotivation values increased in the CG. In conclusion, interdisciplinary educational programs based on the combination of pedagogical models and active methodologies are postulated as methodological alternatives to achieve an integral and multilateral development of children and adolescents, as well as to improve the different learning domains of physical education, such as cognitive, social and motor. It is recommended that future research should consider longitudinal designs with mixed methods and follow-up data to assess learning retention, as well as larger samples and the measurement of a greater number of executive functions (e.g., inhibitory control and attention).}
}

@article{ye2022unreliabilityexplanations,
  title={The Unreliability of Explanations in Few-Shot In-Context Learning},
  author={Xi Ye and Greg Durrett},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2205.03401},
  url={https://www.semanticscholar.org/paper/e811e771f5950d86eafe50655c0d1e5b571e19b6}
}

@article{aurell2022mightyforce,
  title={The mighty force: statistical inference and high-dimensional statistics},
  author={E. Aurell and Jean Barbier and A. Decelle and R. Mulet},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2205.00750},
  url={https://www.semanticscholar.org/paper/603977d9f5a6222ff926044f3089af679c985496},
  abstract={Inference is an English noun formed on the verb infer, from the Latin inferre, meaning to carry (fero) in or into (in-) something. That originally concrete meaning can still be felt in the portal quote of this chapter. In modern non-technical use the meaning of inference is more abstract, and rendered either as “A conclusion reached on the basis of evidence and reasoning” or as “The process of reaching such a conclusion” [1]. In scientific language these translate into characteristics of a phenomenon that are not observed directly, but which are arrived at (inferred) from observations with the help of mathematical and/or statistical methods, and those methods themselves. We will discuss three prominent examples of inference in both senses of modern usage, and how they naturally open up new perspectives and possibilities. Statistical physics is played out on the terrain between individual items and distributions over properties of items. The canonical example is the Langevin equation which describes the motion of a Brownian particle interacting with a thermal reservoir, and the Fokker-Planck equation which describes the evolution of the distribution of possible positions and velocities of the particle. In inference the goal can analogously be to reach one conclusion or retrieve one object, or to establish characteristics of a distribution over objects. The second kind of inference is also called statistical inference. We will here discuss inference in this sense. In the “big-data era”, statistical inference of different kinds is routinely performed based on data sets containing millions or even billions of samples, which themselves may live in spaces of tremendously large dimensionality. In this realm, classical statistical wisdom and tools fail: new mathematics and algorithms able to tackle the phenomena emerging in this regime are necessary. In the very same way, phase transitions were understood to emerge from the complexity (i.e. high-dimensionality) of physical systems more than a century ago, whose understanding required to develop statistical mechanics. It turns out that this is more than an analogy as the theory and methods to perform high-dimensional inference are directly connected to statistical mechanics as we will see in this chapter (and others in the book [2]). High-dimensional inference itself is part of a broader statistical theory of complex systems, referred to as high-dimensional statistics, a very active research field at the crossroads of (statistical) physics, computer science, information theory and machine learning, and which is the powerhouse of modern information processing systems.},
  keywords={arxiv:2205.00750}
}

@article{boers2022theoreticalpaleoclimatic,
  title={Theoretical and paleoclimatic evidence for abrupt transitions in the Earth system},
  author={N. Boers and M. Ghil and T. Stocker},
  year={2022},
  booktitle={Environmental Research Letters},
  doi={10.1088/1748-9326/ac8944},
  url={https://www.semanticscholar.org/paper/8ffbb35aa8fae7df82b860417e8ab13d807413fa},
  abstract={Specific components of the Earth system may abruptly change their state in response to gradual changes in forcing. This possibility has attracted great scientific interest in recent years, and has been recognized as one of the greatest threats associated with anthropogenic climate change. Examples of such components, called tipping elements, include the Atlantic Meridional Overturning Circulation, the polar ice sheets, the Amazon rainforest, as well as the tropical monsoon systems. The mathematical language to describe abrupt climatic transitions is mainly based on the theory of nonlinear dynamical systems and, in particular, on their bifurcations. Applications of this theory to nonautonomous and stochastically forced systems are a very active field of climate research. The empirical evidence that abrupt transitions have indeed occurred in the past stems exclusively from paleoclimate proxy records. In this review, we explain the basic theory needed to describe critical transitions, summarize the proxy evidence for past abrupt climate transitions in different parts of the Earth system, and examine some candidates for future abrupt transitions in response to ongoing anthropogenic forcing. Predicting such transitions remains difficult and is subject to large uncertainties. Substantial improvements in our understanding of the nonlinear mechanisms underlying abrupt transitions of Earth system components are needed. We argue that such an improved understanding requires combining insights from (a) paleoclimatic records; (b) simulations using a hierarchy of models, from conceptual to comprehensive ones; and (c) time series analysis of recent observation-based data that encode the dynamics of the present-day Earth system components that are potentially prone to tipping.}
}

@article{ozturkler2022thinksumprobabilistic,
  title={ThinkSum: Probabilistic reasoning over sets using large language models},
  author={Batu Mehmet Ozturkler and Nikolay Malkin and Zhen Wang and N. Jojic},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.01293},
  url={https://www.semanticscholar.org/paper/370cea8b4220917f45a69358c0303df71f5063c7},
  abstract={Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think – retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum – probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.},
  keywords={arxiv:2210.01293}
}

@article{ferreira2022integerencoding,
  title={To be or not to be an Integer? Encoding Variables for Mathematical Text},
  author={Deborah Ferreira and Mokanarangan Thayaparan and Marco Valentino and Julia Rozanova and André Freitas},
  year={2022},
  booktitle={Findings},
  doi={10.18653/v1/2022.findings-acl.76},
  url={https://www.semanticscholar.org/paper/970d9ffcff27e1e1ce3f45734d59e9f99bef23cc},
  abstract={The application of Natural Language Inference (NLI) methods over large textual corpora can facilitate scientific discovery, reducing the gap between current research and the available large-scale scientific knowledge. However, contemporary NLI models are still limited in interpreting mathematical knowledge written in Natural Language, even though mathematics is an integral part of scientific argumentation for many disciplines. One of the fundamental requirements towards mathematical language understanding, is the creation of models able to meaningfully represent variables. This problem is particularly challenging since the meaning of a variable should be assigned exclusively from its defining type, i.e., the representation of a variable should come from its context. Recent research has formalised the variable typing task, a benchmark for the understanding of abstract mathematical types and variables in a sentence. In this work, we propose VarSlot, a Variable Slot-based approach, which not only delivers state-of-the-art results in the task of variable typing, but is also able to create context-based representations for variables.}
}

@article{chauhan2022towardgenerating,
  title={Toward Generating System Architecture and Formal Functional Description in the Architecture Analysis \& Design Language (AADL) With Structured Natural Language},
  author={Anshumaan Chauhan and Parth Ganeriwala and Chiradeep Sen and S. Bhattacharyya},
  year={2022},
  booktitle={Conference on Computability in Europe},
  doi={10.1115/detc2022-90002},
  url={https://www.semanticscholar.org/paper/765631db254ee8c5df7f8db074ec35a513270728},
  abstract={
 Model based engineering has enabled automated analytical reasoning early in the design phase. As a result, inconsistencies and design errors can be captured early in the development lifecycle. But there is still a gap in the natural language-based specifications and its actual implementation. This is because the formal method-based tools utilize mathematical principles and theories of computation that require specific skills, thus reducing the usability of model-based engineering. Natural language is the most widely used method to represent specifications. So, it is intuitive to utilize natural language-based representation to generate system and formal annotations such that it will enable automated architectural analysis with much wider acceptance leading to a much broader impact. In our paper we focus on designing the above-mentioned approach that integrates representation of the specifications in a subset of English language which can then be used to generate system architecture in Architecture Analysis and Design Language along with the generation of functional specifications. We illustrate our approach by validating it with use cases from the aerospace and electromechanical domains.}
}

@article{hochreiter2022towardbroad,
  title={Toward a broad AI},
  author={Sepp Hochreiter},
  year={2022},
  booktitle={Communications of the ACM},
  doi={10.1145/3512715},
  url={https://www.semanticscholar.org/paper/4510b0c02aca42b7c7ed9931d1c265301c5f5ebd},
  abstract={I M A G E B Y C H R I S T O P H B U R G S T E D T particular aims at a new level of AI—a “broad AI”—with considerably enhanced and broader capabilities for skill acquisition and problem solving.3 We contrast “broad AI” to “narrow AI,” which are the AI systems currently applied. A broad AI considerably surpasses a narrow AI in the following essential properties: when learned models must quickly adapt to new situations, for new customers, new products, new processes, new workflows, or new sensory inputs. With the advent of large corpora of unlabeled data in vision and language, selfsupervised learning based on contrastive learning became very popular. Either views of images are contrasted with views of other images or text descriptions of images are contrasted with text descriptions of other images. Contrastive Language-Image Pretraining (CLIP)10 yielded very impressive results at zero-shot transfer learning. The CLIP model has the potential to become one of the most important foundation models.2 A model with high zero-shot transfer learning knowledge transfer and interaction, adaptability and robustness, abstraction and advanced reasoning, and efficiency (as illustrated in the accompanying figure). A broad AI is a sophisticated and adaptive system, which successfully performs any cognitive task by virtue of its sensory perception, previous experience, and learned skills. To improve adaptability and robustness, a broad AI utilizes few-shot learning, self-supervised learning with contrastive learning, and processes sensory inputs using context and memory. Few-shot learning trains models with a small amount of data using prior knowledge or previous experience. Few-shot learning has a plethora of real-world applications, for example, D E S P I T E B I G}
}

@article{zhou2022towardsidentifying,
  title={Towards Identifying Social Bias in Dialog Systems: Frame, Datasets, and Benchmarks},
  author={Jingyan Zhou and Jiawen Deng and Fei Mi and Yitong Li and Yasheng Wang and Minlie Huang and Xin Jiang and Qun Liu and Helen M. Meng},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/6626dadc76d1af9d19fc4c2a4fa3a4cf414e62e0}
}

@misc{prof2022towardsmultihop,
  title={Towards Multi-Hop Open-Domain Question Answering by Dense Retrieval},
  author={Zhao Meng Prof and Dr. Roger Wattenhofer},
  year={2022},
  url={https://www.semanticscholar.org/paper/bc5def224c0afbd5d32b8542e43dfd774d650202}
}

@article{huang2022towardsreasoning,
  title={Towards Reasoning in Large Language Models: A Survey},
  author={Jie Huang and K. Chang},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10403},
  url={https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45},
  abstract={Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
  keywords={arxiv:2212.10403}
}

@article{shah2022towardssimplifying,
  title={Towards Simplifying and Formalizing UML Class Diagram Generalization/Specialization Relationship with Mathematical Set Theory},
  author={Kruti Shah and Emanuel S. Grant},
  year={2022},
  booktitle={International Conference on Information System and Data Mining},
  doi={10.1145/3546157.3546171},
  url={https://www.semanticscholar.org/paper/5b2b3ec307e1075e9218919e7e635d5ac37120c3},
  abstract={The Unified Modeling Language (UML) is considered the de facto standard for object-oriented software model development. This makes it appropriate to be used in academia courses at both the graduate and undergraduate levels of education. Some challenges to using the UML is academia are its large number of model concepts and the imprecise semantic of some of these concepts. These challenges are daunting for students who are being introduced to the UML. One approach that can be taken in teaching UML towards addressing these concerns is to limit the number of UML concepts taught and recognize that students may not be able to develop correct UML system models. This approach leads to research work that develop a limited set of UML model concepts that are fewer in number and have more precise semantics. In this paper, we present a new approach to resolve an aspect of this problem by simplifying the generalization/specialization semantics of the class diagram through the application of mathematical formality to usage of these class diagram concepts. This research work derives a core set of concepts suitable for graduate and undergraduate comprehension of UML modeling and defines more precise semantics for those modeling concepts. The applicable mathematical principles applied in this work are from the domains of set theory and predicate logic. This approach is particularly relevant for the pedagogy of software engineering and the development of software systems that require a high level of reliability.}
}

@article{wang2022towardsunderstanding,
  title={Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},
  author={Boshi Wang and Sewon Min and Xiang Deng and Jiaming Shen and You Wu and Luke Zettlemoyer and Huan Sun},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10001},
  url={https://www.semanticscholar.org/paper/35922cd0d6b17e45320917338e9f98cb5c1a4f6f},
  abstract={Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations - prompting with invalid reasoning steps can achieve over 80-90\% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these findings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs’ capability to learn to reason in context.},
  keywords={arxiv:2212.10001}
}

@article{shah2022towardsverifying,
  title={Towards Verifying UML Class Diagram and Formalizing Generalization/Specialization Relationship with Mathematical Set Theory},
  author={Kruti Shah and Emanuel S. Grant},
  year={2022},
  journal={Journal of Software},
  doi={10.17706/jsw.17.6.292-303},
  url={https://www.semanticscholar.org/paper/d635df2ad885d6a273a32ae5f4d970b1c7f2f8e3},
  abstract={The Unified Modeling Language (UML) is considered the de facto standard for object-oriented software model development. This makes it appropriate to be used in academia courses at both the graduate and undergraduate levels of education. Some challenges to using the UML is academia are its large number of model concepts and the imprecise semantic of some of these concepts. These challenges are daunting for students who are being introduced to the UML. One approach that can be taken in teaching UML towards addressing these concerns is to limit the number of UML concepts taught and recognize that students may not be able to develop correct UML system models. This approach leads to research work that develop a limited set of UML model concepts that are fewer in number and have more precise semantics. In this paper, we present a new approach to resolve an aspect of this problem by simplifying the generalization/specialization semantics of the class diagram through the application of mathematical formality to usage of these class diagram concepts. Along with that, we discuss the progress of research in the area of verification of UML class models. This research work derives a core set of concepts suitable for graduate and undergraduate comprehension of UML modeling and defines more precise semantics for those modeling concepts. The applicable mathematical principles applied in this work are from the domains of set theory and predicate logic. This approach is particularly relevant for the pedagogy of software engineering and the development of software systems that require a high level of reliability.}
}

@article{agrawal2022towardsmathematics,
  title={Towards a Mathematics Formalisation Assistant using Large Language Models},
  author={Ayush Agrawal and Siddhartha Gadgil and Navin Goyal and Ashvni Narayanan and Anand Tadipatri},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.07524},
  url={https://www.semanticscholar.org/paper/b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce},
  abstract={Mathematics formalisation is the task of writing mathematics (i.e., definitions, theorem statements, proofs) in natural language, as found in books and papers, into a formal language that can then be checked for correctness by a program. It is a thriving activity today, however formalisation remains cumbersome. In this paper, we explore the abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover. We find that with careful inputdependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75\% accuracy for 120 theorem statements. For proofs quantitative analysis is infeasible and we undertake a detailed case study. We choose a diverse set of 13 theorems at undergrad level with proofs that fit in two-three paragraphs. We show that with a new prompting strategy Codex can formalise these proofs in natural language with at least one out of twelve Codex completion being easy to repair into a complete proof. This is surprising as essentially no aligned data exists for formalised mathematics, particularly for proofs. These results suggest that large language models are a promising avenue towards fully or partially automating formalisation.},
  keywords={arxiv:2211.07524}
}

@article{tay2022transcendingscaling,
  title={Transcending Scaling Laws with 0.1\% Extra Compute},
  author={Yi Tay and Jason Wei and Hyung Won Chung and Vinh Q. Tran and David R. So and Siamak Shakeri and Xavier García and H. Zheng and J. Rao and A. Chowdhery and Denny Zhou and Donald Metzler and Slav Petrov and N. Houlsby and Quoc V. Le and Mostafa Dehghani},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.11399},
  url={https://www.semanticscholar.org/paper/1bb6d5761903c7ac978188ae36e2648905e95dc5},
  abstract={Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving \$\textbackslash\{\}sim\$4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.},
  keywords={arxiv:2210.11399}
}

@article{mirzaee2022transferlearning,
  title={Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning},
  author={Roshanak Mirzaee and Parisa Kordjamshidi},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.16952},
  url={https://www.semanticscholar.org/paper/e3cd9f01f87a601b274b4ef6513a84c8cde03214},
  abstract={Recent research shows synthetic data as a source of supervision helps pretrained language models (PLM) transfer learning to new target tasks/domains. However, this idea is less explored for spatial language. We provide two new data resources on multiple spatial language processing tasks. The first dataset is synthesized for transfer learning on spatial question answering (SQA) and spatial role labeling (SpRL). Compared to previous SQA datasets, we include a larger variety of spatial relation types and spatial expressions. Our data generation process is easily extendable with new spatial expression lexicons. The second one is a real-world SQA dataset with human-generated questions built on an existing corpus with SPRL annotations. This dataset can be used to evaluate spatial language processing models in realistic situations. We show pretraining with automatically generated data significantly improves the SOTA results on several SQA and SPRL benchmarks, particularly when the training data in the target domain is small.},
  keywords={arxiv:2210.16952}
}

@article{wu2022triplefactretriever,
  title={Triple-Fact Retriever: An explainable reasoning retrieval model for multi-hop QA problem},
  author={Cheng Wu and Enrui Hu and Ke Zhan and Lan Luo and Xinyu Zhang and Hao Jiang and Qirui Wang and Zhao Cao and Fan Yu and Lei Chen},
  year={2022},
  booktitle={IEEE International Conference on Data Engineering},
  doi={10.1109/icde53745.2022.00095},
  url={https://www.semanticscholar.org/paper/dd77bff42060109c9b2e3e7d87cc9342309c3952},
  abstract={Nowadays, multi-hop question answer (QA) problem is challenging and not well solved in the QA community. The dominant bottleneck of the multi-hop QA problem is the need for a reasoning retriever to fetch a document path from an open-domain corpus (e.g., Wikipedia). A reasoning retriever aims to collect an evidence document from large corpora at one hop retrieval and aggregate the evidence for subsequent hop retrieval, which yields a document path after multi-hop retrieval. There exist two challenges, (1) to fetch the evidence document in an efficient and explainable way at one hop retrieval and (2) to update the question information by aggregating the evidence from the retrieved document after each hop retrieval. To address these two challenges, we propose a triple-fact-based retrieval model to effectively retrieve a related document path in an explainable way for each question. We extract a structured representation from the unstructured document and utilize the knowledge of pre-trained language model (PLM) to do the semantic-level matching between the question and document. We evaluate the proposed Triple-fact Retriever model on the recently proposed open-domain multi-hop QA dataset, HotpotQA, and a cross-document multi-step Reading Comprehension dataset, Wikihop. The results11The source code is available on our website: https://github.com/Rebaccamin/triple\_retriever. demonstrate that the Triple-fact retriever outperforms the existing baseline retrieval works.}
}

@article{del2022truedetective,
  title={True Detective: A Challenging Benchmark for Deep Abductive Reasoning in Foundation Models},
  author={Maksym Del and Mark Fishel},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.10114},
  url={https://www.semanticscholar.org/paper/b1054e448186822bfe9445bb6f4533d157e3da5e}
}

@article{del2022truedetective,
  title={True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4},
  author={Maksym Del and Mark Fishel},
  year={2022},
  booktitle={STARSEM},
  doi={10.18653/v1/2023.starsem-1.28},
  url={https://www.semanticscholar.org/paper/256ef1f8d0ea2982cc50d3e85e5f1b4920f037fe},
  abstract={Large language models (LLMs) have demonstrated solid zero-shot reasoning capabilities, which is reflected in their performance on the current test tasks. This calls for a more challenging benchmark requiring highly advanced reasoning ability to be solved. In this paper, we introduce such a benchmark, consisting of 191 long-form (1200 words on average) mystery narratives constructed as detective puzzles. Puzzles are sourced from the “5 Minute Mystery” platform and include a multiple-choice question for evaluation. Only 47\% of humans solve a puzzle successfully on average, while the best human solvers achieve over 80\% success rate. We show that GPT-3 models barely outperform random on this benchmark (with 28\% accuracy) while state-of-the-art GPT-4 solves only 38\% of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area. Our work introduces a challenging benchmark for future studies on reasoning in language models and contributes to a better understanding of the limits of LLMs’ abilities.},
  keywords={arxiv:2212.10114}
}

@article{roy2022tutorialneurosymbolic,
  title={Tutorial: Neuro-symbolic AI for Mental Healthcare},
  author={Kaushik Roy and Usha Lokala and Manas Gaur and Amit P. Sheth},
  year={2022},
  booktitle={International Conference on AI-ML-Systems},
  doi={10.1145/3564121.3564817},
  url={https://www.semanticscholar.org/paper/2545393a22c688acb3a6c12098479eb6a04ab7d3},
  abstract={Artificial Intelligence (AI) systems for mental healthcare (MHCare) have been ever-growing after realizing the importance of early interventions for patients with chronic mental health (MH) conditions. Social media (SocMedia) emerged as the go-to platform for supporting patients seeking MHCare. The creation of peer-support groups without social stigma has resulted in patients transitioning from clinical settings to SocMedia supported interactions for quick help. Researchers started exploring SocMedia content in search of cues that showcase correlation or causation between different MH conditions to design better interventional strategies. User-level Classification-based AI systems were designed to leverage diverse SocMedia data from various MH conditions, to predict MH conditions. Subsequently, researchers created classification schemes to measure the severity of each MH condition. Such ad-hoc schemes, engineered features, and models not only require a large amount of data but fail to allow clinically acceptable and explainable reasoning over the outcomes. To improve Neural-AI for MHCare, infusion of clinical symbolic knowledge that clinicans use in decision making is required. An impactful use case of Neural-AI systems in MH is conversational systems. These systems require coordination between classification and generation to facilitate humanistic conversation in conversational agents (CA). Current CAs with deep language models lack factual correctness, medical relevance, and safety in their generations, which intertwine with unexplainable statistical classification techniques. This lecture-style tutorial will demonstrate our investigations into Neuro-symbolic methods of infusing clinical knowledge to improve the outcomes of Neural-AI systems to improve interventions for MHCare:(a) We will discuss the use of diverse clinical knowledge in creating specialized datasets to train Neural-AI systems effectively. (b) Patients with cardiovascular disease express MH symptoms differently based on gender differences. We will show that knowledge-infused Neural-AI systems can identify gender-specific MH symptoms in such patients. (c) We will describe strategies for infusing clinical process knowledge as heuristics and constraints to improve language models in generating relevant questions and responses.}
}

@article{levy2022understandingnatural,
  title={Understanding Natural Language in Context},
  author={Avichai Levy and E. Karpas},
  year={2022},
  booktitle={International Conference on Automated Planning and Scheduling},
  doi={10.48550/arXiv.2205.12691},
  url={https://www.semanticscholar.org/paper/b13cb83f9a9e8cea529c527f76ab2b50ab879bbc},
  abstract={Recent years have seen an increasing number of applications that have a natural language interface, either in the form of chatbots or via personal assistants such as Alexa (Amazon), Google Assistant, Siri (Apple), and Cortana (Microsoft). To use these applications, a basic dialog between the assistant and the human is required. While this kind of dialog exists today mainly within static robots that do not make any movement in the household space, the challenge of reasoning about the information conveyed by the environment increases significantly when dealing with robots that can move and manipulate objects in our home environment.
In this paper, we focus on cognitive robots, which have some knowledge-based models of the world and operate by reasoning and planning with this model. Thus, when the robot and the human communicate, there is already some formalism they can use -- the robot’s knowledge representation formalism. In this paper we describe an approach for translating natural language directives into the robot's formalism, allowing much more complicated household tasks to be completed. We do so by combining off-the-shelf SoTA large language models, planning tools, and the robot knowledge of the state of the world and of its own model. This results in much more accurate interpretation of directives in natural language.},
  keywords={arxiv:2205.12691}
}

@article{chen2022unigeounifying,
  title={UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression},
  author={Jiaqi Chen and Tong Li and Jinghui Qin and Pan Lu and Liang Lin and Chongyu Chen and Xiaodan Liang},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2212.02746},
  url={https://www.semanticscholar.org/paper/72fce949725b20428e5f56247fef5c6bd1ce6154},
  abstract={Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning capability on multiple math tasks. However, in essence, these two tasks have similar problem representations and overlapped math knowledge which can improve the understanding and reasoning ability of a deep model on both two tasks. Therefore, we construct a large-scale Unified Geometry problem benchmark, UniGeo, which contains 4,998 calculation problems and 9,543 proving problems. Each proving problem is annotated with a multi-step proof with reasons and mathematical expressions. The proof can be easily reformulated as a proving sequence that shares the same formats with the annotated program sequence for calculation problems. Naturally, we also present a unified multi-task Geometric Transformer framework, Geoformer, to tackle calculation and proving problems simultaneously in the form of sequence generation, which finally shows the reasoning ability can be improved on both two tasks by unifying formulation. Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that aims to predict the mathematical expressions in the problem solution, thus improving the Geoformer model. Experiments on the UniGeo demonstrate that our proposed Geoformer obtains state-of-the-art performance by outperforming task-specific model NGS with over 5.6\% and 3.2\% accuracies on calculation and proving problems, respectively.},
  keywords={arxiv:2212.02746}
}

@article{jiang2022unikgqaunified,
  title={UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph},
  author={Jinhao Jiang and Kun Zhou and Wayne Xin Zhao and Ji-rong Wen},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2212.00959},
  url={https://www.semanticscholar.org/paper/2d01da2c9ece0969d6ec56d22f78caf57050fc03},
  abstract={Multi-hop Question Answering over Knowledge Graph\~{}(KGQA) aims to find the answer entities that are multiple hops away from the topic entities mentioned in a natural language question on a large-scale Knowledge Graph (KG). To cope with the vast search space, existing work usually adopts a two-stage approach: it first retrieves a relatively small subgraph related to the question and then performs the reasoning on the subgraph to find the answer entities accurately. Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence. In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning. For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model\~{}(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs. For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies. Compared with previous studies, our approach is more unified, tightly relating the retrieval and reasoning stages. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our method on the multi-hop KGQA task. Our codes and data are publicly available at\~{}\textbackslash\{\}url\{https://github.com/RUCAIBox/UniKGQA\}.},
  keywords={arxiv:2212.00959}
}

@article{mokhov2022unitedmonoids,
  title={United Monoids: Finding Simplicial Sets and Labelled Algebraic Graphs in Trees},
  author={A. Mokhov},
  year={2022},
  booktitle={The Art, Science, and Engineering of Programming},
  doi={10.22152/programming-journal.org/2022/6/12},
  url={https://www.semanticscholar.org/paper/9af58dfedb0306ff4b5071d3bc64e8c969e5cabb},
  abstract={Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the “algebraic graphs” approach (2017). It uses an algebraic data type to represent graphs and has attracted users, including from industry, due to its emphasis on equational reasoning and making a common class of bugs impossible by eliminating internal invariants. The previous formulation of algebraic graphs did not support edge labels, which was a serious practical limitation. In this paper, we redesign the main algebraic data type and remove this limitation. We follow a fairly standard approach of parameterising a data structure with a semiring of edge labels. The new formulation is both more general and simpler: the two operations for composing graphs used in the previous work can now be obtained from a single operation by fixing the semiring parameter to zero and one, respectively. By instantiating the new data type with different semirings, and working out laws for interpreting the resulting expression trees, we discover an unusual algebraic structure, which we call “united monoids”, that is, a pair of monoids whose unit elements coincide. We believe that it is worth studying united monoids in their full generality, going beyond the graphs which prompted their discovery. To that end, we characterise united monoids with a minimal set of axioms, prove a few basic theorems, and discuss several notable examples. We validate the presented approach by implementing it in the open-source algebraic-graphs library. Our theoretical contributions are supported by proofs that are included in the paper and have also been machinechecked in Agda. By extending algebraic graphs with support for edge labels, we make them suitable for a much larger class of possible applications. By studying united monoids, we provide a theoretical foundation for further research in this area. ACM CCS 2012 Mathematics of computing→ Graph theory;},
  keywords={arxiv:2202.09230}
}

@article{sahu2022unpackinglarge,
  title={Unpacking Large Language Models with Conceptual Consistency},
  author={Pritish Sahu and Michael Cogswell and Yunye Gong and Ajay Divakaran},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2209.15093},
  url={https://www.semanticscholar.org/paper/4f4a80148cb8f328aeaee68b34f9797cfb5ea150},
  abstract={If a Large Language Model (LLM) answers"yes"to the question"Are mountains tall?"then does it know what a mountain is? Can you rely on it responding correctly or incorrectly to other questions about mountains? The success of Large Language Models (LLMs) indicates they are increasingly able to answer queries like these accurately, but that ability does not necessarily imply a general understanding of concepts relevant to the anchor query. We propose conceptual consistency to measure a LLM's understanding of relevant concepts. This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are. To compute it we extract background knowledge by traversing paths between concepts in a knowledge base and then try to predict the model's response to the anchor query from the background knowledge. We investigate the performance of current LLMs in a commonsense reasoning setting using the CSQA dataset and the ConceptNet knowledge base. While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency. Our analysis also shows significant variation in conceptual consistency across different kinds of relations, concepts, and prompts. This serves as a step toward building models that humans can apply a theory of mind to, and thus interact with intuitively.},
  keywords={arxiv:2209.15093}
}

@article{binz2022usingcognitive,
  title={Using cognitive psychology to understand GPT-3},
  author={Marcel Binz and Eric Schulz},
  year={2022},
  booktitle={Proceedings of the National Academy of Sciences of the United States of America},
  doi={10.1073/pnas.2218523120},
  url={https://www.semanticscholar.org/paper/fa3609e00f9f422a309c621a35394c4a38f88687},
  abstract={Significance Language models are trained to predict the next word for a given text. Recently, it has been shown that scaling up these models causes them to not only generate language but also to solve challenging reasoning problems. The present article lets a large language model (GPT-3) do experiments from the cognitive psychology literature. We find that GPT-3 can solve many of these tasks reasonably well, despite being only taught to predict future word occurrences on a vast amount of text from the Internet and books. We additionally utilize analysis tools from the cognitive psychology literature to demystify how GPT-3 solves different tasks and use the thereby acquired insights to make recommendations for how to improve future model iterations.},
  keywords={arxiv:2206.14576}
}

@article{zhang2022utilizingbackground,
  title={Utilizing Background Knowledge for Robust Reasoning over Traffic Situations},
  author={Jiarui Zhang and Filip Ilievski and Aravinda Kollaa and Jonathan M Francis and Kaixin Ma and A. Oltramari},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.07798},
  url={https://www.semanticscholar.org/paper/fb591c46afb5f1f2fc380a5a5a55f9cf9e485edb},
  abstract={Understanding novel situations in the traffic domain requires an intricate combination of domain-specific and causal commonsense knowledge. Prior work has provided sufficient perception-based modalities for traffic monitoring, in this paper, we focus on a complementary research aspect of Intelligent Transportation: traffic understanding. We scope our study to text-based methods and datasets given the abundant commonsense knowledge that can be extracted using language models from large corpus and knowledge graphs. We adopt three knowledge-driven approaches for zero-shot QA over traffic situations, based on prior natural language inference methods, commonsense models with knowledge graph self-supervision, and dense retriever-based models. We constructed two text-based multiple-choice question answering sets: BDD-QA for evaluating causal reasoning in the traffic domain and HDT-QA for measuring the possession of domain knowledge akin to human driving license tests. Among the methods, Unified-QA reaches the best performance on the BDD-QA dataset with the adaptation of multiple formats of question answers. Language models trained with inference information and commonsense knowledge are also good at predicting the cause and effect in the traffic domain but perform badly at answering human-driving QA sets. For such sets, DPR+Unified-QA performs the best due to its efficient knowledge extraction.},
  keywords={arxiv:2212.07798}
}

@article{chochlakis2022vaultaugmenting,
  title={VAuLT: Augmenting the Vision-and-Language Transformer with the Propagation of Deep Language Representations},
  author={Georgios Chochlakis and Tejas Srinivasan and Jesse Thomason and Shrikanth S. Narayanan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2208.09021},
  url={https://www.semanticscholar.org/paper/26bf29a039c09211c854da0419cd17238fce579b}
}

@article{li2022vgstoremultimodal,
  title={VGStore: A Multimodal Extension to SPARQL for Querying RDF Scene Graph},
  author={Yanzeng Li and Zilong Zheng and Wenjuan Han and Lei Zou},
  year={2022},
  booktitle={International Workshop on the Semantic Web},
  doi={10.48550/arXiv.2209.02981},
  url={https://www.semanticscholar.org/paper/73bedd5ea4b4342e1a7389055a051e9f3e2e85af},
  abstract={Semantic Web technology has successfully facilitated many RDF models with rich data representation methods. It also has the potential ability to represent and store multimodal knowledge bases such as multimodal scene graphs. However, most existing query languages, especially SPARQL, barely explore the implicit multimodal relationships like semantic similarity, spatial relations, etc. We first explored this issue by organizing a large-scale scene graph dataset, namely Visual Genome, in the RDF graph database. Based on the proposed RDF-stored multimodal scene graph, we extended SPARQL queries to answer questions containing relational reasoning about color, spatial, etc. Further demo (i.e., VGStore) shows the effectiveness of customized queries and displaying multimodal data.},
  keywords={arxiv:2209.02981}
}

@article{gupta2022vquadvideo,
  title={VQuAD: Video Question Answering Diagnostic Dataset},
  author={Vikrant Gupta and Badri N. Patro and Hemant Parihar and Vinay P. Namboodiri},
  year={2022},
  booktitle={2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)},
  doi={10.1109/WACVW54805.2022.00034},
  url={https://www.semanticscholar.org/paper/234619e9eb3706c0c794e1d31ce75cb07dde39a2},
  abstract={In this paper, we investigate the task of Video based Question Answering. We provide a diagnostic dataset that can be used to evaluate the extent of reasoning abilities of various methods for solving this task. Previous datasets proposed for this task do not have this ability. Our dataset is large scale (around 1.3 million questions jointly for train and test) and evaluates both the spatial and temporal properties and the relationship between various objects for these properties. We evaluate state of the art language model (BERT) as a baseline to understand the extent of correlation based on language features alone. Other existing networks are then used to combine video features along with language features for solving this task. Unfortunately, we observe that the currently prevalent systems do not perform significantly better than the language baseline. We hypothesise that this is due to our efforts in ensuring that no obvious biases exist in this dataset and the dataset is balanced. To make progress, the learning techniques needs to obtain an ability to reason, going beyond basic correlation of biases. This is an interesting and significant challenge provided through our work. We release our dataset and source code for our baseline modules in the following webpage https://delta-lab-iitk.github.io/vquad/.}
}

@article{emslander2022valueaddedscores,
  title={Value-added scores show limited stability over time in primary school},
  author={Valentin Emslander and J. Levy and Ronny Scherer and Antoine Fischbach},
  year={2022},
  booktitle={PLoS ONE},
  doi={10.1371/journal.pone.0279255},
  url={https://www.semanticscholar.org/paper/9197aac8ee8d6f26a8998a93ec4e037cf8986313},
  abstract={Value-added (VA) models are used for accountability purposes and quantify the value a teacher or a school adds to their students’ achievement. If VA scores lack stability over time and vary across outcome domains (e.g., mathematics and language learning), their use for high-stakes decision making is in question and could have detrimental real-life implications: teachers could lose their jobs, or a school might receive less funding. However, school-level stability over time and variation across domains have rarely been studied together. In the present study, we examined the stability of VA scores over time for mathematics and language learning, drawing on representative, large-scale, and longitudinal data from two cohorts of standardized achievement tests in Luxembourg (N = 7,016 students in 151 schools). We found that only 34–38\% of the schools showed stable VA scores over time with moderate rank correlations of VA scores from 2017 to 2019 of r = .34 for mathematics and r = .37 for language learning. Although they showed insufficient stability over time for high-stakes decision making, school VA scores could be employed to identify teaching or school practices that are genuinely effective—especially in heterogeneous student populations.}
}

@article{muravev2022verificationradar,
  title={Verification of radar precipitation nowcasting of significant areas using the generalized Pareto distribution. Part 1: Elements of theory and methods for estimating parameters},
  author={A. Muravev and A. Bundel and D. Kiktev and A. Smirnov},
  year={2022},
  booktitle={Hydrometeorological research and forecasting},
  doi={10.37162/2618-9631-2022-3-6-41},
  url={https://www.semanticscholar.org/paper/4a341336ed762e7d8c33f465305367d0ccea07bb},
  abstract={The assessments of nowcasting of large precipitation areas accumulated in the last few years at the Hydrometeorological Research Center of the Russian Federation are presented in two parts complemented by a discussion of methodological problems in the first part and application problems in the second part of the paper. The division is largely due to the sharp distinction between the theoretical modeling of extremes with a relatively free choice of assumptions and the statistical analysis of the distribution "tails" in rapidly "impoverishing" samples. The contrast between these parts is exacerbated by the responsibility we attribute to the statistical inference relating to extreme and, as a rule, dangerous events. The first part deals with the description of two classical models of the extreme value theory for independent one-dimensional random variables ("block maxima") and for threshold exceedances in stationary time series ("peaks over threshold"). The article explores problems arising from violation of the theoretical results and carries a brief overview of the methods of addressing such problems when extremes are modeled using real data, including those from the field of meteorology. Special attention is given to the distributions with "heavy" tails. Methods and formulas for estimating important characteristics, including the parameters of limiting distributions, are discussed that are borrowed from the references in the documentation of computational mathematical packages of the R language repository. Keywords: precipitation nowcasting, extreme value theory, statistical modeling of extremes, heavy distribution tails, mathematical packages for fitting extreme value distributions}
}

@article{vanhattum2022verifyingdynamic,
  title={Verifying Dynamic Trait Objects in Rust},
  author={Alexa VanHattum and Daniel Schwartz-Narbonne and Nathan Chong and Adrian Sampson},
  year={2022},
  booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP)},
  doi={10.1145/3510457.3513031},
  url={https://www.semanticscholar.org/paper/1ff44db7ee219174273efba0e4a42bf24c1807cf},
  abstract={Rust has risen in prominence as a systems programming language in large part due to its focus on reliability. The language's advanced type system and borrow checker eliminate certain classes of memory safety violations. But for critical pieces of code, teams need assurance beyond what the type checker alone can provide. Verification tools for Rust can check other properties, from memory faults in unsafe Rust code to user-defined correctness assertions. This paper particularly focuses on the challenges in reasoning about Rust's dynamic trait objects, a feature that provides dynamic dispatch for function abstractions. While the explicit dyn keyword that denotes dynamic dispatch is used in 37\% of the 500 most-downloaded Rust libraries (crates), dynamic dispatch is implicitly linked into 70\%. To our knowledge, our open-source Kani Rust Verifier is the first symbolic modeling checking tool for Rust that can verify correctness while supporting the breadth of dynamic trait objects, including dynamically dispatched closures. We show how our system uses semantic trait information from Rust's Mid-level Intermediate Representation (an advantage over targeting a language-agnostic level such as LLVM) to improve verification performance by 5\%–15× for examples from open-source virtualization software. Finally, we share an open-source suite of verification test cases for dynamic trait objects.}
}

@article{meiliasari2022videolearning,
  title={Video as learning tools for middle school mathematics: A case of probability using realistic mathematics education},
  author={Meiliasari Meiliasari and D. A. Wijayanti and Dear Noer},
  year={2022},
  booktitle={AIP Conference Proceedings},
  doi={10.1063/5.0105119},
  url={https://www.semanticscholar.org/paper/a4fa100a2467f7694ca4cb2c24a9db282a7fc00b},
  abstract={The Coronavirus Disease (Covid-19) pandemic in Indonesia has an impact for various sectors, one of which is education. Ministry of Education and Culture provides a policy for school to carry out the learning process from home through online learning or distance learning as an effort to prevent the spread of Covid-19. Videos can be used to support teachers and students in learning mathematics during distance learning. This study aims to develop videos as learning tools of probability in middle school with realistic mathematics education approach. This methodology used is a Research and Development with Brog and Gall model five stages, namely product analysis, product design, product evaluation and revision, small group field test and revision, large group field test and final product. The participants of this study were 26 students of grade 8th. The prototype videos was evaluated by content and language experts obtained a score 83\%, and 84\% by media experts with criteria very feasible. The student questionnaire average for this video product increase from 75\% in small group became 82\% in the large group. This shows that the probability learning video for middle school with realistic mathematics education approach is feasible to be used as mathematics media learning. © 2022 American Institute of Physics Inc.. All rights reserved.}
}

@article{liang2022visualabductive,
  title={Visual Abductive Reasoning},
  author={Chen Liang and Wenguan Wang and Tianfei Zhou and Yi Yang},
  year={2022},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52688.2022.01512},
  url={https://www.semanticscholar.org/paper/838a2297b94f7bad96c4f8370a5f58487f194f44},
  abstract={Abductive reasoning seeks the likeliest possible explanation for partial observations. Although abduction is frequently employed in human daily reasoning, it is rarely explored in computer vision literature. In this paper, we propose a new task and dataset, Visual Abductive Reasoning (VAR), for examining abductive reasoning ability of machine intelligence in everyday visual situations. Given an incomplete set of visual events, AI systems are required to not only describe what is observed, but also infer the hypothesis that can best explain the visual premise. Based on our large-scale VAR dataset, we devise a strong baseline model, REASONER (causal-and-cascaded reasoning Transformer). First, to capture the causal structure of the observations, a contextualized directional position embedding strategy is adopted in the encoder, that yields discriminative represen-tations for the premise and hypothesis. Then, multiple de-coders are cascaded to generate and progressively refine the premise and hypothesis sentences. The prediction scores of the sentences are used to guide cross-sentence information flow in the cascaded reasoning procedure. Our VAR bench-marking results show that REASONER surpasses many famous video-language models, while still being far behind human performance. This work is expected to foster future efforts in the reasoning-beyond-observation paradigm.},
  keywords={arxiv:2203.14040}
}

@misc{chen2022visualknowledge,
  title={Visual Knowledge Learning},
  author={Xinlei Chen},
  year={2022},
  doi={10.1184/r1/21637757.v1},
  url={https://www.semanticscholar.org/paper/b76225abf969f2ccbae2d5a96c8ba1e690547ec3}
}

@article{gupta2022visualprogramming,
  title={Visual Programming: Compositional visual reasoning without training},
  author={Tanmay Gupta and Aniruddha Kembhavi},
  year={2022},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52729.2023.01436},
  url={https://www.semanticscholar.org/paper/af1c871282ec122869d03f5420ef5d9143358a91},
  abstract={We present Visprog, a neuro-symbolic approach to solving complex and compositional visual tasks given natural language instructions. Visprog avoids the need for any task-specific training. Instead, it uses the incontext learning ability of large language models to generate python-like modular programs, which are then executed to get both the solution and a comprehensive and interpretable rationale. Each line of the generated program may invoke one of several off-the-shelf computer vision models, image processing subroutines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program. We demonstrate the flexibility of VIsPROG on 4 diverse tasks - compositional visual question answering, zero-shot reasoning on image pairs, factual knowledge object tagging, and language-guided image editing. We believe neuro-symbolic approaches like Visprog are an exciting avenue to easily and effectively expand the scope of AI systems to serve the long tail of complex tasks that people may wish to perform.},
  keywords={arxiv:2211.11559}
}

@article{liu2022visualspatial,
  title={Visual Spatial Reasoning},
  author={Fangyu Liu and Guy Edward Toh Emerson and Nigel Collier},
  year={2022},
  journal={Transactions of the Association for Computational Linguistics},
  doi={10.1162/tacl_a_00566},
  url={https://www.semanticscholar.org/paper/354b48677e314ef2f47512c5a81723cfd17dd05d},
  abstract={Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (e.g., under, in front of, facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: The human ceiling is above 95\%, while state-of-the-art models only achieve around 70\%. We observe that VLMs’ by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.1},
  keywords={arxiv:2205.00363}
}

@article{wang2022visuallyaugmentedlanguage,
  title={Visually-Augmented Language Modeling},
  author={Weizhi Wang and Li Dong and Hao Cheng and Haoyu Song and Xiaodong Liu and Xifeng Yan and Jianfeng Gao and Furu Wei},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2205.10178},
  url={https://www.semanticscholar.org/paper/6d02cc3e66330fc170a5bde44be7b358149b9c0a},
  abstract={Human language is grounded on multimodal knowledge including visual knowledge like colors, sizes, and shapes. However, current large-scale pre-trained language models rely on text-only self-supervised training with massive text data, which precludes them from utilizing relevant visual information when necessary. To address this, we propose a novel pre-training framework, named VaLM, to Visually-augment text tokens with retrieved relevant images for Language Modeling. Specifically, VaLM builds on a novel latent text-image alignment method via an image retrieval module to fetch corresponding images given a textual context. With the visually-augmented context, VaLM uses a visual knowledge fusion layer to enable multimodal grounded language modeling by attending to both text context and visual knowledge in images. We evaluate VaLM on various visual knowledge-intensive commonsense reasoning tasks, which require visual information to excel. The experimental results illustrate that VaLM outperforms all strong language-only and vision-language baselines with substantial gains in reasoning object commonsense including color, size, and shape. Our code is available at https://github.com/Victorwz/VaLM.},
  keywords={arxiv:2205.10178}
}

@article{liu2022wanliworker,
  title={WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation},
  author={Alisa Liu and Swabha Swayamdipta and Noah A. Smith and Yejin Choi},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2022.findings-emnlp.508},
  url={https://www.semanticscholar.org/paper/5e8d3c2dc0fc53949794fc00600e25558c4a2441},
  abstract={A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11\% on HANS and 9\% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process.},
  keywords={arxiv:2201.05955}
}

@article{tessler2022warmwinter,
  title={Warm (for Winter): Inferring Comparison Classes in Communication},
  author={Michael Henry Tessler and Noah D. Goodman},
  year={2022},
  booktitle={Cognitive Sciences},
  doi={10.1111/cogs.13095},
  url={https://www.semanticscholar.org/paper/d4c4fb5b6902999f5de9c66ca5b4192f0ef9b6f0},
  abstract={Abstract The meanings of natural language utterances depend heavily on context. Yet, what counts as context is often only implicit in conversation. The utterance it's warm outside signals that the temperature outside is relatively high, but the temperature could be high relative to a number of different comparison classes: other days of the year, other weeks, other seasons, etc. Theories of context sensitivity in language agree that the comparison class is a crucial variable for understanding meaning, but little is known about how a listener decides upon the comparison class. Using the case study of gradable adjectives (e.g., warm), we extend a Bayesian model of pragmatic inference to reason flexibly about the comparison class and test its qualitative predictions in a large‐scale free‐production experiment. We find that human listeners infer the comparison class by reasoning about the kinds of observations that would be remarkable enough for a speaker to mention, given the speaker and listener's shared knowledge of the world. Further, we quantitatively synthesize the model and data using Bayesian data analysis, which reveals that usage frequency and a preference for basic‐level categories are two main factors in comparison class inference. This work presents new data and reveals the mechanisms by which human listeners recover the relevant aspects of context when understanding language.}
}

@article{wu2022weaklysupervised,
  title={Weakly Supervised Formula Learner for Solving Mathematical Problems},
  author={Yuxuan Wu and Hideki Nakayama},
  year={2022},
  booktitle={International Conference on Computational Linguistics},
  url={https://www.semanticscholar.org/paper/6b212f99aa29c464207a40c26c529ec552185092}
}

@article{gal2022welcomevague,
  title={Welcome to the era of vague news: a study of the demands of statistical and mathematical products in the COVID-19 pandemic media},
  author={I. Gal and V. Geiger},
  year={2022},
  booktitle={Educational Studies in Mathematics},
  doi={10.1007/s10649-022-10151-7},
  url={https://www.semanticscholar.org/paper/31b44679789c98fffb43ffef879a3957dc3de4bf},
  abstract={In this article, we report on a typology of the demands of statistical and mathematical products (StaMPs) embedded in media items related to the COVID-19 (coronavirus) pandemic. The typology emerged from a content analysis of a large purposive sample of diverse media items selected from digital news sources based in four countries. The findings encompass nine categories of StaMPs: (1) descriptive quantitative information, (2) models, predictions, causality and risk, (3) representations and displays, (4) data quality and strength of evidence, (5) demographics and comparative thinking, (6) heterogeneity and contextual factors, (7) literacy and language demands, (8) multiple information sources, and (9) critical demands. We illustrate these categories via selected media items, substantiate them through relevant research literature, and point to categories that encompass new or enhanced types of demands. Our findings offer insights into the rich set of capabilities that citizens (including both young people and adults) must possess in order to engage these mass media demands, critically analyze statistical and mathematical information in the media, evaluate the meaning and credibility of news reports, understand public policies, and make evidenced-informed judgments. Our conclusions point to the need to revise current curricular frameworks and conceptual models (e.g., regarding statistical and probability literacy, adult numeracy), to better incorporate notions such as blended knowledge, vagueness, risk, strength of evidence, and criticality. Furthermore, more attention is needed to the literacy and language demands of media items involving statistical and mathematical information. Implications for further research and educational practice are discussed.}
}

@article{luo2022whatgoes,
  title={What Goes beyond Multi-modal Fusion in One-stage Referring Expression Comprehension: An Empirical Study},
  author={Gen Luo and Yiyi Zhou and Jiamu Sun and Shubin Huang and Xiaoshuai Sun and Qixiang Ye and Yongjian Wu and Rongrong Ji},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2204.07913},
  url={https://www.semanticscholar.org/paper/73879fcedf89ccf9db86ee55f57db223593c9871}
}

@article{deng2022whatllms,
  title={What do LLMs Know about Financial Markets? A Case Study on Reddit Market Sentiment Analysis},
  author={Xiang Deng and Vasilisa Bashlovkina and Feng Han and Simon Baumgartner and Michael Bendersky},
  year={2022},
  booktitle={The Web Conference},
  doi={10.1145/3543873.3587324},
  url={https://www.semanticscholar.org/paper/52136f813243ac3de8e277906112a41590a376d4},
  abstract={Market sentiment analysis on social media content requires knowledge of both financial markets and social media jargon, which makes it a challenging task for human raters. The resulting lack of high-quality labeled data stands in the way of conventional supervised learning methods. Instead, we approach this problem using semi-supervised learning with a large language model (LLM). Our pipeline generates weak financial sentiment labels for Reddit posts with an LLM and then uses that data to train a small model that can be served in production. We find that prompting the LLM to produce Chain-of-Thought summaries and forcing it through several reasoning paths helps generate more stable and accurate labels, while using a regression loss further improves distillation quality. With only a handful of prompts, the final model performs on par with existing supervised models. Though production applications of our model are limited by ethical considerations, the model’s competitive performance points to the great potential of using LLMs for tasks that otherwise require skill-intensive annotation.},
  keywords={arxiv:2212.11311}
}

@article{madasu2022whatlarge,
  title={What do Large Language Models Learn beyond Language?},
  author={Avinash Madasu and Shashank Srivastava},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.12302},
  url={https://www.semanticscholar.org/paper/5efab88c0cdb11c795fa8f44a5d31b40e2a1c261},
  abstract={Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful `inductive biases' for non-linguistic reasoning. On a set of 19 diverse non-linguistic tasks involving quantitative computations, recognizing regular expressions and reasoning over strings. We find that pretrained models significantly outperform comparable non-pretrained neural models. This remains true also in experiments with training non-pretrained models with fewer parameters to account for model regularization effects. We further explore the effect of text domain on LMs by pretraining models from text from different domains and provenances. Our experiments surprisingly reveal that the positive effects of pre-training persist even when pretraining on multi-lingual text or computer code, and even for text generated from synthetic languages. Our findings suggest a hitherto unexplored deep connection between pre-training and inductive learning abilities of language models.},
  keywords={arxiv:2210.12302}
}

@article{jin2022whenmake,
  title={When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment},
  author={Zhijing Jin and Sydney Levine and Fernando Gonzalez and Ojasv Kamal and Maarten Sap and Mrinmaya Sachan and Rada Mihalcea and J. Tenenbaum and B. Scholkopf},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2210.01478},
  url={https://www.semanticscholar.org/paper/1c1ca2392155ddf30408a442e6b504b5d60d4f2a},
  abstract={AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the flexibility of the human moral mind -- the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule-breaking -- inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MORALCOT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. MORALCOT outperforms seven existing LLMs by 6.2\% F1, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using RBQA. Our data is open-sourced at https://huggingface.co/datasets/feradauto/MoralExceptQA and code at https://github.com/feradauto/MoralCoT},
  keywords={arxiv:2210.01478}
}

@article{ho2022wikiwhyanswering,
  title={WikiWhy: Answering and Explaining Cause-and-Effect Questions},
  author={Matthew Ho and Aditya Sharma and Justin Chang and Michael Stephen Saxon and Sharon Levy and Yujie Lu and William Yang Wang},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.12152},
  url={https://www.semanticscholar.org/paper/8345d757e9127eff382d5285fef99312eaf283cd},
  abstract={As large language models (LLMs) grow larger and more sophisticated, assessing their"reasoning"capabilities in natural language grows more challenging. Recent question answering (QA) benchmarks that attempt to assess reasoning are often limited by a narrow scope of covered situations and subject matters. We introduce WikiWhy, a QA dataset built around a novel auxiliary task: explaining why an answer is true in natural language. WikiWhy contains over 9,000"why"question-answer-rationale triples, grounded on Wikipedia facts across a diverse set of topics. Each rationale is a set of supporting statements connecting the question to the answer. WikiWhy serves as a benchmark for the reasoning capabilities of LLMs because it demands rigorous explicit rationales for each answer to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized. GPT-3 baselines achieve only 38.7\% human-evaluated correctness in the end-to-end answer\&explain condition, leaving significant room for future improvements.},
  keywords={arxiv:2210.12152}
}

@misc{unknown2022workingwith,
  title={Working With Dynamic Crop Models Second Edition Methods Tools And Examples For Agriculture And Environment},
  year={2022},
  url={https://www.semanticscholar.org/paper/7037dc4215b25742c6935bc72a8c80b4b4c7b89e}
}

@article{yang2022zeroshotlearners,
  title={Zero-Shot Learners for Natural Language Understanding via a Unified Multiple Choice Perspective},
  author={Ping Yang and Junjie Wang and Ruyi Gan and Xinyu Zhu and Lin Zhang and Ziwei Wu and Xinyu Gao and Jiaxing Zhang and Tetsuya Sakai},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.08590},
  url={https://www.semanticscholar.org/paper/8862ed012fe06a794fda3ceae3f471a0c2a40fbe},
  abstract={We propose a new paradigm for zero-shot learners that is format agnostic, i.e., it is compatible with any format and applicable to a list of language tasks, such as text classification, commonsense reasoning, coreference resolution, and sentiment analysis. Zero-shot learning aims to train a model on a given task such that it can address new learning tasks without any additional training. Our approach converts zero-shot learning into multiple-choice tasks, avoiding problems in commonly used large-scale generative models such as FLAN. It not only adds generalization ability to models but also significantly reduces the number of parameters. Our method shares the merits of efficient training and deployment. Our approach shows state-of-the-art performance on several benchmarks and produces satisfactory results on tasks such as natural language inference and text classification. Our model achieves this success with only 235M parameters, which is substantially smaller than state-of-the-art models with billions of parameters. The code and pre-trained models are available at https://github.com/IDEA-CCNL/Fengshenbang-LM/tree/main/fengshen/examples/unimc .},
  keywords={arxiv:2210.08590}
}

@article{kuo2022zeroshotprompting,
  title={Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning},
  author={Hui-Chi Kuo and Yun-Nung (Vivian) Chen},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.05901},
  url={https://www.semanticscholar.org/paper/2a4b6fdf4fd74429431a730c14d0087e00b2a4fa},
  abstract={Intelligent virtual assistants are currently designed to perform tasks or services explicitly mentioned by users, so multiple related domains or tasks need to be performed one by one through a long conversation with many explicit intents. Instead, human assistants are capable of reasoning (multiple) implicit intents based on user utterances via commonsense knowledge, reducing complex interactions and improving practicality. Therefore, this paper proposes a framework of multi-domain dialogue systems, which can automatically infer implicit intents based on user utterances and then perform zero-shot prompting using a large pre-trained language model to trigger suitable single task-oriented bots. The proposed framework is demonstrated effective to realize implicit intents and recommend associated bots in a zero-shot manner.},
  keywords={arxiv:2210.05901}
}

@article{galatolo2022zeroshotmathematical,
  title={Zero-shot Mathematical Problem Solving via Generative Pre-trained Transformers},
  author={F. Galatolo and M. Cimino and G. Vaglini},
  year={2022},
  booktitle={International Conference on Enterprise Information Systems},
  doi={10.5220/0011032400003179},
  url={https://www.semanticscholar.org/paper/8a4dce5735a101ff8f64c2b676afb8c24950a5d8},
  abstract={: Mathematics is an effective testbed for measuring the problem-solving ability of machine learning models. The current benchmark for deep learning-based solutions is grade school math problems: given a natural language description of a problem, the task is to analyse the problem, exploit heuristics generated from a very large set of solved examples, and then generate an answer. In this paper, a descendant of the third generation of Generative Pre-trained Transformer Networks (GPT-3) is used to develop a zero-shot learning approach, to solve this problem. The proposed approach shows that coding based problem-solving is more effective than the natural language reasoning based one. Specifically, the architectural solution is built upon OpenAI Codex, a descendant of GPT-3 for programming tasks, trained on public GitHub repositories, the world’s largest source code hosting service. Experimental results clearly show the potential of the approach: by exploiting the Python as programming language, proposed pipeline achieves the 18.63\% solve rate against the 6.82\% of GPT-3. Finally, by using a fine-tuned verifier, the correctness of the answer can be ranked at runtime, and then improved by generating a predefined number of trials. With this approach, for 10 trials and an ideal verifier, the proposed pipeline achieves 54.20\% solve rate.}
}

@misc{lefranc2022simulationpower,
  title={and simulation of power electronics system and simulation of electrical machines and electromagnetic},
  author={O. Lefranc and H. Schneider and C. Turpin and O. Rallières and Maël and Durand and Fernanda Vendrame and Nicolas Damay and H. Rabab and C. Forgez and Marie Sayegh and E. Monmasson and Brian Ospina Agudelo},
  year={2022},
  url={https://www.semanticscholar.org/paper/b6ceb6ee67bbf9e96b2dd5f3383cbd943d8c2d8c}
}

@article{awachat2022technicalreview,
  title={technical review on knowledge intensive NLP for pre-trained language development},
  author={Snehal Awachat and Shwetal Raipure and Kavita B. Kalambe},
  year={2022},
  journal={International Journal of Health Sciences},
  doi={10.53730/ijhs.v6ns2.7510},
  url={https://www.semanticscholar.org/paper/991a4434b906fce5381d701c6e794aa8a5cc3fc9},
  abstract={In today’s world where data plays the very important role, we have various sources of pre-data like online books, equation analysis, encyclopedia, common-sense reasoning, common-sense knowledge, etc. The increasing capacity of pre-training language models have given knowledge intensive natural language processing (KI-NLP) a new boost for advanced functionalities for establishing a stable, flexible, robust and efficient model. Though pre-trained models have its own drawback for handling the KI-NLP tasks, we are here to discuss the challenges faced in this field. A wide variety of pre-trained language models enhanced with external knowledge sources have been proposed and are in rapid development to meet this difficulty. In this research we have also discusses the challenges in NLP in terms of generation of knowledge intensive models. We have also defined some mathematical model and its framework dependability for pre-training different language in NLP. Finally, we have also discussed about variety of literature reviews based on we intend to describe the present progress of pre-trained language model-based knowledge-enhanced models (PLMKEs) in this work by deconstructing their three key elements: information sources, knowledge-intensive NLP tasks, and knowledge fusion methods.}
}

@article{popov2022utomaticconstructing,
  title={Аutomatic Constructing the NURBS Surface of a Ship's Hull},
  author={E. Popov and I. Shorkina},
  year={2022},
  booktitle={Proceedings of the 32nd International Conference on Computer Graphics and Vision},
  doi={10.20948/graphicon-2022-162-169},
  url={https://www.semanticscholar.org/paper/af5cbf15a1fad612cbd1bcebeac81866cc4bc352},
  abstract={The article describes an automated algorithm for the formation of a geometric model of the ship's hull surface according to an electronic theoretical drawing. An analysis of the traditional technology for the formation of ship surfaces based on the Russian CAD system (Sea Solution) is presented. The main disadvantages of traditional technology are determined. The automated algorithm for generating a geometric surface model is designed to reduce the complexity of manual design. The mathematical apparatus for constructing NURBS curves and surfaces serves as the instrumental basis of the algorithm, which provides a simple user control of forms. On the basis of a theoretical drawing generated by the CAD system, a cloud of approximation points was created to build the surface of the ship's hull from them. A general scheme of automatic construction of elements of the surface model in the initial approximation is given. To solve this problem, a software product based on HTML5, JavaScript and Three JS and Verb libraries was created. The use of the JavaScript language is due to its versatility, a large amount of information in the public domain. WebGL is used to implement 3D graphics rendering pipeline. Functions from the Three JS libraries made it possible to implement methods used in analytical geometry. The operability of the developed algorithm is demonstrated on the example of building a 3D model of the hull surface of a fishing trawler.}
}

@article{gupta2022johnyears,
  title={“John is 50 years old, can his son be 65?” Evaluating NLP Models’ Understanding of Feasibility},
  author={Himanshu Gupta and Neeraj Varshney and Swaroop Mishra and Kuntal Kumar Pal and Saurabh Arjun Sawant and Kevin Scaria and Siddharth Goyal and Chitta Baral},
  year={2022},
  booktitle={Conference of the European Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.07471},
  url={https://www.semanticscholar.org/paper/9a3f1a51ab2b3816655e4c4f58a022421ea7b34b},
  abstract={In current NLP research, large-scale language models and their abilities are widely being discussed. Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities. This work focuses on a simple commonsense ability, reasoning about when an action (or its effect) is feasible. To this end, we introduce FeasibilityQA, a question-answering dataset involving binary classification (BCQ) and multi-choice multi-correct questions (MCQ) that test understanding of feasibility. We show that even state-of-the-art models such as GPT-3, GPT-2, and T5 struggle to answer the feasibility questions correctly. Specifically, on (MCQ, BCQ) questions, GPT-3 achieves accuracy of just (19\%, 62\%) and (25\%, 64\%) in zero-shot and few-shot settings, respectively. We also evaluate models by providing relevant knowledge statements required to answer the question and find that the additional knowledge leads to a 7\% gain in performance, but the overall performance still remains low. These results make one wonder how much commonsense knowledge about action feasibility is encoded in state-of-the-art models and how well they can reason about it.},
  keywords={arxiv:2210.07471}
}

@misc{unknown2022workshopproofs,
  title={“Workshop on Proofs and Formalization in Logic, Mathematics and Philosophy”},
  year={2022},
  url={https://www.semanticscholar.org/paper/f7f2276f043ff090c911dc6239f27374fba1869a}
}