
You are an expert in writing PRISMA systematic literature reviews for academic publication.

CRITICAL REQUIREMENT: You MUST cite ALL papers (existing AND new) using LaTeX \cite{} commands throughout the regenerated paper.

=== ALL BIBTEX ENTRIES (INCLUDING NEW PAPERS) ===
@article{wei2022chainthought,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5},
  abstract={We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.}
}

@article{jiang2022draftsketch,
  title={Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
  author={Albert Qiaochu Jiang and S. Welleck and J. Zhou and Wenda Li and Jiacheng Liu and M. Jamnik and Timothée Lacroix and Yuhuai Wu and Guillaume Lample},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.12283},
  url={https://www.semanticscholar.org/paper/7de36d6b14aadc8cdb6ad1340b9ca64b15375bca},
  abstract={The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.}
}

@article{lu2022dynamicprompt,
  title={Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Kai-Wei Chang and Y. Wu and Song-Chun Zhu and Tanmay Rajpurohit and Peter Clark and A. Kalyan},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2209.14610},
  url={https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4},
  abstract={Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.}
}

@article{webb2022emergentanalogical,
  title={Emergent analogical reasoning in large language models},
  author={Taylor W. Webb and K. Holyoak and Hongjing Lu},
  year={2022},
  booktitle={Nature Human Behaviour},
  doi={10.1038/s41562-023-01659-w},
  url={https://www.semanticscholar.org/paper/3cbffab9d7981da6662d474aaa056dcbd3c1701e},
  abstract={The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of Generative Pre-trained Transformer (GPT)-3) on a range of analogical tasks, including a non-visual matrix reasoning task based on the rule structure of Raven’s Standard Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT-4 indicated even better performance. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems. Webb et al. show that new artificial intelligence language models, such as Generative Pre-trained Transformer 3, are able to solve analogical reasoning problems at a human-like level of performance.}
}

@article{creswell2022faithfulreasoning,
  title={Faithful Reasoning Using Large Language Models},
  author={Antonia Creswell and M. Shanahan},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/f0a0e8b6e84207f50db4d24cc4016e40601214ef},
  abstract={Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.}
}

@article{taylor2022galacticalarge,
  title={Galactica: A Large Language Model for Science},
  author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and A. Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1},
  abstract={Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.}
}

@article{saparov2022languagemodels,
  title={Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},
  author={Abulhair Saparov and He He},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.01240},
  url={https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a},
  abstract={Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.}
}

@article{ho2022largelanguage,
  title={Large Language Models Are Reasoning Teachers},
  author={Namgyu Ho and Laura Schmid and Se-Young Yun},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10071},
  url={https://www.semanticscholar.org/paper/a9e3e5dd7b30890553b7ae1c41f932e99192bb44},
  abstract={Recent works have shown that chain-of-thought (CoT) prompting can elicit language models to solve complex reasoning tasks, step-by-step. However, prompt-based CoT methods are dependent on very large models such as GPT-3 175B which are prohibitive to deploy at scale. In this paper, we use these large models as reasoning teachers to enable complex reasoning in smaller models and reduce model size requirements by several orders of magnitude. We propose Fine-tune-CoT, a method that generates reasoning samples from very large teacher models to fine-tune smaller models. We evaluate our method on a wide range of public models and complex tasks. We find that Fine-tune-CoT enables substantial reasoning capability in small models, far outperforming prompt-based baselines and even the teacher model in many tasks. Additionally, we extend our method by leveraging the teacher model’s ability to generate multiple distinct rationales for each original sample. Enriching the fine-tuning data with such diverse reasoning results in a substantial performance boost across datasets, even for very small models. We conduct ablations and sample studies to understand the emergence of reasoning capabilities of student models. Our code implementation and data are available at https://github.com/itsnamgyu/reasoning-teacher.}
}

@article{singhal2022largelanguage,
  title={Large language models encode clinical knowledge},
  author={K. Singhal and Shekoofeh Azizi and T. Tu and S. Mahdavi and Jason Wei and Hyung Won Chung and Nathan Scales and A. Tanwani and H. Cole-Lewis and S. Pfohl and P. Payne and Martin G. Seneviratne and P. Gamble and C. Kelly and Nathaneal Scharli and A. Chowdhery and P. A. Mansfield and B. A. Y. Arcas and D. Webster and Greg S. Corrado and Yossi Matias and K. Chou and Juraj Gottweis and Nenad Tomašev and Yun Liu and A. Rajkomar and J. Barral and Christopher Semturs and A. Karthikesalingam and Vivek Natarajan},
  year={2022},
  booktitle={Nature},
  doi={10.1038/s41586-023-06291-2},
  url={https://www.semanticscholar.org/paper/6052486bc9144dc1730c12bf35323af3792a1fd0},
  abstract={Med-PaLM, a state-of-the-art large language model for medicine, is introduced and evaluated across several medical question answering tasks, demonstrating the promise of these models in this domain. Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model^ 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM^ 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA^ 3 , MedMCQA^ 4 , PubMedQA^ 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics^ 6 ), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.}
}

@article{nam2022learningreason,
  title={Learning to Reason With Relational Abstractions},
  author={A. Nam and Mengye Ren and Chelsea Finn and James L. McClelland},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.02615},
  url={https://www.semanticscholar.org/paper/6d5555348f453bac901c5b57e8a4eeb3074b4071},
  abstract={Large language models have recently shown promising progress in mathematical reasoning when fine-tuned with human-generated sequences walking through a sequence of solution steps. However, the solution sequences are not formally structured and the resulting model-generated sequences may not reflect the kind of systematic reasoning we might expect an expert human to produce. In this paper, we study how to build stronger reasoning capability in language models using the idea of relational abstractions. We introduce new types of sequences that more explicitly provide an abstract characterization of the transitions through intermediate solution steps to the goal state. We find that models that are supplied with such sequences as prompts can solve tasks with a significantly higher accuracy, and models that are trained to produce such sequences solve problems better than those that are trained with previously used human-generated sequences and other baselines. Our work thus takes several steps toward elucidating and improving how language models perform on tasks requiring multi-step mathematical reasoning.}
}

@article{zhou2022leasttomostprompting,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Denny Zhou and Nathanael Scharli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and D. Schuurmans and O. Bousquet and Quoc Le and Ed H. Chi},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2205.10625},
  url={https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321},
  abstract={Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.}
}

@article{welleck2022naturalprovergrounded,
  title={NaturalProver: Grounded Mathematical Proof Generation with Language Models},
  author={S. Welleck and Jiacheng Liu and Ximing Lu and Hannaneh Hajishirzi and Yejin Choi},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2205.12910},
  url={https://www.semanticscholar.org/paper/d593b9b8d63426f0d6a795dd7f2294619bc03610},
  abstract={Theorem proving in natural mathematical language - the mixture of symbolic and natural language used by humans - plays a central role in mathematical advances and education, and tests aspects of reasoning that are core to intelligence. Yet it has remained underexplored with modern generative models. We study large-scale language models on two new generation tasks: suggesting the next step in a mathematical proof, and full proof generation. We develop NaturalProver, a language model that generates proofs by conditioning on background references (e.g. theorems and definitions that are either retrieved or human-provided), and optionally enforces their presence with constrained decoding. On theorems from the NaturalProofs benchmark, NaturalProver improves the quality of next-step suggestions and generated proofs over fine-tuned GPT-3, according to human evaluations from university-level mathematics students. NaturalProver is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40% of the time, which is to our knowledge the first demonstration of these capabilities using neural language models.}
}

@article{mishra2022numgluesuite,
  title={NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks},
  author={Swaroop Mishra and Arindam Mitra and Neeraj Varshney and Bhavdeep Singh Sachdeva and Peter Clark and Chitta Baral and A. Kalyan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2204.05660},
  url={https://www.semanticscholar.org/paper/39238a92de090c104936a4f78375b95600e42ce5},
  abstract={Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.}
}

@article{sharma2022overcomingbarriers,
  title={Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic},
  author={Mandar Sharma and N. Muralidhar and Naren Ramakrishnan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.02098},
  url={https://www.semanticscholar.org/paper/1ec4ae64ded99846bcd8a1a3b16fe2875f55c7c1},
  abstract={Through their transfer learning abilities, highly-parameterized large pre-trained language models have dominated the NLP landscape for a multitude of downstream language tasks. Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning. However, as we illustrate in this paper, building a general purpose language model that also happens to be proficient in mathematical reasoning is not as straight-forward as training it on a numeric dataset. In this work, we develop a novel framework that enables language models to be mathematically proficient while retaining their linguistic prowess. Specifically, we offer information-theoretic interventions to overcome the catastrophic forgetting of linguistic skills that occurs while injecting non-linguistic skills into language models.}
}

@article{yao2022reactsynergizing,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d},
  abstract={While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io}
}

@article{he2022rethinkingwith,
  title={Rethinking with Retrieval: Faithful Large Language Model Inference},
  author={Hangfeng He and Hongming Zhang and D. Roth},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2301.00303},
  url={https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1},
  abstract={Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.}
}

@article{creswell2022selectioninferenceexploiting,
  title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
  author={Antonia Creswell and M. Shanahan and I. Higgins},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd},
  abstract={Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.}
}

@article{valentino2022textgraphs2022,
  title={TextGraphs 2022 Shared Task on Natural Language Premise Selection},
  author={Marco Valentino and Deborah Ferreira and Mokanarangan Thayaparan and André Freitas and Dmitry Ustalov},
  year={2022},
  booktitle={Workshop on Graph-based Methods for Natural Language Processing},
  url={https://www.semanticscholar.org/paper/43845c54af0246baab17d572953e671487e1dcd1}
}

@article{chen2022unigeounifying,
  title={UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression},
  author={Jiaqi Chen and Tong Li and Jinghui Qin and Pan Lu and Liang Lin and Chongyu Chen and Xiaodan Liang},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2212.02746},
  url={https://www.semanticscholar.org/paper/72fce949725b20428e5f56247fef5c6bd1ce6154},
  abstract={Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning capability on multiple math tasks. However, in essence, these two tasks have similar problem representations and overlapped math knowledge which can improve the understanding and reasoning ability of a deep model on both two tasks. Therefore, we construct a large-scale Unified Geometry problem benchmark, UniGeo, which contains 4,998 calculation problems and 9,543 proving problems. Each proving problem is annotated with a multi-step proof with reasons and mathematical expressions. The proof can be easily reformulated as a proving sequence that shares the same formats with the annotated program sequence for calculation problems. Naturally, we also present a unified multi-task Geometric Transformer framework, Geoformer, to tackle calculation and proving problems simultaneously in the form of sequence generation, which finally shows the reasoning ability can be improved on both two tasks by unifying formulation. Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that aims to predict the mathematical expressions in the problem solution, thus improving the Geoformer model. Experiments on the UniGeo demonstrate that our proposed Geoformer obtains state-of-the-art performance by outperforming task-specific model NGS with over 5.6% and 3.2% accuracies on calculation and proving problems, respectively.}
}

=== END BIBTEX ENTRIES ===

=== NEW PAPERS BEING ADDED THIS ITERATION ===
@article{he2022rethinkingwith,
  title={Rethinking with Retrieval: Faithful Large Language Model Inference},
  author={Hangfeng He and Hongming Zhang and D. Roth},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2301.00303},
  url={https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1},
  abstract={Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.}
}

@article{creswell2022selectioninferenceexploiting,
  title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
  author={Antonia Creswell and M. Shanahan and I. Higgins},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd},
  abstract={Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.}
}

@article{valentino2022textgraphs2022,
  title={TextGraphs 2022 Shared Task on Natural Language Premise Selection},
  author={Marco Valentino and Deborah Ferreira and Mokanarangan Thayaparan and André Freitas and Dmitry Ustalov},
  year={2022},
  booktitle={Workshop on Graph-based Methods for Natural Language Processing},
  url={https://www.semanticscholar.org/paper/43845c54af0246baab17d572953e671487e1dcd1}
}

@article{chen2022unigeounifying,
  title={UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression},
  author={Jiaqi Chen and Tong Li and Jinghui Qin and Pan Lu and Liang Lin and Chongyu Chen and Xiaodan Liang},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2212.02746},
  url={https://www.semanticscholar.org/paper/72fce949725b20428e5f56247fef5c6bd1ce6154},
  abstract={Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning capability on multiple math tasks. However, in essence, these two tasks have similar problem representations and overlapped math knowledge which can improve the understanding and reasoning ability of a deep model on both two tasks. Therefore, we construct a large-scale Unified Geometry problem benchmark, UniGeo, which contains 4,998 calculation problems and 9,543 proving problems. Each proving problem is annotated with a multi-step proof with reasons and mathematical expressions. The proof can be easily reformulated as a proving sequence that shares the same formats with the annotated program sequence for calculation problems. Naturally, we also present a unified multi-task Geometric Transformer framework, Geoformer, to tackle calculation and proving problems simultaneously in the form of sequence generation, which finally shows the reasoning ability can be improved on both two tasks by unifying formulation. Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that aims to predict the mathematical expressions in the problem solution, thus improving the Geoformer model. Experiments on the UniGeo demonstrate that our proposed Geoformer obtains state-of-the-art performance by outperforming task-specific model NGS with over 5.6% and 3.2% accuracies on calculation and proving problems, respectively.}
}

=== END NEW PAPERS ===

PREVIOUS DRAFT OF THE PAPER:
=== ABSTRACT ===
This systematic literature review provides a comprehensive overview of the recent advancements in utilizing large language models (LLMs) for mathematical reasoning. The study systematically searched major academic databases for relevant literature, employing specific keywords and inclusion/exclusion criteria. The analysis synthesized findings from numerous studies, highlighting the emergent capabilities of LLMs in various mathematical domains, from arithmetic to complex theorem proving. Key findings indicate that while LLMs demonstrate significant progress, particularly with specialized prompting techniques like Chain-of-Thought, challenges remain in areas such as proof planning, robustness, and faithful multi-step reasoning. The review identifies prominent research themes, including the development of novel prompting strategies, the use of LLMs as reasoning teachers, and their application in specialized scientific domains. This review aims to consolidate the current state of research, identify existing gaps, and suggest promising avenues for future exploration in developing more robust and capable AI systems for mathematical reasoning.

=== INTRODUCTION ===
\section{Introduction}

The field of artificial intelligence has witnessed unprecedented advancements in recent years, largely driven by the development and scaling of large language models (LLMs). These models, trained on vast amounts of text data, have demonstrated remarkable capabilities across a wide spectrum of natural language processing tasks, including text generation, translation, and question answering. A particularly exciting frontier for LLMs is their application to complex cognitive tasks such as mathematical reasoning. Historically, mathematical reasoning has been considered a hallmark of human intelligence, requiring abstract thinking, logical deduction, and systematic problem-solving. The exploration of whether LLMs can truly grasp and perform mathematical reasoning, beyond surface-level pattern matching, is a critical area of contemporary AI research \cite{wei2022chainthought, misra2022numgluesuite, sharma2022overcomingbarriers}.

This review is motivated by the rapid proliferation of research investigating LLMs' mathematical reasoning abilities. Several studies have highlighted the emergent capabilities of these models, particularly with the advent of sophisticated prompting techniques. For instance, Chain-of-Thought (CoT) prompting has been shown to significantly improve the reasoning performance of LLMs by eliciting intermediate reasoning steps \cite{wei2022chainthought}. This approach has been explored in various contexts, including arithmetic reasoning \cite{mishra2022numgluesuite}, commonsense reasoning, and symbolic manipulation \cite{zhou2022leasttomostprompting}. Furthermore, specialized models like Galactica have been developed to tackle scientific knowledge and reasoning \cite{taylor2022galacticalarge}.

Despite these advancements, significant challenges persist. LLMs can exhibit brittle behavior, failing to generalize to problems slightly different from their training data or prompt exemplars \cite{mishra2022numgluesuite}. The interpretability and faithfulness of their reasoning processes also remain concerns \cite{creswell2022faithfulreasoning}. To address these issues and provide a structured understanding of the current landscape, this systematic literature review aims to synthesize the existing research on LLMs and mathematical reasoning. Our primary research questions are:

1. What are the prominent techniques and prompting strategies employed to enhance mathematical reasoning in LLMs?
2. What are the key mathematical reasoning capabilities that LLMs have demonstrated or are being developed for?
3. What are the identified limitations and challenges in current LLM-based mathematical reasoning systems?
4. What are the promising future research directions in this domain?

To address these questions, we followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. This systematic approach ensures transparency and reproducibility of our review process, from initial literature search to the final synthesis of findings. The structure of this paper is as follows: Section \ref{sec:methodology} details our methodology for literature search and selection. Section \ref{sec:results} presents the synthesized findings organized thematically. Section \ref{sec:discussion} discusses the implications of these findings, identifies research gaps, and outlines future research directions. Finally, Section \ref{sec:conclusion} provides a concise summary of the review's key contributions.

We observe a growing interest in enabling LLMs to perform formal reasoning \cite{jiang2022draftsketch, welleck2022naturalprovergrounded} and to serve as teachers for smaller models \cite{ho2022largelanguage}. The ability of LLMs to perform analogical reasoning is also a noteworthy development \cite{webb2022emergentanalogical}. This review aims to provide a holistic overview of these diverse but interconnected research efforts.


=== METHODOLOGY ===
\section{Methodology}

This systematic literature review was conducted following the PRISMA 2020 guidelines \cite{page2021updating} to ensure a rigorous and transparent process. The objective was to identify and synthesize research on the application of large language models (LLMs) to mathematical reasoning.

\subsection{Search Strategy}

A systematic search was performed across major academic digital libraries and preprint repositories, including IEEE Xplore, ACM Digital Library, Scopus, Web of Science, and arXiv. The search queries were constructed using a combination of keywords related to LLMs and mathematical reasoning. The primary search terms included: "large language model", "LLM", "mathematical reasoning", "arithmetic reasoning", "symbolic reasoning", "logic", "theorem proving", and "math word problems". Specific combinations were used, such as "large language model AND mathematical reasoning", "LLM AND arithmetic", and "language model AND formal proof generation". The search was limited to publications released between January 1, 2020, and December 31, 2023, to capture the most recent advancements in this rapidly evolving field.

\subsection{Inclusion and Exclusion Criteria}

Publications were included if they met the following criteria:

*   **Language:** English language publications.
*   **Topic:** Directly addressed the use or capabilities of large language models for any form of mathematical reasoning. This encompassed arithmetic, algebra, symbolic manipulation, logical deduction, theorem proving, and math word problems.
*   **Type of Study:** Peer-reviewed conference papers, journal articles, and reputable preprints reporting original research. Studies that introduced novel methods, datasets, evaluations, or analyses related to LLMs and mathematical reasoning were prioritized.
*   **LLM Focus:** The core of the research involved LLMs, particularly those with a significant parameter count or those exhibiting emergent reasoning abilities.

The following exclusion criteria were applied:

*   **Non-English Publications:** Papers not written in English.
*   **Irrelevant Topics:** Studies focusing solely on natural language processing tasks unrelated to mathematical reasoning, or research on AI systems that did not involve LLMs.
*   **Review Articles:** Surveys, systematic reviews, and meta-analyses were excluded to avoid redundancy and focus on primary research findings. This ensured that our synthesis was based on original studies.
*   **Commentaries and Editorials:** Opinion pieces, short communications, and non-research articles.
*   **Code Repositories without accompanying research papers:** While code is valuable, we focused on published research outlining methodologies and results.

\subsection{Screening Process}

An initial screening of the search results was conducted based on titles and abstracts. Two independent reviewers screened the retrieved records. Any discrepancies or uncertainties regarding inclusion were resolved through discussion and, if necessary, by consulting the full text of the article. Following the initial screening, the full text of the selected articles was retrieved and assessed for eligibility based on the defined inclusion and exclusion criteria.

\subsection{Data Extraction}

For each included study, relevant information was extracted, including:

*   Author(s) and publication year
*   Model(s) used (e.g., GPT-3, PaLM, specific architectures)
*   Mathematical reasoning task(s) addressed (e.g., arithmetic, symbolic, logical)
*   Methodology employed (e.g., prompting techniques, fine-tuning, model architecture)
*   Key findings and performance metrics
*   Identified limitations and future work

\subsection{Quality Assessment}

A formal quality assessment was not conducted as this review aimed to synthesize the breadth of research rather than critically appraise individual study methodologies. However, the inclusion criteria emphasized peer-reviewed publications and reputable preprints, implicitly selecting studies with a degree of methodological rigor.

\subsection{PRISMA Flow Diagram}

Following the PRISMA guidelines, a flow diagram summarizing the study selection process was mentally constructed (as per instructions, not rendered). We identified 19 records through database searches. After removing duplicates (0 records), 19 records remained for screening. After screening titles and abstracts, all 19 records were deemed potentially relevant. Full-text articles of these 19 records were assessed for eligibility, resulting in 19 studies included in this review. No records were excluded during the full-text review based on the defined criteria. Thus, the final number of studies included is 19.


=== RESULTS ===
\section{Results}

Our systematic search identified a substantial body of research investigating the intersection of large language models (LLMs) and mathematical reasoning. A total of 19 studies met our inclusion criteria, representing a significant effort to understand, enhance, and apply LLMs to diverse mathematical tasks. These studies, primarily from 2022, reflect the rapid pace of development in this area.

\subsection{Prompting Strategies for Enhanced Reasoning}

A dominant theme in the literature is the development and application of sophisticated prompting techniques to elicit and improve mathematical reasoning in LLMs. Chain-of-Thought (CoT) prompting, which encourages models to generate intermediate reasoning steps, has been widely adopted and studied \cite{wei2022chainthought}. This approach has shown significant gains across various reasoning tasks, including arithmetic and commonsense reasoning 
\cite{wei2022chainthought}. For example, prompting a large language model with a few CoT demonstrations can achieve state-of-the-art accuracy on benchmarks like GSM8K \cite{wei2022chainthought}.

Beyond standard CoT, researchers have explored variations to address specific challenges. Least-to-Most prompting breaks down complex problems into smaller, sequential subproblems, enabling generalization to harder tasks than those seen in the prompts \cite{zhou2022leasttomostprompting}. This method has demonstrated remarkable success, particularly in symbolic manipulation and compositional generalization tasks \cite{zhou2022leasttomostprompting}. Dynamic prompt learning, utilizing policy gradients, has also been proposed to learn optimal in-context example selection for semi-structured mathematical reasoning, reducing prediction variance and improving accuracy \cite{lu2022dynamicprompt}.

Furthermore, the concept of using LLMs as "reasoning teachers" has emerged, where large models generate reasoning samples to fine-tune smaller models. This approach, Fine-tune-CoT, enables complex reasoning capabilities in smaller models, significantly reducing computational requirements \cite{ho2022largelanguage}. The quality and diversity of generated rationales from teacher models can further boost student model performance \cite{ho2022largelanguage}.

\subsection{LLMs in Formal and Scientific Reasoning}

Research has also focused on leveraging LLMs for more formal aspects of mathematics, including theorem proving. The "Draft, Sketch, and Prove" (DSP) method maps informal proofs to formal proof sketches, which then guide automated provers \cite{jiang2022draftsketch}. This approach has shown that LLMs can generate well-structured sketches that align with human reasoning, improving the performance of automated provers \cite{jiang2022draftsketch}. NaturalProver, another model, focuses on grounded mathematical proof generation by conditioning on background references and optionally enforcing their presence with constrained decoding \cite{welleck2022naturalprovergrounded}. This has demonstrated initial success in suggesting proof steps and generating short proofs \cite{welleck2022naturalprovergrounded}.

In the scientific domain, Galactica, a large language model trained on a vast scientific corpus, has shown impressive performance on technical knowledge probes, mathematical reasoning benchmarks (outperforming PaLM 540B), and downstream scientific tasks \cite{taylor2022galacticalarge}. Med-PaLM, an LLM for medicine, demonstrates the potential of these models in clinical applications, achieving state-of-the-art accuracy on medical question-answering benchmarks, although human evaluations reveal gaps \cite{singhal2022largelanguage}.

\subsection{Emergent Reasoning Capabilities and Limitations}

Studies have highlighted the emergent analogical reasoning capabilities of LLMs, with models like GPT-3 demonstrating a strong capacity for abstract pattern induction, matching or even surpassing human capabilities in certain analogical tasks \cite{webb2022emergentanalogical}. However, a systematic formal analysis reveals that while LLMs are capable of making correct individual deduction steps, they struggle with proof planning – the ability to systematically explore multiple valid deduction options when available \cite{saparov2022languagemodels}.

Faithful reasoning using LLMs is another area of active research, with methods proposed to chain together reasoning steps to mirror the logical structure of a problem. These approaches aim to produce interpretable reasoning traces whose validity can be checked by users, improving performance on multi-step logical deduction and scientific question-answering \cite{creswell2022faithfulreasoning}. The NumGLUE benchmark has been proposed to evaluate fundamental arithmetic reasoning, revealing that even state-of-the-art LLMs perform significantly worse than humans on these tasks, indicating brittleness and a lack of robust arithmetic understanding \cite{mishra2022numgluesuite}.

Overcoming barriers to skill injection, such as arithmetic proficiency, into language models remains a challenge, with issues like catastrophic forgetting of linguistic skills needing to be addressed \cite{sharma2022overcomingbarriers}. The synergy between reasoning and acting in LLMs, as demonstrated by the ReAct framework, shows promise in overcoming issues like hallucination and error propagation by allowing models to interact with external sources like Wikipedia APIs \cite{yao2022reactsynergizing}.

Research into relational abstractions also aims to build stronger reasoning capabilities by providing explicit abstract characterizations of intermediate solution steps, leading to higher accuracy on multi-step mathematical reasoning tasks \cite{nam2022learningreason}.


=== DISCUSSION ===
\section{Discussion}

The synthesized findings from the reviewed literature reveal a dynamic and rapidly advancing field focused on harnessing the power of large language models (LLMs) for mathematical reasoning. The research overwhelmingly points towards the emergent capabilities of LLMs in this domain, particularly when guided by sophisticated prompting strategies \cite{wei2022chainthought, zhou2022leasttomostprompting}.

\subsection{Synthesized Findings}

A primary theme is the efficacy of Chain-of-Thought (CoT) prompting and its derivatives. The ability of CoT to elicit step-by-step reasoning has been a significant breakthrough, enabling LLMs to tackle complex arithmetic and symbolic problems with improved accuracy \cite{wei2022chainthought}. Variations like Least-to-Most prompting address the critical challenge of generalizing to problems harder than training examples, by decomposing them into manageable sub-problems \cite{zhou2022leasttomostprompting}. This hierarchical approach is crucial for building more robust reasoning systems.

The application of LLMs to formal reasoning and scientific domains is another promising area. Methods like "Draft, Sketch, and Prove" \cite{jiang2022draftsketch} and NaturalProver \cite{welleck2022naturalprovergrounded} demonstrate the potential for LLMs to assist in generating and guiding formal mathematical proofs, bridging the gap between informal human reasoning and formal systems. Specialized models like Galactica \cite{taylor2022galacticalarge} and Med-PaLM \cite{singhal2022largelanguage} underscore the feasibility of adapting LLMs for domain-specific scientific and medical reasoning, achieving state-of-the-art results on relevant benchmarks.

Furthermore, the development of LLMs as "reasoning teachers" \cite{ho2022largelanguage} offers a pathway to democratize advanced reasoning capabilities, allowing smaller, more efficient models to inherit complex reasoning skills. The synergy between reasoning and acting, as exemplified by ReAct \cite{yao2022reactsynergizing}, highlights the importance of grounding LLM reasoning with external information and actions to mitigate errors and hallucinations.

\subsection{Research Gaps and Challenges}

Despite considerable progress, significant research gaps and challenges remain. A recurring issue is the brittleness of LLMs, where performance can degrade significantly with minor variations in problem formulation or context \cite{mishra2022numgluesuite}. The NumGLUE benchmark, for instance, explicitly reveals that LLMs lag behind human performance in fundamental arithmetic reasoning \cite{mishra2022numgluesuite}, suggesting a need for deeper, more systematic understanding rather than surface-level pattern recognition.

Proof planning, the ability to strategically select between multiple valid deduction steps, remains a difficult task for LLMs \cite{saparov2022languagemodels}. While individual steps might be correct, the overarching strategy for constructing a proof is often lacking. The faithfulness and interpretability of generated reasoning traces are also critical concerns \cite{creswell2022faithfulreasoning}. Ensuring that the model's internal reasoning process aligns with the presented trace and is verifiable by humans is essential for trust and reliability.

Another challenge is the effective injection of non-linguistic skills, such as precise numerical and arithmetic reasoning, into LLMs without compromising their linguistic abilities \cite{sharma2022overcomingbarriers}. The potential for catastrophic forgetting during fine-tuning for specific skills necessitates novel methods for skill integration.

Finally, while analogical reasoning shows promise \cite{webb2022emergentanalogical}, its full exploitation in complex mathematical problem-solving requires further investigation. Understanding how abstract pattern induction translates to robust multi-step mathematical derivations is an open question.

\subsection{Implications and Future Directions}

The implications of overcoming these challenges are profound. Enhanced LLM reasoning capabilities could revolutionize education, scientific discovery, and problem-solving across numerous industries. Future research should focus on developing methods that promote more robust and generalizable mathematical understanding, moving beyond task-specific performance to true reasoning competence. This could involve:

*   **Hybrid Architectures:** Combining LLMs with symbolic reasoning engines or neuro-symbolic approaches to leverage the strengths of both paradigms \cite{jiang2022draftsketch}.
*   **Improved Training Objectives:** Developing new training objectives that explicitly reward systematic proof planning and logical rigor, rather than solely focusing on end-task accuracy.
*   **Advanced Evaluation Frameworks:** Creating more challenging and diverse benchmarks that rigorously test for generalization, robustness, and true understanding, moving beyond current limitations \cite{mishra2022numgluesuite}.
*   **Interpretability and Verification Tools:** Developing better tools and methodologies to verify the correctness and interpretability of LLM-generated reasoning processes \cite{creswell2022faithfulreasoning}.
*   **Learning Relational Abstractions:** Further exploring methods like learning relational abstractions \cite{nam2022learningreason} to imbue models with a deeper understanding of mathematical structures and transitions.

Continued exploration into synergistic approaches like ReAct \cite{yao2022reactsynergizing} that combine reasoning with interaction and action will also be critical for developing agents that can solve complex, real-world problems requiring mathematical insight.


=== CONCLUSION ===
\section{Conclusion}

This systematic review has synthesized the current landscape of research at the intersection of large language models (LLMs) and mathematical reasoning. Our findings, drawn from 19 diverse studies, highlight significant advancements, particularly in the application of sophisticated prompting techniques like Chain-of-Thought \cite{wei2022chainthought} and its variants \cite{zhou2022leasttomostprompting, lu2022dynamicprompt}. These methods have demonstrably improved LLMs' ability to perform arithmetic, symbolic, and logical reasoning tasks, with emergent capabilities in areas like analogical reasoning \cite{webb2022emergentanalogical} and formal proof generation \cite{jiang2022draftsketch, welleck2022naturalprovergrounded}.

The review underscores the potential of LLMs in specialized domains, as evidenced by models tailored for scientific \cite{taylor2022galacticalarge} and medical applications \cite{singhal2022largelanguage}. The concept of using LLMs as reasoning teachers \cite{ho2022largelanguage} presents a promising avenue for efficient knowledge transfer to smaller models. Furthermore, the synergy between reasoning and acting \cite{yao2022reactsynergizing} offers a path towards more reliable and interpretable problem-solving.

However, critical challenges persist. The brittleness of LLMs in mathematical reasoning \cite{mishra2022numgluesuite}, their difficulties with strategic proof planning \cite{saparov2022languagemodels}, and the need for faithful and interpretable reasoning traces \cite{creswell2022faithfulreasoning} represent significant hurdles. The effective injection of precise skills like arithmetic without compromising linguistic fluency remains an active area of research \cite{sharma2022overcomingbarriers}.

This review contributes by providing a structured overview of the current state-of-the-art, identifying key research themes, and highlighting critical gaps. Future research directions should focus on enhancing robustness, developing more sophisticated proof planning capabilities, improving interpretability, and creating advanced evaluation frameworks that push beyond current limitations. By addressing these challenges, LLMs hold the promise of becoming powerful tools for mathematical discovery and problem-solving, ultimately advancing both AI and our understanding of complex reasoning.


=== END PREVIOUS DRAFT ===

UPDATED STATISTICS:
- Total papers now included: 19
- Records identified: 19
- Studies included: 19

TASK: COMPLETELY REGENERATE the paper integrating the new papers.

REGENERATION INSTRUCTIONS:
1. Read the previous draft to understand existing structure and themes
2. Review the NEW papers being added (listed in "NEW PAPERS BEING ADDED" section)
3. Integrate new papers throughout ALL sections where relevant
4. In Results section:
   - Add new subsections if new themes emerge from new papers
   - Reorganize existing subsections for better coherence
   - CITE every paper discussed using \cite{citationKey}
5. Update all statistics to reflect new paper count
6. Maintain academic quality and narrative flow
7. Ensure EVERY paper (old and new) is cited using \cite{citationKey}

CITATION REQUIREMENTS:
✓ Use \cite{citationKey} format (e.g., \cite{smith2020deep})
✓ Cite papers from BOTH previous draft AND new additions
✓ Introduction: MINIMUM 5-10 citations
✓ Results: MINIMUM 15-25 citations (more with larger paper count)
✓ Discussion: MINIMUM 10-15 citations
✓ Conclusion: MINIMUM 3-5 citations
✓ Each subsection in Results MUST cite papers relevant to that theme

REGENERATE COMPLETE PAPER:
1. ABSTRACT: Update with new paper count, refined findings (NO citations)
2. INTRODUCTION: Integrate relevant new papers, update scope, CITE extensively
3. METHODOLOGY: Update statistics (cite PRISMA guidelines if needed)
4. RESULTS: **CRITICAL** - Reorganize with new papers, cite ALL papers discussed
5. DISCUSSION: Integrate new findings, synthesize across all papers, CITE extensively
6. CONCLUSION: Update with insights from complete set, CITE key papers

CRITICAL: Return your response as VALID JSON with PROPER ESCAPING:

IMPORTANT JSON ESCAPING RULES:
- Every single backslash in LaTeX commands MUST be escaped as double backslash
- \cite{} becomes \\cite{} in JSON
- \subsection{} becomes \\subsection{} in JSON
- Example: "introduction": "Recent work \\cite{smith2020} shows..."

Return ONLY valid JSON in this EXACT format:
{
  "abstract": "...",
  "introduction": "text with \\cite{} properly escaped",
  "methodology": "...",
  "results": "text with \\subsection{} and \\cite{} properly escaped",
  "discussion": "text with \\cite{} properly escaped",
  "conclusion": "text with \\cite{} properly escaped"
}

VERIFY: Check that ALL backslashes are doubled (\\) in JSON before returning!
