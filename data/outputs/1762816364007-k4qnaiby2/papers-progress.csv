ID,Title,Authors,Year,Venue,Citations,URL,PDF URL,DOI,Abstract,Keywords,Included,Exclusion Reason,Extracted At
acausalframeworktoqu-2022,A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models,Alessandro Stolfo; Zhijing Jin; Kumar Shridhar; B. Scholkopf; Mrinmaya Sachan,2022,Annual Meeting of the Association for Computational Linguistics,76,https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe,http://arxiv.org/pdf/2210.12023,10.48550/arXiv.2210.12023,"We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution.By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems.Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.",arxiv:2210.12023,Yes,,2025-11-10T23:12:44.820Z
acontributiononrelat-2022,"A Contribution on Relationship Banking. Economic, Anthropological and Mathematical Reasoning, Empirical Evidence from Italy",Marco Desogus; Elisa Casu,2022,,6,https://www.semanticscholar.org/paper/dd88cc9b1f6d71ef82631b4e1c98c077ccdf291a,,,,,Yes,,2025-11-10T23:12:44.820Z
asurveyofdeeplearnin-2022,A Survey of Deep Learning for Mathematical Reasoning,Pan Lu; Liang Qiu; Wenhao Yu; S. Welleck; Kai-Wei Chang,2022,Annual Meeting of the Association for Computational Linguistics,166,https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d,http://arxiv.org/pdf/2212.10535,10.48550/arXiv.2212.10535,"Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.",arxiv:2212.10535,Yes,,2025-11-10T23:12:44.820Z
alertadaptlanguagemo-2022,ALERT: Adapt Language Models to Reasoning Tasks,Ping Yu; Tianlu Wang; O. Yu. Golovneva; Badr AlKhamissi; Gargi Ghosh; Mona T. Diab; Asli Celikyilmaz,2022,Annual Meeting of the Association for Computational Linguistics,20,https://www.semanticscholar.org/paper/95c11cc5820ba32c60d5f2671f6567b9914a4978,https://arxiv.org/pdf/2212.08286,10.48550/arXiv.2212.08286,"Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.To address this question, we introduce {pasted macro ‘OUR’}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro ‘OUR’}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro ‘OUR’}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.",arxiv:2212.08286,Yes,,2025-11-10T23:12:44.820Z
anoverviewofvadaloga-2022,An Overview of Vadalog: a System for Reasoning over Large Knowledge Graphs,Luigi Bellomarini; Davide Benedetto; Emanuel Sallinger,2022,Sistemi Evoluti per Basi di Dati,1,https://www.semanticscholar.org/paper/83dc0eca1a453e2970d32923bb48bb84976bd968,,,,,Yes,,2025-11-10T23:12:44.820Z
autoformalizationwit-2022,Autoformalization with Large Language Models,Yuhuai Wu; Albert Qiaochu Jiang; Wenda Li; M. Rabe; Charles Staats; M. Jamnik; Christian Szegedy,2022,Neural Information Processing Systems,220,https://www.semanticscholar.org/paper/c28e95a06dfcf13fc65a1cac83722f53e34f12a5,https://arxiv.org/pdf/2205.12615,10.48550/arXiv.2205.12615,"Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from $29.6\%$ to $35.2\%$.",arxiv:2205.12615,Yes,,2025-11-10T23:12:44.820Z
automaticchainofthou-2022,Automatic Chain of Thought Prompting in Large Language Models,Zhuosheng Zhang; Aston Zhang; Mu Li; Alexander J. Smola,2022,International Conference on Learning Representations,779,https://www.semanticscholar.org/paper/90350aa626bed47b02d0c162462e5b0ca82be6b2,,,"Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like""Let's think step by step""to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the""Let's think step by step""prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot",arxiv:2210.03493,Yes,,2025-11-10T23:12:44.820Z
capecorrectiveaction-2022,CAPE: Corrective Actions from Precondition Errors using Large Language Models,S. S. Raman; Vanya Cohen; Eric Rosen; Ifrah Idrees; D. Paulius; Stefanie Tellex,2022,IEEE International Conference on Robotics and Automation,47,https://www.semanticscholar.org/paper/c82d8d80ea68400adb7faebb2f1cff38dd83093a,,10.1109/ICRA57147.2024.10611376,"Extracting knowledge and reasoning from large language models (LLMs) offers a path to designing intelligent robots. Common approaches that leverage LLMs for planning are unable to recover when actions fail and resort to retrying failed actions without resolving the underlying cause. We propose a novel approach (CAPE) that generates corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans through few-shot reasoning on action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while maintaining semantic correctness and minimizing re-prompting. In VirtualHome, CAPE improves a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan, whilst achieving competitive executability. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves correctness by 76.49% with higher executability compared to SayCan. Our approach enables embodied agents to follow natural language commands and robustly recover from failures.",arxiv:2211.09935,Yes,,2025-11-10T23:12:44.820Z
clevrmathadatasetfor-2022,"CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning",Adam Dahlgren Lindström; Savitha Sam Abraham,2022,International Workshop on Neural-Symbolic Learning and Reasoning,80,https://www.semanticscholar.org/paper/74cc3d340039c67bdabaef090d1386fe2c5376ca,http://arxiv.org/pdf/2208.05358,10.48550/arXiv.2208.05358,"We introduce CLEVR-Math, a multi-modal math word problems dataset consisting of simple math word problems involving addition/subtraction, represented partly by a textual description and partly by an image illustrating the scenario. The text describes actions performed on the scene that is depicted in the image. Since the question posed may not be about the scene in the image, but about the state of the scene before or after the actions are applied, the solver envision or imagine the state changes due to these actions. Solving these word problems requires a combination of language, visual and mathematical reasoning. We apply state-of-the-art neural and neuro-symbolic models for visual question answering on CLEVR-Math and empirically evaluate their performances. Our results show how neither method generalise to chains of operations. We discuss the limitations of the two in addressing the task of multi-modal word problem solving.",arxiv:2208.05358,Yes,,2025-11-10T23:12:44.820Z
crosscontaminationac-2022,CROSS-CONTAMINATION: ACCELERATING LARGE LANGUAGE MODELS WITHOUT IMPACTING PERFORMANCE,M. M. Krell; Matej Kosec,2022,,76,https://www.semanticscholar.org/paper/85cac89ba01a07f3dbf6dbb1e0c56067a3105714,,,,,Yes,,2025-11-10T23:12:44.820Z
canretrieveraugmente-2022,Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model,Parishad BehnamGhader; Santiago Miret; Siva Reddy,2022,Conference on Empirical Methods in Natural Language Processing,42,https://www.semanticscholar.org/paper/e4758d05c3d4231dd30c656330e156ccc9dbb07b,http://arxiv.org/pdf/2212.09146,10.48550/arXiv.2212.09146,"Augmenting pretrained language models with retrievers to select the supporting documents has shown promise in effectively solving common NLP problems, including language modeling and question answering, in an interpretable way. In this paper, we first study the strengths and weaknesses of different retriever-augmented language models (REALM, $k$NN-LM, FiD coupled with DPR, and ATLAS and Flan-T5 coupled with Contriever) in reasoning over the retrieved statements in different tasks. We show how the retrieve-then-read models' limitations in reasoning are rooted both in the retriever module as well as the language model. Our experimental results demonstrate that the similarity metric used by the retrievers is generally insufficient for reasoning tasks. Additionally, we show that the language models in retriever-augmented models do not take the complicated relations between the statements into account, which leads to poor reasoning performance even when using the larger models. Moreover, we analyze the reasoning performance of large language models using multihop retrieval but we only observe minor improvements. Overall, this shows great room for further research in this area.",arxiv:2212.09146,Yes,,2025-11-10T23:12:44.820Z
chainofthoughtprompt-2022,Chain of Thought Prompting Elicits Reasoning in Large Language Models,Jason Wei; Xuezhi Wang; Dale Schuurmans; Maarten Bosma; Ed H. Chi; F. Xia; Quoc Le; Denny Zhou,2022,Neural Information Processing Systems,13165,https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5,,,"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",arxiv:2201.11903,Yes,,2025-11-10T23:12:44.820Z
codeaspolicieslangua-2022,Code as Policies: Language Model Programs for Embodied Control,Jacky Liang; Wenlong Huang; F. Xia; Peng Xu; Karol Hausman; Brian Ichter; Peter R. Florence; Andy Zeng,2022,IEEE International Conference on Robotics and Automation,1177,https://www.semanticscholar.org/paper/91deaf9d324c8feafc189da0da03e60a60287bca,https://arxiv.org/pdf/2209.07753,10.1109/ICRA48891.2023.10160591,"Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",arxiv:2209.07753,Yes,,2025-11-10T23:12:44.820Z
complexitybasedpromp-2022,Complexity-Based Prompting for Multi-Step Reasoning,Yao Fu; Hao-Chun Peng; Ashish Sabharwal; Peter Clark; Tushar Khot,2022,International Conference on Learning Representations,517,https://www.semanticscholar.org/paper/c88cafa3e980765a64febe369ceb7c2aa7261d2a,http://arxiv.org/pdf/2210.00720,10.48550/arXiv.2210.00720,"We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",arxiv:2210.00720,Yes,,2025-11-10T23:12:44.820Z
convfinqaexploringth-2022,ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering,Zhiyu Chen; SHIYANG LI; Charese Smiley; Zhiqiang Ma; Sameena Shah; William Yang Wang,2022,Conference on Empirical Methods in Natural Language Processing,161,https://www.semanticscholar.org/paper/d96997265f8146e93b4c9350f19d55e46d1317f0,http://arxiv.org/pdf/2210.03849,10.48550/arXiv.2210.03849,"With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/ConvFinQA.",arxiv:2210.03849,Yes,,2025-11-10T23:12:44.820Z
creativemathematical-2022,Creative Mathematical Reasoning: Does Need for Cognition Matter?,B. Jonsson; Julia Mossegård; Johan Lithner; Linnea Karlsson Wirebring,2022,Frontiers in Psychology,19,https://www.semanticscholar.org/paper/9f6f01cba1158e6bcb17aaa43070ef3b64c59550,https://www.frontiersin.org/articles/10.3389/fpsyg.2021.797807/pdf,10.3389/fpsyg.2021.797807,"A large portion of mathematics education centers heavily around imitative reasoning and rote learning, raising concerns about students’ lack of deeper and conceptual understanding of mathematics. To address these concerns, there has been a growing focus on students learning and teachers teaching methods that aim to enhance conceptual understanding and problem-solving skills. One suggestion is allowing students to construct their own solution methods using creative mathematical reasoning (CMR), a method that in previous studies has been contrasted against algorithmic reasoning (AR) with positive effects on test tasks. Although previous studies have evaluated the effects of CMR, they have ignored if and to what extent intrinsic cognitive motivation play a role. This study investigated the effects of intrinsic cognitive motivation to engage in cognitive strenuous mathematical tasks, operationalized through Need for Cognition (NFC), and working memory capacity (WMC). Two independent groups, consisting of upper secondary students (N = 137, mean age 17.13, SD = 0.62, 63 boys and 74 girls), practiced non-routine mathematical problem solving with CMR and AR tasks and were tested 1 week later. An initial t-test confirmed that the CMR group outperformed the AR group. Structural equation modeling revealed that NFC was a significant predictor of math performance for the CMR group but not for the AR group. The results also showed that WMC was a strong predictor of math performance independent of group. These results are discussed in terms of allowing for time and opportunities for struggle with constructing own solution methods using CMR, thereby enhancing students conceptual understanding.",,Yes,,2025-11-10T23:12:44.820Z
dallevalprobingthere-2022,DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers,Jaemin Cho; Abhaysinh Zala; Mohit Bansal,2022,arXiv.org,130,https://www.semanticscholar.org/paper/804b27dc02becf7bbbd89ba949e1e07e8677c459,,,,,Yes,,2025-11-10T23:12:44.820Z
deplotoneshotvisuall-2022,DePlot: One-shot visual language reasoning by plot-to-table translation,Fangyu Liu; Julian Martin Eisenschlos; Francesco Piccinno; Syrine Krichene; Chenxi Pang; Kenton Lee; Mandar Joshi; Wenhu Chen; Nigel Collier; Y. Altun,2022,Annual Meeting of the Association for Computational Linguistics,133,https://www.semanticscholar.org/paper/4d3a49d1439a0b8fbb0e9f588970ad0f1d70dec8,http://arxiv.org/pdf/2212.10505,10.48550/arXiv.2212.10505,"Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than>28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.",arxiv:2212.10505,Yes,,2025-11-10T23:12:44.820Z
distillingmultistepr-2022,Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions,Kumar Shridhar; Alessandro Stolfo; Mrinmaya Sachan,2022,arXiv.org,47,https://www.semanticscholar.org/paper/72123a86eae2cb5c4eae8650f43524039d48875d,http://arxiv.org/pdf/2212.00193,10.48550/arXiv.2212.00193,,,Yes,,2025-11-10T23:12:44.820Z
distillingreasoningc-2022,Distilling Reasoning Capabilities into Smaller Language Models,Kumar Shridhar; Alessandro Stolfo; Mrinmaya Sachan,2022,Annual Meeting of the Association for Computational Linguistics,202,https://www.semanticscholar.org/paper/8fd462f6248d5e3f1b6602697c09489086b5655f,https://aclanthology.org/2023.findings-acl.441.pdf,10.18653/v1/2023.findings-acl.441,"Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM",arxiv:2212.00193,Yes,,2025-11-10T23:12:44.820Z
draftsketchandproveg-2022,"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs",Albert Qiaochu Jiang; S. Welleck; J. Zhou; Wenda Li; Jiacheng Liu; M. Jamnik; Timothée Lacroix; Yuhuai Wu; Guillaume Lample,2022,International Conference on Learning Representations,226,https://www.semanticscholar.org/paper/7de36d6b14aadc8cdb6ad1340b9ca64b15375bca,http://arxiv.org/pdf/2210.12283,10.48550/arXiv.2210.12283,"The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.",arxiv:2210.12283,Yes,,2025-11-10T23:12:44.820Z
dynamicpromptlearnin-2022,Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning,Pan Lu; Liang Qiu; Kai-Wei Chang; Y. Wu; Song-Chun Zhu; Tanmay Rajpurohit; Peter Clark; A. Kalyan,2022,International Conference on Learning Representations,365,https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4,http://arxiv.org/pdf/2209.14610,10.48550/arXiv.2209.14610,"Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.",arxiv:2209.14610,Yes,,2025-11-10T23:12:44.820Z
emergentanalogicalre-2022,Emergent analogical reasoning in large language models,Taylor W. Webb; K. Holyoak; Hongjing Lu,2022,Nature Human Behaviour,395,https://www.semanticscholar.org/paper/3cbffab9d7981da6662d474aaa056dcbd3c1701e,,10.1038/s41562-023-01659-w,"The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of Generative Pre-trained Transformer (GPT)-3) on a range of analogical tasks, including a non-visual matrix reasoning task based on the rule structure of Raven’s Standard Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT-4 indicated even better performance. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems. Webb et al. show that new artificial intelligence language models, such as Generative Pre-trained Transformer 3, are able to solve analogical reasoning problems at a human-like level of performance.",arxiv:2212.09196,Yes,,2025-11-10T23:12:44.820Z
eventsrealmeventreas-2022,EvEntS ReaLM: Event Reasoning of Entity States via Language Models,Evangelia Spiliopoulou; Artidoro Pagnoni; Yonatan Bisk; E. Hovy,2022,Conference on Empirical Methods in Natural Language Processing,11,https://www.semanticscholar.org/paper/748a2700ec11f51560a69ec05c67ca9f97014be7,https://arxiv.org/pdf/2211.05392,10.48550/arXiv.2211.05392,"This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.",arxiv:2211.05392,Yes,,2025-11-10T23:12:44.820Z
explanationsfromlarg-2022,Explanations from Large Language Models Make Small Reasoners Better,SHIYANG LI; Jianshu Chen; Yelong Shen; Zhiyu Chen; Xinlu Zhang; Zekun Li; Hong Wang; Jingu Qian; Baolin Peng; Yi Mao; Wenhu Chen; Xifeng Yan,2022,arXiv.org,150,https://www.semanticscholar.org/paper/7d29a84a589aa5655e5d3fed8d725ea472816599,http://arxiv.org/pdf/2210.06726,10.48550/arXiv.2210.06726,"Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.",arxiv:2210.06726,Yes,,2025-11-10T23:12:44.820Z
exploringlengthgener-2022,Exploring Length Generalization in Large Language Models,Cem Anil; Yuhuai Wu; Anders Andreassen; Aitor Lewkowycz; Vedant Misra; V. Ramasesh; Ambrose Slone; Guy Gur-Ari; Ethan Dyer; Behnam Neyshabur,2022,Neural Information Processing Systems,196,https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a,http://arxiv.org/pdf/2207.04901,10.48550/arXiv.2207.04901,"The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.",arxiv:2207.04901,Yes,,2025-11-10T23:12:44.820Z
folionaturallanguage-2022,FOLIO: Natural Language Reasoning with First-Order Logic,Simeng Han; Hailey Schoelkopf; Yilun Zhao; Zhenting Qi; Martin Riddell; Luke Benson; Lucy Sun; E. Zubova; Yujie Qiao; Matthew Burtell; David Peng; Jonathan Fan; Yixin Liu; Brian Wong; Malcolm Sailor; Ansong Ni; Linyong Nan; Jungo Kasai; Tao Yu; Rui Zhang; Shafiq R. Joty; Alexander R. Fabbri; Wojciech Kryscinski; Xi Victoria Lin; Caiming Xiong; Dragomir R. Radev,2022,Conference on Empirical Methods in Natural Language Processing,144,https://www.semanticscholar.org/paper/5581bf85386737bd3378eec68189759a05280bea,http://arxiv.org/pdf/2209.00840,10.48550/arXiv.2209.00840,"Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.",arxiv:2209.00840,Yes,,2025-11-10T23:12:44.820Z
faithfulreasoningusi-2022,Faithful Reasoning Using Large Language Models,Antonia Creswell; M. Shanahan,2022,arXiv.org,133,https://www.semanticscholar.org/paper/f0a0e8b6e84207f50db4d24cc4016e40601214ef,,,"Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.",arxiv:2208.14271,Yes,,2025-11-10T23:12:44.820Z
galacticaalargelangu-2022,Galactica: A Large Language Model for Science,Ross Taylor; Marcin Kardas; Guillem Cucurull; Thomas Scialom; A. Hartshorn; Elvis Saravia; Andrew Poulton; Viktor Kerkez; Robert Stojnic,2022,arXiv.org,892,https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1,,,"Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.",arxiv:2211.09085,Yes,,2025-11-10T23:12:44.820Z
greaselmgraphreasoni-2022,GreaseLM: Graph REASoning Enhanced Language Models for Question Answering,Xikun Zhang; Antoine Bosselut; Michihiro Yasunaga; Hongyu Ren; Percy Liang; Christopher D. Manning; J. Leskovec,2022,International Conference on Learning Representations,251,https://www.semanticscholar.org/paper/4ab41d9780f1d1ac34d39fa7e527e73652507fcc,,,"Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.",arxiv:2201.08860,Yes,,2025-11-10T23:12:44.820Z
howwelldoeschatgptdo-2022,How Well Does ChatGPT Do When Taking the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment,A. Gilson; C. Safranek; Ting Huang; V. Socrates; L. Chi; R. A. Taylor; David Chartash,2022,medRxiv,98,https://www.semanticscholar.org/paper/7d4867e28b02059eef4cb25bfcd304b2071b30a9,https://www.medrxiv.org/content/medrxiv/early/2022/12/26/2022.12.23.22283901.full.pdf,10.1101/2022.12.23.22283901,"Background: ChatGPT is a 175 billion parameter natural language processing model which can generate conversation style responses to user input. Objective: To evaluate the performance of ChatGPT on questions within the scope of United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as analyze responses for user interpretability. Methods: We used two novel sets of multiple choice questions to evaluate ChatGPT's performance, each with questions pertaining to Step 1 and Step 2. The first was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the userbase. The second, was the National Board of Medical Examiners (NBME) Free 120-question exams. After prompting ChatGPT with each question, ChatGPT's selected answer was recorded, and the text output evaluated across three qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results: On the four datasets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free- Step2, ChatGPT achieved accuracies of 44%, 42%, 64.4%, and 57.8%. The model demonstrated a significant decrease in performance as question difficulty increased (P=.012) within the AMBOSS- Step1 dataset. We found logical justification for ChatGPT's answer selection was present in 100% of outputs. Internal information to the question was present in >90% of all questions. The presence of information external to the question was respectively 54.5% and 27% lower for incorrect relative to correct answers on the NBME-Free-Step1 and NBME-Free-Step2 datasets (P<=.001). Conclusion: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at greater than 60% threshold on the NBME-Free- Step-1 dataset we show that the model is comparable to a third year medical student. Additionally, due to the dialogic nature of the response to questions, we demonstrate ChatGPT's ability to provide reasoning and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as a medical education tool.",,Yes,,2025-11-10T23:12:44.820Z
humanlanguageunderst-2022,Human Language Understanding & Reasoning,Christopher D. Manning,2022,Daedalus,123,https://www.semanticscholar.org/paper/a0c87ee1b0903c1c9ac72809caf75b6b6997baa0,https://direct.mit.edu/daed/article-pdf/151/2/127/2060607/daed_a_01905.pdf,10.1162/daed_a_01905,"Abstract The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.",,Yes,,2025-11-10T23:12:44.820Z
humanlikeintuitivebe-2022,Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT,Thilo Hagendorff; Sarah Fabi; Michal Kosinski,2022,Nature Computational Science,202,https://www.semanticscholar.org/paper/0bfc05adcddd4fe5d1335d96cc313c41526d4558,https://www.nature.com/articles/s43588-023-00527-x.pdf,10.1038/s43588-023-00527-x,"We design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics. The reasoning capabilities of OpenAI’s generative pre-trained transformer family were tested using semantic illusions and cognitive reflection tests that are typically used in human studies. While early models were prone to human-like cognitive errors, ChatGPT decisively outperformed humans, avoiding the cognitive traps embedded in the tasks.",arxiv:2306.07622,Yes,,2025-11-10T23:12:44.820Z
innermonologueembodi-2022,Inner Monologue: Embodied Reasoning through Planning with Language Models,Wenlong Huang; F. Xia; Ted Xiao; Harris Chan; Jacky Liang; Peter R. Florence; Andy Zeng; Jonathan Tompson; Igor Mordatch; Yevgen Chebotar; P. Sermanet; Noah Brown; Tomas Jackson; Linda Luu; S. Levine; Karol Hausman; Brian Ichter,2022,Conference on Robot Learning,1098,https://www.semanticscholar.org/paper/f3cf71c51b882fe3111d71c4bf104297d38197f8,http://arxiv.org/pdf/2207.05608,10.48550/arXiv.2207.05608,"Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.",arxiv:2207.05608,Yes,,2025-11-10T23:12:44.820Z
kmirabenchmarkforeva-2022,"KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models",Daniel Gao; Yantao Jia; Lei Li; Chengzhen Fu; Zhicheng Dou; Hao Jiang; Xinyu Zhang; Lei Chen; Zhao Cao,2022,arXiv.org,9,https://www.semanticscholar.org/paper/718343008a6cfca9e86ab6160caba353c52c17cf,,,"Previous works show the great potential of pre-trained language models (PLMs) for storing a large amount of factual knowledge. However, to figure out whether PLMs can be reliable knowledge sources and used as alternative knowledge bases (KBs), we need to further explore some critical features of PLMs. Firstly, knowledge memorization and identification abilities: traditional KBs can store various types of entities and relationships; do PLMs have a high knowledge capacity to store different types of knowledge? Secondly, reasoning ability: a qualified knowledge source should not only provide a collection of facts, but support a symbolic reasoner. Can PLMs derive new knowledge based on the correlations between facts? To evaluate these features of PLMs, we propose a benchmark, named Knowledge Memorization, Identification, and Reasoning test (KMIR). KMIR covers 3 types of knowledge, including general knowledge, domain-specific knowledge, and commonsense, and provides 184,348 well-designed questions. Preliminary experiments with various representative pre-training language models on KMIR reveal many interesting phenomenons: 1) The memorization ability of PLMs depends more on the number of parameters than training schemes. 2) Current PLMs are struggling to robustly remember the facts. 3) Model compression technology retains the amount of knowledge well, but hurts the identification and reasoning abilities. We hope KMIR can facilitate the design of PLMs as better knowledge sources.",arxiv:2202.13529,Yes,,2025-11-10T23:12:44.820Z
kritknowledgereasoni-2022,KRIT: Knowledge-Reasoning Intelligence in vision-language Transformer,Kezhen Chen; Qiuyuan Huang; Daniel J. McDuff; Yonatan Bisk; Jianfeng Gao,2022,,3,https://www.semanticscholar.org/paper/5d47143c13591def061e203bf6dd6d97fd110631,,,,,Yes,,2025-11-10T23:12:44.820Z
lambadabackwardchain-2022,LAMBADA: Backward Chaining for Automated Reasoning in Natural Language,Seyed Mehran Kazemi; Najoung Kim; Deepti Bhatia; Xinyuan Xu; Deepak Ramachandran,2022,Annual Meeting of the Association for Computational Linguistics,87,https://www.semanticscholar.org/paper/03fb95e6be583ca954c3d00812a9e9a40f118e51,http://arxiv.org/pdf/2212.13894,10.48550/arXiv.2212.13894,"Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.",arxiv:2212.13894,Yes,,2025-11-10T23:12:44.820Z
languagemodelsaregre-2022,Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought,Abulhair Saparov; He He,2022,International Conference on Learning Representations,384,https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a,http://arxiv.org/pdf/2210.01240,10.48550/arXiv.2210.01240,"Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.",arxiv:2210.01240,Yes,,2025-11-10T23:12:44.820Z
languagemodelsshowhu-2022,Language models show human-like content effects on reasoning,Ishita Dasgupta; Andrew Kyle Lampinen; Stephanie C. Y. Chan; Antonia Creswell; D. Kumaran; James L. McClelland; Felix Hill,2022,arXiv.org,207,https://www.semanticscholar.org/paper/290732e9fb08a29af8892a7c1f73c9d2a1b9d7db,http://arxiv.org/pdf/2207.07051,10.48550/arXiv.2207.07051,"Reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable""content effects""; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large language models, as well as humans, and find that the language models reflect many of the same patterns observed in humans across these tasks $\unicode{x2014}$ like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected both in answer patterns, and in lower-level features like the relationship between model answer distributions and human response times. Our findings have implications for understanding both these cognitive effects in humans, and the factors that contribute to language model performance.",arxiv:2207.07051,Yes,,2025-11-10T23:12:44.820Z
largelanguagemodelsc-2022,Large Language Models Can Self-Improve,Jiaxin Huang; S. Gu; Le Hou; Yuexin Wu; Xuezhi Wang; Hongkun Yu; Jiawei Han,2022,Conference on Empirical Methods in Natural Language Processing,708,https://www.semanticscholar.org/paper/3fa70115248377c3d1517c9f978791a296fbc1dd,http://arxiv.org/pdf/2210.11610,10.48550/arXiv.2210.11610,"Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate""high-confidence""rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",arxiv:2210.11610,Yes,,2025-11-10T23:12:44.820Z
largelanguagemodelsa-2022,Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors,Mohammad Reza Taesiri; Finlay Macklon; Yihe Wang; Hengshuo Shen; C. Bezemer,2022,arXiv.org,18,https://www.semanticscholar.org/paper/55e3fe05598be7c3dd357d51166869f6571b824f,http://arxiv.org/pdf/2210.02506,10.48550/arXiv.2210.02506,"Video game testing requires game-specific knowledge as well as common sense reasoning about the events in the game. While AI-driven agents can satisfy the first requirement, it is not yet possible to meet the second requirement automatically. Therefore, video game testing often still relies on manual testing, and human testers are required to play the game thoroughly to detect bugs. As a result, it is challenging to fully automate game testing. In this study, we explore the possibility of leveraging the zero-shot capabilities of large language models for video game bug detection. By formulating the bug detection problem as a question-answering task, we show that large language models can identify which event is buggy in a sequence of textual descriptions of events from a game. To this end, we introduce the GameBugDescriptions benchmark dataset, which consists of 167 buggy gameplay videos and a total of 334 question-answer pairs across 8 games. We extensively evaluate the performance of six models across the OPT and InstructGPT large language model families on our benchmark dataset. Our results show promising results for employing language models to detect video game bugs. With the proper prompting technique, we could achieve an accuracy of 70.66%, and on some video games, up to 78.94%. Our code, evaluation data and the benchmark can be found on https://asgaardlab.github.io/LLMxBugs",arxiv:2210.02506,Yes,,2025-11-10T23:12:44.820Z
largelanguagemodelse-2022,Large language models encode clinical knowledge,K. Singhal; Shekoofeh Azizi; T. Tu; S. Mahdavi; Jason Wei; Hyung Won Chung; Nathan Scales; A. Tanwani; H. Cole-Lewis; S. Pfohl; P. Payne; Martin G. Seneviratne; P. Gamble; C. Kelly; Nathaneal Scharli; A. Chowdhery; P. A. Mansfield; B. A. Y. Arcas; D. Webster; Greg S. Corrado; Yossi Matias; K. Chou; Juraj Gottweis; Nenad Tomašev; Yun Liu; A. Rajkomar; J. Barral; Christopher Semturs; A. Karthikesalingam; Vivek Natarajan,2022,Nature,3053,https://www.semanticscholar.org/paper/6052486bc9144dc1730c12bf35323af3792a1fd0,https://www.nature.com/articles/s41586-023-06291-2.pdf,10.1038/s41586-023-06291-2,"Med-PaLM, a state-of-the-art large language model for medicine, is introduced and evaluated across several medical question answering tasks, demonstrating the promise of these models in this domain. Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model^ 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM^ 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA^ 3 , MedMCQA^ 4 , PubMedQA^ 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics^ 6 ), including 67.6% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.",arxiv:2212.13138,Yes,,2025-11-10T23:12:44.820Z
learntoexplainmultim-2022,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,Pan Lu; Swaroop Mishra; Tony Xia; Liang Qiu; Kai-Wei Chang; Song-Chun Zhu; Oyvind Tafjord; Peter Clark; A. Kalyan,2022,Neural Information Processing Systems,1695,https://www.semanticscholar.org/paper/d3135733aa39dec20ce72aa138589dda27c8406d,http://arxiv.org/pdf/2209.09513,10.48550/arXiv.2209.09513,"When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https://scienceqa.github.io.",arxiv:2209.09513,Yes,,2025-11-10T23:12:44.820Z
learningtoreasonwith-2022,Learning to Reason With Relational Abstractions,A. Nam; Mengye Ren; Chelsea Finn; James L. McClelland,2022,arXiv.org,5,https://www.semanticscholar.org/paper/6d5555348f453bac901c5b57e8a4eeb3074b4071,https://arxiv.org/pdf/2210.02615,10.48550/arXiv.2210.02615,"Large language models have recently shown promising progress in mathematical reasoning when fine-tuned with human-generated sequences walking through a sequence of solution steps. However, the solution sequences are not formally structured and the resulting model-generated sequences may not reflect the kind of systematic reasoning we might expect an expert human to produce. In this paper, we study how to build stronger reasoning capability in language models using the idea of relational abstractions. We introduce new types of sequences that more explicitly provide an abstract characterization of the transitions through intermediate solution steps to the goal state. We find that models that are supplied with such sequences as prompts can solve tasks with a significantly higher accuracy, and models that are trained to produce such sequences solve problems better than those that are trained with previously used human-generated sequences and other baselines. Our work thus takes several steps toward elucidating and improving how language models perform on tasks requiring multi-step mathematical reasoning.",arxiv:2210.02615,Yes,,2025-11-10T23:12:44.820Z
leasttomostprompting-2022,Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,Denny Zhou; Nathanael Scharli; Le Hou; Jason Wei; Nathan Scales; Xuezhi Wang; D. Schuurmans; O. Bousquet; Quoc Le; Ed H. Chi,2022,International Conference on Learning Representations,1366,https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321,http://arxiv.org/pdf/2205.10625,10.48550/arXiv.2205.10625,"Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",arxiv:2205.10625,Yes,,2025-11-10T23:12:44.820Z
legalpromptingteachi-2022,Legal Prompting: Teaching a Language Model to Think Like a Lawyer,Fang Yu; Lee Quartey; Frank Schilder,2022,arXiv.org,81,https://www.semanticscholar.org/paper/cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3,http://arxiv.org/pdf/2212.01326,10.48550/arXiv.2212.01326,"Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought (CoT) prompts can improve arithmetic or common sense tasks significantly. We explore how such approaches fare with legal reasoning tasks and take the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning approaches. Our findings show that while CoT prompting and fine-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from specific legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.",arxiv:2212.01326,Yes,,2025-11-10T23:12:44.820Z
lilaaunifiedbenchmar-2022,Lila: A Unified Benchmark for Mathematical Reasoning,Swaroop Mishra; Pan Lu; A. Kalyan,2022,,0,https://www.semanticscholar.org/paper/a630c70aed27b52f6d04d1e772b153c5a7b6f6fe,,,"Mathematical reasoning skills are essential for general-purpose intelligent systems to perform tasks from grocery shopping to climate modeling. Towards evaluating and improving AI systems in this domain, we propose LILA, a unified mathematical reasoning benchmark consisting of 23 diverse tasks along four dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs, thereby obtaining explainable solutions in addition to the correct answer. We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation. Finally, we introduce BHASKARA, a general-purpose mathematical reasoning model trained on LILA. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83% F1 score vs. single-task models), while the best performing model only obtains 60.40%, indicating the room for improvement in general mathematical reasoning and understanding.",arxiv:2210.17517,Yes,,2025-11-10T23:12:44.820Z
littleredridinghoodg-2022,Little Red Riding Hood Goes around the Globe: Crosslingual Story Planning and Generation with Large Language Models,E. Razumovskaia; Joshua Maynez; Annie Louis; Mirella Lapata; Shashi Narayan,2022,International Conference on Language Resources and Evaluation,5,https://www.semanticscholar.org/paper/30cc7ae95583ade1f05226c08c6f6609777aeedd,http://arxiv.org/pdf/2212.10471,10.48550/arXiv.2212.10471,"Previous work has demonstrated the effectiveness of planning for story generation exclusively in a monolingual setting focusing primarily on English. We consider whether planning brings advantages to automatic story generation across languages. We propose a new task of crosslingual story generation with planning and present a new dataset for this task. We conduct a comprehensive study of different plans and generate stories in several languages, by leveraging the creative and reasoning capabilities of large pretrained language models. Our results demonstrate that plans which structure stories into three acts lead to more coherent and interesting narratives, while allowing to explicitly control their content and structure.",arxiv:2212.10471,Yes,,2025-11-10T23:12:44.820Z
mrklsystemsamodularn-2022,"MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning",Ehud Karpas; Omri Abend; Yonatan Belinkov; Barak Lenz; Opher Lieber; Nir Ratner; Y. Shoham; Hofit Bata; Yoav Levine; Kevin Leyton-Brown; Dor Muhlgay; N. Rozen; Erez Schwartz; Gal Shachaf; Shai Shalev-Shwartz; A. Shashua; Moshe Tenenholtz,2022,arXiv.org,93,https://www.semanticscholar.org/paper/1bcde55995a957b3e8a595d536b816cb8989cf1d,http://arxiv.org/pdf/2205.00445,10.48550/arXiv.2205.00445,"Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced""miracle"") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.",arxiv:2205.00445,Yes,,2025-11-10T23:12:44.820Z
mathematicallanguage-2022,Mathematical language shapes how we understand the economy,W. Arthur,2022,,0,https://www.semanticscholar.org/paper/51aae5840a02bafe5368753e31b4168eeacdf841,,,,,Yes,,2025-11-10T23:12:44.820Z
measurementevaluatio-2022,"Measurement, Evaluation, and Model Construction of Mathematical Literacy Based on IoT and PISA",Yunfeng Chen,2022,Mathematical Problems in Engineering,4,https://www.semanticscholar.org/paper/da5c08f5175374114940aec48ddc6dcba494dab3,https://downloads.hindawi.com/journals/mpe/2022/3278401.pdf,10.1155/2022/3278401,"“Mathematics Curriculum Standard for Ordinary Senior High School” points out that mathematical modelling literacy is the literacy of abstracting real problems mathematically, expressing problems with mathematical language, and building models with mathematical methods to solve problems. It assesses the amount to which students who are about to finish compulsory schooling have the knowledge and aptitude to deal with future life issues, based on the notion of lifelong learning. The usage of a local integrated development environment requires a number of complex processes, including installation and setup, as well as the procurement of necessary hardware. In reality, in order to nurture students, the core literacy of mathematics is to completely execute quality education, further identify the training and development direction of talents, and infiltrate the core literacy material into junior high school mathematics classroom instruction. For such a large-scale international education measurement project, the research and development of test questions is very important. Therefore, detailed technical consideration has been carried out and test volumes have been designed that are relatively suitable for different countries. The test questions are closely related to life, while there are few questions related to real situational problems in the test, which are mathematically processed first. Due to the international background of evaluation, it is a great challenge for researchers to realize the localization of evaluation on the basis of adapting to the domestic situation. At the same time, there is no scientific and perfect mathematical modelling literacy evaluation system in China’s basic education research. How can middle school front-line teachers better evaluate students’ mathematical modelling literacy? As a result, it is critical to develop a scientific and quantitative mathematical literacy measurement model that will aid in the teaching and research of mathematical modelling by middle school front-line teachers, as well as contributing theoretically to the evaluation and research of mathematical modelling literacy in China. Therefore, based on mathematics literacy evaluation, this paper studies the hierarchy of the IoT and the measurement and evaluation model of mathematics literacy based on the IoT, as well as its enlightenment to the compilation of mathematics academic test questions in China.",,Yes,,2025-11-10T23:12:44.820Z
medmcqaalargescalemu-2022,MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering,Ankit Pal; Logesh Kumar Umapathi; Malaikannan Sankarasubbu,2022,"ACM Conference on Health, Inference, and Learning",461,https://www.semanticscholar.org/paper/741776172685b9717159a9fcd21841461bb33b14,http://arxiv.org/pdf/2203.14371,10.48550/arXiv.2203.14371,"This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS \&NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects \&topics. A detailed explanation of the solution, along with the above information, is provided in this study.",arxiv:2203.14371,Yes,,2025-11-10T23:12:44.820Z
mindseyegroundedlang-2022,Mind's Eye: Grounded Language Model Reasoning through Simulation,Ruibo Liu; Jason Wei; S. Gu; Te-Yen Wu; Soroush Vosoughi; Claire Cui; Denny Zhou; Andrew M. Dai,2022,International Conference on Learning Representations,89,https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8,http://arxiv.org/pdf/2210.05359,10.48550/arXiv.2210.05359,"Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.",arxiv:2210.05359,Yes,,2025-11-10T23:12:44.820Z
multistepdeductivere-2022,Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation,Qiming Bao; A. Peng; Tim Hartill; N. Tan; Zhenyun Deng; M. Witbrock; Jiamou Liu,2022,International Workshop on Neural-Symbolic Learning and Reasoning,16,https://www.semanticscholar.org/paper/4a52399e66da3fb1406132ecedf274925b5f7972,http://arxiv.org/pdf/2207.14000,10.48550/arXiv.2207.14000,"Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gated attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datasets, we develop PARARULE-Plus, a large dataset with more examples that require deeper reasoning steps. Experimental results show that the addition of PARARULE-Plus can increase the model's performance on examples requiring deeper reasoning depths. The source code and data are available at https://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.",arxiv:2207.14000,Yes,,2025-11-10T23:12:44.820Z
nlxgptamodelfornatur-2022,NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks,Fawaz Sammani; Tanmoy Mukherjee; Nikos Deligiannis,2022,Computer Vision and Pattern Recognition,72,https://www.semanticscholar.org/paper/bc64190d42d9dc34077b6a096d9053bb88deaa3a,https://arxiv.org/pdf/2203.05081,10.1109/CVPR52688.2022.00814,"Natural language explanation (NLE) models aim at explaining the decision-making process of a black box system via generating natural language sentences which are human-friendly, high-level and fine-grained. Current NLE models11Throughout this paper, we refer to NLE models as Natural Language Explanation models aimed for vision and vision-language tasks. explain the decision-making process of a vision or vision-language model (a.k.a., task model), e.g., a VQA model, via a language model (a.k.a., explanation model), e.g., GPT. Other than the additional memory resources and inference time required by the task model, the task and explanation models are completely independent, which disassociates the explanation from the reasoning process made to predict the answer. We introduce NLX-GPT, a general, compact and faithful language model that can simultaneously predict an answer and explain it. We first conduct pre-training on large scale data of image-caption pairs for general understanding of images, and then formulate the answer as a text prediction task along with the explanation. Without region proposals nor a task model, our resulting overall framework attains better evaluation scores, contains much less parameters and is 15× faster than the current SoA model. We then address the problem of evaluating the explanations which can be in many times generic, data-biased and can come in several forms. We therefore design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based attack, a selfevaluation framework that requires no labels. Code is at: https://github.com/fawazsammani/nlxgpt.",arxiv:2203.05081,Yes,,2025-11-10T23:12:44.820Z
naturalprovergrounde-2022,NaturalProver: Grounded Mathematical Proof Generation with Language Models,S. Welleck; Jiacheng Liu; Ximing Lu; Hannaneh Hajishirzi; Yejin Choi,2022,Neural Information Processing Systems,85,https://www.semanticscholar.org/paper/d593b9b8d63426f0d6a795dd7f2294619bc03610,https://arxiv.org/pdf/2205.12910,10.48550/arXiv.2205.12910,"Theorem proving in natural mathematical language - the mixture of symbolic and natural language used by humans - plays a central role in mathematical advances and education, and tests aspects of reasoning that are core to intelligence. Yet it has remained underexplored with modern generative models. We study large-scale language models on two new generation tasks: suggesting the next step in a mathematical proof, and full proof generation. We develop NaturalProver, a language model that generates proofs by conditioning on background references (e.g. theorems and definitions that are either retrieved or human-provided), and optionally enforces their presence with constrained decoding. On theorems from the NaturalProofs benchmark, NaturalProver improves the quality of next-step suggestions and generated proofs over fine-tuned GPT-3, according to human evaluations from university-level mathematics students. NaturalProver is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40% of the time, which is to our knowledge the first demonstration of these capabilities using neural language models.",arxiv:2205.12910,Yes,,2025-11-10T23:12:44.820Z
numglueasuiteoffunda-2022,NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks,Swaroop Mishra; Arindam Mitra; Neeraj Varshney; Bhavdeep Singh Sachdeva; Peter Clark; Chitta Baral; A. Kalyan,2022,Annual Meeting of the Association for Computational Linguistics,118,https://www.semanticscholar.org/paper/39238a92de090c104936a4f78375b95600e42ce5,http://arxiv.org/pdf/2204.05660,10.48550/arXiv.2204.05660,"Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 %). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 % on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.",arxiv:2204.05660,Yes,,2025-11-10T23:12:44.820Z
optimlscalinglanguag-2022,OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization,S. Iyer; Xi Victoria Lin; Ramakanth Pasunuru; Todor Mihaylov; Daniel Simig; Ping Yu; Kurt Shuster; Tianlu Wang; Qing Liu; Punit Singh Koura; Xian Li; Brian O'Horo; Gabriel Pereyra; Jeff Wang; Christopher Dewan; Asli Celikyilmaz; Luke S. Zettlemoyer; Veselin Stoyanov,2022,arXiv.org,291,https://www.semanticscholar.org/paper/e965e93e76a9e6c4e4863d145b5c007b540d575d,,,"Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.",arxiv:2212.12017,Yes,,2025-11-10T23:12:44.820Z
onsecondthoughtletsn-2022,"On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",Omar Shaikh; Hongxin Zhang; William B. Held; Michael Bernstein; Diyi Yang,2022,Annual Meeting of the Association for Computational Linguistics,229,https://www.semanticscholar.org/paper/b1b8c3e47f44158d22fb70bb453d2494ed013b70,http://arxiv.org/pdf/2212.08061,10.48550/arXiv.2212.08061,"Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model’s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.",arxiv:2212.08061,Yes,,2025-11-10T23:12:44.820Z
optimizinglanguagemo-2022,Optimizing Language Models for Argumentative Reasoning,Luke Thorburn; Ariel Kruger,2022,ArgML@COMMA,15,https://www.semanticscholar.org/paper/ae3a6bbe22ea136280e2927807775b3ac8356440,,,,,Yes,,2025-11-10T23:12:44.820Z
overcomingbarriersto-2022,Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic,Mandar Sharma; N. Muralidhar; Naren Ramakrishnan,2022,arXiv.org,6,https://www.semanticscholar.org/paper/1ec4ae64ded99846bcd8a1a3b16fe2875f55c7c1,http://arxiv.org/pdf/2211.02098,10.48550/arXiv.2211.02098,"Through their transfer learning abilities, highly-parameterized large pre-trained language models have dominated the NLP landscape for a multitude of downstream language tasks. Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning. However, as we illustrate in this paper, building a general purpose language model that also happens to be proficient in mathematical reasoning is not as straight-forward as training it on a numeric dataset. In this work, we develop a novel framework that enables language models to be mathematically proficient while retaining their linguistic prowess. Specifically, we offer information-theoretic interventions to overcome the catastrophic forgetting of linguistic skills that occurs while injecting non-linguistic skills into language models.",arxiv:2211.02098,Yes,,2025-11-10T23:12:44.820Z
palmscalinglanguagem-2022,PaLM: Scaling Language Modeling with Pathways,A. Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; P. Barham; Hyung Won Chung; Charles Sutton; Sebastian Gehrmann; Parker Schuh; Kensen Shi; Sasha Tsvyashchenko; Joshua Maynez; Abhishek Rao; Parker Barnes; Yi Tay; Noam M. Shazeer; Vinodkumar Prabhakaran; Emily Reif; Nan Du; Ben Hutchinson; Reiner Pope; James Bradbury; Jacob Austin; M. Isard; Guy Gur-Ari; Pengcheng Yin; Toju Duke; Anselm Levskaya; S. Ghemawat; Sunipa Dev; H. Michalewski; Xavier García; Vedant Misra; Kevin Robinson; L. Fedus; Denny Zhou; Daphne Ippolito; D. Luan; Hyeontaek Lim; Barret Zoph; A. Spiridonov; Ryan Sepassi; David Dohan; Shivani Agrawal; Mark Omernick; Andrew M. Dai; Thanumalayan Sankaranarayana Pillai; Marie Pellat; Aitor Lewkowycz; Erica Moreira; R. Child; Oleksandr Polozov; Katherine Lee; Zongwei Zhou; Xuezhi Wang; Brennan Saeta; Mark Díaz; Orhan Firat; Michele Catasta; Jason Wei; K. Meier-Hellstern; D. Eck; J. Dean; Slav Petrov; Noah Fiedel,2022,Journal of machine learning research,7113,https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb,,,"Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.",arxiv:2204.02311,Yes,,2025-11-10T23:12:44.820Z
parselaunifiednatura-2022,Parsel: A Unified Natural Language Framework for Algorithmic Reasoning,E. Zelikman; Qian Huang; Gabriel Poesia; Noah D. Goodman; Nick Haber,2022,arXiv.org,16,https://www.semanticscholar.org/paper/239b5649b12f28fd610de036afba41b9246db6c9,http://arxiv.org/pdf/2212.10561,10.48550/arXiv.2212.10561,,,Yes,,2025-11-10T23:12:44.820Z
parselalgorithmicrea-2022,Parsel🦆: Algorithmic Reasoning with Language Models by Composing Decompositions,E. Zelikman; Qian Huang; Gabriel Poesia; Noah D. Goodman; Nick Haber,2022,Neural Information Processing Systems,67,https://www.semanticscholar.org/paper/e325fe41c8c1d547ccd102ac82be3ec8b23960f2,,,"Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\% to 85\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel",arxiv:2212.10561,Yes,,2025-11-10T23:12:44.820Z
planbenchanextensibl-2022,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,Karthik Valmeekam; Alberto Olmo; S. Sreedharan; Subbarao Kambhampati,2022,Neural Information Processing Systems,306,https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc,,,"Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.",arxiv:2206.10498,Yes,,2025-11-10T23:12:44.820Z
propositionalreasoni-2022,Propositional Reasoning via Neural Transformer Language Models,Oscar J. Romero; A. Tomasic; A. Steinfeld; John Zimmerman,2022,International Workshop on Neural-Symbolic Learning and Reasoning,3,https://www.semanticscholar.org/paper/ca68b7b6a6f062da58453a48898e1f14b4200a27,,,,,Yes,,2025-11-10T23:12:44.820Z
roscoeasuiteofmetric-2022,ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning,O. Yu. Golovneva; Moya Chen; Spencer Poff; Martin Corredor; Luke Zettlemoyer; Maryam Fazel-Zarandi; Asli Celikyilmaz,2022,arXiv.org,189,https://www.semanticscholar.org/paper/391246ce9c59d61c94cca3f8bef56c95542a4708,https://arxiv.org/pdf/2212.07919,10.48550/arXiv.2212.07919,"Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics.",arxiv:2212.07919,Yes,,2025-11-10T23:12:44.820Z
reactsynergizingreas-2022,ReAct: Synergizing Reasoning and Acting in Language Models,Shunyu Yao; Jeffrey Zhao; Dian Yu; Nan Du; Izhak Shafran; Karthik Narasimhan; Yuan Cao,2022,International Conference on Learning Representations,4452,https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d,,,"While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",arxiv:2210.03629,Yes,,2025-11-10T23:12:44.820Z
responsiblereasoning-2022,Responsible Reasoning with Large Language Models and the Impact of Proper Nouns,Sumit Kumar Jha,2022,,6,https://www.semanticscholar.org/paper/a70fcd82d8d26bf4c8b0bdde88e96b6ce2c80ecf,,,,,Yes,,2025-11-10T23:12:44.820Z
rethinkingwithretrie-2022,Rethinking with Retrieval: Faithful Large Language Model Inference,Hangfeng He; Hongming Zhang; D. Roth,2022,arXiv.org,191,https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1,http://arxiv.org/pdf/2301.00303,10.48550/arXiv.2301.00303,"Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.",arxiv:2301.00303,Yes,,2025-11-10T23:12:44.820Z
starbootstrappingrea-2022,STaR: Bootstrapping Reasoning With Reasoning,E. Zelikman; Yuhuai Wu; Noah D. Goodman,2022,,651,https://www.semanticscholar.org/paper/23dd78e424d32f6a48660dcd67ce994b8a7db8be,,,"Generating step-by-step""chain-of-thought""rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the""Self-Taught Reasoner""(STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30$\times$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.",arxiv:2203.14465,Yes,,2025-11-10T23:12:44.820Z
selectioninferenceex-2022,Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning,Antonia Creswell; M. Shanahan; I. Higgins,2022,International Conference on Learning Representations,408,https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd,,,"Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",arxiv:2205.09712,Yes,,2025-11-10T23:12:44.820Z
selfconsistencyimpro-2022,Self-Consistency Improves Chain of Thought Reasoning in Language Models,Xuezhi Wang; Jason Wei; D. Schuurmans; Quoc Le; Ed H. Chi; Denny Zhou,2022,International Conference on Learning Representations,4975,https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2,,,"Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",arxiv:2203.11171,Yes,,2025-11-10T23:12:44.820Z
smoothquantaccuratea-2022,SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models,Guangxuan Xiao; Ji Lin; Mickael Seznec; Julien Demouth; Song Han,2022,International Conference on Machine Learning,1083,https://www.semanticscholar.org/paper/2c994fadbb84fb960d8306ee138dbeef41a5b323,http://arxiv.org/pdf/2211.10438,10.48550/arXiv.2211.10438,"Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.",arxiv:2211.10438,Yes,,2025-11-10T23:12:44.820Z
socraticmodelscompos-2022,Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language,Andy Zeng; Adrian S. Wong; Stefan Welker; K. Choromanski; F. Tombari; Aveek Purohit; M. Ryoo; Vikas Sindhwani; Johnny Lee; Vincent Vanhoucke; Peter R. Florence,2022,International Conference on Learning Representations,645,https://www.semanticscholar.org/paper/ada81a4de88a6ce474df2e2446ad11fea480616e,,,"Large pretrained (e.g.,""foundation"") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",arxiv:2204.00598,Yes,,2025-11-10T23:12:44.820Z
solvingmathwordprobl-2022,Solving Math Word Problem via Cooperative Reasoning induced Language Models,Xinyu Zhu; Junjie Wang; Lin Zhang; Yuxiang Zhang; Ruyi Gan; Jiaxing Zhang; Yujiu Yang,2022,arXiv.org,24,https://www.semanticscholar.org/paper/01f7bb1f9c611b5e849558e445fdccb98a3a3040,http://arxiv.org/pdf/2210.16257,10.48550/arXiv.2210.16257,,,Yes,,2025-11-10T23:12:44.820Z
solvingquantitativer-2022,Solving Quantitative Reasoning Problems with Language Models,Aitor Lewkowycz; Anders Andreassen; David Dohan; Ethan Dyer; H. Michalewski; V. Ramasesh; Ambrose Slone; Cem Anil; Imanol Schlag; Theo Gutman-Solo; Yuhuai Wu; Behnam Neyshabur; Guy Gur-Ari; Vedant Misra,2022,Neural Information Processing Systems,1192,https://www.semanticscholar.org/paper/ab0e3d3e4d42369de5933a3b4c237780b41c0d77,http://arxiv.org/pdf/2206.14858,10.48550/arXiv.2206.14858,"Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.",arxiv:2206.14858,Yes,,2025-11-10T23:12:44.820Z
structuredflexiblean-2022,"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",K. M. Collins; Catherine Wong; Jiahai Feng; Megan Wei; J. Tenenbaum,2022,Annual Meeting of the Cognitive Science Society,66,https://www.semanticscholar.org/paper/7ef9aafc68511afab5b287e62b754576ea37b4ce,http://arxiv.org/pdf/2205.05718,10.48550/arXiv.2205.05718,"Human language offers a powerful window into our thoughts -- we tell stories, give explanations, and express our beliefs and goals through words. Abundant evidence also suggests that language plays a developmental role in structuring our learning. Here, we ask: how much of human-like thinking can be captured by learning statistical patterns in language alone? We first contribute a new challenge benchmark for comparing humans and distributional large language models (LLMs). Our benchmark contains two problem-solving domains (planning and explanation generation) and is designed to require generalization to new, out-of-distribution problems expressed in language. We find that humans are far more robust than LLMs on this benchmark. Next, we propose a hybrid Parse-and-Solve model, which augments distributional LLMs with a structured symbolic reasoning module. We find that this model shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.",arxiv:2205.05718,Yes,,2025-11-10T23:12:44.820Z
symbolicmathreasonin-2022,Symbolic Math Reasoning with Language Models,Vedant Gaur; Nikunj Saunshi,2022,2022 IEEE MIT Undergraduate Research Technology Conference (URTC),12,https://www.semanticscholar.org/paper/f557f3a32d309373e7d31bb93ca1b80b4a6e39e7,,10.1109/URTC56832.2022.10002218,"The emergence of large language models (LLMs) such as OpenAI’s GPT-3, Google’s LaMDA, Meta’s OPT [2, 3, 7, 10] etc. have revolutionized the field of natural language processing (NLP). These models with upwards of hundreds of billions of parameters are trained on large unlabeled text corpora and can subsequently solve downstream tasks with little to no labeled data. While these models are increasingly versatile in their abilities, e.g., solving math word problems, the larger question of their ability to reason remains. Using and modifying the SVAMP dataset, we find that GPT-3’s davinci-002 model, in addition to having good performance on numerical math word problems, also performs well on the potentially harder symbolic version of the same problems. Furthermore, adopting a two-step approach (solve symbolically and then substitute numerical values) leads to better accuracy on the numerical test set in the zero-shot regime. Additionally, we find that the use of specific prompting techniques pushes the model, in many cases, to actively describe its thought process and aid in the final answer output when faced with a complex, multi-step problem, aligning with recent observations.",,Yes,,2025-11-10T23:12:44.820Z
teachingalgorithmicr-2022,Teaching Algorithmic Reasoning via In-context Learning,Hattie Zhou; Azade Nova; H. Larochelle; Aaron C. Courville; Behnam Neyshabur; Hanie Sedghi,2022,arXiv.org,126,https://www.semanticscholar.org/paper/4d17732d90440682b0500f4e209c6cc4fac20e0e,http://arxiv.org/pdf/2211.09066,10.48550/arXiv.2211.09066,"Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.",arxiv:2211.09066,Yes,,2025-11-10T23:12:44.820Z
testinglargelanguage-2022,Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment,Lorenzo Bertolini; Julie Weeds; David Weir,2022,International Conference on Computational Linguistics,15,https://www.semanticscholar.org/paper/1606793daaa20d4a4a78e859c2fd6b4f7535680c,,,,,Yes,,2025-11-10T23:12:44.820Z
textgraphs2022shared-2022,TextGraphs 2022 Shared Task on Natural Language Premise Selection,Marco Valentino; Deborah Ferreira; Mokanarangan Thayaparan; André Freitas; Dmitry Ustalov,2022,Workshop on Graph-based Methods for Natural Language Processing,12,https://www.semanticscholar.org/paper/43845c54af0246baab17d572953e671487e1dcd1,,,,,Yes,,2025-11-10T23:12:44.820Z
theunreliabilityofex-2022,The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning,Xi Ye; Greg Durrett,2022,Neural Information Processing Systems,214,https://www.semanticscholar.org/paper/9ffefdf1fcd780cb71450b0a7a29247c66aa87be,,,"Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",arxiv:2205.03401,Yes,,2025-11-10T23:12:44.820Z
thinksumprobabilisti-2022,ThinkSum: Probabilistic reasoning over sets using large language models,Batu Mehmet Ozturkler; Nikolay Malkin; Zhen Wang; N. Jojic,2022,Annual Meeting of the Association for Computational Linguistics,23,https://www.semanticscholar.org/paper/370cea8b4220917f45a69358c0303df71f5063c7,http://arxiv.org/pdf/2210.01293,10.48550/arXiv.2210.01293,"Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think – retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum – probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.",arxiv:2210.01293,Yes,,2025-11-10T23:12:44.820Z
towardsreasoninginla-2022,Towards Reasoning in Large Language Models: A Survey,Jie Huang; K. Chang,2022,Annual Meeting of the Association for Computational Linguistics,758,https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45,http://arxiv.org/pdf/2212.10403,10.48550/arXiv.2212.10403,"Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.",arxiv:2212.10403,Yes,,2025-11-10T23:12:44.820Z
towardsamathematicsf-2022,Towards a Mathematics Formalisation Assistant using Large Language Models,Ayush Agrawal; Siddhartha Gadgil; Navin Goyal; Ashvni Narayanan; Anand Tadipatri,2022,arXiv.org,18,https://www.semanticscholar.org/paper/b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce,https://arxiv.org/pdf/2211.07524,10.48550/arXiv.2211.07524,"Mathematics formalisation is the task of writing mathematics (i.e., definitions, theorem statements, proofs) in natural language, as found in books and papers, into a formal language that can then be checked for correctness by a program. It is a thriving activity today, however formalisation remains cumbersome. In this paper, we explore the abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover. We find that with careful inputdependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75% accuracy for 120 theorem statements. For proofs quantitative analysis is infeasible and we undertake a detailed case study. We choose a diverse set of 13 theorems at undergrad level with proofs that fit in two-three paragraphs. We show that with a new prompting strategy Codex can formalise these proofs in natural language with at least one out of twelve Codex completion being easy to repair into a complete proof. This is surprising as essentially no aligned data exists for formalised mathematics, particularly for proofs. These results suggest that large language models are a promising avenue towards fully or partially automating formalisation.",arxiv:2211.07524,Yes,,2025-11-10T23:12:44.820Z
triplefactretrievera-2022,Triple-Fact Retriever: An explainable reasoning retrieval model for multi-hop QA problem,Cheng Wu; Enrui Hu; Ke Zhan; Lan Luo; Xinyu Zhang; Hao Jiang; Qirui Wang; Zhao Cao; Fan Yu; Lei Chen,2022,IEEE International Conference on Data Engineering,8,https://www.semanticscholar.org/paper/dd77bff42060109c9b2e3e7d87cc9342309c3952,,10.1109/icde53745.2022.00095,"Nowadays, multi-hop question answer (QA) problem is challenging and not well solved in the QA community. The dominant bottleneck of the multi-hop QA problem is the need for a reasoning retriever to fetch a document path from an open-domain corpus (e.g., Wikipedia). A reasoning retriever aims to collect an evidence document from large corpora at one hop retrieval and aggregate the evidence for subsequent hop retrieval, which yields a document path after multi-hop retrieval. There exist two challenges, (1) to fetch the evidence document in an efficient and explainable way at one hop retrieval and (2) to update the question information by aggregating the evidence from the retrieved document after each hop retrieval. To address these two challenges, we propose a triple-fact-based retrieval model to effectively retrieve a related document path in an explainable way for each question. We extract a structured representation from the unstructured document and utilize the knowledge of pre-trained language model (PLM) to do the semantic-level matching between the question and document. We evaluate the proposed Triple-fact Retriever model on the recently proposed open-domain multi-hop QA dataset, HotpotQA, and a cross-document multi-step Reading Comprehension dataset, Wikihop. The results11The source code is available on our website: https://github.com/Rebaccamin/triple_retriever. demonstrate that the Triple-fact retriever outperforms the existing baseline retrieval works.",,Yes,,2025-11-10T23:12:44.820Z
unigeounifyinggeomet-2022,UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression,Jiaqi Chen; Tong Li; Jinghui Qin; Pan Lu; Liang Lin; Chongyu Chen; Xiaodan Liang,2022,Conference on Empirical Methods in Natural Language Processing,133,https://www.semanticscholar.org/paper/72fce949725b20428e5f56247fef5c6bd1ce6154,https://arxiv.org/pdf/2212.02746,10.48550/arXiv.2212.02746,"Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning capability on multiple math tasks. However, in essence, these two tasks have similar problem representations and overlapped math knowledge which can improve the understanding and reasoning ability of a deep model on both two tasks. Therefore, we construct a large-scale Unified Geometry problem benchmark, UniGeo, which contains 4,998 calculation problems and 9,543 proving problems. Each proving problem is annotated with a multi-step proof with reasons and mathematical expressions. The proof can be easily reformulated as a proving sequence that shares the same formats with the annotated program sequence for calculation problems. Naturally, we also present a unified multi-task Geometric Transformer framework, Geoformer, to tackle calculation and proving problems simultaneously in the form of sequence generation, which finally shows the reasoning ability can be improved on both two tasks by unifying formulation. Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that aims to predict the mathematical expressions in the problem solution, thus improving the Geoformer model. Experiments on the UniGeo demonstrate that our proposed Geoformer obtains state-of-the-art performance by outperforming task-specific model NGS with over 5.6% and 3.2% accuracies on calculation and proving problems, respectively.",arxiv:2212.02746,Yes,,2025-11-10T23:12:44.820Z
unikgqaunifiedretrie-2022,UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph,Jinhao Jiang; Kun Zhou; Wayne Xin Zhao; Ji-rong Wen,2022,International Conference on Learning Representations,122,https://www.semanticscholar.org/paper/2d01da2c9ece0969d6ec56d22f78caf57050fc03,http://arxiv.org/pdf/2212.00959,10.48550/arXiv.2212.00959,"Multi-hop Question Answering over Knowledge Graph~(KGQA) aims to find the answer entities that are multiple hops away from the topic entities mentioned in a natural language question on a large-scale Knowledge Graph (KG). To cope with the vast search space, existing work usually adopts a two-stage approach: it first retrieves a relatively small subgraph related to the question and then performs the reasoning on the subgraph to find the answer entities accurately. Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence. In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning. For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model~(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs. For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies. Compared with previous studies, our approach is more unified, tightly relating the retrieval and reasoning stages. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our method on the multi-hop KGQA task. Our codes and data are publicly available at~\url{https://github.com/RUCAIBox/UniKGQA}.",arxiv:2212.00959,Yes,,2025-11-10T23:12:44.820Z
unpackinglargelangua-2022,Unpacking Large Language Models with Conceptual Consistency,Pritish Sahu; Michael Cogswell; Yunye Gong; Ajay Divakaran,2022,arXiv.org,18,https://www.semanticscholar.org/paper/4f4a80148cb8f328aeaee68b34f9797cfb5ea150,http://arxiv.org/pdf/2209.15093,10.48550/arXiv.2209.15093,"If a Large Language Model (LLM) answers""yes""to the question""Are mountains tall?""then does it know what a mountain is? Can you rely on it responding correctly or incorrectly to other questions about mountains? The success of Large Language Models (LLMs) indicates they are increasingly able to answer queries like these accurately, but that ability does not necessarily imply a general understanding of concepts relevant to the anchor query. We propose conceptual consistency to measure a LLM's understanding of relevant concepts. This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are. To compute it we extract background knowledge by traversing paths between concepts in a knowledge base and then try to predict the model's response to the anchor query from the background knowledge. We investigate the performance of current LLMs in a commonsense reasoning setting using the CSQA dataset and the ConceptNet knowledge base. While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency. Our analysis also shows significant variation in conceptual consistency across different kinds of relations, concepts, and prompts. This serves as a step toward building models that humans can apply a theory of mind to, and thus interact with intuitively.",arxiv:2209.15093,Yes,,2025-11-10T23:12:44.820Z
visualspatialreasoni-2022,Visual Spatial Reasoning,Fangyu Liu; Guy Edward Toh Emerson; Nigel Collier,2022,Transactions of the Association for Computational Linguistics,238,https://www.semanticscholar.org/paper/354b48677e314ef2f47512c5a81723cfd17dd05d,https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00566/2138360/tacl_a_00566.pdf,10.1162/tacl_a_00566,"Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (e.g., under, in front of, facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: The human ceiling is above 95%, while state-of-the-art models only achieve around 70%. We observe that VLMs’ by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.1",arxiv:2205.00363,Yes,,2025-11-10T23:12:44.820Z
whatdolargelanguagem-2022,What do Large Language Models Learn beyond Language?,Avinash Madasu; Shashank Srivastava,2022,Conference on Empirical Methods in Natural Language Processing,5,https://www.semanticscholar.org/paper/5efab88c0cdb11c795fa8f44a5d31b40e2a1c261,http://arxiv.org/pdf/2210.12302,10.48550/arXiv.2210.12302,"Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful `inductive biases' for non-linguistic reasoning. On a set of 19 diverse non-linguistic tasks involving quantitative computations, recognizing regular expressions and reasoning over strings. We find that pretrained models significantly outperform comparable non-pretrained neural models. This remains true also in experiments with training non-pretrained models with fewer parameters to account for model regularization effects. We further explore the effect of text domain on LMs by pretraining models from text from different domains and provenances. Our experiments surprisingly reveal that the positive effects of pre-training persist even when pretraining on multi-lingual text or computer code, and even for text generated from synthetic languages. Our findings suggest a hitherto unexplored deep connection between pre-training and inductive learning abilities of language models.",arxiv:2210.12302,Yes,,2025-11-10T23:12:44.820Z
