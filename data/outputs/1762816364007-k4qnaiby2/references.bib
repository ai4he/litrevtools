@article{stolfo2022causalframework,
  title={A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models},
  author={Alessandro Stolfo and Zhijing Jin and Kumar Shridhar and B. Scholkopf and Mrinmaya Sachan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.12023},
  url={https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe},
  abstract={We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution.By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems.Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.},
  keywords={arxiv:2210.12023}
}

@misc{desogus2022contributionrelationship,
  title={A Contribution on Relationship Banking. Economic, Anthropological and Mathematical Reasoning, Empirical Evidence from Italy},
  author={Marco Desogus and Elisa Casu},
  year={2022},
  url={https://www.semanticscholar.org/paper/dd88cc9b1f6d71ef82631b4e1c98c077ccdf291a}
}

@article{lu2022surveydeep,
  title={A Survey of Deep Learning for Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Wenhao Yu and S. Welleck and Kai-Wei Chang},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10535},
  url={https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d},
  abstract={Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.},
  keywords={arxiv:2212.10535}
}

@article{yu2022alertadapt,
  title={ALERT: Adapt Language Models to Reasoning Tasks},
  author={Ping Yu and Tianlu Wang and O. Yu. Golovneva and Badr AlKhamissi and Gargi Ghosh and Mona T. Diab and Asli Celikyilmaz},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.08286},
  url={https://www.semanticscholar.org/paper/95c11cc5820ba32c60d5f2671f6567b9914a4978},
  abstract={Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.To address this question, we introduce \{pasted macro ‘OUR’\}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. \{pasted macro ‘OUR’\}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using \{pasted macro ‘OUR’\}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.},
  keywords={arxiv:2212.08286}
}

@article{bellomarini2022overviewvadalog,
  title={An Overview of Vadalog: a System for Reasoning over Large Knowledge Graphs},
  author={Luigi Bellomarini and Davide Benedetto and Emanuel Sallinger},
  year={2022},
  booktitle={Sistemi Evoluti per Basi di Dati},
  url={https://www.semanticscholar.org/paper/83dc0eca1a453e2970d32923bb48bb84976bd968}
}

@article{wu2022autoformalizationwith,
  title={Autoformalization with Large Language Models},
  author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2205.12615},
  url={https://www.semanticscholar.org/paper/c28e95a06dfcf13fc65a1cac83722f53e34f12a5},
  abstract={Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion (\$25.3\textbackslash\{\}\%\$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from \$29.6\textbackslash\{\}\%\$ to \$35.2\textbackslash\{\}\%\$.},
  keywords={arxiv:2205.12615}
}

@article{zhang2022automaticchain,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alexander J. Smola},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/90350aa626bed47b02d0c162462e5b0ca82be6b2},
  abstract={Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like"Let's think step by step"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the"Let's think step by step"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
  keywords={arxiv:2210.03493}
}

@article{raman2022capecorrective,
  title={CAPE: Corrective Actions from Precondition Errors using Large Language Models},
  author={S. S. Raman and Vanya Cohen and Eric Rosen and Ifrah Idrees and D. Paulius and Stefanie Tellex},
  year={2022},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA57147.2024.10611376},
  url={https://www.semanticscholar.org/paper/c82d8d80ea68400adb7faebb2f1cff38dd83093a},
  abstract={Extracting knowledge and reasoning from large language models (LLMs) offers a path to designing intelligent robots. Common approaches that leverage LLMs for planning are unable to recover when actions fail and resort to retrying failed actions without resolving the underlying cause. We propose a novel approach (CAPE) that generates corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans through few-shot reasoning on action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while maintaining semantic correctness and minimizing re-prompting. In VirtualHome, CAPE improves a human-annotated plan correctness metric from 28.89\% to 49.63\% over SayCan, whilst achieving competitive executability. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves correctness by 76.49\% with higher executability compared to SayCan. Our approach enables embodied agents to follow natural language commands and robustly recover from failures.},
  keywords={arxiv:2211.09935}
}

@article{lindstrm2022clevrmathdataset,
  title={CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning},
  author={Adam Dahlgren Lindström and Savitha Sam Abraham},
  year={2022},
  booktitle={International Workshop on Neural-Symbolic Learning and Reasoning},
  doi={10.48550/arXiv.2208.05358},
  url={https://www.semanticscholar.org/paper/74cc3d340039c67bdabaef090d1386fe2c5376ca},
  abstract={We introduce CLEVR-Math, a multi-modal math word problems dataset consisting of simple math word problems involving addition/subtraction, represented partly by a textual description and partly by an image illustrating the scenario. The text describes actions performed on the scene that is depicted in the image. Since the question posed may not be about the scene in the image, but about the state of the scene before or after the actions are applied, the solver envision or imagine the state changes due to these actions. Solving these word problems requires a combination of language, visual and mathematical reasoning. We apply state-of-the-art neural and neuro-symbolic models for visual question answering on CLEVR-Math and empirically evaluate their performances. Our results show how neither method generalise to chains of operations. We discuss the limitations of the two in addressing the task of multi-modal word problem solving.},
  keywords={arxiv:2208.05358}
}

@misc{krell2022crosscontaminationaccelerating,
  title={CROSS-CONTAMINATION: ACCELERATING LARGE LANGUAGE MODELS WITHOUT IMPACTING PERFORMANCE},
  author={M. M. Krell and Matej Kosec},
  year={2022},
  url={https://www.semanticscholar.org/paper/85cac89ba01a07f3dbf6dbb1e0c56067a3105714}
}

@article{behnamghader2022retrieveraugmentedlanguage,
  title={Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model},
  author={Parishad BehnamGhader and Santiago Miret and Siva Reddy},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2212.09146},
  url={https://www.semanticscholar.org/paper/e4758d05c3d4231dd30c656330e156ccc9dbb07b},
  abstract={Augmenting pretrained language models with retrievers to select the supporting documents has shown promise in effectively solving common NLP problems, including language modeling and question answering, in an interpretable way. In this paper, we first study the strengths and weaknesses of different retriever-augmented language models (REALM, \$k\$NN-LM, FiD coupled with DPR, and ATLAS and Flan-T5 coupled with Contriever) in reasoning over the retrieved statements in different tasks. We show how the retrieve-then-read models' limitations in reasoning are rooted both in the retriever module as well as the language model. Our experimental results demonstrate that the similarity metric used by the retrievers is generally insufficient for reasoning tasks. Additionally, we show that the language models in retriever-augmented models do not take the complicated relations between the statements into account, which leads to poor reasoning performance even when using the larger models. Moreover, we analyze the reasoning performance of large language models using multihop retrieval but we only observe minor improvements. Overall, this shows great room for further research in this area.},
  keywords={arxiv:2212.09146}
}

@article{wei2022chainthought,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5},
  abstract={We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  keywords={arxiv:2201.11903}
}

@article{liang2022codepolicies,
  title={Code as Policies: Language Model Programs for Embodied Control},
  author={Jacky Liang and Wenlong Huang and F. Xia and Peng Xu and Karol Hausman and Brian Ichter and Peter R. Florence and Andy Zeng},
  year={2022},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA48891.2023.10160591},
  url={https://www.semanticscholar.org/paper/91deaf9d324c8feafc189da0da03e60a60287bca},
  abstract={Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  keywords={arxiv:2209.07753}
}

@article{fu2022complexitybasedprompting,
  title={Complexity-Based Prompting for Multi-Step Reasoning},
  author={Yao Fu and Hao-Chun Peng and Ashish Sabharwal and Peter Clark and Tushar Khot},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.00720},
  url={https://www.semanticscholar.org/paper/c88cafa3e980765a64febe369ceb7c2aa7261d2a},
  abstract={We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.},
  keywords={arxiv:2210.00720}
}

@article{chen2022convfinqaexploring,
  title={ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering},
  author={Zhiyu Chen and SHIYANG LI and Charese Smiley and Zhiqiang Ma and Sameena Shah and William Yang Wang},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.03849},
  url={https://www.semanticscholar.org/paper/d96997265f8146e93b4c9350f19d55e46d1317f0},
  abstract={With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/ConvFinQA.},
  keywords={arxiv:2210.03849}
}

@article{jonsson2022creativemathematical,
  title={Creative Mathematical Reasoning: Does Need for Cognition Matter?},
  author={B. Jonsson and Julia Mossegård and Johan Lithner and Linnea Karlsson Wirebring},
  year={2022},
  booktitle={Frontiers in Psychology},
  doi={10.3389/fpsyg.2021.797807},
  url={https://www.semanticscholar.org/paper/9f6f01cba1158e6bcb17aaa43070ef3b64c59550},
  abstract={A large portion of mathematics education centers heavily around imitative reasoning and rote learning, raising concerns about students’ lack of deeper and conceptual understanding of mathematics. To address these concerns, there has been a growing focus on students learning and teachers teaching methods that aim to enhance conceptual understanding and problem-solving skills. One suggestion is allowing students to construct their own solution methods using creative mathematical reasoning (CMR), a method that in previous studies has been contrasted against algorithmic reasoning (AR) with positive effects on test tasks. Although previous studies have evaluated the effects of CMR, they have ignored if and to what extent intrinsic cognitive motivation play a role. This study investigated the effects of intrinsic cognitive motivation to engage in cognitive strenuous mathematical tasks, operationalized through Need for Cognition (NFC), and working memory capacity (WMC). Two independent groups, consisting of upper secondary students (N = 137, mean age 17.13, SD = 0.62, 63 boys and 74 girls), practiced non-routine mathematical problem solving with CMR and AR tasks and were tested 1 week later. An initial t-test confirmed that the CMR group outperformed the AR group. Structural equation modeling revealed that NFC was a significant predictor of math performance for the CMR group but not for the AR group. The results also showed that WMC was a strong predictor of math performance independent of group. These results are discussed in terms of allowing for time and opportunities for struggle with constructing own solution methods using CMR, thereby enhancing students conceptual understanding.}
}

@article{cho2022dallevalprobing,
  title={DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers},
  author={Jaemin Cho and Abhaysinh Zala and Mohit Bansal},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/804b27dc02becf7bbbd89ba949e1e07e8677c459}
}

@article{liu2022deplotoneshot,
  title={DePlot: One-shot visual language reasoning by plot-to-table translation},
  author={Fangyu Liu and Julian Martin Eisenschlos and Francesco Piccinno and Syrine Krichene and Chenxi Pang and Kenton Lee and Mandar Joshi and Wenhu Chen and Nigel Collier and Y. Altun},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10505},
  url={https://www.semanticscholar.org/paper/4d3a49d1439a0b8fbb0e9f588970ad0f1d70dec8},
  abstract={Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than>28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0\% improvement over finetuned SOTA on human-written queries from the task of chart QA.},
  keywords={arxiv:2212.10505}
}

@article{shridhar2022distillingmultistep,
  title={Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions},
  author={Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.00193},
  url={https://www.semanticscholar.org/paper/72123a86eae2cb5c4eae8650f43524039d48875d}
}

@article{shridhar2022distillingreasoning,
  title={Distilling Reasoning Capabilities into Smaller Language Models},
  author={Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2023.findings-acl.441},
  url={https://www.semanticscholar.org/paper/8fd462f6248d5e3f1b6602697c09489086b5655f},
  abstract={Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70\% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM},
  keywords={arxiv:2212.00193}
}

@article{jiang2022draftsketch,
  title={Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
  author={Albert Qiaochu Jiang and S. Welleck and J. Zhou and Wenda Li and Jiacheng Liu and M. Jamnik and Timothée Lacroix and Yuhuai Wu and Guillaume Lample},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.12283},
  url={https://www.semanticscholar.org/paper/7de36d6b14aadc8cdb6ad1340b9ca64b15375bca},
  abstract={The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9\% to 39.3\% on a collection of mathematical competition problems.},
  keywords={arxiv:2210.12283}
}

@article{lu2022dynamicprompt,
  title={Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Kai-Wei Chang and Y. Wu and Song-Chun Zhu and Tanmay Rajpurohit and Peter Clark and A. Kalyan},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2209.14610},
  url={https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4},
  abstract={Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31\% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.},
  keywords={arxiv:2209.14610}
}

@article{webb2022emergentanalogical,
  title={Emergent analogical reasoning in large language models},
  author={Taylor W. Webb and K. Holyoak and Hongjing Lu},
  year={2022},
  booktitle={Nature Human Behaviour},
  doi={10.1038/s41562-023-01659-w},
  url={https://www.semanticscholar.org/paper/3cbffab9d7981da6662d474aaa056dcbd3c1701e},
  abstract={The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of Generative Pre-trained Transformer (GPT)-3) on a range of analogical tasks, including a non-visual matrix reasoning task based on the rule structure of Raven’s Standard Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT-4 indicated even better performance. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems. Webb et al. show that new artificial intelligence language models, such as Generative Pre-trained Transformer 3, are able to solve analogical reasoning problems at a human-like level of performance.},
  keywords={arxiv:2212.09196}
}

@article{spiliopoulou2022eventsrealm,
  title={EvEntS ReaLM: Event Reasoning of Entity States via Language Models},
  author={Evangelia Spiliopoulou and Artidoro Pagnoni and Yonatan Bisk and E. Hovy},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2211.05392},
  url={https://www.semanticscholar.org/paper/748a2700ec11f51560a69ec05c67ca9f97014be7},
  abstract={This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.},
  keywords={arxiv:2211.05392}
}

@article{li2022explanationsfrom,
  title={Explanations from Large Language Models Make Small Reasoners Better},
  author={SHIYANG LI and Jianshu Chen and Yelong Shen and Zhiyu Chen and Xinlu Zhang and Zekun Li and Hong Wang and Jingu Qian and Baolin Peng and Yi Mao and Wenhu Chen and Xifeng Yan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.06726},
  url={https://www.semanticscholar.org/paper/7d29a84a589aa5655e5d3fed8d725ea472816599},
  abstract={Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5\% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.},
  keywords={arxiv:2210.06726}
}

@article{anil2022exploringlength,
  title={Exploring Length Generalization in Large Language Models},
  author={Cem Anil and Yuhuai Wu and Anders Andreassen and Aitor Lewkowycz and Vedant Misra and V. Ramasesh and Ambrose Slone and Guy Gur-Ari and Ethan Dyer and Behnam Neyshabur},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2207.04901},
  url={https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a},
  abstract={The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.},
  keywords={arxiv:2207.04901}
}

@article{han2022folionatural,
  title={FOLIO: Natural Language Reasoning with First-Order Logic},
  author={Simeng Han and Hailey Schoelkopf and Yilun Zhao and Zhenting Qi and Martin Riddell and Luke Benson and Lucy Sun and E. Zubova and Yujie Qiao and Matthew Burtell and David Peng and Jonathan Fan and Yixin Liu and Brian Wong and Malcolm Sailor and Ansong Ni and Linyong Nan and Jungo Kasai and Tao Yu and Rui Zhang and Shafiq R. Joty and Alexander R. Fabbri and Wojciech Kryscinski and Xi Victoria Lin and Caiming Xiong and Dragomir R. Radev},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2209.00840},
  url={https://www.semanticscholar.org/paper/5581bf85386737bd3378eec68189759a05280bea},
  abstract={Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.},
  keywords={arxiv:2209.00840}
}

@article{creswell2022faithfulreasoning,
  title={Faithful Reasoning Using Large Language Models},
  author={Antonia Creswell and M. Shanahan},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/f0a0e8b6e84207f50db4d24cc4016e40601214ef},
  abstract={Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.},
  keywords={arxiv:2208.14271}
}

@article{taylor2022galacticalarge,
  title={Galactica: A Large Language Model for Science},
  author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and A. Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1},
  abstract={Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.},
  keywords={arxiv:2211.09085}
}

@article{zhang2022greaselmgraph,
  title={GreaseLM: Graph REASoning Enhanced Language Models for Question Answering},
  author={Xikun Zhang and Antoine Bosselut and Michihiro Yasunaga and Hongyu Ren and Percy Liang and Christopher D. Manning and J. Leskovec},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/4ab41d9780f1d1ac34d39fa7e527e73652507fcc},
  abstract={Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.},
  keywords={arxiv:2201.08860}
}

@article{gilson2022welldoes,
  title={How Well Does ChatGPT Do When Taking the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment},
  author={A. Gilson and C. Safranek and Ting Huang and V. Socrates and L. Chi and R. A. Taylor and David Chartash},
  year={2022},
  booktitle={medRxiv},
  doi={10.1101/2022.12.23.22283901},
  url={https://www.semanticscholar.org/paper/7d4867e28b02059eef4cb25bfcd304b2071b30a9},
  abstract={Background: ChatGPT is a 175 billion parameter natural language processing model which can generate conversation style responses to user input. Objective: To evaluate the performance of ChatGPT on questions within the scope of United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as analyze responses for user interpretability. Methods: We used two novel sets of multiple choice questions to evaluate ChatGPT's performance, each with questions pertaining to Step 1 and Step 2. The first was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the userbase. The second, was the National Board of Medical Examiners (NBME) Free 120-question exams. After prompting ChatGPT with each question, ChatGPT's selected answer was recorded, and the text output evaluated across three qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results: On the four datasets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free- Step2, ChatGPT achieved accuracies of 44\%, 42\%, 64.4\%, and 57.8\%. The model demonstrated a significant decrease in performance as question difficulty increased (P=.012) within the AMBOSS- Step1 dataset. We found logical justification for ChatGPT's answer selection was present in 100\% of outputs. Internal information to the question was present in >90\% of all questions. The presence of information external to the question was respectively 54.5\% and 27\% lower for incorrect relative to correct answers on the NBME-Free-Step1 and NBME-Free-Step2 datasets (P<=.001). Conclusion: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at greater than 60\% threshold on the NBME-Free- Step-1 dataset we show that the model is comparable to a third year medical student. Additionally, due to the dialogic nature of the response to questions, we demonstrate ChatGPT's ability to provide reasoning and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as a medical education tool.}
}

@article{manning2022humanlanguage,
  title={Human Language Understanding \& Reasoning},
  author={Christopher D. Manning},
  year={2022},
  booktitle={Daedalus},
  doi={10.1162/daed_a_01905},
  url={https://www.semanticscholar.org/paper/a0c87ee1b0903c1c9ac72809caf75b6b6997baa0},
  abstract={Abstract The last decade has yielded dramatic and quite surprising breakthroughs in natural language processing through the use of simple artificial neural network computations, replicated on a very large scale and trained over exceedingly large amounts of data. The resulting pretrained language models, such as BERT and GPT-3, have provided a powerful universal language understanding and generation base, which can easily be adapted to many understanding, writing, and reasoning tasks. These models show the first inklings of a more general form of artificial intelligence, which may lead to powerful foundation models in domains of sensory experience beyond just language.}
}

@article{hagendorff2022humanlikeintuitive,
  title={Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT},
  author={Thilo Hagendorff and Sarah Fabi and Michal Kosinski},
  year={2022},
  booktitle={Nature Computational Science},
  doi={10.1038/s43588-023-00527-x},
  url={https://www.semanticscholar.org/paper/0bfc05adcddd4fe5d1335d96cc313c41526d4558},
  abstract={We design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics. The reasoning capabilities of OpenAI’s generative pre-trained transformer family were tested using semantic illusions and cognitive reflection tests that are typically used in human studies. While early models were prone to human-like cognitive errors, ChatGPT decisively outperformed humans, avoiding the cognitive traps embedded in the tasks.},
  keywords={arxiv:2306.07622}
}

@article{huang2022innermonologue,
  title={Inner Monologue: Embodied Reasoning through Planning with Language Models},
  author={Wenlong Huang and F. Xia and Ted Xiao and Harris Chan and Jacky Liang and Peter R. Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and P. Sermanet and Noah Brown and Tomas Jackson and Linda Luu and S. Levine and Karol Hausman and Brian Ichter},
  year={2022},
  booktitle={Conference on Robot Learning},
  doi={10.48550/arXiv.2207.05608},
  url={https://www.semanticscholar.org/paper/f3cf71c51b882fe3111d71c4bf104297d38197f8},
  abstract={Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
  keywords={arxiv:2207.05608}
}

@article{gao2022kmirbenchmark,
  title={KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models},
  author={Daniel Gao and Yantao Jia and Lei Li and Chengzhen Fu and Zhicheng Dou and Hao Jiang and Xinyu Zhang and Lei Chen and Zhao Cao},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/718343008a6cfca9e86ab6160caba353c52c17cf},
  abstract={Previous works show the great potential of pre-trained language models (PLMs) for storing a large amount of factual knowledge. However, to figure out whether PLMs can be reliable knowledge sources and used as alternative knowledge bases (KBs), we need to further explore some critical features of PLMs. Firstly, knowledge memorization and identification abilities: traditional KBs can store various types of entities and relationships; do PLMs have a high knowledge capacity to store different types of knowledge? Secondly, reasoning ability: a qualified knowledge source should not only provide a collection of facts, but support a symbolic reasoner. Can PLMs derive new knowledge based on the correlations between facts? To evaluate these features of PLMs, we propose a benchmark, named Knowledge Memorization, Identification, and Reasoning test (KMIR). KMIR covers 3 types of knowledge, including general knowledge, domain-specific knowledge, and commonsense, and provides 184,348 well-designed questions. Preliminary experiments with various representative pre-training language models on KMIR reveal many interesting phenomenons: 1) The memorization ability of PLMs depends more on the number of parameters than training schemes. 2) Current PLMs are struggling to robustly remember the facts. 3) Model compression technology retains the amount of knowledge well, but hurts the identification and reasoning abilities. We hope KMIR can facilitate the design of PLMs as better knowledge sources.},
  keywords={arxiv:2202.13529}
}

@misc{chen2022kritknowledgereasoning,
  title={KRIT: Knowledge-Reasoning Intelligence in vision-language Transformer},
  author={Kezhen Chen and Qiuyuan Huang and Daniel J. McDuff and Yonatan Bisk and Jianfeng Gao},
  year={2022},
  url={https://www.semanticscholar.org/paper/5d47143c13591def061e203bf6dd6d97fd110631}
}

@article{kazemi2022lambadabackward,
  title={LAMBADA: Backward Chaining for Automated Reasoning in Natural Language},
  author={Seyed Mehran Kazemi and Najoung Kim and Deepti Bhatia and Xinyuan Xu and Deepak Ramachandran},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.13894},
  url={https://www.semanticscholar.org/paper/03fb95e6be583ca954c3d00812a9e9a40f118e51},
  abstract={Remarkable progress has been made on automated reasoning with natural text, by using Large Language Models (LLMs) and methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from intended conclusion to supporting axioms) is significantly more efficient at proof-finding. Importing this intuition into the LM setting, we develop a Backward Chaining algorithm, called LAMBADA, that decomposes reasoning into four sub-modules, that are simply implemented by few-shot prompted LLM inference. We show that LAMBADA achieves sizable accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.},
  keywords={arxiv:2212.13894}
}

@article{saparov2022languagemodels,
  title={Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},
  author={Abulhair Saparov and He He},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.01240},
  url={https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a},
  abstract={Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.},
  keywords={arxiv:2210.01240}
}

@article{dasgupta2022languagemodels,
  title={Language models show human-like content effects on reasoning},
  author={Ishita Dasgupta and Andrew Kyle Lampinen and Stephanie C. Y. Chan and Antonia Creswell and D. Kumaran and James L. McClelland and Felix Hill},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2207.07051},
  url={https://www.semanticscholar.org/paper/290732e9fb08a29af8892a7c1f73c9d2a1b9d7db},
  abstract={Reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable"content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models \$\textbackslash\{\}unicode\{x2014\}\$ whose prior expectations capture some aspects of human knowledge \$\textbackslash\{\}unicode\{x2014\}\$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large language models, as well as humans, and find that the language models reflect many of the same patterns observed in humans across these tasks \$\textbackslash\{\}unicode\{x2014\}\$ like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected both in answer patterns, and in lower-level features like the relationship between model answer distributions and human response times. Our findings have implications for understanding both these cognitive effects in humans, and the factors that contribute to language model performance.},
  keywords={arxiv:2207.07051}
}

@article{huang2022largelanguage,
  title={Large Language Models Can Self-Improve},
  author={Jiaxin Huang and S. Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.11610},
  url={https://www.semanticscholar.org/paper/3fa70115248377c3d1517c9f978791a296fbc1dd},
  abstract={Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate"high-confidence"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4\%->82.1\% on GSM8K, 78.2\%->83.0\% on DROP, 90.0\%->94.4\% on OpenBookQA, and 63.4\%->67.9\% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.},
  keywords={arxiv:2210.11610}
}

@article{taesiri2022largelanguage,
  title={Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors},
  author={Mohammad Reza Taesiri and Finlay Macklon and Yihe Wang and Hengshuo Shen and C. Bezemer},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.02506},
  url={https://www.semanticscholar.org/paper/55e3fe05598be7c3dd357d51166869f6571b824f},
  abstract={Video game testing requires game-specific knowledge as well as common sense reasoning about the events in the game. While AI-driven agents can satisfy the first requirement, it is not yet possible to meet the second requirement automatically. Therefore, video game testing often still relies on manual testing, and human testers are required to play the game thoroughly to detect bugs. As a result, it is challenging to fully automate game testing. In this study, we explore the possibility of leveraging the zero-shot capabilities of large language models for video game bug detection. By formulating the bug detection problem as a question-answering task, we show that large language models can identify which event is buggy in a sequence of textual descriptions of events from a game. To this end, we introduce the GameBugDescriptions benchmark dataset, which consists of 167 buggy gameplay videos and a total of 334 question-answer pairs across 8 games. We extensively evaluate the performance of six models across the OPT and InstructGPT large language model families on our benchmark dataset. Our results show promising results for employing language models to detect video game bugs. With the proper prompting technique, we could achieve an accuracy of 70.66\%, and on some video games, up to 78.94\%. Our code, evaluation data and the benchmark can be found on https://asgaardlab.github.io/LLMxBugs},
  keywords={arxiv:2210.02506}
}

@article{singhal2022largelanguage,
  title={Large language models encode clinical knowledge},
  author={K. Singhal and Shekoofeh Azizi and T. Tu and S. Mahdavi and Jason Wei and Hyung Won Chung and Nathan Scales and A. Tanwani and H. Cole-Lewis and S. Pfohl and P. Payne and Martin G. Seneviratne and P. Gamble and C. Kelly and Nathaneal Scharli and A. Chowdhery and P. A. Mansfield and B. A. Y. Arcas and D. Webster and Greg S. Corrado and Yossi Matias and K. Chou and Juraj Gottweis and Nenad Tomašev and Yun Liu and A. Rajkomar and J. Barral and Christopher Semturs and A. Karthikesalingam and Vivek Natarajan},
  year={2022},
  booktitle={Nature},
  doi={10.1038/s41586-023-06291-2},
  url={https://www.semanticscholar.org/paper/6052486bc9144dc1730c12bf35323af3792a1fd0},
  abstract={Med-PaLM, a state-of-the-art large language model for medicine, is introduced and evaluated across several medical question answering tasks, demonstrating the promise of these models in this domain. Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model\^{} 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM\^{} 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA\^{} 3 , MedMCQA\^{} 4 , PubMedQA\^{} 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics\^{} 6 ), including 67.6\% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17\%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.},
  keywords={arxiv:2212.13138}
}

@article{lu2022learnexplain,
  title={Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
  author={Pan Lu and Swaroop Mishra and Tony Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and A. Kalyan},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2209.09513},
  url={https://www.semanticscholar.org/paper/d3135733aa39dec20ce72aa138589dda27c8406d},
  abstract={When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of \~{}21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20\% in few-shot GPT-3 and 3.99\% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96\%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40\% of the data. The data and code are available at https://scienceqa.github.io.},
  keywords={arxiv:2209.09513}
}

@article{nam2022learningreason,
  title={Learning to Reason With Relational Abstractions},
  author={A. Nam and Mengye Ren and Chelsea Finn and James L. McClelland},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.02615},
  url={https://www.semanticscholar.org/paper/6d5555348f453bac901c5b57e8a4eeb3074b4071},
  abstract={Large language models have recently shown promising progress in mathematical reasoning when fine-tuned with human-generated sequences walking through a sequence of solution steps. However, the solution sequences are not formally structured and the resulting model-generated sequences may not reflect the kind of systematic reasoning we might expect an expert human to produce. In this paper, we study how to build stronger reasoning capability in language models using the idea of relational abstractions. We introduce new types of sequences that more explicitly provide an abstract characterization of the transitions through intermediate solution steps to the goal state. We find that models that are supplied with such sequences as prompts can solve tasks with a significantly higher accuracy, and models that are trained to produce such sequences solve problems better than those that are trained with previously used human-generated sequences and other baselines. Our work thus takes several steps toward elucidating and improving how language models perform on tasks requiring multi-step mathematical reasoning.},
  keywords={arxiv:2210.02615}
}

@article{zhou2022leasttomostprompting,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Denny Zhou and Nathanael Scharli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and D. Schuurmans and O. Bousquet and Quoc Le and Ed H. Chi},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2205.10625},
  url={https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321},
  abstract={Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
  keywords={arxiv:2205.10625}
}

@article{yu2022legalprompting,
  title={Legal Prompting: Teaching a Language Model to Think Like a Lawyer},
  author={Fang Yu and Lee Quartey and Frank Schilder},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.01326},
  url={https://www.semanticscholar.org/paper/cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3},
  abstract={Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought (CoT) prompts can improve arithmetic or common sense tasks significantly. We explore how such approaches fare with legal reasoning tasks and take the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning approaches. Our findings show that while CoT prompting and fine-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from specific legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.},
  keywords={arxiv:2212.01326}
}

@misc{mishra2022lilaunified,
  title={Lila: A Unified Benchmark for Mathematical Reasoning},
  author={Swaroop Mishra and Pan Lu and A. Kalyan},
  year={2022},
  url={https://www.semanticscholar.org/paper/a630c70aed27b52f6d04d1e772b153c5a7b6f6fe},
  abstract={Mathematical reasoning skills are essential for general-purpose intelligent systems to perform tasks from grocery shopping to climate modeling. Towards evaluating and improving AI systems in this domain, we propose LILA, a unified mathematical reasoning benchmark consisting of 23 diverse tasks along four dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs, thereby obtaining explainable solutions in addition to the correct answer. We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation. Finally, we introduce BHASKARA, a general-purpose mathematical reasoning model trained on LILA. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83\% F1 score vs. single-task models), while the best performing model only obtains 60.40\%, indicating the room for improvement in general mathematical reasoning and understanding.},
  keywords={arxiv:2210.17517}
}

@article{razumovskaia2022littleriding,
  title={Little Red Riding Hood Goes around the Globe: Crosslingual Story Planning and Generation with Large Language Models},
  author={E. Razumovskaia and Joshua Maynez and Annie Louis and Mirella Lapata and Shashi Narayan},
  year={2022},
  booktitle={International Conference on Language Resources and Evaluation},
  doi={10.48550/arXiv.2212.10471},
  url={https://www.semanticscholar.org/paper/30cc7ae95583ade1f05226c08c6f6609777aeedd},
  abstract={Previous work has demonstrated the effectiveness of planning for story generation exclusively in a monolingual setting focusing primarily on English. We consider whether planning brings advantages to automatic story generation across languages. We propose a new task of crosslingual story generation with planning and present a new dataset for this task. We conduct a comprehensive study of different plans and generate stories in several languages, by leveraging the creative and reasoning capabilities of large pretrained language models. Our results demonstrate that plans which structure stories into three acts lead to more coherent and interesting narratives, while allowing to explicitly control their content and structure.},
  keywords={arxiv:2212.10471}
}

@article{karpas2022mrklsystems,
  title={MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning},
  author={Ehud Karpas and Omri Abend and Yonatan Belinkov and Barak Lenz and Opher Lieber and Nir Ratner and Y. Shoham and Hofit Bata and Yoav Levine and Kevin Leyton-Brown and Dor Muhlgay and N. Rozen and Erez Schwartz and Gal Shachaf and Shai Shalev-Shwartz and A. Shashua and Moshe Tenenholtz},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2205.00445},
  url={https://www.semanticscholar.org/paper/1bcde55995a957b3e8a595d536b816cb8989cf1d},
  abstract={Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced"miracle") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.},
  keywords={arxiv:2205.00445}
}

@misc{arthur2022mathematicallanguage,
  title={Mathematical language shapes how we understand the economy},
  author={W. Arthur},
  year={2022},
  url={https://www.semanticscholar.org/paper/51aae5840a02bafe5368753e31b4168eeacdf841}
}

@article{chen2022measurementevaluation,
  title={Measurement, Evaluation, and Model Construction of Mathematical Literacy Based on IoT and PISA},
  author={Yunfeng Chen},
  year={2022},
  booktitle={Mathematical Problems in Engineering},
  doi={10.1155/2022/3278401},
  url={https://www.semanticscholar.org/paper/da5c08f5175374114940aec48ddc6dcba494dab3},
  abstract={“Mathematics Curriculum Standard for Ordinary Senior High School” points out that mathematical modelling literacy is the literacy of abstracting real problems mathematically, expressing problems with mathematical language, and building models with mathematical methods to solve problems. It assesses the amount to which students who are about to finish compulsory schooling have the knowledge and aptitude to deal with future life issues, based on the notion of lifelong learning. The usage of a local integrated development environment requires a number of complex processes, including installation and setup, as well as the procurement of necessary hardware. In reality, in order to nurture students, the core literacy of mathematics is to completely execute quality education, further identify the training and development direction of talents, and infiltrate the core literacy material into junior high school mathematics classroom instruction. For such a large-scale international education measurement project, the research and development of test questions is very important. Therefore, detailed technical consideration has been carried out and test volumes have been designed that are relatively suitable for different countries. The test questions are closely related to life, while there are few questions related to real situational problems in the test, which are mathematically processed first. Due to the international background of evaluation, it is a great challenge for researchers to realize the localization of evaluation on the basis of adapting to the domestic situation. At the same time, there is no scientific and perfect mathematical modelling literacy evaluation system in China’s basic education research. How can middle school front-line teachers better evaluate students’ mathematical modelling literacy? As a result, it is critical to develop a scientific and quantitative mathematical literacy measurement model that will aid in the teaching and research of mathematical modelling by middle school front-line teachers, as well as contributing theoretically to the evaluation and research of mathematical modelling literacy in China. Therefore, based on mathematics literacy evaluation, this paper studies the hierarchy of the IoT and the measurement and evaluation model of mathematics literacy based on the IoT, as well as its enlightenment to the compilation of mathematics academic test questions in China.}
}

@article{pal2022medmcqalargescale,
  title={MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering},
  author={Ankit Pal and Logesh Kumar Umapathi and Malaikannan Sankarasubbu},
  year={2022},
  booktitle={ACM Conference on Health, Inference, and Learning},
  doi={10.48550/arXiv.2203.14371},
  url={https://www.semanticscholar.org/paper/741776172685b9717159a9fcd21841461bb33b14},
  abstract={This paper introduces MedMCQA, a new large-scale, Multiple-Choice Question Answering (MCQA) dataset designed to address real-world medical entrance exam questions. More than 194k high-quality AIIMS \textbackslash\{\}\&NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects are collected with an average token length of 12.77 and high topical diversity. Each sample contains a question, correct answer(s), and other options which requires a deeper language understanding as it tests the 10+ reasoning abilities of a model across a wide range of medical subjects \textbackslash\{\}\&topics. A detailed explanation of the solution, along with the above information, is provided in this study.},
  keywords={arxiv:2203.14371}
}

@article{liu2022mindsgrounded,
  title={Mind's Eye: Grounded Language Model Reasoning through Simulation},
  author={Ruibo Liu and Jason Wei and S. Gu and Te-Yen Wu and Soroush Vosoughi and Claire Cui and Denny Zhou and Andrew M. Dai},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.05359},
  url={https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8},
  abstract={Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9\% zero-shot, and 46.0\% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.},
  keywords={arxiv:2210.05359}
}

@article{bao2022multistepdeductive,
  title={Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation},
  author={Qiming Bao and A. Peng and Tim Hartill and N. Tan and Zhenyun Deng and M. Witbrock and Jiamou Liu},
  year={2022},
  booktitle={International Workshop on Neural-Symbolic Learning and Reasoning},
  doi={10.48550/arXiv.2207.14000},
  url={https://www.semanticscholar.org/paper/4a52399e66da3fb1406132ecedf274925b5f7972},
  abstract={Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gated attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gated attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datasets, we develop PARARULE-Plus, a large dataset with more examples that require deeper reasoning steps. Experimental results show that the addition of PARARULE-Plus can increase the model's performance on examples requiring deeper reasoning depths. The source code and data are available at https://github.com/Strong-AI-Lab/Multi-Step-Deductive-Reasoning-Over-Natural-Language.},
  keywords={arxiv:2207.14000}
}

@article{sammani2022nlxgptmodel,
  title={NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks},
  author={Fawaz Sammani and Tanmoy Mukherjee and Nikos Deligiannis},
  year={2022},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52688.2022.00814},
  url={https://www.semanticscholar.org/paper/bc64190d42d9dc34077b6a096d9053bb88deaa3a},
  abstract={Natural language explanation (NLE) models aim at explaining the decision-making process of a black box system via generating natural language sentences which are human-friendly, high-level and fine-grained. Current NLE models11Throughout this paper, we refer to NLE models as Natural Language Explanation models aimed for vision and vision-language tasks. explain the decision-making process of a vision or vision-language model (a.k.a., task model), e.g., a VQA model, via a language model (a.k.a., explanation model), e.g., GPT. Other than the additional memory resources and inference time required by the task model, the task and explanation models are completely independent, which disassociates the explanation from the reasoning process made to predict the answer. We introduce NLX-GPT, a general, compact and faithful language model that can simultaneously predict an answer and explain it. We first conduct pre-training on large scale data of image-caption pairs for general understanding of images, and then formulate the answer as a text prediction task along with the explanation. Without region proposals nor a task model, our resulting overall framework attains better evaluation scores, contains much less parameters and is 15× faster than the current SoA model. We then address the problem of evaluating the explanations which can be in many times generic, data-biased and can come in several forms. We therefore design 2 new evaluation measures: (1) explain-predict and (2) retrieval-based attack, a selfevaluation framework that requires no labels. Code is at: https://github.com/fawazsammani/nlxgpt.},
  keywords={arxiv:2203.05081}
}

@article{welleck2022naturalprovergrounded,
  title={NaturalProver: Grounded Mathematical Proof Generation with Language Models},
  author={S. Welleck and Jiacheng Liu and Ximing Lu and Hannaneh Hajishirzi and Yejin Choi},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2205.12910},
  url={https://www.semanticscholar.org/paper/d593b9b8d63426f0d6a795dd7f2294619bc03610},
  abstract={Theorem proving in natural mathematical language - the mixture of symbolic and natural language used by humans - plays a central role in mathematical advances and education, and tests aspects of reasoning that are core to intelligence. Yet it has remained underexplored with modern generative models. We study large-scale language models on two new generation tasks: suggesting the next step in a mathematical proof, and full proof generation. We develop NaturalProver, a language model that generates proofs by conditioning on background references (e.g. theorems and definitions that are either retrieved or human-provided), and optionally enforces their presence with constrained decoding. On theorems from the NaturalProofs benchmark, NaturalProver improves the quality of next-step suggestions and generated proofs over fine-tuned GPT-3, according to human evaluations from university-level mathematics students. NaturalProver is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40\% of the time, which is to our knowledge the first demonstration of these capabilities using neural language models.},
  keywords={arxiv:2205.12910}
}

@article{mishra2022numgluesuite,
  title={NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks},
  author={Swaroop Mishra and Arindam Mitra and Neeraj Varshney and Bhavdeep Singh Sachdeva and Peter Clark and Chitta Baral and A. Kalyan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2204.05660},
  url={https://www.semanticscholar.org/paper/39238a92de090c104936a4f78375b95600e42ce5},
  abstract={Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 \%). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 \% on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.},
  keywords={arxiv:2204.05660}
}

@article{iyer2022optimlscaling,
  title={OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
  author={S. Iyer and Xi Victoria Lin and Ramakanth Pasunuru and Todor Mihaylov and Daniel Simig and Ping Yu and Kurt Shuster and Tianlu Wang and Qing Liu and Punit Singh Koura and Xian Li and Brian O'Horo and Gabriel Pereyra and Jeff Wang and Christopher Dewan and Asli Celikyilmaz and Luke S. Zettlemoyer and Veselin Stoyanov},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/e965e93e76a9e6c4e4863d145b5c007b540d575d},
  abstract={Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.},
  keywords={arxiv:2212.12017}
}

@article{shaikh2022secondthought,
  title={On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning},
  author={Omar Shaikh and Hongxin Zhang and William B. Held and Michael Bernstein and Diyi Yang},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.08061},
  url={https://www.semanticscholar.org/paper/b1b8c3e47f44158d22fb70bb453d2494ed013b70},
  abstract={Generating a Chain of Thought (CoT) has been shown to consistently improve large language model (LLM) performance on a wide range of NLP tasks. However, prior work has mainly focused on logical reasoning tasks (e.g. arithmetic, commonsense QA); it remains unclear whether improvements hold for more diverse types of reasoning, especially in socially situated contexts. Concretely, we perform a controlled evaluation of zero-shot CoT across two socially sensitive domains: harmful questions and stereotype benchmarks. We find that zero-shot CoT reasoning in sensitive domains significantly increases a model’s likelihood to produce harmful or undesirable output, with trends holding across different prompt formats and model variants. Furthermore, we show that harmful CoTs increase with model size, but decrease with improved instruction following. Our work suggests that zero-shot CoT should be used with caution on socially important tasks, especially when marginalized groups or sensitive topics are involved.},
  keywords={arxiv:2212.08061}
}

@article{thorburn2022optimizinglanguage,
  title={Optimizing Language Models for Argumentative Reasoning},
  author={Luke Thorburn and Ariel Kruger},
  year={2022},
  booktitle={ArgML@COMMA},
  url={https://www.semanticscholar.org/paper/ae3a6bbe22ea136280e2927807775b3ac8356440}
}

@article{sharma2022overcomingbarriers,
  title={Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic},
  author={Mandar Sharma and N. Muralidhar and Naren Ramakrishnan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.02098},
  url={https://www.semanticscholar.org/paper/1ec4ae64ded99846bcd8a1a3b16fe2875f55c7c1},
  abstract={Through their transfer learning abilities, highly-parameterized large pre-trained language models have dominated the NLP landscape for a multitude of downstream language tasks. Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning. However, as we illustrate in this paper, building a general purpose language model that also happens to be proficient in mathematical reasoning is not as straight-forward as training it on a numeric dataset. In this work, we develop a novel framework that enables language models to be mathematically proficient while retaining their linguistic prowess. Specifically, we offer information-theoretic interventions to overcome the catastrophic forgetting of linguistic skills that occurs while injecting non-linguistic skills into language models.},
  keywords={arxiv:2211.02098}
}

@article{chowdhery2022palmscaling,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={A. Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and P. Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and M. Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and S. Ghemawat and Sunipa Dev and H. Michalewski and Xavier García and Vedant Misra and Kevin Robinson and L. Fedus and Denny Zhou and Daphne Ippolito and D. Luan and Hyeontaek Lim and Barret Zoph and A. Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and R. Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Díaz and Orhan Firat and Michele Catasta and Jason Wei and K. Meier-Hellstern and D. Eck and J. Dean and Slav Petrov and Noah Fiedel},
  year={2022},
  journal={Journal of machine learning research},
  url={https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb},
  abstract={Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.},
  keywords={arxiv:2204.02311}
}

@article{zelikman2022parselunified,
  title={Parsel: A Unified Natural Language Framework for Algorithmic Reasoning},
  author={E. Zelikman and Qian Huang and Gabriel Poesia and Noah D. Goodman and Nick Haber},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.10561},
  url={https://www.semanticscholar.org/paper/239b5649b12f28fd610de036afba41b9246db6c9}
}

@article{zelikman2022parselalgorithmic,
  title={Parsel🦆: Algorithmic Reasoning with Language Models by Composing Decompositions},
  author={E. Zelikman and Qian Huang and Gabriel Poesia and Noah D. Goodman and Nick Haber},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/e325fe41c8c1d547ccd102ac82be3ec8b23960f2},
  abstract={Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\textbackslash\{\}\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\textbackslash\{\}\% to 85\textbackslash\{\}\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel},
  keywords={arxiv:2212.10561}
}

@article{valmeekam2022planbenchextensible,
  title={PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change},
  author={Karthik Valmeekam and Alberto Olmo and S. Sreedharan and Subbarao Kambhampati},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc},
  abstract={Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.},
  keywords={arxiv:2206.10498}
}

@article{romero2022propositionalreasoning,
  title={Propositional Reasoning via Neural Transformer Language Models},
  author={Oscar J. Romero and A. Tomasic and A. Steinfeld and John Zimmerman},
  year={2022},
  booktitle={International Workshop on Neural-Symbolic Learning and Reasoning},
  url={https://www.semanticscholar.org/paper/ca68b7b6a6f062da58453a48898e1f14b4200a27}
}

@article{golovneva2022roscoesuite,
  title={ROSCOE: A Suite of Metrics for Scoring Step-by-Step Reasoning},
  author={O. Yu. Golovneva and Moya Chen and Spencer Poff and Martin Corredor and Luke Zettlemoyer and Maryam Fazel-Zarandi and Asli Celikyilmaz},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.07919},
  url={https://www.semanticscholar.org/paper/391246ce9c59d61c94cca3f8bef56c95542a4708},
  abstract={Large language models show improved downstream task performance when prompted to generate step-by-step reasoning to justify their final answers. These reasoning steps greatly improve model interpretability and verification, but objectively studying their correctness (independent of the final answer) is difficult without reliable methods for automatic evaluation. We simply do not know how often the stated reasoning steps actually support the final end task predictions. In this work, we present ROSCOE, a suite of interpretable, unsupervised automatic scores that improve and extend previous text generation evaluation metrics. To evaluate ROSCOE against baseline metrics, we design a typology of reasoning errors and collect synthetic and human evaluation scores on commonly used reasoning datasets. In contrast with existing metrics, ROSCOE can measure semantic consistency, logicality, informativeness, fluency, and factuality - among other traits - by leveraging properties of step-by-step rationales. We empirically verify the strength of our metrics on five human annotated and six programmatically perturbed diagnostics datasets - covering a diverse set of tasks that require reasoning skills and show that ROSCOE can consistently outperform baseline metrics.},
  keywords={arxiv:2212.07919}
}

@article{yao2022reactsynergizing,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d},
  abstract={While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  keywords={arxiv:2210.03629}
}

@misc{jha2022responsiblereasoning,
  title={Responsible Reasoning with Large Language Models and the Impact of Proper Nouns},
  author={Sumit Kumar Jha},
  year={2022},
  url={https://www.semanticscholar.org/paper/a70fcd82d8d26bf4c8b0bdde88e96b6ce2c80ecf}
}

@article{he2022rethinkingwith,
  title={Rethinking with Retrieval: Faithful Large Language Model Inference},
  author={Hangfeng He and Hongming Zhang and D. Roth},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2301.00303},
  url={https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1},
  abstract={Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.},
  keywords={arxiv:2301.00303}
}

@misc{zelikman2022starbootstrapping,
  title={STaR: Bootstrapping Reasoning With Reasoning},
  author={E. Zelikman and Yuhuai Wu and Noah D. Goodman},
  year={2022},
  url={https://www.semanticscholar.org/paper/23dd78e424d32f6a48660dcd67ce994b8a7db8be},
  abstract={Generating step-by-step"chain-of-thought"rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the"Self-Taught Reasoner"(STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30\$\textbackslash\{\}times\$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.},
  keywords={arxiv:2203.14465}
}

@article{creswell2022selectioninferenceexploiting,
  title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
  author={Antonia Creswell and M. Shanahan and I. Higgins},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd},
  abstract={Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100\% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.},
  keywords={arxiv:2205.09712}
}

@article{wang2022selfconsistencyimproves,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Xuezhi Wang and Jason Wei and D. Schuurmans and Quoc Le and Ed H. Chi and Denny Zhou},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2},
  abstract={Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  keywords={arxiv:2203.11171}
}

@article{xiao2022smoothquantaccurate,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Guangxuan Xiao and Ji Lin and Mickael Seznec and Julien Demouth and Song Han},
  year={2022},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2211.10438},
  url={https://www.semanticscholar.org/paper/2c994fadbb84fb960d8306ee138dbeef41a5b323},
  abstract={Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.},
  keywords={arxiv:2211.10438}
}

@article{zeng2022socraticmodels,
  title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  author={Andy Zeng and Adrian S. Wong and Stefan Welker and K. Choromanski and F. Tombari and Aveek Purohit and M. Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Peter R. Florence},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/ada81a4de88a6ce474df2e2446ad11fea480616e},
  abstract={Large pretrained (e.g.,"foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.},
  keywords={arxiv:2204.00598}
}

@article{zhu2022solvingmath,
  title={Solving Math Word Problem via Cooperative Reasoning induced Language Models},
  author={Xinyu Zhu and Junjie Wang and Lin Zhang and Yuxiang Zhang and Ruyi Gan and Jiaxing Zhang and Yujiu Yang},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.16257},
  url={https://www.semanticscholar.org/paper/01f7bb1f9c611b5e849558e445fdccb98a3a3040}
}

@article{lewkowycz2022solvingquantitative,
  title={Solving Quantitative Reasoning Problems with Language Models},
  author={Aitor Lewkowycz and Anders Andreassen and David Dohan and Ethan Dyer and H. Michalewski and V. Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2206.14858},
  url={https://www.semanticscholar.org/paper/ab0e3d3e4d42369de5933a3b4c237780b41c0d77},
  abstract={Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.},
  keywords={arxiv:2206.14858}
}

@article{collins2022structuredflexible,
  title={Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks},
  author={K. M. Collins and Catherine Wong and Jiahai Feng and Megan Wei and J. Tenenbaum},
  year={2022},
  booktitle={Annual Meeting of the Cognitive Science Society},
  doi={10.48550/arXiv.2205.05718},
  url={https://www.semanticscholar.org/paper/7ef9aafc68511afab5b287e62b754576ea37b4ce},
  abstract={Human language offers a powerful window into our thoughts -- we tell stories, give explanations, and express our beliefs and goals through words. Abundant evidence also suggests that language plays a developmental role in structuring our learning. Here, we ask: how much of human-like thinking can be captured by learning statistical patterns in language alone? We first contribute a new challenge benchmark for comparing humans and distributional large language models (LLMs). Our benchmark contains two problem-solving domains (planning and explanation generation) and is designed to require generalization to new, out-of-distribution problems expressed in language. We find that humans are far more robust than LLMs on this benchmark. Next, we propose a hybrid Parse-and-Solve model, which augments distributional LLMs with a structured symbolic reasoning module. We find that this model shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.},
  keywords={arxiv:2205.05718}
}

@article{gaur2022symbolicmath,
  title={Symbolic Math Reasoning with Language Models},
  author={Vedant Gaur and Nikunj Saunshi},
  year={2022},
  booktitle={2022 IEEE MIT Undergraduate Research Technology Conference (URTC)},
  doi={10.1109/URTC56832.2022.10002218},
  url={https://www.semanticscholar.org/paper/f557f3a32d309373e7d31bb93ca1b80b4a6e39e7},
  abstract={The emergence of large language models (LLMs) such as OpenAI’s GPT-3, Google’s LaMDA, Meta’s OPT [2, 3, 7, 10] etc. have revolutionized the field of natural language processing (NLP). These models with upwards of hundreds of billions of parameters are trained on large unlabeled text corpora and can subsequently solve downstream tasks with little to no labeled data. While these models are increasingly versatile in their abilities, e.g., solving math word problems, the larger question of their ability to reason remains. Using and modifying the SVAMP dataset, we find that GPT-3’s davinci-002 model, in addition to having good performance on numerical math word problems, also performs well on the potentially harder symbolic version of the same problems. Furthermore, adopting a two-step approach (solve symbolically and then substitute numerical values) leads to better accuracy on the numerical test set in the zero-shot regime. Additionally, we find that the use of specific prompting techniques pushes the model, in many cases, to actively describe its thought process and aid in the final answer output when faced with a complex, multi-step problem, aligning with recent observations.}
}

@article{zhou2022teachingalgorithmic,
  title={Teaching Algorithmic Reasoning via In-context Learning},
  author={Hattie Zhou and Azade Nova and H. Larochelle and Aaron C. Courville and Behnam Neyshabur and Hanie Sedghi},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.09066},
  url={https://www.semanticscholar.org/paper/4d17732d90440682b0500f4e209c6cc4fac20e0e},
  abstract={Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.},
  keywords={arxiv:2211.09066}
}

@article{bertolini2022testinglarge,
  title={Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment},
  author={Lorenzo Bertolini and Julie Weeds and David Weir},
  year={2022},
  booktitle={International Conference on Computational Linguistics},
  url={https://www.semanticscholar.org/paper/1606793daaa20d4a4a78e859c2fd6b4f7535680c}
}

@article{valentino2022textgraphs2022,
  title={TextGraphs 2022 Shared Task on Natural Language Premise Selection},
  author={Marco Valentino and Deborah Ferreira and Mokanarangan Thayaparan and André Freitas and Dmitry Ustalov},
  year={2022},
  booktitle={Workshop on Graph-based Methods for Natural Language Processing},
  url={https://www.semanticscholar.org/paper/43845c54af0246baab17d572953e671487e1dcd1}
}

@article{ye2022unreliabilityexplanations,
  title={The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning},
  author={Xi Ye and Greg Durrett},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/9ffefdf1fcd780cb71450b0a7a29247c66aa87be},
  abstract={Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.},
  keywords={arxiv:2205.03401}
}

@article{ozturkler2022thinksumprobabilistic,
  title={ThinkSum: Probabilistic reasoning over sets using large language models},
  author={Batu Mehmet Ozturkler and Nikolay Malkin and Zhen Wang and N. Jojic},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.01293},
  url={https://www.semanticscholar.org/paper/370cea8b4220917f45a69358c0303df71f5063c7},
  abstract={Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think – retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum – probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.},
  keywords={arxiv:2210.01293}
}

@article{huang2022towardsreasoning,
  title={Towards Reasoning in Large Language Models: A Survey},
  author={Jie Huang and K. Chang},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10403},
  url={https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45},
  abstract={Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
  keywords={arxiv:2212.10403}
}

@article{agrawal2022towardsmathematics,
  title={Towards a Mathematics Formalisation Assistant using Large Language Models},
  author={Ayush Agrawal and Siddhartha Gadgil and Navin Goyal and Ashvni Narayanan and Anand Tadipatri},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.07524},
  url={https://www.semanticscholar.org/paper/b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce},
  abstract={Mathematics formalisation is the task of writing mathematics (i.e., definitions, theorem statements, proofs) in natural language, as found in books and papers, into a formal language that can then be checked for correctness by a program. It is a thriving activity today, however formalisation remains cumbersome. In this paper, we explore the abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover. We find that with careful inputdependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75\% accuracy for 120 theorem statements. For proofs quantitative analysis is infeasible and we undertake a detailed case study. We choose a diverse set of 13 theorems at undergrad level with proofs that fit in two-three paragraphs. We show that with a new prompting strategy Codex can formalise these proofs in natural language with at least one out of twelve Codex completion being easy to repair into a complete proof. This is surprising as essentially no aligned data exists for formalised mathematics, particularly for proofs. These results suggest that large language models are a promising avenue towards fully or partially automating formalisation.},
  keywords={arxiv:2211.07524}
}

@article{wu2022triplefactretriever,
  title={Triple-Fact Retriever: An explainable reasoning retrieval model for multi-hop QA problem},
  author={Cheng Wu and Enrui Hu and Ke Zhan and Lan Luo and Xinyu Zhang and Hao Jiang and Qirui Wang and Zhao Cao and Fan Yu and Lei Chen},
  year={2022},
  booktitle={IEEE International Conference on Data Engineering},
  doi={10.1109/icde53745.2022.00095},
  url={https://www.semanticscholar.org/paper/dd77bff42060109c9b2e3e7d87cc9342309c3952},
  abstract={Nowadays, multi-hop question answer (QA) problem is challenging and not well solved in the QA community. The dominant bottleneck of the multi-hop QA problem is the need for a reasoning retriever to fetch a document path from an open-domain corpus (e.g., Wikipedia). A reasoning retriever aims to collect an evidence document from large corpora at one hop retrieval and aggregate the evidence for subsequent hop retrieval, which yields a document path after multi-hop retrieval. There exist two challenges, (1) to fetch the evidence document in an efficient and explainable way at one hop retrieval and (2) to update the question information by aggregating the evidence from the retrieved document after each hop retrieval. To address these two challenges, we propose a triple-fact-based retrieval model to effectively retrieve a related document path in an explainable way for each question. We extract a structured representation from the unstructured document and utilize the knowledge of pre-trained language model (PLM) to do the semantic-level matching between the question and document. We evaluate the proposed Triple-fact Retriever model on the recently proposed open-domain multi-hop QA dataset, HotpotQA, and a cross-document multi-step Reading Comprehension dataset, Wikihop. The results11The source code is available on our website: https://github.com/Rebaccamin/triple\_retriever. demonstrate that the Triple-fact retriever outperforms the existing baseline retrieval works.}
}

@article{chen2022unigeounifying,
  title={UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression},
  author={Jiaqi Chen and Tong Li and Jinghui Qin and Pan Lu and Liang Lin and Chongyu Chen and Xiaodan Liang},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2212.02746},
  url={https://www.semanticscholar.org/paper/72fce949725b20428e5f56247fef5c6bd1ce6154},
  abstract={Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning capability on multiple math tasks. However, in essence, these two tasks have similar problem representations and overlapped math knowledge which can improve the understanding and reasoning ability of a deep model on both two tasks. Therefore, we construct a large-scale Unified Geometry problem benchmark, UniGeo, which contains 4,998 calculation problems and 9,543 proving problems. Each proving problem is annotated with a multi-step proof with reasons and mathematical expressions. The proof can be easily reformulated as a proving sequence that shares the same formats with the annotated program sequence for calculation problems. Naturally, we also present a unified multi-task Geometric Transformer framework, Geoformer, to tackle calculation and proving problems simultaneously in the form of sequence generation, which finally shows the reasoning ability can be improved on both two tasks by unifying formulation. Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that aims to predict the mathematical expressions in the problem solution, thus improving the Geoformer model. Experiments on the UniGeo demonstrate that our proposed Geoformer obtains state-of-the-art performance by outperforming task-specific model NGS with over 5.6\% and 3.2\% accuracies on calculation and proving problems, respectively.},
  keywords={arxiv:2212.02746}
}

@article{jiang2022unikgqaunified,
  title={UniKGQA: Unified Retrieval and Reasoning for Solving Multi-hop Question Answering Over Knowledge Graph},
  author={Jinhao Jiang and Kun Zhou and Wayne Xin Zhao and Ji-rong Wen},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2212.00959},
  url={https://www.semanticscholar.org/paper/2d01da2c9ece0969d6ec56d22f78caf57050fc03},
  abstract={Multi-hop Question Answering over Knowledge Graph\~{}(KGQA) aims to find the answer entities that are multiple hops away from the topic entities mentioned in a natural language question on a large-scale Knowledge Graph (KG). To cope with the vast search space, existing work usually adopts a two-stage approach: it first retrieves a relatively small subgraph related to the question and then performs the reasoning on the subgraph to find the answer entities accurately. Although these two stages are highly related, previous work employs very different technical solutions for developing the retrieval and reasoning models, neglecting their relatedness in task essence. In this paper, we propose UniKGQA, a novel approach for multi-hop KGQA task, by unifying retrieval and reasoning in both model architecture and parameter learning. For model architecture, UniKGQA consists of a semantic matching module based on a pre-trained language model\~{}(PLM) for question-relation semantic matching, and a matching information propagation module to propagate the matching information along the directed edges on KGs. For parameter learning, we design a shared pre-training task based on question-relation matching for both retrieval and reasoning models, and then propose retrieval- and reasoning-oriented fine-tuning strategies. Compared with previous studies, our approach is more unified, tightly relating the retrieval and reasoning stages. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our method on the multi-hop KGQA task. Our codes and data are publicly available at\~{}\textbackslash\{\}url\{https://github.com/RUCAIBox/UniKGQA\}.},
  keywords={arxiv:2212.00959}
}

@article{sahu2022unpackinglarge,
  title={Unpacking Large Language Models with Conceptual Consistency},
  author={Pritish Sahu and Michael Cogswell and Yunye Gong and Ajay Divakaran},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2209.15093},
  url={https://www.semanticscholar.org/paper/4f4a80148cb8f328aeaee68b34f9797cfb5ea150},
  abstract={If a Large Language Model (LLM) answers"yes"to the question"Are mountains tall?"then does it know what a mountain is? Can you rely on it responding correctly or incorrectly to other questions about mountains? The success of Large Language Models (LLMs) indicates they are increasingly able to answer queries like these accurately, but that ability does not necessarily imply a general understanding of concepts relevant to the anchor query. We propose conceptual consistency to measure a LLM's understanding of relevant concepts. This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are. To compute it we extract background knowledge by traversing paths between concepts in a knowledge base and then try to predict the model's response to the anchor query from the background knowledge. We investigate the performance of current LLMs in a commonsense reasoning setting using the CSQA dataset and the ConceptNet knowledge base. While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency. Our analysis also shows significant variation in conceptual consistency across different kinds of relations, concepts, and prompts. This serves as a step toward building models that humans can apply a theory of mind to, and thus interact with intuitively.},
  keywords={arxiv:2209.15093}
}

@article{liu2022visualspatial,
  title={Visual Spatial Reasoning},
  author={Fangyu Liu and Guy Edward Toh Emerson and Nigel Collier},
  year={2022},
  journal={Transactions of the Association for Computational Linguistics},
  doi={10.1162/tacl_a_00566},
  url={https://www.semanticscholar.org/paper/354b48677e314ef2f47512c5a81723cfd17dd05d},
  abstract={Spatial relations are a basic part of human cognition. However, they are expressed in natural language in a variety of ways, and previous work has suggested that current vision-and-language models (VLMs) struggle to capture relational information. In this paper, we present Visual Spatial Reasoning (VSR), a dataset containing more than 10k natural text-image pairs with 66 types of spatial relations in English (e.g., under, in front of, facing). While using a seemingly simple annotation format, we show how the dataset includes challenging linguistic phenomena, such as varying reference frames. We demonstrate a large gap between human and model performance: The human ceiling is above 95\%, while state-of-the-art models only achieve around 70\%. We observe that VLMs’ by-relation performances have little correlation with the number of training examples and the tested models are in general incapable of recognising relations concerning the orientations of objects.1},
  keywords={arxiv:2205.00363}
}

@article{madasu2022whatlarge,
  title={What do Large Language Models Learn beyond Language?},
  author={Avinash Madasu and Shashank Srivastava},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.12302},
  url={https://www.semanticscholar.org/paper/5efab88c0cdb11c795fa8f44a5d31b40e2a1c261},
  abstract={Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful `inductive biases' for non-linguistic reasoning. On a set of 19 diverse non-linguistic tasks involving quantitative computations, recognizing regular expressions and reasoning over strings. We find that pretrained models significantly outperform comparable non-pretrained neural models. This remains true also in experiments with training non-pretrained models with fewer parameters to account for model regularization effects. We further explore the effect of text domain on LMs by pretraining models from text from different domains and provenances. Our experiments surprisingly reveal that the positive effects of pre-training persist even when pretraining on multi-lingual text or computer code, and even for text generated from synthetic languages. Our findings suggest a hitherto unexplored deep connection between pre-training and inductive learning abilities of language models.},
  keywords={arxiv:2210.12302}
}