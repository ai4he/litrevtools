
You are an expert in writing PRISMA systematic literature reviews for academic publication.

CRITICAL REQUIREMENT: You MUST cite ALL papers (existing AND new) using LaTeX \cite{} commands throughout the regenerated paper.

=== ALL BIBTEX ENTRIES (INCLUDING NEW PAPERS) ===
@article{cohen2022thisunicorn,
  title={"This is my unicorn, Fluffy": Personalizing frozen vision-language representations},
  author={Niv Cohen and Rinon Gal and E. Meirom and Gal Chechik and Y. Atzmon},
  year={2022},
  booktitle={European Conference on Computer Vision},
  doi={10.48550/arXiv.2204.01694},
  url={https://www.semanticscholar.org/paper/0791a0441e1f672c43aecb2d6708fbc8725c8cad},
  abstract={Large Vision&Language models pretrained on web-scale data provide representations that are invaluable for numerous V&L problems. However, it is unclear how they can be used for reasoning about user-specific visual concepts in unstructured language. This problem arises in multiple domains, from personalized image retrieval to personalized interaction with smart devices. We introduce a new learning setup called Personalized Vision&Language (PerVL) with two new benchmark datasets for retrieving and segmenting user-specific"personalized"concepts"in the wild". In PerVL, one should learn personalized concepts (1) independently of the downstream task (2) allowing a pretrained model to reason about them with free language, and (3) does not require personalized negative examples. We propose an architecture for solving PerVL that operates by extending the input vocabulary of a pretrained model with new word embeddings for the new personalized concepts. The model can then reason about them by simply using them in a sentence. We demonstrate that our approach learns personalized visual concepts from a few examples and can effectively apply them in image retrieval and semantic segmentation using rich textual queries.}
}

@article{stolfo2022causalframework,
  title={A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models},
  author={Alessandro Stolfo and Zhijing Jin and Kumar Shridhar and B. Scholkopf and Mrinmaya Sachan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.12023},
  url={https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe},
  abstract={We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution.By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems.Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.}
}

@article{snchez2022clusteringapproach,
  title={A Clustering Approach for the Optimal Siting of Recharging Stations in the Electric Vehicle Routing Problem with Time Windows},
  author={Danny García Sánchez and Alejandra Tabares and L. Faria and Juan Carlos Rivera and J. Franco},
  year={2022},
  booktitle={Energies},
  doi={10.3390/en15072372},
  url={https://www.semanticscholar.org/paper/f1164514c7180331c3b059c19eab5169c9c921a7},
  abstract={Transportation has been incorporating electric vehicles (EVs) progressively. EVs do not produce air or noise pollution, and they have high energy efficiency and low maintenance costs. In this context, the development of efficient techniques to overcome the vehicle routing problem becomes crucial with the proliferation of EVs. The vehicle routing problem concerns the freight capacity and battery autonomy limitations in different delivery-service scenarios, and the challenge of best locating recharging stations. This work proposes a mixed-integer linear programming model to solve the electric location routing problem with time windows (E-LRPTW) considering the state of charge, freight and battery capacities, and customer time windows in the decision model. A clustering strategy based on the k-means algorithm is proposed to divide the set of vertices (EVs) into small areas and define potential sites for recharging stations, while reducing the number of binary variables. The proposed model for E-LRPTW was implemented in Python and solved using mathematical modeling language AMPL together with CPLEX. Performed tests on instances with 5 and 10 clients showed a large reduction in the time required to find the solution (by about 60 times in one instance). It is concluded that the strategy of dividing customers by sectors has the potential to be applied and generate solutions for larger geographical areas and numbers of recharging stations, and determine recharging station locations as part of planning decisions in more realistic scenarios.}
}

@misc{desogus2022contributionrelationship,
  title={A Contribution on Relationship Banking. Economic, Anthropological and Mathematical Reasoning, Empirical Evidence from Italy},
  author={Marco Desogus and Elisa Casu},
  year={2022},
  url={https://www.semanticscholar.org/paper/dd88cc9b1f6d71ef82631b4e1c98c077ccdf291a}
}

@article{wang2022hybridgenetic,
  title={A Hybrid Genetic Algorithm for Flexible Job Shop Scheduling Problem},
  author={Xianglong Wang and Changyi Liu},
  year={2022},
  booktitle={2022 5th World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM)},
  doi={10.1109/WCMEIM56910.2022.10021523},
  url={https://www.semanticscholar.org/paper/3cff420b5a41a06291b68f5b6600935c090f8ad8},
  abstract={Partially flexible job shop scheduling problem (P-FJSP) is a NP Hard problem more complex than fully flexi-ble job shop scheduling problem (T -FJSP). In this paper, the mathematical model of flexible job shop scheduling is established with the goal of minimizing the maximum completion time (makespan). It combines the local search ability of simu-lated annealing algorithm and the global search ability of ge-netic algorithm. In the process of chromosome decoding, greedy decoding method is used to get a better scheduling solution as far as possible. The hybrid scheduling algorithm is implemented based on Visual Studio and C # language. Finally, 8×8 classic scheduling instance are used for simulation scheduling experiments to verify that the hybrid genetic algorithm proposed in this paper is effective in solving large-scale FJSP.}
}

@article{zhang2022multilayerattention,
  title={A Multi-Layer Attention Network for Visual Commonsense Reasoning},
  author={Wenqi Zhang and Yongchao Gao and Heng Qian and Hongli Lyu},
  year={2022},
  booktitle={International Conference on Data Science and Information Technology},
  doi={10.1109/DSIT55514.2022.9943834},
  url={https://www.semanticscholar.org/paper/0e0f20f3af3650b5a97b0ec3f046ba8160b45279},
  abstract={Visual Commonsense Reasoning (VCR) is a challenging multimodal task involving several research fields such as vision, cognition, and reasoning, which combines images and natural language for reasoning. Existing VCR methods focus on global attention or use pre-training models, but these methods lack attention to local features of visual and language. In this paper, a multi-layer attention network is proposed for the VCR task, including an intra-modal attention module and an inter-modal attention module. The intra-modal attention module complements important features of visual and language modalities with fine-grained visual attention to improve the relevance of visual and language. The inter-modal attention module captures the internal dependencies between visual and language. Finally, the two modules are integrated into an end-to-end reasoning framework. Experiments on the VCR large-scale dataset show that the proposed method exhibits a decent improvement in the VCR task and illustrates the effectiveness of the method on three subtasks.}
}

@article{ricci2022petrinetbasedapproach,
  title={A Petri-Net-Based Approach for Enhancing Clinical Reasoning in Medical Education},
  author={F. Ricci and F. Consorti and F. Pecoraro and D. Luzi and Oscar Tamburis},
  year={2022},
  journal={IEEE Transactions on Learning Technologies},
  doi={10.1109/tlt.2022.3157391},
  url={https://www.semanticscholar.org/paper/98b0fb67a6fb222998e3449621f3f5eecaed758e},
  abstract={Medical students are called to acquire competence to manage disease in its dynamic evolution over time, learning to analyze how clinical conditions evolve in a patient's history and how each condition interferes with the evolution of the other coexisting conditions. In this article, the health issue network (HIN) approach is introduced as a formal language based on Petri nets (PNs) to model properties that are particularly apposite for the graphical representation of HIN evolutionary paths. Moreover, the PNs’ underlying mathematical model allows users to draw coherent and well-formed graphs representing rather complex clinical cases. Finally, HIN can be easily integrated into a simulation environment to support case-based learning activities and assessment. The examples of the exercises provided in this article show, on the one hand, the ways the introduced methodology is figured out and implemented; on the other hand, they outline the variety of learning questions that users may deal with when deploying the HIN approach.}
}

@article{ekong2022ratiocinativestudy,
  title={A Ratiocinative Study and Assessment of W. V. O. Quine’s “Criterion of Ontological Commitment”},
  author={Joseph T. Ekong},
  year={2022},
  journal={International Journal of Philosophy},
  doi={10.47941/ijp.1052},
  url={https://www.semanticscholar.org/paper/7b6955111d3bd91b13e7a9c7fbdfd75d43825c36},
  abstract={Purpose: This work has three main objectives: Firstly, it offers an elucidation of the notion of ontological commitment. Secondly, it assesses the adequacy of the criterion of ontological commitment for different languages. Thirdly, it offers some speculative and evaluative remarks regarding the significance of Quine’s criterion of ontological commitment. Many ontologists, within the analytic tradition, often appeal to Quine's criterion of ontological commitment, when debating whether an assertion or theory implies the existence of a certain entity. Regarding his goal in formulating this criterion, he says that the criterion does not aim to help us discover what it is that there is, but only what a theory says there is: “I look to variables and quantification for evidence as to what a theory says that there is, not for evidence as to what there is” (Quine, 1960: 225). Its most popular formulation, using textual evidence from Quine's oeuvre, is: “To be is to be the value of a bound variable,” (Quine, 1961: 15). However, this formulation is susceptible to gross misunderstanding, especially if one is influenced by the formalities and technical maneuvers of model theory. In mathematical logic, model theory is the study of the relationship between formal theories (a collection of sentences in a formal language expressing statements about a mathematical structure), and their models (those structures in which the statements of the theory hold). Model theory is a branch of mathematical logic where we study mathematical structures by considering the first-order sentences true in those structures and the sets definable by first-order formulas. Model theory studies the relations between sentences of a formal language and the interpretations (or ‘structures’) which make these sentences true or false. It offers precise definitions of truth, logical truth and consequence, meanings and modalities. 
Methodology: This work is expository, analytic, critical and evaluative in its methodology. Of course, there are familiar philosophical problems which are within the discursive framework of ‘ontology,’ often phrased by asking if something or some category of things are “real,” or whether “they exist,” concretely. An outstanding example is provided by the traditional problem of universals, which issues in the nominalist-realist controversy, as to the real existence of universals, or of abstract entities such as classes (in the mathematical sense) or propositions (in the abstract sense, referring to the content of an assertion in abstraction from the particular words used to convey it). 
Results: In as much as one might agree with Quine’s Criterion of Ontological Commitment, one might also opine that it is nonetheless a feature of first-order language (i.e. the language embodied in first-order logic; a symbolized reasoning process comprising relations, functions and constants, in which each sentence or statement is broken down into a subject and a predicate. In this regard, the predicate modifies or defines the properties of the subject) that there should be an exact correspondence between the ontological commitments carried by a sentence and the objects that must be counted among the values of the variables in order for the sentence to be true. However, this in itself is not a reason for thinking that such a feature will generalize beyond first-order languages. It is possible for Quine’s Criterion to degenerate, when the language contains atomic predicates expressing extrinsic properties. 
Unique Contribution to theory, practice and policy: Based on Quine’s analysis, a theory is committed to those and only those entities that in the last analysis serve as the values of its bound variables. Thus, ordinary first-order theory commits one to an ontology only of individuals (particulars), whereas higher order logic commits one to the existence of sets, i.e. of collections of definite and distinct entities (or, alternatively, of properties and relations). Likewise, if bound first-order variables are assumed to range over sets (as they do in set theory), a commitment to the existence of these sets is incurred. Admittedly, the precise import of Quine’s criterion of ontological commitment, however, is not completely clear, nor is it clear in what other sense one is perhaps committed by a theory to those entities that are named or otherwise referred to in it, but not quantified over in it. However, it despite its limitations, it has made is possible for one to measure the ontological cost of theories, an important component in deciding which theories to accept, thus offering a partial foundation for theory choice.}
}

@article{li2022scenariobasedexploration,
  title={A Scenario-based Exploration of Expected Usefulness, Privacy Concerns, and Adoption Likelihood of Learning Analytics},
  author={X. Li and M. Rosson and Jenay Robert},
  year={2022},
  booktitle={ACM Conference on Learning @ Scale},
  doi={10.1145/3491140.3528271},
  url={https://www.semanticscholar.org/paper/067b8489b028d931b751cb9413225b761e51dcf3},
  abstract={Learning analytics has become a robust research area in the last decade, as innovative analytic models of learning data have been created with the goal of enhancing teaching and learning. However, barriers to large scale adoption of such technologies in higher education still exist. In recent years, a strand of research has begun to investigate stakeholders' expectations of learning analytics, hoping to find ways to integrate the innovations into everyday teaching practices. For instance, studies have investigated instructors' ideas about how learning analytics might be helpful, as well as concerns about student data privacy. However, most studies have taken a general approach rather than considering instructors' day-to-day experiences. Using survey methods, we presented instructors with hypothetical scenarios of learning analytics in use across disciplines, class sizes, teaching activities, and types of student data. We asked for ratings of both usefulness and privacy concerns for each proposed teaching situation. Our respondents considered scenarios involving learning outcomes-related data (e.g. grades) to be more useful than those that involve student interactions (e.g. language, social activity). In contrast, privacy concerns were lower for outcomes-oriented scenarios than interactions-focused scenarios. An interesting new finding was a negative correlation of usefulness and privacy; we discuss this in the context of instructors' possible cost-benefit reasoning. We reflect on our findings with respect to future efforts in developing and fielding learning analytics tools.}
}

@article{lu2022surveydeep,
  title={A Survey of Deep Learning for Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Wenhao Yu and S. Welleck and Kai-Wei Chang},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10535},
  url={https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d},
  abstract={Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.}
}

@article{hu2022surveyknowledge,
  title={A Survey of Knowledge Enhanced Pre-Trained Language Models},
  author={Linmei Hu and Zeyi Liu and Ziwang Zhao and Lei Hou and Liqiang Nie and Juanzi Li},
  year={2022},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  doi={10.1109/TKDE.2023.3310002},
  url={https://www.semanticscholar.org/paper/a26623d52d24e03044a158cddad931ec5ab7304c},
  abstract={Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are categorized into KG-based and retrieval-based methods. Finally, we point out some promising future directions of KE-PLMs.}
}

@article{zhou2022surveyneural,
  title={A Survey on Neural Open Information Extraction: Current Status and Future Directions},
  author={Shaowen Zhou and Yu Bowen and Aixin Sun and Cheng Long and Jingyang Li and Haiyang Yu and Jianguo Sun},
  year={2022},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2205.11725},
  url={https://www.semanticscholar.org/paper/5de6ecf62f14c9263882f9f30d6448df9efd34e0},
  abstract={Open Information Extraction (OpenIE) facilitates domain-independent discovery of relational facts from large corpora. The technique well suits many open-world natural language understanding scenarios, such as automatic knowledge base construction, open-domain question answering, and explicit reasoning. Thanks to the rapid development in deep learning technologies, numerous neural OpenIE architectures have been proposed and achieve considerable performance improvement. In this survey, we provide an extensive overview of the state-of-the-art neural OpenIE models, their key design decisions, strengths and weakness. Then, we discuss limitations of current solutions and the open issues in OpenIE problem itself. Finally we list recent trends that could help expand its scope and applicability, setting up promising directions for future research in OpenIE. To our best knowledge, this paper is the first review on neural OpenIE.}
}

@article{wankmller2022comparisonapproaches,
  title={A comparison of approaches for imbalanced classification problems in the context of retrieving relevant documents for an analysis},
  author={Sandra Wankmüller},
  year={2022},
  journal={Journal of Computational Social Science},
  doi={10.1007/s42001-022-00191-7},
  url={https://www.semanticscholar.org/paper/a12e9a6863c8453787575172599389d2ddcd9f62},
  abstract={One of the first steps in many text-based social science studies is to retrieve documents that are relevant for an analysis from large corpora of otherwise irrelevant documents. The conventional approach in social science to address this retrieval task is to apply a set of keywords and to consider those documents to be relevant that contain at least one of the keywords. But the application of incomplete keyword lists has a high risk of drawing biased inferences. More complex and costly methods such as query expansion techniques, topic model-based classification rules, and active as well as passive supervised learning could have the potential to more accurately separate relevant from irrelevant documents and thereby reduce the potential size of bias. Yet, whether applying these more expensive approaches increases retrieval performance compared to keyword lists at all, and if so, by how much, is unclear as a comparison of these approaches is lacking. This study closes this gap by comparing these methods across three retrieval tasks associated with a data set of German tweets (Linder in SSRN, 2017. https://doi.org/10.2139/ssrn.3026393 ), the Social Bias Inference Corpus (SBIC) (Sap et al. in Social bias frames: reasoning about social and power implications of language. In: Jurafsky et al. (eds) Proceedings of the 58th annual meeting of the association for computational linguistics. Association for Computational Linguistics, p 5477–5490, 2020. https://doi.org/10.18653/v1/2020.aclmain.486 ), and the Reuters-21578 corpus (Lewis in Reuters-21578 (Distribution 1.0). [Data set], 1997. http://www.daviddlewis.com/resources/testcollections/reuters21578/ ). Results show that query expansion techniques and topic model-based classification rules in most studied settings tend to decrease rather than increase retrieval performance. Active supervised learning, however, if applied on a not too small set of labeled training instances (e.g. 1000 documents), reaches a substantially higher retrieval performance than keyword lists.}
}

@article{wang2022comparisonthree,
  title={A comparison of three approaches to covariate effects on latent factors},
  author={Ze Wang},
  year={2022},
  booktitle={Large-scale Assessments in Education},
  doi={10.1186/s40536-022-00148-2},
  url={https://www.semanticscholar.org/paper/c40412109167ae57baab6505edf9b628efca6d3a},
  abstract={In educational and psychological research, it is common to use latent factors to represent constructs and then to examine covariate effects on these latent factors. Using empirical data, this study applied three approaches to covariate effects on latent factors: the multiple-indicator multiple-cause (MIMIC) approach, multiple group confirmatory factor analysis (MG-CFA) approach, and the structural equation model trees (SEM Trees) approach. The MIMIC approach directly models covariate effects on latent factors. The MG-CFA approach allows testing of measurement invariance before latent factor means could be compared. The more recently developed SEM Trees approach partitions the sample into homogenous subsets based on the covariate space; model parameters are estimated separately for each subgroup. We applied the three approaches using an empirical dataset extracted from the eighth-grade U.S. data from the Trends in International Mathematics and Science Study 2019 database. All approaches suggested differences among mathematics achievement categories for the latent factor of mathematics self-concept. In addition, language spoken at home did not seem to affect students’ mathematics self-concept. Despite these general findings, the three approaches provided different pieces of information regarding covariate effects. For all models, we appropriately considered the complex data structure and sampling weights following recent recommendations for analyzing large-scale assessment data.}
}

@misc{alemany2022methodologycharacterize,
  title={A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America},
  author={L. A. Alemany and Luciana Benotti and Hernán Maina and Luc'ia M. Gonz'alez and Mariela Rajngewerc and Lautaro Mart'inez and Jos'e L. S'anchez and M. Schilman and Guido Ivetta and Alexia Halvorsen and Amanda Rojo and M. Bordone and Beatriz Busaniche},
  year={2022},
  url={https://www.semanticscholar.org/paper/a82a08b5e6a11f4d6fdff95dd30177957ed7855e},
  abstract={Automated decision-making systems, especially those based on natural language processing, are pervasive in our lives. They are not only behind the internet search engines we use daily, but also take more critical roles: selecting candidates for a job, determining suspects of a crime, diagnosing autism and more. Such automated systems make errors, which may be harmful in many ways, be it because of the severity of the consequences (as in health issues) or because of the sheer number of people they affect. When errors made by an automated system affect a population more than others, we call the system \textit{biased}. Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to audit them. In this paper, we present a methodology that spells out how social scientists, domain experts, and machine learning experts can collaboratively explore biases and harmful stereotypes in word embeddings and large language models. Our methodology is based on the following principles: * focus on the linguistic manifestations of discrimination on word embeddings and language models, not on the mathematical properties of the models * reduce the technical barrier for discrimination experts%, be it social scientists, domain experts or other * characterize through a qualitative exploratory process in addition to a metric-based approach * address mitigation as part of the training process, not as an afterthought}
}

@article{kim2022novelmodular,
  title={A novel modular modeling approach for understanding different electromechanics between left and right heart in rat},
  author={Nari Kim and Julius D. Pronto and D. Nickerson and A. Taberner and Peter J. Hunter},
  year={2022},
  booktitle={Frontiers in Physiology},
  doi={10.3389/fphys.2022.965054},
  url={https://www.semanticscholar.org/paper/2fba0d7b1293e4b13180fb3bccf86ed52ddcaf70},
  abstract={While ion channels and transporters involved in excitation-contraction coupling have been linked and constructed as comprehensive computational models, validation of whether each individual component of a model can be reused has not been previously attempted. Here we address this issue while using a novel modular modeling approach to investigate the underlying mechanism for the differences between left ventricle (LV) and right ventricle (RV). Our model was developed from modules constructed using the module assembly principles of the CellML model markup language. The components of three existing separate models of cardiac function were disassembled as to create smaller modules, validated individually, and then the component parts were combined into a new integrative model of a rat ventricular myocyte. The model was implemented in OpenCOR using the CellML standard in order to ensure reproducibility. Simulated action potential (AP), Ca2+ transient, and tension were in close agreement with our experimental measurements: LV AP showed a prolonged duration and a more prominent plateau compared with RV AP; Ca2+ transient showed prolonged duration and slow decay in LV compared to RV; the peak value and relaxation of tension were larger and slower, respectively, in LV compared to RV. Our novel approach of module-based mathematical modeling has established that the ionic mechanisms underlying the APs and Ca2+ handling play a role in the variation in force production between ventricles. This simulation process also provides a useful way to reuse and elaborate upon existing models in order to develop a new model.}
}

@article{mi2022reviewdevelopment,
  title={A review: development of named entity recognition (NER) technology for aeronautical information intelligence},
  author={Baigang Mi and Fan Yi},
  year={2022},
  booktitle={Artificial Intelligence Review},
  doi={10.1007/s10462-022-10197-2},
  url={https://www.semanticscholar.org/paper/ca2da2420fd25c8633641542730d3f0867c50f60}
}

@article{poythress2022semioticanalysis,
  title={A semiotic analysis of multiple systems of logic: using tagmemic theory to assess the usefulness and limitations of formal logics, and to produce a mathematical lattice model including multiple systems of logic},
  author={V. Poythress},
  year={2022},
  journal={Semiotica: Journal of the International Association for Semiotic Studies},
  doi={10.1515/sem-2020-0051},
  url={https://www.semanticscholar.org/paper/606db29a9d5cad5cd06b8eeb1f8beee390c87ca4},
  abstract={Abstract Tagmemic theory as a semiotic theory can be used to analyze multiple systems of logic and to assess their strengths and weaknesses. This analysis constitutes an application of semiotics and also a contribution to understanding of the nature of logic within the context of human meaning. Each system of logic is best adapted to represent one portion of human rationality. Acknowledging this correlation between systems and their targets helps explain the usefulness of more than one system. Among these systems, the two-valued system of classical logic takes its place. All the systems of logic can be incorporated into a complex mathematical model that has a place for each system and that represents a larger whole in human reasoning. The model can represent why tight formal systems of logic can be applied in some contexts with great success, but in other contexts are not directly applicable. The result suggests that human reasoning is innately richer than any one formal system of logic.}
}

@misc{song2022thesissubmitted,
  title={A thesis submitted to the Faculty of Graduate and Postdoctoral Affairs in partial fulfillment of the requirements for the degree of Master of Arts},
  author={Charlene Song},
  year={2022},
  url={https://www.semanticscholar.org/paper/25be22274b72f1337e977d94d0c94026d13a67d0}
}

@article{markta2022accuracypupils,
  title={ACCURACY OF PUPILS´ SELF-ASSESSMENT},
  author={Švamberk Šauerová Markéta and Smetáčková Irena},
  year={2022},
  booktitle={EduPort},
  doi={10.21062/edp.2022.009},
  url={https://www.semanticscholar.org/paper/fcfabc1d551304cde28a4f0658ed20e36559e05f},
  abstract={In this study, we investigated the accuracy of pupils´ self-assessment in two main school domains – mathematics and Czech language. The analysis explores whether pupils are able to evaluate adequately their own results in the didactic tests and then use some individual parameters to explain the level of self-assessment. The aim of the study was to analyze whether groups of pupils with different self-assessments of school tasks in the Czech language and mathematics (significant underestimation, adequate self-assessment, significant overestimation) differ in some of the cognitive skills studied. Our study questions were as follows: (1) Do pupils assess their achievements in particular school tasks accurately, or inaccurately? (2) Do pupils´ self-assessments differ in mathematics and language? (3) Do the pupil´s self-assessment correlate with individual parameters? The main tool used in the study was a didactic test on mathematics and a didactic test on the Czech language based on the Czech National Curricula Document and created by an expert team. In addition, Raven's Color Progressive Matrices (CPM), Similarities from the Wechsler Intelligence (WISC-SIM), and the Rey-Osterrieth Complex Figure (ROCF) were used. Considering the nature of the data, the non-parametric Kruskal-Wallis ANOVA was used. The present study is a part of the larger research project, involving 29 primary school classes, 657 pupils in total. Based on the data obtained, it can be concluded that the accuracy of pupils' self-assessments is low, while the accuracy of pupils' self-assessments in mathematics and Czech language differs (in mathematics there are more children with more accurate estimates and more pupils who underestimate themselves, in Czech language there are more pupils who overestimate their performance. Statistically significant differences were observed in the domains of Raven's Color Progressive Matrices and Rey-Osterrieth Figure, and in terms of the focus of each test, it could be concluded that there are significant differences between the groups in the domain of non-verbal reasoning skills and in the domain of analytical and organizational perceptual activity and memory. In the area of verbal intellectual abilities, there were no significant differences between the groups.}
}

@article{ji2022afrbertattentionbased,
  title={AFR-BERT: Attention-based mechanism feature relevance fusion multimodal sentiment analysis model},
  author={Mingyu Ji and Jiawei Zhou and Wei Ning},
  year={2022},
  booktitle={PLoS ONE},
  doi={10.1371/journal.pone.0273936},
  url={https://www.semanticscholar.org/paper/918f34bd4274316d684dd6c267b13fe010a74a6e},
  abstract={Multimodal sentiment analysis is an essential task in natural language processing which refers to the fact that machines can analyze and recognize emotions through logical reasoning and mathematical operations after learning multimodal emotional features. For the problem of how to consider the effective fusion of multimodal data and the relevance of multimodal data in multimodal sentiment analysis, we propose an attention-based mechanism feature relevance fusion multimodal sentiment analysis model (AFR-BERT). In the data pre-processing stage, text features are extracted using the pre-trained language model BERT (Bi-directional Encoder Representation from Transformers), and the BiLSTM (Bi-directional Long Short-Term Memory) is used to obtain the internal information of the audio. In the data fusion phase, the multimodal data fusion network effectively fuses multimodal features through the interaction of text and audio information. During the data analysis phase, the multimodal data association network analyzes the data by exploring the correlation of fused information between text and audio. In the data output phase, the model outputs the results of multimodal sentiment analysis. We conducted extensive comparative experiments on the publicly available sentiment analysis datasets CMU-MOSI and CMU-MOSEI. The experimental results show that AFR-BERT improves on the classical multimodal sentiment analysis model in terms of relevant performance metrics. In addition, ablation experiments and example analysis show that the multimodal data analysis network in AFR-BERT can effectively capture and analyze the sentiment features in text and audio.}
}

@article{gulwani2022aiassistedprogramming,
  title={AI-assisted programming: applications, user experiences, and neuro-symbolic techniques (keynote)},
  author={Sumit Gulwani},
  year={2022},
  booktitle={ESEC/SIGSOFT FSE},
  doi={10.1145/3540250.3569444},
  url={https://www.semanticscholar.org/paper/11230f03465d8ab073815397717d8afa3f3dae1c}
}

@article{yu2022alertadapt,
  title={ALERT: Adapt Language Models to Reasoning Tasks},
  author={Ping Yu and Tianlu Wang and O. Yu. Golovneva and Badr AlKhamissi and Gargi Ghosh and Mona T. Diab and Asli Celikyilmaz},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.08286},
  url={https://www.semanticscholar.org/paper/95c11cc5820ba32c60d5f2671f6567b9914a4978},
  abstract={Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.To address this question, we introduce {pasted macro ‘OUR’}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. {pasted macro ‘OUR’}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using {pasted macro ‘OUR’}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.}
}

@article{2022algorithmmethod,
  title={ALGORITHM METHOD IN TEACHING RUSSIAN AT SECONDARY SCHOOL},
  author={Юлия Владимировна Подкина},
  year={2022},
  booktitle={Tomsk state pedagogical university bulletin},
  doi={10.23951/1609-624x-2022-6-80-87},
  url={https://www.semanticscholar.org/paper/ded393f5b3432f3d0b9258fff2b9db33b204bf84},
  abstract={Введение. Обучение русскому языку в средней школе, развитие речи и формирование орфографических и пунктуационных навыков – важная задача, которая сопряжена с рядом трудностей. Эффективному изучению русского языка в общеобразовательной школе зачастую препятствуют такие факторы, как плохая усидчивость, отсутствие интереса к предмету, билингвизм и другое. Метод алгоритмизированного представления правил русской орфографии и пунктуации способствует наилучшему усвоению учебного материала и позволяет повысить качество обучения русскому языку школьников среднего и старшего звена. Цель − обоснование эффективности метода алгоритма в обучении русскому языку детей общеобразовательных средних школ, рассмотрение примерных моделей обучающих алгоритмов. Материал и методы. В работе применялись теоретические методы (моделирование, анализ, синтез); эмпирические методы (наблюдение, сравнение, эксперимент). Результаты и обсуждение. Простое заучивание правил не всегда приводит к повышению грамотности учащихся. Метод алгоритма предусматривает совместное с учениками составление алгоритмизированных схем различных видов, которые иллюстрируют изучаемое правило, позволяют пошагово отработать механизм рассуждения при выполнении орфографических и пунктуационных заданий. Такой подход способствует достижению высокого качества знаний путем систематической отработки практических навыков с помощью схем, адаптируемых под потребности каждого ребенка. Обучающий алгоритм может иметь разные виды: от четко сформулированной схемы (похожей на математический пример) до красочной иллюстрации, которая будет понятна детям с творческими способностями. Заключение. Метод алгоритма применяют для изучения практически любого правила русской орфографии и пунктуации. В созданной совместно с учащимися схеме должно быть отведено место для исключений и для примеров, которые ребенок впишет самостоятельно. При создании обучающей схемы школьник является активным соавтором. Схема никогда не является замкнутой системой. Она дорабатывается и совершенствуется в процессе практической деятельности учащихся. У детей из одного класса схемы могут быть совершенно различны, так как усовершенствованы и доработаны самостоятельно под руководством учителя.
 Introduction. Teaching Russian in secondary school, speech development and the formation of spelling and punctuation skills is an important task that involves a number of difficulties. Effective study of the Russian language in a secondary school is often hindered by factors such as poor perseverance, lack of interest in the subject, bilingualism, and more. Russian Russian spelling rules algorithmized representation method is considered in this paper, which allows to improve the quality of teaching Russian to middle and senior school students. The purpose is to substantiate the effectiveness of the algorithm method in teaching the Russian language to children of secondary schools, to consider approximate models of training algorithms. Material and methods. Theoretical methods (modeling, analysis, synthesis) were used in the work; empirical methods (observation, comparison, experiment). Results and discussion. Simple memorizing of the rules does not always lead to increased literacy of students. The algorithm method provides for the joint compilation of algorithmic schemes of various types with students, which illustrate the rule being studied, allow you to work out the mechanism of reasoning step by step when performing spelling and punctuation tasks. This approach contributes to the achievement of a high quality of knowledge through the systematic development of practical skills with the help of schemes adapted to the needs of each child. The training algorithm can have different types: from a clearly formulated scheme (similar to a mathematical example) up to a colorful illustration that will be understandable to children with creative abilities. Conclusion. The algorithm method can be applied to study almost any rule of Russian spelling and punctuation. In the scheme created jointly with the students, there should be a place for exceptions and for examples that the child will enter independently. When creating a training scheme, the student is an active co-author. A circuit is never a closed system. It is being refined and improved in the process of practical activity of students. For children from the same class, the schemes can be completely different, as they have been improved and finalized independently.}
}

@article{mare2022updatethermal,
  title={AN UPDATE OF THERMAL ERROR COMPENSATION MODEL VIA ON-MACHINE MEASUREMENT},
  author={M. Mareš and O. Horejš and Michal Straka and J. Švéda and Tomáš Kozlok},
  year={2022},
  journal={MM Science Journal},
  doi={10.17973/mmsj.2022_12_2022150},
  url={https://www.semanticscholar.org/paper/796f47a4059604f27ad57c3760cc7ebea9f6a020},
  abstract={Software compensation is state-of-the-art technology used to reduce CNC machine tool thermal errors, and it belongs to a key intelligent functions of modern machine tools. However, a pretrained and nonadaptive model may not be accurate and robust enough for long-term application. This research presents a transfer function based thermal error compensation model updated via on-machine measurement. A mathematical model is implemented into the machine management software of a large horizontal machining centre to compensate for thermal errors in real time using C#/C++ programming language. The results show that after the thermal error compensation model is updated via on-machine measurement, the prediction accuracy, measured as peak-to-peak values, and the normalized root mean squared error are significantly improved. The prediction accuracy of the compensation model updated via on-machine measurement strongly depends on the sampling interval of the on machine measurements.}
}

@misc{nam2022achievingunderstanding,
  title={Achieving and Understanding Out-of-Distribution Generalization in Systematic Reasoning in Small-Scale Transformers},
  author={A. Nam and Mustafa Abdool and Trevor Maxfield and James L. McClelland},
  year={2022},
  url={https://www.semanticscholar.org/paper/8283064365ae7594d891e8b7daf36fd37ca809b0},
  abstract={Out-of-distribution generalization (OODG) is a longstanding challenge for neural networks. This challenge is quite apparent in tasks with well-defined variables and rules, where explicit use of the rules could solve problems independently of the particular values of the variables, but networks tend to be tied to the range of values sampled in their training data. Large transformer-based language models have pushed the boundaries on how well neural networks can solve previously unseen problems, but their complexity and lack of clarity about the relevant content in their training data obfuscates how they achieve such robustness. As a step toward understanding how transformer-based systems generalize, we explore the question of OODG in small scale transformers trained with examples from a known distribution. Using a reasoning task based on the puzzle Sudoku, we show that OODG can occur on a complex problem if the training set includes examples sampled from the whole distribution of simpler component tasks. Successful generalization depends on carefully managing positional alignment when absolute position encoding is used, but we find that suppressing sensitivity to absolute positions overcomes this limitation. Taken together our results represent a small step toward understanding and promoting systematic generalization in transformers.}
}

@article{hppner2022advantagesdisadvantages,
  title={Advantages and disadvantages of (dedicated) model transformation languages},
  author={S. Höppner and Yves Haas and Matthias Tichy and Katharina Juhnke},
  year={2022},
  booktitle={Empirical Software Engineering},
  doi={10.1007/s10664-022-10194-7},
  url={https://www.semanticscholar.org/paper/d96fa397010fa107aadcedbff577feead334e3be},
  abstract={Model driven development envisages the use of model transformations to evolve models. Model transformation languages, developed for this task, are touted with many benefits over general purpose programming languages. However, a large number of these claims have not yet been substantiated. They are also made without the context necessary to be able to critically assess their merit or built meaningful empirical studies around them. The objective of our work is to elicit the reasoning, influences and background knowledge that lead people to assume benefits or drawbacks of model transformation languages. We conducted a large-scale interview study involving 56 participants from research and industry. Interviewees were presented with claims about model transformation languages and were asked to provide reasons for their assessment thereof. We qualitatively analysed the responses to find factors that influence the properties of model transformation languages as well as explanations as to how exactly they do so. Our interviews show, that general purpose expressiveness of GPLs, domain specific capabilities of MTLs as well as tooling all have strong influences on how people view properties of model transformation languages. Moreover, the Choice of MTL, the Use Case for which a transformation should be developed as well as the Skill s of involved stakeholders have a moderating effect on the influences, by changing the context to consider. There is a broad body of experience, that suggests positive and negative influences for properties of MTLs. Our data suggests, that much needs to be done in order to convey the viability of model transformation languages. Efforts to provide more empirical substance need to be undergone and lacklustre language capabilities and tooling need to be improved upon. We suggest several approaches for this that can be based on the results of the presented study.}
}

@article{abramson2022applicationpseudologlikelihoods,
  title={An Application of Pseudo-Log-Likelihoods to Natural Language Scoring},
  author={Darren Abramson and Ali Emami},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/16bf88a6d172699cb9a26a6936efb4941e3f3c13},
  abstract={Language models built using semi-supervised machine learning on large corpora of natural language have very quickly enveloped the fields of natural language generation and understanding. In this paper we apply a zero-shot approach independently developed by a number of researchers now gaining recognition as a significant alternative to fine-tuning for evaluation on common sense tasks. A language model with relatively few parameters and training steps compared to a more recent language model (T5) can outperform it on a recent large data set (TimeDial), while displaying robustness in its performance across a similar class of language tasks. Surprisingly, this result is achieved by using a hyperparameter-free zero-shot method with the smaller model, compared to fine-tuning to the larger model. We argue that robustness of the smaller model ought to be understood in terms of compositionality, in a sense that we draw from recent literature on a class of similar models. We identify a practical cost for our method and model: high GPU-time for natural language evaluation. The zero-shot measurement technique that produces remarkable stability, both for ALBERT and other BERT variants, is an application of pseudo-log-likelihoods to masked language models for the relative measurement of probability for substitution alternatives in forced choice language tasks such as the Winograd Schema Challenge, Winogrande, and others. One contribution of this paper is to bring together a number of similar, but independent strands of research. We produce some absolute state-of-the-art results for common sense reasoning in binary choice tasks, performing better than any published result in the literature, including fine-tuned efforts. We show a remarkable consistency of the model's performance under adversarial settings, which we argue is best explained by the model's compositionality of representations.}
}

@article{zhang2022empiricalinvestigation,
  title={An Empirical Investigation of Commonsense Self-Supervision with Knowledge Graphs},
  author={Jiarui Zhang and Filip Ilievski and Kaixin Ma and Jonathan M Francis and A. Oltramari},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2205.10661},
  url={https://www.semanticscholar.org/paper/651ae53112e73b02440773727b68cedbf8322705},
  abstract={Self-supervision based on the information extracted from large knowledge graphs has been shown to improve the generalization of language models, in zero-shot evaluation on various downstream language reasoning tasks. Since these improvements are reported in aggregate, however, little is known about (i) how to select the appropriate knowledge for solid performance across tasks, (ii) how to combine this knowledge with neural language models, and (iii) how these pairings affect granular task performance. In this paper, we study the effect of knowledge sampling strategies and sizes that can be used to generate synthetic data for adapting language models. We study the effect of different synthetic datasets on language models with various architectures and sizes. The resulting models are evaluated against four task properties: domain overlap, answer similarity, vocabulary overlap, and answer length. Our experiments show that encoder-decoder models benefit from more data to learn from, whereas sampling strategies that balance across different aspects yield best performance. Most of the improvement occurs on questions with short answers and dissimilar answer candidates, which corresponds to the characteristics of the data used for pre-training.}
}

@article{khan2022executableformal,
  title={An Executable Formal Model of the VHDL in Isabelle/HOL},
  author={Wilayat Khan and Zhé Hóu and David Sanán and J. Nebhen and Yang Liu and Alwen Tiu},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/37b0b6db785f8c37460e2bb80da138c1443af5b4},
  abstract={In the hardware design process, hardware components are usually described in a hardware description language. Most of the hardware description languages, such as Verilog and VHDL, do not have mathematical foundation and hence are not fit for formal reasoning about the design. To enable formal reasoning in one of the most commonly used description language VHDL, we define a formal model of the VHDL language in Isabelle/HOL. Our model targets the functional part of VHDL designs used in industry, specifically the design of the LEON3 processor's integer unit. We cover a wide range of features in the VHDL language that are usually not modelled in the literature and define a novel operational semantics for it. Furthermore, our model can be exported to OCaml code for execution, turning the formal model into a VHDL simulator. We have tested our simulator against simple designs used in the literature, as well as the div32 module in the LEON3 design. The Isabelle/HOL code is publicly available: https://zhehou.github.io/apps/VHDLModel.zip}
}

@article{katra2022experimentationframework,
  title={An Experimentation Framework for Specification and Verification of Web Services},
  author={Szymon Katra and Wiktor B. Daszczuk and Danny Czejdo},
  year={2022},
  booktitle={Conference on Computer Science and Information Systems},
  doi={10.15439/2022F188},
  url={https://www.semanticscholar.org/paper/9fbe3dc7a2229a5435fc7ace6978550af5ac3268},
  abstract={Designing and implementing Web Services constitutes a large and constantly growing part of the information technology market. Web Services have specific scenarios in which distributed processes and network resources are used. This aspect of services requires integration with the model checkers. This article presents the experimentation framework in which services can be specified and then formally analyzed for deadlock-freedom, achievement of process goals, and similar features. Rybu4WS language enriches the basic Rybu language with the ability to use variables in processes, service calls between servers, new structural instructions, and other constructions known to programmers while remaining in line with declarative, mathematical IMDS formalism. Additionally, the development environment allows simulation of a counterexample or a witness - obtained as a result of the model checking - in a similar way to traditional debuggers.}
}

@article{jeon2022informationtheoreticanalysis,
  title={An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws},
  author={Hong Jun Jeon and Benjamin Van Roy},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.01365},
  url={https://www.semanticscholar.org/paper/dab053b7713b77ab09f50b90b3176607912e913a},
  abstract={We study the compute-optimal trade-off between model and training data set sizes for large neural networks. Our result suggests a linear relation similar to that supported by the empirical analysis of chinchilla. While that work studies transformer-based large language models trained on the MassiveText corpus gopher, as a starting point for development of a mathematical theory, we focus on a simpler learning model and data generating process, each based on a neural network with a sigmoidal output unit and single hidden layer of ReLU activation units. We introduce general error upper bounds for a class of algorithms which incrementally update a statistic (for example gradient descent). For a particular learning model inspired by barron 1993, we establish an upper bound on the minimal information-theoretically achievable expected error as a function of model and data set sizes. We then derive allocations of computation that minimize this bound. We present empirical results which suggest that this approximation correctly identifies an asymptotic linear compute-optimal scaling. This approximation also generates new insights. Among other things, it suggests that, as the input dimension or latent space complexity grows, as might be the case for example if a longer history of tokens is taken as input to a language model, a larger fraction of the compute budget should be allocated to growing the learning model rather than training data.}
}

@article{bellomarini2022overviewvadalog,
  title={An Overview of Vadalog: a System for Reasoning over Large Knowledge Graphs},
  author={Luigi Bellomarini and Davide Benedetto and Emanuel Sallinger},
  year={2022},
  booktitle={Sistemi Evoluti per Basi di Dati},
  url={https://www.semanticscholar.org/paper/83dc0eca1a453e2970d32923bb48bb84976bd968}
}

@article{amaliyah2022analisiskesulitan,
  title={Analisis Kesulitan Belajar Matematika dalam Menyelesaikan Soal Cerita di Kelas IV Sekolah Dasar Negeri Pakujaya 02},
  author={Aam Amaliyah and Luthfia Nur Maulida and N. Safitri and Ratri Hersita Dewi and Sabgi Wulan Septiara},
  year={2022},
  booktitle={ALSYS},
  doi={10.58578/alsys.v2i3.386},
  url={https://www.semanticscholar.org/paper/5023bebd78bb5f55a0d706f94b27f718b9c83cfc},
  abstract={Students with learning difficulties in mathematics often make mistakes in solving story problems on fractional material. This research uses descriptive qualitative. The purpose of this study was to determine the types of learning difficulties in mathematics experienced by students, the factors that influence learning difficulties, and to reveal the efforts that can be made to overcome the difficulties in learning mathematics in grade IV Pakujaya 02 State Elementary School. Data collection techniques were observation and interviews. . Based on data analysis and discussion, students experienced errors, namely: 1. Understanding the problem, namely errors in interpreting language and making mathematical models. The reason is incomplete/wrong reasoning and low student ability. 2. Planning for problem solving is an error in connecting one concept with another concept. The cause of this error is the humanistic thinking of students. 3. Implement problem solving planning, namely errors in implementing incorrect formulas. Errors in this aspect are caused by incomplete or incorrect reasoning and students' humanistic thinking.}
}

@article{shidqiya2022analysisstudents,
  title={Analysis of Students’ Mathematical Thinking Ability in Terms of Self Efficacy},
  author={Adiba Idlal Shidqiya and Sukestiyarno Sukestiyarno},
  year={2022},
  journal={Unnes Journal of Mathematics Education},
  doi={10.15294/ujme.v11i3.58772},
  url={https://www.semanticscholar.org/paper/fa5b5d97f15b5244e34a49e44317a1822b3e0daa},
  abstract={Mathematical thinking ability must be owned by students to solve various problems. Students are considered capable of fulfilling the indicators of mathematical thinking ability properly if they are balanced with good self-efficacy abilities. This research method is qualitative which aims to find new indicators and describe mathematical thinking ability in terms of self-efficacy and provide recommendations for teachers. The research subjects were six students from the first year of senior high school using purposive sampling. Indicators of mathematical thinking ability, include 1) Reasoning: identifying concepts and problems; 2) Generalizing: demonstrating mathematical ideas in writing and using mathematical language to express ideas correctly; 3) Critical Thinking: using representations to create mathematical models; 4) Problem Solving: planning problem solving strategies, implementing and checking results. 5) Communicating: revealing the results of problem solving. The results: 1) low self-efficacy’s students were only able to master reasoning; 2) moderate self-efficacy’s students are able to master reasoning, generalizing, and critical thinking; 3) high self-efficacy’s students are able to master all indicators. Recommendations for teachers are by giving opportunity to low self-efficacy’s students to speak in public, give appreciation for their efforts and reprimand if it doesn’t lower their confidence when they make mistakes.}
}

@article{yu2022analysiscorrelation,
  title={Analysis of the Correlation between Academic Performance and Learning Motivation in English Course under a Corpus-Data-Driven Blended Teaching Model},
  author={Lan Yu and Jun Shen},
  year={2022},
  booktitle={Scientific Programming},
  doi={10.1155/2022/3407270},
  url={https://www.semanticscholar.org/paper/6f554d023d8e403e5ee70268e55f5b2fe1be574e},
  abstract={To explore the correlation between academic performance and learning motivation in English course under a corpus-data-driven blended teaching model, this study set research objects as 62 year-2020-enrolled undergraduate students majoring in English from a university in Jinan City, Shandong Province, eastern China. According to their previous frequencies of using information technology to learn English, these 62 students were divided into two groups: practice group with high frequency and control group with low frequency, with 31 students in each group. The two groups of students were taught 3 English lessons per week for a total of 15 weeks by the exact same teachers using a corpus-data-driven blended teaching model. The students’ English academic performances were assessed by well-organized final tests, and their English learning motivations were measured by a motivation scale and questionnaires. The results show that the correlation coefficients between the average score of motivation questionnaires, intrinsic motivation factors, extrinsic motivation factors, and the average score of academic performances in practice group were 0.894, 0.682, and 0.724, respectively, while those in control group were 0.749, 0.836, and 0.904. In all the above correlation analyses, the significance level is 0.01, and all coefficient values are higher than critical value. Hence, there is a positive correlation between learning motivation and academic performance of the two groups of subjects. It is found that the corpus-data-driven blended teaching model has a significant impact on college students’ English academic performance and learning motivation, and it has a positive effect on the improvement of their English academic performance and the cultivation of learning motivation. In general, the key to this teaching model lies in reasoning and acquisition by analyzing the language provided by the corpus, and the whole process of data-driven learning is student-centered. Students are exposed to a large number of authentic language knowledge and cultural information, which promotes the sensitivity to relevant points. The results of this paper provide a reference for further research on the analysis of the correlation between academic performance and learning motivation in English course under the corpus-data-driven blended teaching model.}
}

@article{kumar2022answerlevelcalibration,
  title={Answer-level Calibration for Free-form Multiple Choice Question Answering},
  author={Sawan Kumar},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2022.acl-long.49},
  url={https://www.semanticscholar.org/paper/a5584d2d9b0de9e1692241d46d0c70942919cd60},
  abstract={Pre-trained language models have recently shown that training on large corpora using the language modeling objective enables few-shot and zero-shot capabilities on a variety of NLP tasks, including commonsense reasoning tasks. This is achieved using text interactions with the model, usually by posing the task as a natural language text completion problem. While using language model probabilities to obtain task specific scores has been generally useful, it often requires task-specific heuristics such as length normalization, or probability calibration. In this work, we consider the question answering format, where we need to choose from a set of (free-form) textual choices of unspecified lengths given a context. We present ALC (Answer-Level Calibration), where our main suggestion is to model context-independent biases in terms of the probability of a choice without the associated context and to subsequently remove it using an unsupervised estimate of similarity with the full context. We show that our unsupervised answer-level calibration consistently improves over or is competitive with baselines using standard evaluation metrics on a variety of tasks including commonsense reasoning tasks. Further, we show that popular datasets potentially favor models biased towards easy cues which are available independent of the context. We analyze such biases using an associated F1-score. Our analysis indicates that answer-level calibration is able to remove such biases and leads to a more robust measure of model capability.}
}

@article{zhou2022applicationthreeflow,
  title={Application of Three-Flow Fusion Technology Based on Modelica in Thermal Power Digital Twin},
  author={Dongyan Zhou and Haidong Gao and Wenyu Wang and Jun Cao and Wenfei Yang and Ruirui Zeng and Yuan He},
  year={2022},
  journal={IEEE Journal of Radio Frequency Identification},
  doi={10.1109/JRFID.2022.3205855},
  url={https://www.semanticscholar.org/paper/5391cf3bf8f2cc858ee1a532be7d9e2e6b6f6983},
  abstract={Thermal power plants gather large energy infrastructure; therefore, massive historical and real-time data of equipment operation will be generated in daily operation. Digital industrialization puts forward higher requirements for the use of big data than simple tasks, such as generating reports. MWorks is a multidomain unified modeling and simulation platform based on the Modelica language. In this study, MWorks is used to realize the modeling of multidomain systems, including electrical, thermal, mechanical, fluid, and heat transfer, in power plants. The test, calibration, verification, parameter optimization, and fault diagnosis of the thermal power plant mathematical models, which are historical and real-time data-driven, are discussed. The technology of three-flow fusion, including material flow, energy flow, and information flow, and its application in thermal power digital twin are explored.}
}

@article{alghamdi2022armathdataset,
  title={ArMATH: a Dataset for Solving Arabic Math Word Problems},
  author={Reem Alghamdi and Zhenwen Liang and Xiangliang Zhang},
  year={2022},
  booktitle={International Conference on Language Resources and Evaluation},
  url={https://www.semanticscholar.org/paper/4aca69be58a271b1be45ec7ebb3586569cec50b0}
}

@article{kar2022arggenprompting,
  title={ArgGen: Prompting Text Generation Models for Document-Level Event-Argument Aggregation},
  author={Debanjana Kar and S. Sarkar and Pawan Goyal},
  year={2022},
  booktitle={AACL/IJCNLP},
  doi={10.18653/v1/2022.findings-aacl.37},
  url={https://www.semanticscholar.org/paper/61f49465c0d53663ad5264c8f683c6724d31eef1},
  abstract={Most of the existing discourse-level Information Extraction tasks have been modeled to be extractive in nature. However, we argue that extracting information from larger bodies of discourse-like documents requires more natural language understanding and reasoning capabilities. In our work, we propose the novel task of document-level event argument aggregation which generates consolidated event-arguments at a document-level with minimal loss of information. More specifically, we focus on generating precise document-level information frames in a multilingual setting using prompt-based methods. In this paper, we show the effectiveness of prompt-based text generation approach to generate document-level argument spans in a low-resource and zero-shot setting. We also release the first of its kind multilingual event argument aggregation dataset that can be lever-aged in other related multilingual text generation tasks as well: https://github.com/}
}

@article{tewes2022artificialintelligence,
  title={Artificial Intelligence in the American Healthcare Industry: Looking Forward to 2030},
  author={F. Tewes},
  year={2022},
  journal={Journal of Medical Research and Surgery},
  doi={10.52916/jmrs224089},
  url={https://www.semanticscholar.org/paper/6baa97e2ca007eb2eeb51490f604d2bfd767fa0c},
  abstract={Artificial intelligence (AI) has the potential to speed up the exponential growth of cutting-edge technology, much way the Internet did. Due to intense competition from the private sector, governments, and businesspeople around the world, the Internet has already reached its peak as an exponential technology. In contrast, artificial intelligence is still in its infancy, and people all over the world are unsure of how it will impact their lives in the future. Artificial intelligence, is a field of technology that enables robots and computer programmes to mimic human intellect by teaching a predetermined set of software rules to learn by repetitive learning from experience and slowly moving toward maximum performance. Although this intelligence is still developing, it has already demonstrated five different levels of independence. Utilized initially to resolve issues. Next, think about solutions. Third, respond to inquiries. Fourth, use data analytics to generate forecasts. Fifth, make tactical recommendations. Massive data sets and "iterative algorithms," which use lookup tables and other data structures like stacks and queues to solve issues, make all of this possible. Iteration is a strategy where software rules are regularly adjusted to patterns in the data for a certain number of iterations. The artificial intelligence continuously makes small, incremental improvements that result in exponential growth, which enables the computer to become incredibly proficient at whatever it is trained to do. For each round of data processing, the artificial intelligence tests and measures its performance to develop new expertise. In order to address complicated problems, artificial intelligence aims to create computer systems that can mimic human behavior and exhibit human-like thought processes [1]. Artificial intelligence technology is being developed to give individualized medication in the field of healthcare. By 2030, six different artificial intelligence sectors will have considerably improved healthcare delivery through the utilization of larger, more accessible data sets. The first is machine learning. This area of artificial intelligence learns automatically and produces improved results based on identifying patterns in the data, gaining new insights, and enhancing the outcomes of whatever activity the system is intended to accomplish. It does this without being trained to learn a particular topic. Here are several instances of machine learning in the healthcare industry. The first is the IBM Watson Genomics, which aids in rapid disease diagnosis and identification by fusing cognitive computing with genome-based tumour sequencing. Second, a project called Nave Bayes allows for the prediction of diabetes years before an official diagnosis, before it results in harm to the kidneys, the heart, and the nerves. Third, employing two machine learning approaches termed classification and clustering to analyse the Indian Liver Patient Data (ILPD) set in order to predict liver illness before this organ that regulates metabolism becomes susceptible to chronic hepatitis, liver cancer, and cirrhosis [2]. Second, deep learning. Deep learning employs artificial intelligence to learn from data processing, much like machine learning does. Deep learning, on the other hand, makes use of synthetic neural networks that mimic human brain function to analyse data, identify relationships between the data, and provide outputs based on positive and negative reinforcement. For instance, in the fields of Magnetic Resonance Imaging (MRI) and Computed Tomography (CT), deep learning aids in the processes of picture recognition and object detection. Deep learning algorithms for the early identification of Alzheimer's, diabetic retinopathy, and breast nodule ultrasound detection are three applications of this cutting-edge technology in the real world. Future developments in deep learning will make considerable improvements in pathology and radiology pictures [3]. Third, neural networks. The artificial intelligence system can now accept massive data sets, find patterns within the data, and respond to queries regarding the information processed because the computer learning process resembles a network of neurons in the human brain. Let's examine a few application examples that are now applicable to the healthcare sector. According to studies from John Hopkins University, surgical errors are a major contributor to medical malpractice claims since they happen more than 4,000 times a year in just the United States due to the human error of surgeons. Neural networks can be used in robot-assisted surgery to model and plan procedures, evaluate the abilities of the surgeon, and streamline surgical activities. In one study of 379 orthopaedic patients, it was discovered that robotic surgery using neural networks results in five times fewer complications than surgery performed by a single surgeon. Another application of neural networks is in visualising diagnostics, which was proven to physicians by Harvard University researchers who inserted an image of a gorilla to x-rays. Of the radiologists who saw the images, 83% did not recognise the gorilla. The Houston Medical Research Institute has created a breast cancer early detection programme that can analyse mammograms with 99 percent accuracy and offer diagnostic information 30 times faster than a human [4]. Cognitive computing is the fourth. Aims to replicate the way people and machines interact, showing how a computer may operate like the human brain when handling challenging tasks like text, speech, or image analysis. Large volumes of patient data have been analysed, with the majority of the research to date focusing on cancer, diabetes, and cardiovascular disease. Companies like Google, IBM, Facebook, and Apple have shown interest in this work. Cognitive computing made up the greatest component of the artificial market in 2020, with 39% of the total [5]. Hospitals made up 42% of the market for cognitive computing end users because of the rising demand for individualised medical data. IBM invested more than $1 billion on the development of the WATSON analytics platform ecosystem and collaboration with startups committed to creating various cloud and application-based systems for the healthcare business in 2014 because it predicted the demand for cognitive computing in this sector. Natural Language Processing (NLP) is the fifth. This area of artificial intelligence enables computers to comprehend and analyse spoken language. The initial phase of this pre-processing is to divide the data up into more manageable semantic units, which merely makes the information simpler for the NLP system to understand. Clinical trial development is experiencing exponential expansion in the healthcare sector thanks to NLP. First, the NLP uses speech-to-text dictation and structured data entry to extract clinical data at the point of care, reducing the need for manual assessment of complex clinical paperwork. Second, using NLP technology, healthcare professionals can automatically examine enormous amounts of unstructured clinical and patient data to select the most suitable patients for clinical trials, perhaps leading to an improvement in the patients' health [6]. Computer vision comes in sixth. Computer vision, an essential part of artificial intelligence, uses visual data as input to process photos and videos continuously in order to get better results faster and with higher quality than would be possible if the same job were done manually. Simply put, doctors can now diagnose their patients with diseases like cancer, diabetes, and cardiovascular disorders more quickly and at an earlier stage. Here are a few examples of real-world applications where computer vision technology is making notable strides. Mammogram images are analysed by visual systems that are intended to spot breast cancer at an early stage. Automated cell counting is another example from the real world that dramatically decreases human error and raises concerns about the accuracy of the results because they might differ greatly depending on the examiner's experience and degree of focus. A third application of computer vision in the real world is the quick and painless early-stage tumour detection enabled by artificial intelligence. Without a doubt, computer vision has the unfathomable potential to significantly enhance how healthcare is delivered. Other than for visual data analysis, clinicians can use this technology to enhance their training and skill development. Currently, Gramener is the top company offering medical facilities and research organisations computer vision solutions [7]. The usage of imperative rather than functional programming languages is one of the key difficulties in creating artificial intelligence software. As artificial intelligence starts to increase exponentially, developers employing imperative programming languages must assume that the machine is stupid and supply detailed instructions that are subject to a high level of maintenance and human error. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures of research and development. As a result, software developers have contributed to the unreasonably high cost of medical care. Functional programming languages, on the other hand, demand that the developer use their problem-solving abilities as though the computer were a mathematician. As a result, compared to the number of lines of code needed by the programme to perform the same operation, mathematical functions are orders of magnitude shorter. In software with hundreds of thousands of lines of code, human error detection is challenging. Therefore, the substantial amount of ensuing maintenance may become ridiculously expensive, maintaining the high expenditures o}
}

@article{zimmerman2022assessingphysics,
  title={Assessing physics quantitative literacy in algebra-based physics: lessons learned},
  author={Charlotte Zimmerman and Andrew McCarty and Suzanne White Brahmia and Alexis Olsho and Mieke De Cock and A. Boudreaux and Trevor I. Smith and Philip Eaton},
  year={2022},
  booktitle={Physics Education Research Conference Proceedings},
  doi={10.1119/perc.2022.pr.zimmerman},
  url={https://www.semanticscholar.org/paper/8f16d42771af2c1227c7a4cf6ad219e54351c9f7},
  abstract={Physics quantitative literacy (PQL)—applying familiar mathematics in novel ways in the context of physics— is ubiquitous across physics classrooms. The Physics Inventory for Quantitative Literacy, or PIQL, is a recently published reasoning inventory that can be used to assess PQL from calculus-based introductory physics through upper division courses (White Brahmia et al. 2021). There remains a need, however, for assessment of quantitative reasoning at the algebra-based level which includes not only algebra-based college courses but also pre-college physics courses. We present recent work adapting the PIQL to an algebra-based context towards developing the GERQN—the Generalized Equation-based Reasoning inventory for Quantities and Negativity. We report lessons learned from our efforts to adapt items from the calculus-based PIQL to the algebra-based GERQN, and provide examples of how items were revised to be within students proximal zone. We also report on our experience translating the GERQN into Flemish as part of a larger, on-going research project, and what we learned about language accessibility for native and non-native English speakers alike for developing assessment items, curricular materials, and when speaking with students.}
}

@misc{kogan2022assessingacademic,
  title={Assessing the Academic Recovery of Ohio Students: An Analysis of Spring 2022 Ohio State Tests},
  author={Vladimir Kogan},
  year={2022},
  url={https://www.semanticscholar.org/paper/76ac1af061d5d2ccf19d748ca8b744a9461260f3}
}

@article{gao2022attributedtext,
  title={Attributed Text Generation via Post-hoc Research and Revision},
  author={Luyu Gao and Zhuyun Dai and Panupong Pasupat and Anthony Chen and Arun Tejasvi Chaganty and Yicheng Fan and Vincent Zhao and N. Lao and Hongrae Lee and Da-Cheng Juan and Kelvin Guu},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.08726},
  url={https://www.semanticscholar.org/paper/4ef5410ec4b546eda642fe786cc1bdbb5a7251e1}
}

@misc{wu2022autoformalizationneural,
  title={Autoformalization for Neural Theorem Proving},
  author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
  year={2022},
  url={https://www.semanticscholar.org/paper/15e767aa26a14455da95a2b2f11e3d59f2c250f6}
}

@article{wu2022autoformalizationwith,
  title={Autoformalization with Large Language Models},
  author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2205.12615},
  url={https://www.semanticscholar.org/paper/c28e95a06dfcf13fc65a1cac83722f53e34f12a5},
  abstract={Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from $29.6\%$ to $35.2\%$.}
}

@article{zhang2022automaticchain,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alexander J. Smola},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/90350aa626bed47b02d0c162462e5b0ca82be6b2},
  abstract={Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like"Let's think step by step"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the"Let's think step by step"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot}
}

@article{shridhar2022automaticgeneration,
  title={Automatic Generation of Socratic Subquestions for Teaching Math Word Problems},
  author={Kumar Shridhar and Jakub Macina and Mennatallah El-Assady and Tanmay Sinha and Manu Kapur and Mrinmaya Sachan},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2211.12835},
  url={https://www.semanticscholar.org/paper/e6745fb621481ccb0ed53c267a37292e499c1b42},
  abstract={Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers.In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning.On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.}
}

@article{xiao2022auxiliaryteaching,
  title={Auxiliary Teaching System of Higher Mathematics Based on Random Matrix Model},
  author={Yabin Xiao and Bing Zhou and Dan-ni He and Jingzhong Liu},
  year={2022},
  booktitle={Mathematical Problems in Engineering},
  doi={10.1155/2022/7983989},
  url={https://www.semanticscholar.org/paper/869011d58c272450dc8cf95bd4f81601e17b8511},
  abstract={With the development of computer technology, computers have become a part of people’s lives and the Internet has connected the world’s networks as a whole. Computer technology is changing people’s study, life, and work. People’s traditional education mode, thinking, content, method, and talent training program have a significant impact. The development from traditional to computer technology-based teaching methods has brought new developments and leaps in educational technology. This paper analyzes the research background, significance, and research status of the advanced mathematics auxiliary teaching system, introduces the related technologies and development modes used in the development of the system, and especially discusses the access database technology by ADO and the mathematical expression based on MathML language. Secondly, starting from the actual teaching, we analyze the functional requirements and performance requirements of the system in detail and make detailed planning and design for the system architecture, database selection, functional modules, etc. The design and implementation process of this teaching system are summarized. The teaching strategy inference engine is the key to the personalization and intelligence of the ICAI system. According to the learning models provided by different students, the system designs a corresponding teaching sequence for the learners by controlling the meta-knowledge of the domain knowledge base. The teaching strategy inference engine cuts the domain knowledge tree, selects the knowledge points suitable for the student, and sorts the selected knowledge points reasonably to generate an optimal teaching sequence. According to the students’ learning situation, combined with the teaching rules in the teaching rule library, the students’ grades are dynamically adjusted, so as to select new learning content for students and provide teaching suggestions in time. The student model is the premise of the ICAI system to achieve individualization and intelligence. The system makes a comprehensive evaluation and diagnosis of students through fuzzy comprehensive evaluation and fuzzy reasoning. On this basis, a cognitive student model is established, which is the teaching strategy that provided the basis for the formulation.}
}

@misc{an2022bevbertmultimodal,
  title={BEVBert: Multimodal Map Pre-training for Language-guided Navigation},
  author={Dongyan An and Yuankai Qi and Yangguang Li and Yan Huang and Liangsheng Wang and T. Tan and Jing Shao},
  year={2022},
  url={https://www.semanticscholar.org/paper/d7abc3bcf368c7c0e3487da7cecae1ac209a7284},
  abstract={Large-scale pre-training has shown promising results on the vision-and-language navigation (VLN) task. However, most existing pre-training methods employ discrete panoramas to learn visual-textual associations. This requires the model to implicitly correlate incomplete, duplicate observations within the panoramas, which may impair an agent's spatial understanding. Thus, we propose a new map-based pre-training paradigm that is spatial-aware for use in VLN. Concretely, we build a local metric map to explicitly aggregate incomplete observations and remove duplicates, while modeling navigation dependency in a global topological map. This hybrid design can balance the demand of VLN for both short-term reasoning and long-term planning. Then, based on the hybrid map, we devise a pre-training framework to learn a multimodal map representation, which enhances spatial-aware cross-modal reasoning thereby facilitating the language-guided navigation goal. Extensive experiments demonstrate the effectiveness of the map-based pre-training route for VLN, and the proposed method achieves state-of-the-art on four VLN benchmarks.}
}

@article{chen2022btpkbasedlearning,
  title={BTPK-based learning: An Interpretable Method for Named Entity Recognition},
  author={Yulin Chen and Zelai Yao and Haixiao Chi and D. Gabbay and Bo Yuan and Bruno Bentzen and Beishui Liao},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/811151315ac5fefb1629a4d02c0274370db468a7}
}

@article{hua2022bayesvarbrulunified,
  title={BayesVarbrul: a unified multidimensional analysis of language change in a speaker community},
  author={Xia Hua},
  year={2022},
  journal={Journal of Language Evolution},
  doi={10.1093/jole/lzac004},
  url={https://www.semanticscholar.org/paper/c294f2479c50d70b8e6b32b630e197aea1d9309d},
  abstract={
 Exchange in ideas between language evolution and biological evolution has a long history, due to a shared theoretical foundation between language and biology as two evolving systems. Both systems evolve in terms of the frequency of a variant in a population for each of a large number of variables, that is how often a particular variant of a language variable is used in a speaker community and how many individuals in a biological population carry a particular variant of a gene. The way these frequencies change has been modelled under a similar mathematical framework. Here, I show how we can use concepts from genome wide association studies that identify the source of natural selection and the genes under selection in a biological population to study how social factors affect the usage of language variables in a speaker community or how some social groups use some language variables differently from other groups. Using the Gurindji Kriol language as a case study, I show how this approach unifies existing mathematical and statistical tools in studying language evolution over a large number of speakers and a large number of language variables, which provides a promising link between micro- and macro-evolution in language. The approach is named BayesVarbrul and is ready to apply to datasets other than the Gurindji Kriol dataset, including existing corpus data. The code and the instructions are available at https://github.com/huaxia1985/BayesVarbrul.}
}

@misc{si2022benchmarkinggpt3,
  title={Benchmarking GPT-3 For Closed-Book QA: Strengths and Weaknesses},
  author={Chenglei Si and Naman Molri and Gurmehar Cheema and Elliot Huang and Arjun Akkiraju},
  year={2022},
  url={https://www.semanticscholar.org/paper/b9a0bc80aa136027327697fe40189792a32c8b0c}
}

@article{gopinath2022benchmarkinglargescale,
  title={Benchmarking Large-Scale ACOPF Solutions and Optimality Bounds},
  author={S. Gopinath and H. Hijazi},
  year={2022},
  booktitle={IEEE Power & Energy Society General Meeting},
  doi={10.1109/PESGM48719.2022.9916662},
  url={https://www.semanticscholar.org/paper/dea559bde46a1b9efc64c4418eebb3e9f3b775b7},
  abstract={We present the results of a comprehensive bench-marking effort aimed at evaluating and comparing state-of-the-art open-source tools for solving the Alternating-Current Optimal Power Flow (ACOPF) problem. Our numerical experiments include all instances found in the public library PGLIB with network sizes up to 30,000 nodes. The benchmarked tools span a number of programming languages (Python, Julia, Matlab/Octave, and C++), nonlinear optimization solvers (Ipopt, MIPS, and INLP) as well as different mathematical modeling tools (JuMP and Gravity). We also present state-of-the-art optimality bounds obtained using sparsity-exploiting semidefinite programming approaches and corresponding computational times.}
}

@article{gokhale2022benchmarkingspatial,
  title={Benchmarking Spatial Relationships in Text-to-Image Generation},
  author={Tejas Gokhale and Hamid Palangi and Besmira Nushi and Vibhav Vineet and E. Horvitz and Ece Kamar and Chitta Baral and Yezhou Yang},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.10015},
  url={https://www.semanticscholar.org/paper/4bf77d64b860ed0cd84a63aecd92a3cb295b88ee},
  abstract={Spatial understanding is a fundamental aspect of computer vision and integral for human-level reasoning about images, making it an important component for grounded language understanding. While recent text-to-image synthesis (T2I) models have shown unprecedented improvements in photorealism, it is unclear whether they have reliable spatial understanding capabilities. We investigate the ability of T2I models to generate correct spatial relationships among objects and present VISOR, an evaluation metric that captures how accurately the spatial relationship described in text is generated in the image. To benchmark existing models, we introduce a dataset, $\mathrm{SR}_{2D}$, that contains sentences describing two or more objects and the spatial relationships between them. We construct an automated evaluation pipeline to recognize objects and their spatial relationships, and employ it in a large-scale evaluation of T2I models. Our experiments reveal a surprising finding that, although state-of-the-art T2I models exhibit high image quality, they are severely limited in their ability to generate multiple objects or the specified spatial relations between them. Our analyses demonstrate several biases and artifacts of T2I models such as the difficulty with generating multiple objects, a bias towards generating the first object mentioned, spatially inconsistent outputs for equivalent relationships, and a correlation between object co-occurrence and spatial understanding capabilities. We conduct a human study that shows the alignment between VISOR and human judgement about spatial understanding. We offer the $\mathrm{SR}_{2D}$ dataset and the VISOR metric to the community in support of T2I reasoning research.}
}

@article{srivastava2022beyondimitation,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and A. Dsouza and Ambrose Slone and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Madotto and Andrea Santilli and Andreas Stuhlmuller and Andrew M. Dai and A. La and Andrew Kyle Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and A. Tabassum and Arul Menezes and Arun Kirubarajan and A. Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karakacs and B. R. Roberts and B. S. Loe and Barret Zoph and Bartlomiej Bojanowski and Batuhan Ozyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and B. Howald and Bryan Orinion and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and C'esar Ferri Ram'irez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and D. Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and D. Gonz'alez and Danielle R. Perszyk and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and D. Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Dylan Schrader and Ekaterina Shutova and E. D. Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodolà and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and E. Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Mart'inez-Plumed and Francesca Happ'e and François Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germán Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Xinyue Wang and Gonzalo Jaimovitch-L'opez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and H. Bogar and Henry Shevlin and Hinrich Schutze and H. Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and John Kernion and Jacob Hilton and Jaehoon Lee and J. Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Koco'n and Jana Thompson and Janelle Wingfield and Jared Kaplan and Jarema Radom and Jascha Narain Sohl-Dickstein and Jason Phang and Jason Wei and J. Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Oluwadara Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Jane W Waweru and John Burden and John Miller and John U. Balis and Jonathan Batchelder and Jonathan Berant and Jorg Frohberg and Jos Rozen and J. Hernández-Orallo and Joseph Boudeman and J. Guerr and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and K. Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and K. Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and K. Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-philippe Morency and Luca Moschella and Luca Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Col'on and Luke Metz and Lutfi Kerem cSenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ram’irez Quintana and M. Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and M. Schubert and Medina Baitemirova and Melody Arnaud and M. McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and M. Strube and Michal Swkedrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mitch Walker and Monica Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and T. MukundVarma and Nanyun Peng and Nathan A. Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nicole Martinez and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and N. Keskar and Niveditha Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and P. Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and P. Eckersley and Phu Mon Htut and P. Hwang and P. Milkowski and P. Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ramon Risco and Raphael Milliere and Rhythm Garg and Richard Barnes and R. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan Le Bras and Rosanne Liu and Rowan Jacobs and Rui Zhang and R. Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and R. Teehan and Rylan Yang and Sahib Singh and Saif Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi S. Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and S. Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima Debnath and Siamak Shakeri and Simon Thormeyer and S. Melzi and Siva Reddy and S. Makini and Soo-Hwan Lee and Spencer Bradley Torene and Sriharsha Hatwar and S. Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T Piantadosi and Stuart M. Shieber and Summer Misherghi and S. Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsunori Hashimoto and Te-Lin Wu and T. Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and T. Kornev and T. Tunduny and Tobias Gerstenberg and T. Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and V. Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and W. Fedus and W. Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinran Zhao and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yu Hou and Yuntao Bai and Zachary Seid and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881},
  abstract={Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit"breakthrough"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.}
}

@article{jung2022blankcollapse,
  title={Blank Collapse: Compressing CTC emission for the faster decoding},
  author={Minkyu Jung and Ohhyeok Kwon and S. Seo and Soonshin Seo},
  year={2022},
  booktitle={Interspeech},
  doi={10.48550/arXiv.2210.17017},
  url={https://www.semanticscholar.org/paper/6498d95d3f988e684bc6a70004decbefec655222},
  abstract={Connectionist Temporal Classification (CTC) model is a very efficient method for modeling sequences, especially for speech data. In order to use CTC model as an Automatic Speech Recognition (ASR) task, the beam search decoding with an external language model like n-gram LM is necessary to obtain reasonable results. In this paper we analyze the blank label in CTC beam search deeply and propose a very simple method to reduce the amount of calculation resulting in faster beam search decoding speed. With this method, we can get up to 78% faster decoding speed than ordinary beam search decoding with a very small loss of accuracy in LibriSpeech datasets. We prove this method is effective not only practically by experiments but also theoretically by mathematical reasoning. We also observe that this reduction is more obvious if the accuracy of the model is higher.}
}

@article{wan2022bridgingbetween,
  title={Bridging the Gap between Recognition-level Pre-training and Commonsensical Vision-language Tasks},
  author={Yue Wan and Yueen Ma and Haoxuan You and Zhecan Wang and Shih-Fu Chang},
  year={2022},
  booktitle={CSRR},
  doi={10.18653/v1/2022.csrr-1.4},
  url={https://www.semanticscholar.org/paper/ac13afe6c5f456a1f250e03e3258f5cfecc99373},
  abstract={Large-scale visual-linguistic pre-training aims to capture the generic representations from multimodal features, which are essential for downstream vision-language tasks. Existing methods mostly focus on learning the semantic connections between visual objects and linguistic content, which tend to be recognitionlevel information and may not be sufficient for commonsensical reasoning tasks like VCR. In this paper, we propose a novel commonsensical vision-language pre-training framework to bridge the gap. We first augment the conventional image-caption pre-training datasets with commonsense inferences from a visuallinguistic GPT-2. To pre-train models on image, caption and commonsense inferences together, we propose two new tasks: masked commonsense modeling (MCM) and commonsense type prediction (CTP). To reduce the shortcut effect between captions and commonsense inferences, we further introduce the domain-wise adaptive masking that dynamically adjusts the masking ratio. Experimental results on downstream tasks, VCR and VQA, show the improvement of our pre-training strategy over previous methods. Human evaluation also validates the relevance, informativeness, and diversity of the generated commonsense inferences. Overall, we demonstrate the potential of incorporating commonsense knowledge into the conventional recognition-level visual-linguistic pre-training.}
}

@misc{leemann2022oherencevaluation,
  title={C OHERENCE E VALUATION OF V ISUAL C ONCEPTS WITH O BJECTS AND L ANGUAGE},
  author={Tobias Leemann and Yao Rong and Stefan Kraft and Enkelejda Kasneci and Gjergji Kasneci},
  year={2022},
  url={https://www.semanticscholar.org/paper/75e3f61b69dc6bc8bfb7fd28aa1001edbbc8eab4}
}

@article{raman2022capecorrective,
  title={CAPE: Corrective Actions from Precondition Errors using Large Language Models},
  author={S. S. Raman and Vanya Cohen and Eric Rosen and Ifrah Idrees and D. Paulius and Stefanie Tellex},
  year={2022},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA57147.2024.10611376},
  url={https://www.semanticscholar.org/paper/c82d8d80ea68400adb7faebb2f1cff38dd83093a},
  abstract={Extracting knowledge and reasoning from large language models (LLMs) offers a path to designing intelligent robots. Common approaches that leverage LLMs for planning are unable to recover when actions fail and resort to retrying failed actions without resolving the underlying cause. We propose a novel approach (CAPE) that generates corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans through few-shot reasoning on action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while maintaining semantic correctness and minimizing re-prompting. In VirtualHome, CAPE improves a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan, whilst achieving competitive executability. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves correctness by 76.49% with higher executability compared to SayCan. Our approach enables embodied agents to follow natural language commands and robustly recover from failures.}
}

@article{lindstrm2022clevrmathdataset,
  title={CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning},
  author={Adam Dahlgren Lindström and Savitha Sam Abraham},
  year={2022},
  booktitle={International Workshop on Neural-Symbolic Learning and Reasoning},
  doi={10.48550/arXiv.2208.05358},
  url={https://www.semanticscholar.org/paper/74cc3d340039c67bdabaef090d1386fe2c5376ca},
  abstract={We introduce CLEVR-Math, a multi-modal math word problems dataset consisting of simple math word problems involving addition/subtraction, represented partly by a textual description and partly by an image illustrating the scenario. The text describes actions performed on the scene that is depicted in the image. Since the question posed may not be about the scene in the image, but about the state of the scene before or after the actions are applied, the solver envision or imagine the state changes due to these actions. Solving these word problems requires a combination of language, visual and mathematical reasoning. We apply state-of-the-art neural and neuro-symbolic models for visual question answering on CLEVR-Math and empirically evaluate their performances. Our results show how neither method generalise to chains of operations. We discuss the limitations of the two in addressing the task of multi-modal word problem solving.}
}

@article{dong2022corrpusdetecting,
  title={CORRPUS: Detecting Story Inconsistencies via Codex-Bootstrapped Neurosymbolic Reasoning},
  author={Yi Dong and Lara J. Martin and Chris Callison-Burch},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.10754},
  url={https://www.semanticscholar.org/paper/4bea09d4c897fb201c032b9eb605a943b1e70435}
}

@misc{krell2022crosscontaminationaccelerating,
  title={CROSS-CONTAMINATION: ACCELERATING LARGE LANGUAGE MODELS WITHOUT IMPACTING PERFORMANCE},
  author={M. M. Krell and Matej Kosec},
  year={2022},
  url={https://www.semanticscholar.org/paper/85cac89ba01a07f3dbf6dbb1e0c56067a3105714}
}

@article{lin2022curriculumlearning,
  title={CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction},
  author={Jiaju Lin and Qin Chen and Jie Zhou and Jian Jin and Liangye He},
  year={2022},
  booktitle={International Joint Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2205.00498},
  url={https://www.semanticscholar.org/paper/65d88194a902332b78dd5a7b919fa577bfa7ee9f},
  abstract={Implicit event argument extraction (EAE) aims to identify arguments that could scatter over the document. Most previous work focuses on learning the direct relations between arguments and the given trigger, while the implicit relations with long-range dependency are not well studied. Moreover, recent neural network based approaches rely on a large amount of labeled data for training, which is unavailable due to the high labelling cost. In this paper, we propose a Curriculum learning based Prompt tuning (CUP) approach, which resolves implicit EAE by four learning stages. The stages are defined according to the relations with the trigger node in a semantic graph, which well captures the long-range dependency between arguments and the trigger. In addition, we integrate a prompt-based encoder-decoder model to elicit related knowledge from pre-trained language models (PLMs) in each stage, where the prompt templates are adapted with the learning progress to enhance the reasoning for arguments. Experimental results on two well-known benchmark datasets show the great advantages of our proposed approach. In particular, we outperform the state-of-the-art models in both fully-supervised and low-data scenarios.}
}

@article{willig2022foundationmodels,
  title={Can Foundation Models Talk Causality?},
  author={Moritz Willig and M. Zecevic and D. Dhami and K. Kersting},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2206.10591},
  url={https://www.semanticscholar.org/paper/6745381bfa99a3b979766cca05e91559f1b770e3},
  abstract={Foundation models are subject to an ongoing heated debate, leaving open the question of progress towards AGI and dividing the community into two camps: the ones who see the arguably impressive results as evidence to the scaling hypothesis, and the others who are worried about the lack of interpretability and reasoning capabilities. By investigating to which extent causal representations might be captured by these large scale language models, we make a humble efforts towards resolving the ongoing philosophical conflicts.}
}

@article{tefnik2022incontextlearners,
  title={Can In-context Learners Learn a Reasoning Concept from Demonstrations?},
  author={Michal Tefnik and Marek Kadlcík},
  year={2022},
  booktitle={NLRSE},
  doi={10.18653/v1/2023.nlrse-1.8},
  url={https://www.semanticscholar.org/paper/e7cfc3362dd85b17c747e9f9636749696f87a88b},
  abstract={Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations.However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input.However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models’ ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.To disentangle models’ in-context learning ability independent of models’ memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in few-shot demonstrations.We find that smaller models are more sensitive to the presented concepts. While some of the models are able to benefit from concept-presenting demonstrations for each assessed concept, we find that none of the assessed in-context learners can benefit from all presented reasoning concepts consistently, leaving the in-context concept learning an open challenge.}
}

@article{behnamghader2022retrieveraugmentedlanguage,
  title={Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model},
  author={Parishad BehnamGhader and Santiago Miret and Siva Reddy},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2212.09146},
  url={https://www.semanticscholar.org/paper/e4758d05c3d4231dd30c656330e156ccc9dbb07b},
  abstract={Augmenting pretrained language models with retrievers to select the supporting documents has shown promise in effectively solving common NLP problems, including language modeling and question answering, in an interpretable way. In this paper, we first study the strengths and weaknesses of different retriever-augmented language models (REALM, $k$NN-LM, FiD coupled with DPR, and ATLAS and Flan-T5 coupled with Contriever) in reasoning over the retrieved statements in different tasks. We show how the retrieve-then-read models' limitations in reasoning are rooted both in the retriever module as well as the language model. Our experimental results demonstrate that the similarity metric used by the retrievers is generally insufficient for reasoning tasks. Additionally, we show that the language models in retriever-augmented models do not take the complicated relations between the statements into account, which leads to poor reasoning performance even when using the larger models. Moreover, we analyze the reasoning performance of large language models using multihop retrieval but we only observe minor improvements. Overall, this shows great room for further research in this area.}
}

@article{schlegel2022transformersreason,
  title={Can Transformers Reason in Fragments of Natural Language?},
  author={Viktor Schlegel and Kamen V. Pavlov and Ian Pratt-Hartmann},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2211.05417},
  url={https://www.semanticscholar.org/paper/8ee376114a43432399554be39a79c1a2b6c65d51},
  abstract={State-of-the-art deep-learning-based approaches to Natural Language Processing (NLP) are credited with various capabilities that involve reasoning with natural language texts. %However, reasoning in this setting is often ill-defined and shallow. In this paper we carry out a large-scale empirical study investigating the detection of formally valid inferences in controlled fragments of natural language for which the satisfiability problem becomes increasingly complex. We find that, while transformer-based language models perform surprisingly well in these scenarios, a deeper analysis reveals that they appear to overfit to superficial patterns in the data rather than acquiring the logical principles governing the reasoning in these fragments.}
}

@article{carette2022centralsubmonads,
  title={Central Submonads and Notions of Computation},
  author={T. Carette and Louis Lemonnier and V. Zamdzhiev},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2207.09190},
  url={https://www.semanticscholar.org/paper/af36ab7fe4e10aad2e01be5dcde0784241742832}
}

@article{wei2022chainthought,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5},
  abstract={We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.}
}

@misc{unknown2022chartingspace,
  title={Charting the Space of Quantum Field Theories},
  year={2022},
  url={https://www.semanticscholar.org/paper/cb39717895b9fa4cd2cc59748d59e20ce5eb4521}
}

@article{wang2022chiqalarge,
  title={ChiQA: A Large Scale Image-based Real-World Question Answering Dataset for Multi-Modal Understanding},
  author={Bingning Wang and Feiya Lv and Ting Yao and Yiming Yuan and Jin Ma and Yu Luo and Haijin Liang},
  year={2022},
  booktitle={International Conference on Information and Knowledge Management},
  doi={10.1145/3511808.3557258},
  url={https://www.semanticscholar.org/paper/730efc9d93a2b34f02a98aa46d9357f05111fd99},
  abstract={Visual question answering is an important task in both natural language and vision understanding. However, in most of the public visual question answering datasets such as VQA, CLEVR, the questions are human generated that specific to the given image, such as 'What color are her eyes?'. The human generated crowdsourcing questions are relatively simple and sometimes have the bias toward certain entities or attributes [1, 55]. In this paper, we introduce a new question answering dataset based on image-ChiQA. It contains the real-world queries issued by internet users, combined with several related open-domain images. The system should determine whether the image could answer the question or not. Different from previous VQA datasets, the questions are real-world image-independent queries that are more various and unbiased. Compared with previous image-retrieval or image-caption datasets, the ChiQA not only measures the relatedness but also measures the answerability, which demands more fine-grained vision and language reasoning. ChiQA contains more than 40K questions and more than 200K question-images pairs. A three-level 2/1/0 label is assigned to each pair indicating perfect answer, partially answer and irrelevant. Data analysis shows ChiQA requires a deep understanding of both language and vision, including grounding, comparisons, and reading. We evaluate several state-of-the-art visual-language models such as ALBEF, demonstrating that there is still a large room for improvements on ChiQA.}
}

@article{wilson2022classificationopenended,
  title={Classification of open-ended responses to a research-based assessment using natural language processing},
  author={Joseph Wilson and Benjamin Pollard and J. M. Aiken and Marcos D. Caballero and H. Lewandowski},
  year={2022},
  booktitle={Physical Review Physics Education Research},
  doi={10.1103/physrevphyseducres.18.010141},
  url={https://www.semanticscholar.org/paper/932cb50f541c3141fabe156ecf3bbafb0aa61c29},
  abstract={Surveys have long been used in physics education research to understand student reasoning and inform course improvements. However, to make analysis of large sets of responses practical, most surveys use a closed-response format with a small set of potential responses. Open-ended formats, such as written free response, can provide deeper insights into student thinking, but take much longer to analyze, especially with a large number of responses. Here, we explore natural language processing as a computational solution to this problem. We create a machine learning model that can take student responses from the Physics Measurement Questionnaire as input, and output a categorization of student reasoning based on different reasoning paradigms. Our model yields classifications with the same level of agreement as that between two humans categorizing the data, but can be done by a computer, and thus can be scaled for large datasets. In this work, we describe the algorithms and methodologies used to create, train, and test our natural language processing system. We also present the results of the analysis and discuss the utility of these approaches for analyzing open-response data in education research. DOI: 10.1103/PhysRevPhysEducRes.18.010141}
}

@article{dong2022corrpuscodebased,
  title={CoRRPUS: Code-based Structured Prompting for Neurosymbolic Story Understanding},
  author={Yi Dong and Lara J. Martin and Chris Callison-Burch},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2023.findings-acl.832},
  url={https://www.semanticscholar.org/paper/76f54657eb0893a0b203da57dcf0b4fffeebfc2c},
  abstract={Story generation and understanding -- as with all NLG/NLU tasks -- has seen a surge in neurosymbolic work. Researchers have recognized that, while large language models (LLMs) have tremendous utility, they can be augmented with symbolic means to be even better and to make up for any flaws that the neural networks might have. However, symbolic methods are extremely costly in terms of the amount of time and expertise needed to create them. In this work, we capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use of symbolic methods for tracking the state of stories and aiding in story understanding. We show that our CoRRPUS system and abstracted prompting procedures can beat current state-of-the-art structured LLM techniques on pre-existing story understanding tasks (bAbI Task 2 and Re^3) with minimal hand engineering. We hope that this work can help highlight the importance of symbolic representations and specialized prompting for LLMs as these models require some guidance for performing reasoning tasks properly.}
}

@article{kim2022cosimcommonsense,
  title={CoSIm: Commonsense Reasoning for Counterfactual Scene Imagination},
  author={Hyounghun Kim and Abhaysinh Zala and Mohit Bansal},
  year={2022},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2207.03961},
  url={https://www.semanticscholar.org/paper/153b51c7871f82c8966a8d744d3630ef791f00f4},
  abstract={As humans, we can modify our assumptions about a scene by imagining alternative objects or concepts in our minds. For example, we can easily anticipate the implications of the sun being overcast by rain clouds (e.g., the street will get wet) and accordingly prepare for that. In this paper, we introduce a new dataset called Commonsense Reasoning for Counterfactual Scene Imagination (CoSIm) which is designed to evaluate the ability of AI systems to reason about scene change imagination. To be specific, in this multimodal task/dataset, models are given an image and an initial question-response pair about the image. Next, a counterfactual imagined scene change (in textual form) is applied, and the model has to predict the new response to the initial question based on this scene change. We collect 3.5K high-quality and challenging data instances, with each instance consisting of an image, a commonsense question with a response, a description of a counterfactual change, a new response to the question, and three distractor responses. Our dataset contains various complex scene change types (such as object addition/removal/state change, event description, environment change, etc.) that require models to imagine many different scenarios and reason about the changed scenes. We present a baseline model based on a vision-language Transformer (i.e., LXMERT) and ablation studies. Through human evaluation, we demonstrate a large human-model performance gap, suggesting room for promising future work on this challenging, counterfactual multimodal task.}
}

@article{liang2022codepolicies,
  title={Code as Policies: Language Model Programs for Embodied Control},
  author={Jacky Liang and Wenlong Huang and F. Xia and Peng Xu and Karol Hausman and Brian Ichter and Peter R. Florence and Andy Zeng},
  year={2022},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA48891.2023.10160591},
  url={https://www.semanticscholar.org/paper/91deaf9d324c8feafc189da0da03e60a60287bca},
  abstract={Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io}
}

@article{sahu2022codequeriesdataset,
  title={CodeQueries: A Dataset of Semantic Queries over Code},
  author={Surya Prakash Sahu and Madhurima Mandal and Shikhar Bharadwaj and Aditya Kanade and Petros Maniatis and S. Shevade},
  year={2022},
  booktitle={International Symposium on Electronic Commerce},
  doi={10.1145/3641399.3641408},
  url={https://www.semanticscholar.org/paper/cd937849a314b3e5eb4862a3b55aa823811a5996},
  abstract={Developers often have questions about semantic aspects of code they are working on, e.g., “Is there a class whose parent classes declare a conflicting attribute?”. Answering them requires understanding code semantics such as attributes and inheritance relation of classes. An answer to such a question should identify code spans constituting the answer (e.g., the declaration of the subclass) as well as supporting facts (e.g., the definitions of the conflicting attributes). The existing work on question-answering over code has considered yes/no questions or method-level context. We contribute a labeled dataset, called CodeQueries, of semantic queries over Python code. Compared to the existing datasets, in CodeQueries, the queries are about code semantics, the context is file level and the answers are code spans. We curate the dataset based on queries supported by a widely-used static analysis tool, CodeQL, and include both positive and negative examples, and queries requiring single-hop and multi-hop reasoning. To assess the value of our dataset, we evaluate baseline neural approaches. We study a large language model (GPT3.5-Turbo) in zero-shot and few-shot settings on a subset of CodeQueries. We also evaluate a BERT style model (CuBERT) with fine-tuning. We find that these models achieve limited success on CodeQueries. CodeQueries is thus a challenging dataset to test the ability of neural models, to understand code semantics, in the extractive question-answering setting.}
}

@article{zhao2022collaborativereasoning,
  title={Collaborative Reasoning on Multi-Modal Semantic Graphs for Video-Grounded Dialogue Generation},
  author={Xueliang Zhao and Yuxuan Wang and Chongyang Tao and Chenshuo Wang and Dongyan Zhao},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.12460},
  url={https://www.semanticscholar.org/paper/256fd60c692ebe12fe2bbf65d46722f511aa3117},
  abstract={We study video-grounded dialogue generation, where a response is generated based on the dialogue context and the associated video. The primary challenges of this task lie in (1) the difficulty of integrating video data into pre-trained language models (PLMs) which presents obstacles to exploiting the power of large-scale pre-training; and (2) the necessity of taking into account the complementarity of various modalities throughout the reasoning process. Although having made remarkable progress in video-grounded dialogue generation, existing methods still fall short when it comes to integrating with PLMs in a way that allows information from different modalities to complement each other. To alleviate these issues, we first propose extracting pertinent information from videos and turning it into reasoning paths that are acceptable to PLMs. Additionally, we propose a multi-agent reinforcement learning method to collaboratively perform reasoning on different modalities (i.e., video and dialogue context). Empirical experiment results on two public datasets indicate that the proposed model can significantly outperform state-of-the-art models by large margins on both automatic and human evaluations.}
}

@article{gouhar2022combininglocal,
  title={Combining local and global approaches to ascertain semantic similarity},
  author={Shahrukh Gouhar and Anupam Misra and Radha Rathore and Mansoor Ali Shaik and Dr. Subhasis Dasgupta},
  year={2022},
  booktitle={2022 IEEE India Council International Subsections Conference (INDISCON)},
  doi={10.1109/INDISCON54605.2022.9862898},
  url={https://www.semanticscholar.org/paper/1f9ae8a3f6be60d10e9d1d3eeecc0a8fda0404b2},
  abstract={Interviewing potential candidates is both time consuming and resource intensive. This is particularly prominent in organizations which go for large scale recruitment processes. In the current study, a client based application has been proposed for interviewing candidates for data science profiles where interviewee’s answers are scored using machine learning. Different approaches were tried with pretrained models but as the application was very much domain specific, those models did not provide good results. Hence a custom embedding layer was built on open source data science textbooks. These embeddings were used with Gated Recurrent Units (GRU) to capture a local approach (subject specific) in the interview answers. However, this neglected the nuances of the English language involved in critical reasoning. Hence Bi-directional Encoder Representation from Transformers (BERT) was employed to capture the global approach (interaction between words in the English language) in the interview answers. The similarity scores from these two approaches were ensembled into a machine learning model which allotted the final score to the interviewee’s answer. The proposed method outperformed the pretrained models with significant margin when tested with the validation data.}
}

@misc{albalak2022commonsensereasoning,
  title={Commonsense Reasoning for Conversational AI: A Survey of Recent Datasets and Benchmarks},
  author={Alon Albalak and Varun R. Embar and Yi-Lin Tuan and L. Getoor and Ankur Bapna and Gokhan Tur and Dilek Hakkani-Tur and Larry Heck. 2017 and Lisa Bauer and Yicheng Wang and Mohit Bansal and Antoine Bosselut and Hannah Rashkin and Maarten Sap and Chaitanya Malaviya and Asli Celikyilmaz and Yejin Choi and Yulong Chen and Y. Liu and Liang Chen and Leyang Cui and Yu Wu and Shujie Liu and Yue-Feng Zhang and J. Devlin and Ming-Wei Chang and Kenton Lee and Leilei Gan and Yating Zhang and Kun Kuang and Shuo Lin Yuan and Changlong Li and Xiaozhong Sun and Liu Fei and Wu and R. Speer and Joshua Chin and Catherine Havasi and Kai Sun and Dian Yu and Jianshu Chen and Dong Yu and Jai Desai and Aaron Wade and Haoran Li and Asli Celikyil-879 maz and Yashar Mehdad and Dragomir R. Radev and Geng Tu and Ji-Rong Wen and Cheng Liu and Dazhi Jiang and A. Stolcke and Lynn Voss and Dilek Peters and John Hakkani-Tur and Benoit Dowding and Raquel Favre and Matthew Fernández and Mike Frampton and Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Llion Uszkoreit and Aidan N Jones and Łukasz Gomez and Kaiser Illia and Polosukhin. 2017 and Attention and S. Welleck and Jason Weston and Arthur Szlam},
  year={2022},
  url={https://www.semanticscholar.org/paper/4e37589fb896d1578ba4282f40c20708079ae8e5}
}

@article{ye2022complementaryexplanations,
  title={Complementary Explanations for Effective In-Context Learning},
  author={Xi Ye and Srini Iyer and Asli Celikyilmaz and Ves Stoyanov and Greg Durrett and Ramakanth Pasunuru},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2211.13892},
  url={https://www.semanticscholar.org/paper/097dc73d5d422b3c09286e72d16b2561ae5fb395},
  abstract={Large language models (LLMs) have exhibited remarkable capabilities in learning from explanations in prompts, but there has been limited understanding of exactly how these explanations function or why they are effective. This work aims to better understand the mechanisms by which explanations are used for in-context learning. We first study the impact of two different factors on the performance of prompts with explanations: the computation trace (the way the solution is decomposed) and the natural language used to express the prompt. By perturbing explanations on three controlled tasks, we show that both factors contribute to the effectiveness of explanations. We further study how to form maximally effective sets of explanations for solving a given test query. We find that LLMs can benefit from the complementarity of the explanation set: diverse reasoning skills shown by different exemplars can lead to better performance. Therefore, we propose a maximal marginal relevance-based exemplar selection approach for constructing exemplar sets that are both relevant as well as complementary, which successfully improves the in-context learning performance across three real-world tasks on multiple LLMs.}
}

@article{fu2022complexitybasedprompting,
  title={Complexity-Based Prompting for Multi-Step Reasoning},
  author={Yao Fu and Hao-Chun Peng and Ashish Sabharwal and Peter Clark and Tushar Khot},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.00720},
  url={https://www.semanticscholar.org/paper/c88cafa3e980765a64febe369ceb7c2aa7261d2a},
  abstract={We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.}
}

@article{li2022composingensembles,
  title={Composing Ensembles of Pre-trained Models via Iterative Consensus},
  author={Shuang Li and Yilun Du and J. Tenenbaum and A. Torralba and Igor Mordatch},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.11522},
  url={https://www.semanticscholar.org/paper/f3a13abf23afecf534c955954d70c3b0fc41d334},
  abstract={Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information, while vision models such as DALL-E can generate photorealistic photos but fail to understand complex language descriptions. In this work, we propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as"generators"or"scorers"and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5%, without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation. Project page: https://energy-based-model.github.io/composing-pretrained-models.}
}

@article{kuculo2022comprehensiveevent,
  title={Comprehensive Event Representations using Event Knowledge Graphs and Natural Language Processing},
  author={Tin Kuculo},
  year={2022},
  booktitle={The Web Conference},
  doi={10.1145/3487553.3524199},
  url={https://www.semanticscholar.org/paper/7fcb917cd4b6d0c77c41b4a1b1dc0c4d1965ba7b},
  abstract={Recent work has utilised knowledge-aware approaches to natural language understanding, question answering, recommendation systems, and other tasks. These approaches rely on well-constructed and large-scale knowledge graphs that can be useful for many downstream applications and empower knowledge-aware models with commonsense reasoning. Such knowledge graphs are constructed through knowledge acquisition tasks such as relation extraction and knowledge graph completion. This work seeks to utilise and build on the growing body of work that uses findings from the field of natural language processing (NLP) to extract knowledge from text and build knowledge graphs. The focus of this research project is on how we can use transformer-based approaches to extract and contextualise event information, matching it to existing ontologies, to build comprehensive knowledge graph-based event representations. Specifically, sub-event extraction is used as a way of creating sub-event-aware event representations. These event representations are then further enriched through fine-grained location extraction and contextualised through the alignment of historically relevant quotes.}
}

@article{evtikhov2022computationalexperiment,
  title={Computational experiment – nondimensionalization of equations, computational stability and program testing},
  author={M. G. Evtikhov and V. G. Evtikhov},
  year={2022},
  booktitle={Radioelectronics Nanosystems Information Technologies},
  doi={10.17725/rensit.2022.14.331},
  url={https://www.semanticscholar.org/paper/094d38867c42533f3d61f76b92c0c3b82f54e3fb},
  abstract={From the stages of the computational experiment, the stage of non-dimensionalization of the initial equation (system of equations) of the problem is considered - the replacement of its variables by the product of the corresponding dimensionless quantities by their units of measurement with subsequent transformations. Such a transition from a physical model to a mathematical (dimensionless) one makes it possible to obtain software implementations for research. A critical evaluation of its complexity is carried out and possible errors in the results are evaluated. At the same time, new versions of software are formed. Object-oriented programming tools and version control systems (for example, git) allow you to create versions of software tools adapted to different conditions of their use and for different types of users. Parallelization of work on versions is carried out. At the same time, for further software implementation, the set-theoretic language of formulas with partially recursive functions is effective. To implement versions with large amounts of calculations and data, high-performance computing systems based on software and hardware acceleration, parallel information processing and cloud architectures are used. As a rule, a difference model of the problem and iterative methods for solving it are constructed for a program version. Computational stability conditions are usually stipulated in modern instructions for standard program libraries. For new algorithms, it is necessary to analyze the stability of difference schemes based on the refinement of their spectral properties and the use of functional analysis methods. For storage and subsequent application of the results of computational experiments, it is advisable to use modern databases. As a kind of computational experiment, testing of alpha and beta versions of programs and their releases is also considered.}
}

@article{khuralay2022computersimulation,
  title={Computer Simulation of Intelligent Control Systems for High-Precision Cruise Missiles},
  author={Moldamurat Khuralay and Akhmetov Kayrat Telektesovich and Otegen Alikhan Serikovich and Brimzhanova Saule Serikovna and Otyzbayeva Karlygash Zhalenovna and Zhiyenbek Arailym Oteulievna},
  year={2022},
  booktitle={2022 International Conference on Smart Information Systems and Technologies (SIST)},
  doi={10.1109/SIST54437.2022.9945703},
  url={https://www.semanticscholar.org/paper/84b3bd3fbcf52e2aa7c2595a6880586f12ce8f59}
}

@misc{unknown2022computerverifiedfoundations,
  title={Computer-Verified Foundations of Metaphysics and an Ontology of Natural Numbers in Isabelle/HOL},
  year={2022},
  url={https://www.semanticscholar.org/paper/02ad77812348e299794d5bc83a999090a7ee2139}
}

@article{smith2022constructvldatafree,
  title={ConStruct-VL: Data-Free Continual Structured VL Concepts Learning*},
  author={James Smith and Paola Cascante-Bonilla and Assaf Arbelle and Donghyun Kim and Rameswar Panda and David D. Cox and Diyi Yang and Z. Kira and R. Feris and Leonid Karlinsky},
  year={2022},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52729.2023.01440},
  url={https://www.semanticscholar.org/paper/6b3e939d93c82c269f552e7e2050524c3ad9b73b},
  abstract={Recently, large-scale pre-trained Vision-and-Language (VL) foundation models have demonstrated remarkable capabilities in many zero-shot downstream tasks, achieving competitive results for recognizing objects defined by as little as short text prompts. However, it has also been shown that VL models are still brittle in Structured VL Concept (SVLC) reasoning, such as the ability to recognize object attributes, states, and inter-object relations. This leads to reasoning mistakes, which need to be corrected as they occur by teaching VL models the missing SVLC skills; often this must be done using private data where the issue was found, which naturally leads to a data-free continual (no task-id) VL learning setting. In this work, we introduce the first Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark11Our code is publicly available at https://github.com/jamessealesmith/ConStruct-VL and show it is challenging for many existing data-free CL strategies. We, therefore, propose a data-free method comprised of a new approach of Adversarial Pseudo-Replay (APR) which generates adversarial reminders of past tasks from past task models. To use this method efficiently, we also propose a continual parameter-efficient Layered-LoRA (LaLo) neural architecture allowing no-memory-cost access to all past models at train time. We show this approach outperforms all data-free methods by as much as ~ 7% while even matching some levels of experience-replay (prohibitive for applications where data-privacy must be preserved).}
}

@article{wagemaker2022concurrentnetkat,
  title={Concurrent NetKAT: Modeling and analyzing stateful, concurrent networks},
  author={J. Wagemaker and Nate Foster and Tobias Kapp'e and D. Kozen and J. Rot and Alexandra Silva},
  year={2022},
  booktitle={European Symposium on Programming},
  doi={10.1007/978-3-030-99336-8_21},
  url={https://www.semanticscholar.org/paper/eb5a72315f84c7234ca6697d327de571f96ffb28},
  abstract={We introduce Concurrent NetKAT (CNetKAT), an extension of NetKAT with operators for specifying and reasoning about concurrency in scenarios where multiple packets interact through state. We provide a model of the language based on partially-ordered multisets (pomsets), which are a well-established mathematical structure for defining the denotational semantics of concurrent languages. We provide a sound and complete axiomatization of this model, and we illustrate the use of CNetKAT through examples. More generally, CNetKAT can be understood as an algebraic framework for reasoning about programs with both local state (in packets) and global state (in a global store).}
}

@article{hurst2022connectingsymbolic,
  title={Connecting symbolic fractions to their underlying proportions using iterative partitioning.},
  author={M. Hurst and Jacob R Butts and S. Levine},
  year={2022},
  booktitle={Developmental Psychology},
  doi={10.1037/dev0001384},
  url={https://www.semanticscholar.org/paper/60e87d430f117b616c456cf8f1955926036f128a},
  abstract={Fractions are a challenging mathematics topic for many elementary and middle school students, and even for adults. However, a growing body of developmental research suggests that young children can reason about visually presented proportions, well before fraction instruction, providing insight into how fractions might be introduced to improve learning. We designed a card game to teach first and second grade children (N = 195, including a racially and economically diverse sample from the United States) about fractions in one of three ways. In the Actively Divided condition we iteratively divided an area model into equal-sized units, in the Predivided condition we used an area model with the end-state of the Actively Divided condition, and in the Nondivided condition we used a continuous representation of the fraction magnitude that was not divided into unit-sized parts. Children in the actively divided condition demonstrated larger improvements matching symbolic fractions and visual fractions (i.e., pie charts) than children in the other two conditions. Posthoc analyses of children's gameplay revealed that the actively divided condition may have provided a more optimal level of difficulty for young children than the predivided condition, which was particularly difficult, and the nondivided condition, which was trivially easy. These differences in gameplay performance provide insights into possible mechanisms for our results. We discuss open research questions highlighted by this work and implications of these findings for both the development of proportional reasoning and fraction learning. (PsycInfo Database Record (c) 2022 APA, all rights reserved).}
}

@article{albilali2022constructingarabic,
  title={Constructing Arabic Reading Comprehension Datasets: Arabic WikiReading and KaifLematha},
  author={Eman Albilali and Nora Al-Twairesh and M. Hosny},
  year={2022},
  booktitle={Language Resources and Evaluation},
  doi={10.1007/s10579-022-09577-5},
  url={https://www.semanticscholar.org/paper/169f4557b4b9909c82eb1fb5e621c68763ddec2d}
}

@article{rozora2022constructiongoodnessoffit,
  title={Construction of goodness-of-fit criteria for the type of impulse response function},
  author={I. Rozora and A. Melnyk},
  year={2022},
  booktitle={Science Technology and Innovation},
  doi={10.35668/2520-6524-2022-2-07},
  url={https://www.semanticscholar.org/paper/d01bb60413ff14940ec6af9fdf0a1868d1d2acef},
  abstract={The article is devoted to the study of the impulse response function, its estimation and properties, square-Gaussian random variables and processes, the rate of convergence of the unknown impulse response function, testing the hypothesis about the type of impulse response function, building a simulation model. The study showed that the pulse response function is the output signal of the system during signal processing, when the input signal is a short pulse. In a more general form, the impulse response function describes the response or output of the system as a function of time. Also, the impulse response function is considered a property of linear displacement systems. During the study of the estimation of the impulse response function on orthonormal and trigonometric bases, two conditions A, B and remarks to them were formed, which are used in the future to find different coefficients. The study of square-Gaussian random variables and processes has shown the benefits of using them in relation to the impulse response function. A theorem was also presented, which estimated the probability of a large deviation of the square-Gaussian process in the norm of a continuous function. To study the rate of convergence of the unknown impulse response function in the space of continuous functions and in the space L2, a lemma was formed, as well as a theorem that directly showed the rate of convergence of the impulse response function in the space of continuous functions. Zero and alternative hypotheses were formed. The null hypothesis claimed that the impulse response function existed, and the alternative hypothesis suggested the opposite. To test the hypothesis about the form of the impulse response function, a theorem was used by which a criterion was formed. Visual Studio Community 2022 integrated development environment (C ++ programming language) and Wolfram Mathematica computer algebra system for analytical transformations and numerical calculations were used to build the simulation model, which allowed to make mathematical calculations quite accurately.}
}

@article{zamorski2022continuallearning,
  title={Continual learning on 3D point clouds with random compressed rehearsal},
  author={M. Zamorski and Michal Stypulkowski and Konrad Karanowski and Tomasz Trzciński and Maciej Ziȩba},
  year={2022},
  booktitle={Computer Vision and Image Understanding},
  doi={10.48550/arXiv.2205.08013},
  url={https://www.semanticscholar.org/paper/0fbb0c96dd9bf65e1af877a377bb4c63767906b8},
  abstract={Contemporary deep neural networks offer state-of-the-art results when applied to visual reasoning, e.g., in the context of 3D point cloud data. Point clouds are important datatype for precise modeling of three-dimensional environments, but effective processing of this type of data proves to be challenging. In the world of large, heavily-parameterized network architectures and continuously-streamed data, there is an increasing need for machine learning models that can be trained on additional data. Unfortunately, currently available models cannot fully leverage training on additional data without losing their past knowledge. Combating this phenomenon, called catastrophic forgetting, is one of the main objectives of continual learning. Continual learning for deep neural networks has been an active field of research, primarily in 2D computer vision, natural language processing, reinforcement learning, and robotics. However, in 3D computer vision, there are hardly any continual learning solutions specifically designed to take advantage of point cloud structure. This work proposes a novel neural network architecture capable of continual learning on 3D point cloud data. We utilize point cloud structure properties for preserving a heavily compressed set of past data. By using rehearsal and reconstruction as regularization methods of the learning process, our approach achieves a significant decrease of catastrophic forgetting compared to the existing solutions on several most popular point cloud datasets considering two continual learning settings: when a task is known beforehand, and in the challenging scenario of when task information is unknown to the model.}
}

@article{pan2022contrastivelanguageimage,
  title={Contrastive Language-Image Pre-Training with Knowledge Graphs},
  author={Xuran Pan and Tianzhu Ye and Dongchen Han and S. Song and Gao Huang},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2210.08901},
  url={https://www.semanticscholar.org/paper/b3d8233b1d1368ccfe691f3a0cc80d5874439198},
  abstract={Recent years have witnessed the fast development of large-scale pre-training frameworks that can extract multi-modal representations in a unified form and achieve promising performances when transferred to downstream tasks. Nevertheless, existing approaches mainly focus on pre-training with simple image-text pairs, while neglecting the semantic connections between concepts from different modalities. In this paper, we propose a knowledge-based pre-training framework, dubbed Knowledge-CLIP, which injects semantic information into the widely used CLIP model. Through introducing knowledge-based objectives in the pre-training process and utilizing different types of knowledge graphs as training data, our model can semantically align the representations in vision and language with higher quality, and enhance the reasoning ability across scenarios and modalities. Extensive experiments on various vision-language downstream tasks demonstrate the effectiveness of Knowledge-CLIP compared with the original CLIP and competitive baselines.}
}

@article{chen2022convfinqaexploring,
  title={ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering},
  author={Zhiyu Chen and SHIYANG LI and Charese Smiley and Zhiqiang Ma and Sameena Shah and William Yang Wang},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.03849},
  url={https://www.semanticscholar.org/paper/d96997265f8146e93b4c9350f19d55e46d1317f0},
  abstract={With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/ConvFinQA.}
}

@article{ehberger2022correctionlanguage,
  title={Correction to: “The language of Dirac’s theory of radiation”: the inception and initial reception of a tool for the quantum field theorist},
  author={Markus Ehberger},
  year={2022},
  booktitle={Archive for History of Exact Sciences},
  doi={10.1007/s00407-022-00293-8},
  url={https://www.semanticscholar.org/paper/5bf8381ea5755c176bdd7e3d7cf576554ed7d697},
  abstract={In 1927, Paul Dirac first explicitly introduced the idea that electrodynamical processes can be evaluated by decomposing them into virtual (modern terminology), energy non-conserving subprocesses. This mode of reasoning structured a lot of the perturbative evaluations of quantum electrodynamics during the 1930s. Although the physical picture connected to Feynman diagrams is no longer based on energy non-conserving transitions but on off-shell particles, emission and absorption subprocesses still remain their fundamental constituents. This article will access the introduction and the initial reception of this picture of subsequent transitions (PST) by conceiving of concepts, models, and their representations as tools for the practitioners. I will argue for a multi-factorial explanation of Dirac’s initial, verbally explicit introduction: the mathematical representation he had developed was highly suggestive and already partly conceptualized; Dirac was philosophical flexible enough to talk about transitions when no actual transitions, according to the general interpretation of quantum mechanics of the time, occurred; and, importantly, Dirac eventually used the verbal exposition in the same paper in which he introduced it. The direct impact of PST on the conception of quantum electrodynamical processes will be exemplified by its reflection in diagrammatical representations. The study of the diverging ontological commitments towards PST immediately after its introduction opens up the prehistory of a philosophical debate that stretches out into the present: the dispute about the representational and ontological status of the physical picture connected to the evaluation of the perturbative series of QED and QFT.}
}

@misc{chen2022counterfactualdecoding,
  title={Counterfactual Decoding for Anti-Hallucination Knowledge-grounded Dialogue Generation},
  author={Anthony Chen and Chris DuBois and Sameer Singh},
  year={2022},
  url={https://www.semanticscholar.org/paper/708a9cfc47136377f192f996329d8ff3289280e4}
}

@article{li2022counterfactualreasoning,
  title={Counterfactual reasoning: Do language models need world knowledge for causal understanding?},
  author={Jiaxuan Li and Lang-Chi Yu and Allyson Ettinger},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.03278},
  url={https://www.semanticscholar.org/paper/91a82593721c03ecffdef1c72ea55c6d87c42473},
  abstract={Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.}
}

@misc{ignacio2022courseguides,
  title={Course guides 270180 - DCS - Curve and Surface Design},
  author={Rodrigo Ignacio and Silveira Isoba and 10 SILVEIRAISOBA-},
  year={2022},
  url={https://www.semanticscholar.org/paper/ee2f4d01ddee42df906209ec07ae060971148ac0}
}

@article{jonsson2022creativemathematical,
  title={Creative Mathematical Reasoning: Does Need for Cognition Matter?},
  author={B. Jonsson and Julia Mossegård and Johan Lithner and Linnea Karlsson Wirebring},
  year={2022},
  booktitle={Frontiers in Psychology},
  doi={10.3389/fpsyg.2021.797807},
  url={https://www.semanticscholar.org/paper/9f6f01cba1158e6bcb17aaa43070ef3b64c59550},
  abstract={A large portion of mathematics education centers heavily around imitative reasoning and rote learning, raising concerns about students’ lack of deeper and conceptual understanding of mathematics. To address these concerns, there has been a growing focus on students learning and teachers teaching methods that aim to enhance conceptual understanding and problem-solving skills. One suggestion is allowing students to construct their own solution methods using creative mathematical reasoning (CMR), a method that in previous studies has been contrasted against algorithmic reasoning (AR) with positive effects on test tasks. Although previous studies have evaluated the effects of CMR, they have ignored if and to what extent intrinsic cognitive motivation play a role. This study investigated the effects of intrinsic cognitive motivation to engage in cognitive strenuous mathematical tasks, operationalized through Need for Cognition (NFC), and working memory capacity (WMC). Two independent groups, consisting of upper secondary students (N = 137, mean age 17.13, SD = 0.62, 63 boys and 74 girls), practiced non-routine mathematical problem solving with CMR and AR tasks and were tested 1 week later. An initial t-test confirmed that the CMR group outperformed the AR group. Structural equation modeling revealed that NFC was a significant predictor of math performance for the CMR group but not for the AR group. The results also showed that WMC was a strong predictor of math performance independent of group. These results are discussed in terms of allowing for time and opportunities for struggle with constructing own solution methods using CMR, thereby enhancing students conceptual understanding.}
}

@article{kumar2022criticalanalysis,
  title={Critical Analysis of Big Data Applications using Functional Linguistics and Diversified Integration},
  author={D. Kumar and Srinivasa Rao and R. Vijaya and Kumar Reddy and D. Kumar and D. Sai and Assoc.Professor},
  year={2022},
  booktitle={2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC)},
  doi={10.1109/ICAAIC53929.2022.9792589},
  url={https://www.semanticscholar.org/paper/d0d2a1b809cf3a78f3439b89b39ce5f11e571664},
  abstract={In recent times, the top-down use of big information data analysis and the mechanical manipulation of man-made intellectual abilities has provided the center with specialized means to advance the practical coordination of semantic structure. Potentially related to the great information research of artificial reasoning (AI), the various embodiments of utilitarian phonetics have routinely experienced problems in applications. As a result, it created critical difficulties for the far-reaching improvement of useful etymology. In this unique situation, ideas related to the improvement of useful phonetics and artificial reasoning are explained. Starting from this premise, the useful etymologies of innovative union and internal incorporation are analyzed with respect to the scenario of large-scale information AI research. In addition, in order to stimulate the useful advancement of phonetics inside and outside, this study makes some proposals, among which are the rationalization of a dispersed group climate, the promotion of a practical phase of information about the language, the assumption of models, modalities of equality of information and the transmission of an equitable preparation of the brain network. These ideas constitute an initial phase of hypothetical etymological analyzes and capacity for improvement. In addition, some ideas were made to advance the internal and external improvement of the utilitarian etymology, such as structuring a useful linguistic information phase, updating an adequate group climate, preparing the circulating brain network, assuming equal information patterns and modalities to provide a reference to skills development and hypothetical phonetic exploration.}
}

@misc{wolf2022crosslingualspeaker,
  title={Cross-Lingual Speaker Identification from Weak Local Evidence},
  author={Thomas Wolf and Lysandre Debut and Julien Victor Sanh and Clement Chaumond and Anthony Delangue and Pier-339 Moi and Clara ric Cistac and Yacine Ma and Julien Jernite and Plu and Teven Xu and Sylvain Le Scao and Gugger and Mariama and Quentin Drame and M. LhoestAlexander and Rush and Michael Miller Yoder and Sopan Khosla and Qinlan Shen and Ben Zhou and Qiang Ning and Daniel Khashabi and Kyle Richardson and Tushar Khot},
  year={2022},
  url={https://www.semanticscholar.org/paper/385b71fb56b54b019b855ff0265bbdbb01ad01ea}
}

@article{yu2022crunchqasynthetic,
  title={CrunchQA: A Synthetic Dataset for Question Answering over Crunchbase Knowledge Graph},
  author={Lifan Yu and Nadya Abdel Madjid and D. Difallah},
  year={2022},
  booktitle={2022 IEEE International Conference on Big Data (Big Data)},
  doi={10.1109/BigData55660.2022.10021012},
  url={https://www.semanticscholar.org/paper/dcb17d546ce8aec11174bafad2fb3d913e6b2e98},
  abstract={The digital transformation in the finance and enterprise sector has been driven by the advances made in big data and artificial intelligence technologies. For instance, data integration enables businesses to make better decisions by consolidating and mining heterogeneous data repositories. In particular, knowledge graphs (KGs) are used to facilitate the integration of disparate data sources and can be utilized to answer complex queries. This work proposes a new dataset for question-answering on knowledge graphs (KGQA) to reflect the challenges we identified in real-world applications which are not covered by existing benchmarks, namely, multi-hop constraints, numeric and literal embeddings, ranking, reification, and hyper-relations. To build the dataset, we create a new Knowledge Graph from the Crunchbase database using a lightweight schema to support high-quality entity embeddings in large graphs. Next, we create a Question Answering dataset based on natural language question generation using predefined multiple-hop templates and paraphrasing. Finally, we conduct extensive experiments with state-of-the-art KGQA models and compare their performance on CrunchQA. The results show that the existing models do not perform well, for example, on multi-hop constrained queries. Hence, CrunchQA can be used as a challenging benchmark dataset for future KGQA reasoning models. The dataset and scripts are available on the project repository. 1}
}

@article{chen2022curriculumbroadcoverage,
  title={Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding},
  author={Zeming Chen and Qiyue Gao},
  year={2022},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2204.06283},
  url={https://www.semanticscholar.org/paper/32c6607346e0bbe21844275f55fb368bbffd4699},
  abstract={In the age of large transformer language models, linguistic evaluation play an important role in diagnosing models’ abilities and limitations on natural language understanding. However, current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets. In this paper, we introduce Curriculum as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. Curriculum contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. We show that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality. In addition, our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models that may encourage future research on re-designing datasets, model architectures, and learning objectives.}
}

@article{cho2022dallevalprobing,
  title={DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers},
  author={Jaemin Cho and Abhaysinh Zala and Mohit Bansal},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/804b27dc02becf7bbbd89ba949e1e07e8677c459}
}

@article{nuraina2022desainbahan,
  title={DESAIN BAHAN AJAR BERBASIS AKTIVITAS PENALARAN MATEMATIS MENGGUNAKAN MODEL MISSOURI MATHEMATIC PROJECT MATA KULIAH ANALISIS KOMPLEKS},
  author={Nuraina Nuraina and Muliana - Muliana and M. Mursalin and Mila Kartika Sari Bangun and U. Rahayu},
  year={2022},
  booktitle={Numeracy},
  doi={10.46244/numeracy.v9i2.1891},
  url={https://www.semanticscholar.org/paper/24f768f302b2a18082d523c11bc1c60a89fa76db},
  abstract={The failure of students in studying complex analysis courses is based on student learning habits that only focus on memorizing concepts from the material studied without understanding it properly, and lack of motivation to repeat the material that has been studied. The teaching materials used in this course are textbooks. However, the textbook used still does not contain activities for students' mathematical reasoning abilities, students cannot learn independently from the book, because the material presented is difficult to understand, even lecturers who teach this complex analysis course have to redesign the material, writing in written form. hand is then given to the student. One effort that could be to improve students' mathematical reasoning abilities is to facilitate learning resources with supporting teaching materials. In this study, teaching materials will be designed based on mathematical reasoning activities using the Missauri Mathematical Project (MMP) learning model. The research method used in this research is the Analysis, Design, Development, Implementation, and Evaluation (ADDIE) development model. The development of complex analysis textbooks based on reasoning activities using the Missauri Mathematical Project learning model is feasible to be developed with the percentage of assessment by media expert validators obtained an average score of 91.79% in the "very valid" categories, and media expert validators obtained the average score. a score of 88.62% with the "very valid" categories. The results of the small group validation obtained a score of 83.5% with the "very valid" criteria. The results of the large group trial obtained a score of 85.5% with the "very practical" criteria. 
Abstrak 
Salah satu kendala dalam mata kuliah analisis kompleks adalah kegagalan mahasiswa dalam mempelajari materi yang disebabkan oleh kebiasaan belajar mahasiswa yang hanya terfokus untuk menghafal konsep dari materi yang diajarkan tanpa memahaminya secara mendalam, serta kurangnya motivasi untuk mengkaji ulang materi yang telah dipelajari. Sumber utama yang digunakan sebagai bahan ajar pada mata kuliah ini yaitu buku paket. Namun, buku paket yang digunakan masih belum memuat  aktivitas kemampuan penalaran matematis mahasiswa, mahasiswa tidak bisa belajar secara mandiri dari buku tersebut, karena materi yang disajikan sulit untuk dipahami, bahkan dosen  yang  mengajarkan  mata  kuliah  analisis  kompleks  ini  harus  mendesain  ulang materinya,  menulis  dengan  tulisan  tangan  kemudian  diberikan  kepada  mahasiswa. Salah satu usaha yang bisa dilakukan untuk meningkatkan kemampuan penalaran matematis mahasiswa adalah dengan memfasilitasi sumber belajar dengan bahan ajar yang mendukung. Dalam penelitian pengembangan ini, bahan ajar akan disusun dengan berbasis aktivitas penalaran matematis menggunakan model pembelajaran missauri mathematic project. Metode penelitian yang dipakai dalam penelitian ini yaitu model pengembangan  analysis, design, development, implementation, and evaluation. Pengembangan buku ajar analisis kompleks berbasis aktivitas penlaran dengan menggunakan model pembelajaran missauri mathematic project layak dikembangkan dengan persentase penilaian oleh validator ahli media diperoleh skor rata-rata sebesar 91,79% dengan kriteria “sangat valid”, dan validator ahli media didapat hasil skor rata-rata sebesar 88,62% dengan kriteria “sangat valid”. Hasil validasi kelompok kecil didapat nilai sebesar 83,5% dengan kriteria “sangat valid”. Hasil uji coba kelompok besar didapat nilai sebesar 85,5%  dengan kriteria “sangat praktis”.}
}

@article{liu2022deplotoneshot,
  title={DePlot: One-shot visual language reasoning by plot-to-table translation},
  author={Fangyu Liu and Julian Martin Eisenschlos and Francesco Piccinno and Syrine Krichene and Chenxi Pang and Kenton Lee and Mandar Joshi and Wenhu Chen and Nigel Collier and Y. Altun},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10505},
  url={https://www.semanticscholar.org/paper/4d3a49d1439a0b8fbb0e9f588970ad0f1d70dec8},
  abstract={Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than>28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.}
}

@article{tian2022debiasingmodels,
  title={Debiasing NLU Models via Causal Intervention and Counterfactual Reasoning},
  author={Bing Tian and Yixin Cao and Yong Zhang and Chunxiao Xing},
  year={2022},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.1609/aaai.v36i10.21389},
  url={https://www.semanticscholar.org/paper/d1eb051c6b13eba8a9b333d5ee0a55250717195d},
  abstract={Recent studies have shown that strong Natural Language Understanding (NLU) models are prone to relying on annotation biases of the datasets as a shortcut, which goes against the underlying mechanisms of the task of interest. To reduce such biases, several recent works introduce debiasing methods to regularize the training process of targeted NLU models. In this paper, we provide a new perspective with causal inference to find out the bias. On one hand, we show that there is an unobserved confounder for the natural language utterances and their respective classes, leading to spurious correlations from training data. To remove such confounder, the backdoor adjustment with causal intervention is utilized to find the true causal effect, which makes the training process fundamentally different from the traditional likelihood estimation. On the other hand, in inference process, we formulate the bias as the direct causal effect and remove it by pursuing the indirect causal effect with counterfactual reasoning. We conduct experiments on large-scale natural language inference and fact verification benchmarks, evaluating on bias sensitive datasets that are specifically designed to assess the robustness of models against known biases in the training data. Experimental results show that our proposed debiasing framework outperforms previous state-of-the-art debiasing methods while maintaining the original in-distribution performance.}
}

@article{khot2022decomposedprompting,
  title={Decomposed Prompting: A Modular Approach for Solving Complex Tasks},
  author={Tushar Khot and H. Trivedi and Matthew Finlayson and Yao Fu and Kyle Richardson and Peter Clark and Ashish Sabharwal},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.02406},
  url={https://www.semanticscholar.org/paper/07955e96cbd778d0ae2a68f09d073b866dd84c2a},
  abstract={Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.}
}

@article{rasal2022deepstructural,
  title={Deep Structural Causal Shape Models},
  author={Rajat Rasal and Daniel Coelho de Castro and Nick Pawlowski and Ben Glocker},
  year={2022},
  booktitle={ECCV Workshops},
  doi={10.48550/arXiv.2208.10950},
  url={https://www.semanticscholar.org/paper/8975f550eed321c203d3990692f82e3f7b112b8f},
  abstract={Causal reasoning provides a language to ask important interventional and counterfactual questions beyond purely statistical association. In medical imaging, for example, we may want to study the causal effect of genetic, environmental, or lifestyle factors on the normal and pathological variation of anatomical phenotypes. However, while anatomical shape models of 3D surface meshes, extracted from automated image segmentation, can be reliably constructed, there is a lack of computational tooling to enable causal reasoning about morphological variations. To tackle this problem, we propose deep structural causal shape models (CSMs), which utilise high-quality mesh generation techniques, from geometric deep learning, within the expressive framework of deep structural causal models. CSMs enable subject-specific prognoses through counterfactual mesh generation ("How would this patient's brain structure change if they were ten years older?"), which is in contrast to most current works on purely population-level statistical shape modelling. We demonstrate the capabilities of CSMs at all levels of Pearl's causal hierarchy through a number of qualitative and quantitative experiments leveraging a large dataset of 3D brain structures.}
}

@article{hodge2022designplanning,
  title={Design and Planning of a Transdisciplinary Investigation into Farmland Pollinators: Rationale, Co-Design, and Lessons Learned},
  author={S. Hodge and O. Schweiger and A. Klein and S. Potts and Cecilia Costa and M. Albrecht and J. D. de Miranda and M. Mand and P. De la Rúa and M. Rundlöf and Eleanor Attridge and R. Dean and P. Bulet and D. Michez and R. Paxton and A. Babin and N. Cougoule and M. Laurent and Anne-Claire Martel and Laurianne Paris and M. Rivière and E. Dubois and M. Chauzat and K. Arafah and Dalel Askri and S. Voisin and T. Kiljanek and Irene Bottero and Christophe Dominik and Giovanni Tamburini and M. Pereira-Peixoto and Dimitry Wintermantel and T. Breeze and E. Cini and D. Senapathi and G. Di Prisco and P. Mędrzycki and S. Hagenbucher and A. Knauer and Janine M. Schwarz and Risto Raimets and Vicente Martínez-López and K. Ivarsson and C. Hartfield and P. Hunter and Mark Brown and J C Stout},
  year={2022},
  booktitle={Sustainability},
  doi={10.3390/su141710549},
  url={https://www.semanticscholar.org/paper/400a8763ed493d109f63a7ca7529811630839d8d},
  abstract={To provide a complete portrayal of the multiple factors negatively impacting insects in agricultural landscapes it is necessary to assess the concurrent incidence, magnitude, and interactions among multiple stressors over substantial biogeographical scales. Trans-national ecological field investigations with wide-ranging stakeholders typically encounter numerous challenges during the design planning stages, not least that the scientific soundness of a spatially replicated study design must account for the substantial geographic and climatic variation among distant sites. ‘PoshBee’ (Pan-European assessment, monitoring, and mitigation of Stressors on the Health of Bees) is a multi-partner transdisciplinary agroecological project established to investigate the suite of stressors typically encountered by pollinating insects in European agricultural landscapes. To do this, PoshBee established a network of 128 study sites across eight European countries and collected over 50 measurements and samples relating to the nutritional, toxicological, pathogenic, and landscape components of the bees’ environment. This paper describes the development process, rationale, and end-result of each aspect of the of the PoshBee field investigation. We describe the main issues and challenges encountered during the design stages and highlight a number of actions or processes that may benefit other multi-partner research consortia planning similar large-scale studies. It was soon identified that in a multi-component study design process, the development of interaction and communication networks involving all collaborators and stakeholders requires considerable time and resources. It was also necessary at each planning stage to be mindful of the needs and objectives of all stakeholders and partners, and further challenges inevitably arose when practical limitations, such as time restrictions and labour constraints, were superimposed upon prototype study designs. To promote clarity for all stakeholders, for each sub-component of the study, there should be a clear record of the rationale and reasoning that outlines how the final design transpired, what compromises were made, and how the requirements of different stakeholders were accomplished. Ultimately, multi-national agroecological field studies such as PoshBee benefit greatly from the involvement of diverse stakeholders and partners, ranging from field ecologists, project managers, policy legislators, mathematical modelers, and farmer organisations. While the execution of the study highlighted the advantages and benefits of large-scale transdisciplinary projects, the long planning period emphasized the need to formally describe a design framework that could facilitate the design process of future multi-partner collaborations.}
}

@article{li2022designsimulation,
  title={Design and Simulation Application of Fuzzy Controller Based on Granular Computing},
  author={Huiyue Li and Jianhua Yang and Wei Lu},
  year={2022},
  booktitle={International Conference on Computer and Information Application},
  doi={10.1109/iccia55271.2022.9828430},
  url={https://www.semanticscholar.org/paper/c54f2d59dca66c50ddc4e8f745f76ac659a1ac6f},
  abstract={Fuzzy control theory generates fuzzy rules based on expert experience and experimental data, so fuzzy control method can control complex and large-scale systems without precise mathematical models. But fuzzy control method still has problems: complex structure and rule explosion problem. Aiming at the above problems, this paper proposes a fuzzy control method based on granular computing. Firstly, design the fuzzy controller, the fuzzy rules are generated by the fuzzy space division method. Then, using the information granulation method, each fuzzy rule is granulated into an information granule. Taking points in the information granules to fit the realization function of the granular function, using that function control the object instead of the fuzzy controller. The fuzzy reasoning process is omitted, and the number of rules has nothing to do with the accuracy, it is only related to the number of granules. Therefore, the structure of the control system can be simplified under the condition of ensuring the accuracy of the system, and the problem of rule explosion can be avoided at the same time. The simulation experiment using the second-order inverted pendulum as the control object proves the feasibility and effectiveness of the fuzzy control method based on granular computing.}
}

@article{tsukanov2022designcircular,
  title={Design of circular air intakes for subsonic turbofans},
  author={Ruslan Tsukanov},
  year={2022},
  booktitle={Aerospace technic and technology},
  doi={10.32620/aktt.2022.4.01},
  url={https://www.semanticscholar.org/paper/e3b0d513d05ced7c6323462e74e858db318a2247},
  abstract={The subject matter of this article is the process of subsonic air intake shaping for high-bypass ratio turbofan at the airplane preliminarily designing stage. The goal was to improve a mathematical model of V. I. Polikovskii method of subsonic air intake shaping for high-bypass ratio turbofan. The tasks are to consider the presence of cant of inlet cross-section, required to perform effective operation at airplane cruising angle-of-attacks; to increase the radius of curvature of the air intake lip to provide air flow near it without flow separation, which was definitely determined and could not be increased in the existing method; to improve constant length velocity gradient law (used in this method) so that too large duct expansion angles near the air intake outlet cross-section can be avoided; to consider the engine inlet spinner presence. The methods used are analytical and digital mathematical methods, implemented in MathCAD and Microsoft Visual Studio systems. The following results were obtained: based on the proposed method, new calculation module for the Power Unit software version 11.8 has been developed (С-language Win32 UNICODE application) with a friendly user interface. Conclusions. The scientific novelty of the results obtained is as follows: 1) mathematical model (algorithm and its program implementation) for circular turbofan air intake shaping has been improved considering cant of the inlet cross-section, air intake lip rounding with two radiuses, presence of engine inlet spinner, and zero expansion angles in the diffuser outlet cross-section; 2) adequacy of calculation results using the improved mathematical model is shown using comparison with shapes of circular turbofan air intakes, developed by the leading aviation companies.}
}

@article{heru2022designsupplementary,
  title={Design of supplementary mathematics module for preparation of minimum competency assessment for fifth grade elementary school students},
  author={Heru Heru and R. E. Yuliani and Izah Zulpah},
  year={2022},
  booktitle={Jurnal Math Educator Nusantara Wahana Publikasi Karya Tulis Ilmiah di Bidang Pendidikan Matematika},
  doi={10.29407/jmen.v8i1.17682},
  url={https://www.semanticscholar.org/paper/11fa440d51f0257ebeb2b6e7e08d75b6d6c83530},
  abstract={Students' skills in solving numeracy questions in the AKM are still varied, especially for SD Negeri 3 Mendo Barat students. The implementation of AKM, especially numeracy skills, impacts the learning given to students. Students are not only required to understand mathematical concepts but must demonstrate other mathematical abilities such as reasoning and problem-solving abilities. Therefore, teaching materials are needed that can support the AKM-based learning process. At SD Negeri 3 Mendo Barat, there are still very few teaching materials owned by students, especially those based on AKM. This study aims to develop a companion mathematics module for preparing AKM for fifth-grade elementary school students that is valid, practical, and potentially affects learning outcomes. Development research using a 4-D model is used as a research technique. The research subjects were 22 students of class V SD Negeri 3 Mendo Barat, as many as 22 people. The results showed that the mathematics module was valid and practical—valid seen from the experts' assessment of the module. Experts consist of material, media, and language experts. Practicality is obtained from the results of student questionnaire analysis in a limited field test which shows a practicality percentage of 91.32 per cent. Based on the operational field test results, the average final score of students is 87.05, which indicates that student learning outcomes are in the very good category. The module can potentially affect student learning outcomes in the good category.}
}

@article{zoph2022designingeffective,
  title={Designing Effective Sparse Expert Models},
  author={Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and J. Dean and Noam M. Shazeer and W. Fedus},
  year={2022},
  booktitle={IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum},
  doi={10.1109/IPDPSW55747.2022.00171},
  url={https://www.semanticscholar.org/paper/e47da75675b9a3fe02ef1efadca39bc8cdfcdc17},
  abstract={Scale has opened new frontiers in natural language processing -- but at a high cost. In response, Mixture-of-Experts (MoE) and Switch Transformers have been proposed as an energy efficient path to even larger and more capable language models. But advancing the state-of-the-art across a broad set of natural language tasks has been hindered by training instabilities and uncertain quality during fine-tuning. Our work focuses on these issues and acts as a design guide. We conclude by scaling a sparse model to 269B parameters, with a computational cost comparable to a 32B dense encoder-decoder Transformer (Stable and Transferable Mixture-of-Experts or ST-MoE-32B). For the first time, a sparse model achieves state-of-the-art performance in transfer learning, across a diverse set of tasks including reasoning (SuperGLUE, ARC Easy, ARC Challenge), summarization (XSum, CNN-DM), closed book question answering (WebQA, Natural Questions), and adversarially constructed tasks (Winogrande, ANLI R3).}
}

@article{albrecht2022despitesuperhuman,
  title={Despite "super-human" performance, current LLMs are unsuited for decisions about ethics and safety},
  author={Joshua Albrecht and Ellie Kitanidis and Abraham J. Fetterman},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.06295},
  url={https://www.semanticscholar.org/paper/7d5175db1b99552491063d2d9581b0b51e1d2932},
  abstract={Large language models (LLMs) have exploded in popularity in the past few years and have achieved undeniably impressive results on benchmarks as varied as question answering and text summarization. We provide a simple new prompting strategy that leads to yet another supposedly"super-human"result, this time outperforming humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset). Unfortunately, we find that relying on average performance to judge capabilities can be highly misleading. LLM errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to flip the output label. We also observe signs of inverse scaling with model size on some examples, and show that prompting models to"explain their reasoning"often leads to alarming justifications of unethical actions. Our results highlight how human-like performance does not necessarily imply human-like understanding or reasoning.}
}

@article{khokhlova2022developmentalgorithm,
  title={Development of an Algorithm to Analyze Vacancies in the Labor Market Based on Open-Source Data},
  author={O. Khokhlova and A. N. Khokhlova and A. Choyzhalsanova},
  year={2022},
  booktitle={Voprosy statistiki},
  doi={10.34023/2313-6383-2022-29-4-33-41},
  url={https://www.semanticscholar.org/paper/e58a983ea50412432b36dfe9c4c8b935e1d57c3d},
  abstract={In the introductory part of the article, the authors substantiate the relevance of developing methodological tools for analyzing job vacancies in the labor market in the context of the modern technological revolution, which significantly increases requirements for professional knowledge and experience of working personnel and changes the ratio between traditional and new professions.To assess the current situation on the labor market and the demand for currently existing professions, the main section of the published results of the study presents the algorithm for analyzing vacancies using large data arrays from open sources using mathematical and statistical tools and machine learning methods using the Python programming language and the IBM SPSS modeler analytical platform. The algorithm includes: parsing data on vacancies, analyzing vacancies by the main criteria, clustering vacancies by salary level and building a neural network model – a multilayer perceptron of the dependence of salary on a number of predictors. It should be noted that the developed algorithm is universal, because it can be used to analyze big data from any open source at a certain point in time.The results of the analysis will allow researchers and specialists of management structures to more realistically assess the current situation on the labor market, educational institutions will be able to adjust training programs in accordance with the modern requirements of employers, employers will make decisions on the development of competencies in their field of activity and conduct a comparative analysis of demanded vacancies in terms of quantitative and qualitative characteristics, and for the applicant it will be easier to see the demand for vacancies in the labor market and develop new skills.}
}

@article{tekin2022developmentattitude,
  title={Development of an Attitude Scale for Moral Literacy Skills: Validity and Reliability Study},
  author={İshak Tekin},
  year={2022},
  booktitle={Marife Dini Araştırmalar Dergisi},
  doi={10.33420/marife.1104203},
  url={https://www.semanticscholar.org/paper/eef5d97b3b2d750009d76c0b814a7bbb4cfe6964},
  abstract={In recent years, some changes have emerged in the general aims of education programs, and in this context, educational authorities have adopted new approaches aiming at nurturing skills. Especially in moral education, which is an indispensable part of citizenship education, the concept of moral literacy is one of the important isssue of educational discussions. In the literature, there are many studies asserting that schools should educate children for moral literacy as well as primitive language literacy, science and mathematics literacy, intercultural literacy. On the other hand, it should be noted that there are not enough studies dealing with the application of the subject in the field. In addition, it is seen that countries have not yet taken sufficient steps for a change in their educational systems and have not demonstrated a strong will. 
Moral literacy refers to a skill that includes thinking about one's own moral values, determining the possible consequences of various alternatives and their effects, making logical decisions about which option is compatible with one's values, acting in line with one's values, and taking responsibility for one's own actions. Moral literacy, conceptualized by Nancy Tuana, consists of three basic elements, namely moral moral sensitivity, moral reasoning skills, and moral imagination, and three sub-skills under them. These skills generally include the intellectual skills that lead the moral decision-making process. 
This study aimed to develop a valid and reliable attitude scale based on Tuana's theory. Within the scope of the research designed in the general survey model, the relevant literature was scanned and the studies on moral literacy were examined. With the clarification of the theoretical structure, the scale development process was started. The following stages were followed in the development of the moral literacy scale: i. Creating the item pool, ii. Content validaty, iii. Reviewing and finalizing the draft form, iv. Application of the scale, v. Item analysis, analysis of construct validity and reliability analysis, vi. Examining the correlation between subscales and the total score, vii. Putting into the final form of the scale and reporting. 
In the first stage, the draft form consisting of 27 items was sent to five field experts and a language expert, 1 item was removed from the scale as they were not compatible with the scope in line with the suggestions of the field experts, and 1 more item was added. The draft form, which was finalized with changes, was applied to 653 university students studying at Eskişehir Osmangazi University and Anadolu University in the fall semester of the 2018-2019 academic year. 88 data, which were not marked carefully and were not found reliable during the data entry to excel, were excluded from the analysis. In the item analysis of the scale, 3 items were excluded from the draft scale and the construct validity was passed. 
In the exploratory factor analysis, 7 more items with values less than .40 and overlapping were excluded from the analysis. As a result of the factor analysis performed after this process, a structure consisting of 5 factors and 20 items was obtained. As a result of the reliability analysis, it was seen that the whole scale had high reliability, while the sub-dimensions had medium and low reliability levels. It was also determined that the structure obtained by exploratory factor analysis was confirmed as a result of testing with confirmatory factor analysis. According to the path analysis, it can be said that the existing structure fits well. When the relationship between the sub-dimensions of the scale is examined, it can be stated that there is a significant positive relationship between the sub-dimensions and a total score can be obtained for the attitudes towards the moral literacy skill of the scale. As a result, it can be said that the scale obtained as a result of these processes can measure the attitudes of university students towards moral literacy skills in a reliable and valid way.}
}

@article{ziborov2022developmentselflearning,
  title={Development of self-learning intelligent decision support system to control of steel production technological processes},
  author={I. Ziborov and T. Zheldak},
  year={2022},
  booktitle={Systems and Technologies},
  doi={10.34185/1562-9945-3-140-2022-04},
  url={https://www.semanticscholar.org/paper/4c4ca4d83f47ba86df0706bd0271f82a1a879316},
  abstract={Taking to the consideration the current state of converter production and measuring equipment at Ukrainian enterprises, it follows that the smelting process is based on a complex dynamic non-deterministic system. The process is complicated by the large number of param-eters, the inability to accurately identify the state of the system at any time, as well as the dif-ficulty of forecasting system requirements. Preliminary analysis has shown that in the conditions of this production converter manufacturing efficiency increase can be reached at the expense of: - reducing the cost of raw materials, such as iron-containing additives, deoxidizers, non-metallic elements in steel; - reduction of melting time, especially blowing time; - reducing defects and improving product quality. It is proposed the architecture of integrated control DSS in converter steel production based on the principle of minimal interference in the production process. The primary aim of such a system is to predict the behavior of the production process, providing the recommen-dations for its impact in order to optimize the external criterion of efficiency. The source and amount of data required for the database formation and DSS knowledge base are substantiated. The mechanism of self-learning in the course of technological tasks is described. The structural scheme of self-learning DSS, self-learning algorithm, which is mainly featured with modularity, is offered in the paper. The approach allows testing of any number of existing algorithms for learning, forecasting and optimization in order to further select the most effective ones, modifies the system in the future and allows the parallel use of a number of com-peting algorithms. The operator has the opportunity to choose as a control solution one of the proposed systems, or the formation of its own, better by a certain external criterion of result quality. Based on the suggested software structure, a number of tasks are formulated that need to be performed to build a decision support system. It is also considered to apply the mathematical apparatus of fuzzy sets to describe certain pa-rameters of the technological process and quality criteria, fuzzy neural network for modeling reasoning processes and the choice of algorithm for its training.}
}

@article{li2022differentiablereasoning,
  title={Differentiable Reasoning over Long Stories - Assessing Systematic Generalisation in Neural Models},
  author={Wanshui Li and Pasquale Minervini},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2203.10620},
  url={https://www.semanticscholar.org/paper/59494dcb572cebb577a1bcb2d6f87dfca93d6591},
  abstract={Contemporary neural networks have achieved a series of developments and successes in many aspects; however, when exposed to data outside the training distribution, they may fail to predict correct answers. In this work, we were concerned about this generalisation issue and thus analysed a broad set of models systematically and robustly over long stories. Related experiments were conducted based on the CLUTRR, which is a diagnostic benchmark suite that can analyse generalisation of natural language understanding (NLU) systems by training over small story graphs and testing on larger ones. In order to handle the multi-relational story graph, we consider two classes of neural models:"E-GNN", the graph-based models that can process graph-structured data and consider the edge attributes simultaneously; and"L-Graph", the sequence-based models which can process linearized version of the graphs. We performed an extensive empirical evaluation, and we found that the modified recurrent neural network yield surprisingly accurate results across every systematic generalisation tasks which outperform the modified graph neural network, while the latter produced more robust models.}
}

@misc{aralikatte2022discursivesocratic,
  title={Discursive Socratic Questioning: (Unsupervised) Interpreting Neural Language Models for Discourse Understanding},
  author={Rahul Aralikatte and Matthew Lamm and Daniel Hardt and Regina Barzilay and Mirella Lapata. 2008 and Modeling and Yonatan Belinkov and Sebastian Gehrmann and Ayal Klein and Eran Hirsch and Ron Eliav and Valentina Pyatkin and J. Mamou and Wei-Jen Ko and Cutter Dalton and Mark Simmons and Greg Fisher and Durrett Junyi and Jessy Li and Dis-737 and Yating Wu and Dananjay Srini-740 and Meichun Webber and Tat-Seng Liu and F. ChuaNancy and 746 and Wenqiang Lei and Yuanxin Xiang and Qian Yuwei Wang and Meichun Zhong and Liu Min-Yen and Kan and Lin-753 and Belinda Z. Li and Maxwell I. Nye and Hwee Ziheng Lin and Tou Ng and Min-Yen Kan and Au-766},
  year={2022},
  url={https://www.semanticscholar.org/paper/76e5069425547d4f53b5aa843a765a305b7fa470}
}

@article{shridhar2022distillingmultistep,
  title={Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions},
  author={Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.00193},
  url={https://www.semanticscholar.org/paper/72123a86eae2cb5c4eae8650f43524039d48875d}
}

@article{shridhar2022distillingreasoning,
  title={Distilling Reasoning Capabilities into Smaller Language Models},
  author={Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2023.findings-acl.441},
  url={https://www.semanticscholar.org/paper/8fd462f6248d5e3f1b6602697c09489086b5655f},
  abstract={Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM}
}

@article{imberti2022divinginto,
  title={Diving into the Deep End: Machine Learning for the Chemist},
  author={S. Imberti},
  year={2022},
  booktitle={ACS Omega},
  doi={10.1021/acsomega.2c04373},
  url={https://www.semanticscholar.org/paper/7c838fcffb45a6c191130627911ef50c5795e9d3},
  abstract={I the past year, ACS Omega has seen a dramatic increase in the number of articles published with an Artificial Intelligence (AI) or Machine Learning, Deep Learning, Neural Networks theme. In 2021, 105 articles were published vs 45 articles published in the previous year, an increase of 133%. This exceptional growth for a fully open access broad scope journal pairs with the growth seen at many other journals in the ACS portfolio. Interestingly, the growth registered in the past 2−3 years is not confined to the journals that specialize in chemical informatics: the Journal of Chemical Information and Modeling, the Journal of Chemical Theory and Computation, and, to some extent, the Journal of Physical Chemistry C. It also encompasses journals in the materials sciences, the physical sciences, measurement science, chemical engineering, and environmental science in the broader ACS portfolio (Figure 1). Looking at the wider publication landscape, the Directory of Open Access Journal lists 65 journal entries for scientific publications that pertain to the topic of AI. Eleven of these were added in the last year alone, and this includes only those journals queried in the computational science category. In addition to these, numerous other open access, broader scope journals also publish work without any perceived evaluation of immediate impact and where existing AI tools have been successfully applied to a variety of chemistry questions. Over the past 10 to 15 years, AI, especially deep learning, has effected dramatic technological progress and proven success in areas such as computer vision, speech recognition, natural language processing, common sense knowledge, strategic reasoning, and robotics. Exceptional results have also been reported in the medical sciences; for example, deep neural networks facilitated accurate diagnosis of skin cancer, and deep learning enabled extraction of new knowledge from old data, enabling accurate prediction of age, gender, smoking status, blood pressure, and heart attack propensity of individuals just by analyzing previously acquired retinal images. But, what about the status of AI and its perception in chemistry? In the ensuing text, as an entreé to the associated Virtual Issue, I will present a brief overview of the perceived usefulness of AI at this time in some fields of the chemical sciences and related areas, based on recently published reviews and perspectives by experts in the area, as well as other resources. A review by Baum et al. charted the growth and distribution of AI-related chemistry publications in the last two decades using the CAS Content Collection, which includes patents as well as research articles. In their paper, they refer to the “Hype Cycle of Emerging Technologies”, and from the data gathered, they determine that AI adoption in life sciences and analytical chemistry has navigated the so-called “peak of inflated expectations” and “trough of disillusionment” and successfully progressed to the “plateau of productivity”. It is common to overestimate the effect of a technology in the short run and underestimate it in the long run (anecdotally known as Amara’s law). For example, despite commercial interests and consequent investments being enormous, to this day, no new drug has yet been synthesized using AI. As another (counter) example, Peiretti and Brunel, in their Perspective published in 2018, ponder whether organic chemistry and in particular retrosynthesis might be the ideal next application of AI techniques. After all, it “f its perfectly” the definition of AI as a problem with “complex input−output relationships [that] are dif f icult or impractical to model procedurally”. However, Baum et al. firmly assess that “there are still areas of Chemistry like organic synthetic chemistry where AI is yet to make an impact”. Still, work is in progress, as standardized formats for reporting a chemical synthesis procedure are being developed and even classified (an essential step for scientific fields to exist) in the new taxonomy of digital chemistry or chemputation. At this stage in the discussion, it may be interesting to explore what the drivers are to greater or lesser success in applying AI methods to scientific problems. This point is examined in the Editorial by Jones et al., which accompanies an excellent JACS Au special collection on “Emerging Chemistry & Machine Learning”. In their overview, three main reasons are indicated. Two of them are quite intrinsic to the method (algorithm development and theoretical derivation of descriptors), while one of them is, notably, not specific but resides in the availability of a collection of standardized, highquality data. A case in point, and unanimously defined as the most spectacular recent success of AI, is the recent prediction of a protein’s three-dimensional structure from its amino acid sequence via AlphaFold. The tool was trained on large publicly available databases, such as the RSCB Protein Data Bank, as well as protein sequences of unknown structure. In return, it is somewhat expected, although not guaranteed, that the new information generated is also made available to the public. The AlphaFold code is now publicly available. It can}
}

@article{mathur2022docinferdocumentlevel,
  title={DocInfer: Document-level Natural Language Inference using Optimal Evidence Selection},
  author={Puneet Mathur and Gautam Kunapuli and Riyaz Ahmad Bhat and Manish Shrivastava and Dinesh Manocha and M. Singh},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2022.emnlp-main.51},
  url={https://www.semanticscholar.org/paper/4430cb7ddb3c4a9860ddabf4f92568a8a03c2b18},
  abstract={We present DocInfer - a novel, end-to-end Document-level Natural Language Inference model that builds a hierarchical document graph enriched through inter-sentence relations (topical, entity-based, concept-based), performs paragraph pruning using the novel SubGraph Pooling layer, followed by optimal evidence selection based on REINFORCE algorithm to identify the most important context sentences for a given hypothesis. Our evidence selection mechanism allows it to transcend the input length limitation of modern BERT-like Transformer models while presenting the entire evidence together for inferential reasoning. We show this is an important property needed to reason on large documents where the evidence may be fragmented and located arbitrarily far from each other. Extensive experiments on popular corpora - DocNLI, ContractNLI, and ConTRoL datasets, and our new proposed dataset called CaseHoldNLI on the task of legal judicial reasoning, demonstrate significant performance gains of 8-12% over SOTA methods. Our ablation studies validate the impact of our model. Performance improvement of 3-6% on annotation-scarce downstream tasks of fact verification, multiple-choice QA, and contract clause retrieval demonstrates the usefulness of DocInfer beyond primary NLI tasks.}
}

@article{lewis2022doesclip,
  title={Does CLIP Bind Concepts? Probing Compositionality in Large Image Models},
  author={Martha Lewis and Nihal V. Nayak and Qinan Yu and Jack Merullo and Ellie Pavlick},
  year={2022},
  booktitle={Findings},
  doi={10.48550/arXiv.2212.10537},
  url={https://www.semanticscholar.org/paper/2de7790ed868510c8001a90c11737fe4e8a01930},
  abstract={Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ‘red cube’ by reasoning over the constituents ‘red’ and ‘cube’. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ‘cube behind sphere’ from ‘sphere behind cube’). To inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We benchmark them on three synthetic datasets – single-object, two-object, and relational – designed to test concept binding. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance drops dramatically. At the same time, CDSMs also perform poorly, with best performance at chance level.}
}

@misc{elsaesser2022downloadfree,
  title={Download Free Film Theory An Introduction Through The Senses Thomas Elsaesser Pdf File Free},
  author={T. Elsaesser},
  year={2022},
  url={https://www.semanticscholar.org/paper/213b3e97d6cbf31ba582b4787c612507a2d545cf}
}

@article{jiang2022draftsketch,
  title={Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
  author={Albert Qiaochu Jiang and S. Welleck and J. Zhou and Wenda Li and Jiacheng Liu and M. Jamnik and Timothée Lacroix and Yuhuai Wu and Guillaume Lample},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.12283},
  url={https://www.semanticscholar.org/paper/7de36d6b14aadc8cdb6ad1340b9ca64b15375bca},
  abstract={The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.}
}

@article{lu2022dynamicprompt,
  title={Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Kai-Wei Chang and Y. Wu and Song-Chun Zhu and Tanmay Rajpurohit and Peter Clark and A. Kalyan},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2209.14610},
  url={https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4},
  abstract={Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.}
}

@article{melnyk2022economicmathematical,
  title={ECONOMIC AND MATHEMATICAL TOOLS FOR PREDICTING THE CURRENCY EXCHANGE RATE},
  author={Ostap Melnyk and Oleksandr Novoseletskyy},
  year={2022},
  booktitle={Scientific opinion: Economics and Management},
  doi={10.32836/2521-666x/2022-78-24},
  url={https://www.semanticscholar.org/paper/46d121662cd6b020e9945b20c399559bd4c276ab},
  abstract={The article deals with the analysis of existing approaches to exchange rate forecasting. It also includes the review of Ukrainian and foreign scientists on this topic. The authors of this article have considered the main disadvantages and benefits of existing forecasting dimensions, as well as individual methods and models. They indicated ways to facilitate the implementation of currency exchange rate forecasting using neural networks with software libraries for various programming languages and individual software applications, as well. As a result, the authors have systematized knowledge about existing approaches used in the process of currency exchange rate forecasting. There are two dimensions of currency exchange rate forecasting, in particular, intuitive and formalized ones. The intuitive dimension is peculiar to short-term forecasting and is often used in trading. Its main advantages include the ability to consider structural changes in the economy that can significantly affect the exchange rate formation itself and the speed of forecasting. However, the disadvantage of intuitive methods is the inability to prove formally the quality of the obtained forecasts. The advantages of the formalized dimension of forecasting include the ability to prove the quality. Businesses and government agencies use it the most often. Extrapolation methods and machine learning methods are mainly used to predict the exchange rate using formalized methods. Moreover, the reviewed studies indicate that among the well-known extrapolation methods for predicting the exchange rate, autoregressive models (VAR, AR, ARMA, ARIMA, SARIMA, ARCH, GARCH, ARDL) and smoothing methods (floating averages, adaptive methods and models) are used the most frequently. Machine learning methods include neural networks. Trend models have proved to be ineffective for currency exchange rate forecasting. The reason for this appeared to be using large amounts of data for currency exchange rate forecasting, and each fluctuation there directly affects the whole phenomenon.}
}

@article{joshi2022ertestevaluating,
  title={ER-TEST Evaluating Explanation Regularization Methods for NLP Models},
  author={Brihi Joshi and Aaron Chan and Ziyi Liu and Shaoliang Nie and Maziar Sanjabi and Hamed Firooz and Xiang Ren},
  year={2022},
  booktitle={TRUSTNLP},
  doi={10.48550/arXiv.2205.12542},
  url={https://www.semanticscholar.org/paper/506f4614f2be3b02984a1b293553ce07d18b38bb}
}

@article{wang2022erecenhanced,
  title={EREC: Enhanced Language Representations with Event Chains},
  author={Huajie Wang and Yinglin Wang},
  year={2022},
  booktitle={Inf.},
  doi={10.3390/info13120582},
  url={https://www.semanticscholar.org/paper/a41c89afad3e5cfbcaa6dcd4acb02d0cd53a15b1},
  abstract={The natural language model BERT uses a large-scale unsupervised corpus to accumulate rich linguistic knowledge during its pretraining stage, and then, the information is fine-tuned for specific downstream tasks, which greatly improves the understanding capability of various natural language tasks. For some specific tasks, the capability of the model can be enhanced by introducing external knowledge. In fact, these methods, such as ERNIE, have been proposed for integrating knowledge graphs into BERT models, which significantly enhanced its capabilities in related tasks such as entity recognition. However, for two types of tasks, commonsense causal reasoning and predicting the ending of stories, few previous studies have combined model modification and process optimization to integrate external knowledge. Therefore, referring to ERNIE, in this paper, we propose enhanced language representation with event chains (EREC), which focuses on keywords in the text corpus and their implied relations. Event chains are integrated into EREC as external knowledge. Furthermore, various graph networks are used to generate embeddings and to associate keywords in the corpus. Finally, via multi-task training, external knowledge is integrated into the model generated in the pretraining stage so as to enhance the effect of the model in downstream tasks. The experimental process of the EREC model is carried out with a three-stage design, and the experimental results show that EREC has a deeper understanding of the causal relationship and event relationship contained in the text by integrating the event chains, and it achieved significant improvements on two specific tasks.}
}

@article{fredericks2022editorial,
  title={Editorial},
  author={B. Fredericks and M. Nakata and Katelyn Barney},
  year={2022},
  journal={The Australian Journal of Indigenous Education},
  doi={10.55146/ajie.v51i2.624},
  url={https://www.semanticscholar.org/paper/c5186d69aaa32a3f34d8b77217995114ded864db},
  abstract={Welcome to Volume 51.2 of The Australian Journal of Indigenous Education. This is our second volume since our shift to being an open access journal. We are very pleased that AJIE has recently been accepted into the Directory of Open Access Journals and was awarded the DOAJ Seal for best practice in open access. DOAJ is an extensive index of diverse open access journals internationally and their aim is to increase the visibility, accessibility, reputation, usage and impact of quality, peer-reviewed, open access scholarly research journals globally. We are also excited that since the journal became open access in August 2022 there has been over 20,000 views of whole articles and over 24,000 views of abstracts on our new open access website. 
This is a larger volume of AJIE than usual, and we thank the authors and reviewers for their contributions. You play a vital role in ensuring the quality of the journal. We would also like to thank Michelle James for her detailed and astute copyediting for the journal. Special thanks to Senior Publications Officer Sonia Nitchell for her continuing work on importing the large AJIE archive onto the new platform. 
The first suite of articles in this volume focuses on the early childhood context with articles by Locke and Webb providing us with insights into the inclusion of Indigenous knowledges and perspectives in early education and care settings in the first paper and how Aboriginal educators integrated their cultural knowledge and experiences to develop Aboriginal children’s skills in the second. In a South Saami context, Kroik explores preschool teachers’ identity as linguistic role models by means of analysing their own descriptions of language learning. In Canada, Schroeder et al. demonstrate the importance of making curricula relevant to Indigenous children by including content that is culturally relevant and developmentally appropriate. The interrelationships between language, identity and culture from Māori kaumātua (elders both male and female) and whānau (parents and extended family members) from Aotearoa (New Zealand) is explored by Berryman et al. 
The second suite of papers take us into the context of schools. Johnson and Flückiger explore the important role for Aboriginal Education Workers in remote Australian communities, while Goodall et al. draw on student and teacher memories of the early days of Indigenous-controlled adult education provider Tranby Aboriginal Co-operative Ltd. The paper by Guenther et al. analyses My School data for Very Remote Aboriginal schools, showing how the Remote School Attendance Strategy school attendance results compare with similar non-Remote School Attendance Strategy schools. Their findings raise ethical and accountability concerns about the Remote School Attendance Strategy, which they argue lacks evidence of attendance improvement, and which potentially causes harm. Whitau et al. also examine school attendance but in relation to Western Australian Aboriginal young women and the links between racism, teacher–student relationships, and peer connectedness, and how these were related to participant attendance and engagement at school. Moore et al. discuss the Whole of Community Engagement (WCE) initiative, which sought to identify barriers and enablers in Aboriginal students’ pathways to post-compulsory education in six remote communities in Arnhem Land and central Australia. They describe the features that led them to characterise the initiative and the remote community and school context as intercultural and complex. Also in relation to the Whole of Community Engagement initiative, Moore et al. propose an intercultural perspective as a refinement to the both-ways approach to remote education. Osborne et al. focus on aspirations of students, their families and communities at Nyangatjatjara College an independent Aboriginal school distributed across three campuses in the southern region of the Northern Territory. Macdonald and Gringart present a new measurement instrument, the Multi-Dimensional Student Perceptions of School Questionnaire (MSPSQ), validated with a moderate-sized sample of Indigenous and non-Indigenous secondary students in Western Australia. 
The next suite of papers has an international focus with papers from Canada, Aotearoa (New Zealand), Brazil, and Tonga. Stavrou and Murphy explore tensions surrounding Indigenising school mathematics in a Western Canadian prairie province conducted with three Cree elementary school teachers while Denston et al. examine teachers’ perceptions and experiences of a collaborative case study to adapt a literacy approach originally designed for an Aotearoa (New Zealand) English-medium context. Ioris et al. explore the main trends and pending gaps related to indigenous education in Brazil while Fonua et al. shares the stories of 26 successful Tongan science learners who participated in talanoa (open discussion without an agenda) about their engagement, enjoyment, and success in secondary and university science education in Aotearoa (New Zealand). 
The final papers in this volume shift to the university context with Hogarth exploring a small pilot study conducted at a Queensland university examining how academics perceive the inclusion of Indigenous Knowledges within institutional and professional contexts and initial teacher education programs. Forsyth et al. speak to the importance of employing Indigenous methodologies when conducting Indigenous research to improve dental and medical health outcomes for Indigenous peoples. Hook and Jessen reflect on the contentious nature of non-Indigenous academics teaching Indigenous Studies and draw on student survey data to illustrate the conflict between their pedagogic practices, student expectations and the structural impediments to their teaching aims. Smith et al. also provide a personal reflection on the higher education context by discussing the need to have institutional conversations about coloniality, institutional racism and white fragility within tertiary institutions. The final paper in this volume by Gibbs et al. explores the relationships between racism, cultural resilience, and educational engagement and academic outcomes for Aboriginal tertiary students. They highlight that cultural resilience and support is critical to Aboriginal student success within universities. Racism continues to be particularly important to address because, as the 2022 Australian Reconciliation Barometer recently highlighted, experiences of racial prejudice have increased for Aboriginal and Torres Strait Islander people over the last two years and certainly there is much work needed to improve relationships between Indigenous and non-Indigenous people. 
We hope you enjoy reading the articles in this volume and hope the articles lead to further dialogue and discussion about Indigenous educational success both in Australia and internationally. 
Bronwyn Fredericks, Martin Nakata, and Katelyn Barney}
}

@article{brabec2022editorialinvestigation,
  title={Editorial for “Investigation of the Inter‐ and Intra‐Scanner Reproducibility and Repeatability of Radiomics Features in Magnetic Resonance Imaging”},
  author={J. Brabec and F. Lennartsson},
  year={2022},
  journal={Journal of Magnetic Resonance Imaging},
  doi={10.1002/jmri.28190},
  url={https://www.semanticscholar.org/paper/26f5fc56ff98df31c9e54b1e760e976eb46028a2},
  abstract={Radiomics refers to a multistep process of feature extraction from medical images and their analysis by computer algorithms. The key idea is that the information contained in an image may not reveal itself by a naked eye but may become apparent through algorithms. This could lead to new imaging contrasts, perhaps because of the fundamentally different nature of how we, man and computer, arrive at image characterization—in the case of radiologists, the analysis relies on the wiring of a brain, whereas radiomics relies on simple but incremental mathematical operations. Furthermore, radiomics as a process can be divided into consecutive steps: acquisition of the medical images, identification of the volume of interest that relates to the pathology and its segmentation, extraction of relevant features describing the pathology, creating or sharing databases, and, finally, proposition with validation of predictive models. This is the main reason why radiomics holds the promise of discovering hidden correlations and bringing transparency into radiologists’ decisionmaking, respectively. One could summarize that radiomics is rather an approach than a specific method and that images are regarded as data rather than pictures. Radiomics is used to solve tasks by utilizing several families of features: these may be based on morphologic characteristics such as lesion size or volume, image intensity-based histograms or descriptors of the relationships between image voxels, such as gray-level co-occurrence, run-length, size-zone, or distance zone matrices. Features can also be extracted from images that are preprocessed by filters (eg, wavelet or Laplacian filters), as well as may stem from image fractal features. They can also be, however, based on machine learning techniques. One of such important tasks is a texture analysis—which is defined as a spatial distribution of an image that contains information on the local aspects of the tissue. To describe texture, we can use words in natural language such as coarseness, smoothness, or perhaps because it is difficult to grasp this concept in natural language a description by quantifiable features could be orthogonal and thus useful. MRI-based radiomics has already been applied in research, for example, to predict treatment response or outcome based on intensity histogram-based radiomic features. Radiomics could also bridge the gap between radiology and biology by correlating the features with underlying tumor genetics. Other applications include identification of malignant tissue, tumor classification, image guided radiotherapy, or distinguishing true progression from pseudo-progression. The results can be complementary to information obtained from histology, genotyping, laboratory results, clinical reports, or standard radiological reports or can be even corroborated in a decision support system. Although the possibilities of applications seem to be vast, and the theory is appealing, radiomics has not yet lived up to this promise and has not yet transitioned into the clinics because the key limitation of radiomics is its reproducibility. It is an issue even in the case when we use the same computational method and scan the participants on the same MR scanners! This is the question that is addressed in current issue of Journal of Magnetic Resonance in Imaging by the work titled “Investigation of the interand intra-scanner reproducibility and repeatability of radiomics features in magnetic resonance imaging” where the authors studied interand intra-scanner variability of radiomics variables in MRI trough multicenter validation. The authors found that, albeit few do, most of the radiomics features do not pass this minimal test! A way forward is to find a consensus on features that are reproducible, which is where this study contributes. However, one cannot be successful without a deeper understanding of the dependencies: the feature values depend both on parameter choices during calculations, but also on the parameters of the MR pulse-sequence. Based on this understanding, we can define features that are robust with respect to the dependencies. To further facilitate the data-driven approach, although challenging, it is crucial to create large and curated databases that also include a large span of the underlying dependencies. Because, if these are not at hand,}
}

@article{garcaros2022effectsselfregulated,
  title={Effects of self-regulated learning and procrastination on academic stress, subjective well-being, and academic achievement in secondary education},
  author={R. García-Ros and F. Pérez-Gónzalez and J. Tomás and P. Sancho},
  year={2022},
  booktitle={Current Psychology},
  doi={10.1007/s12144-022-03759-8},
  url={https://www.semanticscholar.org/paper/0e9087ca2feb8400039833999290ec0ef09d7bc0},
  abstract={The main objective of this study was to test a structural theoretical model of the effects of self-regulated learning on academic stress, subjective well-being, and academic achievement in Secondary Education, considering academic procrastination as a mediator. An additional aim was to explore whether these relationships were moderated by gender and educational level. Participants were 728 students in compulsory and post-compulsory secondary education in a large city in Eastern Spain. Path analysis results indicated that the proposed model showed satisfactory fit, with the three dimensions of self-regulated learning significantly predicting the educational outcomes considered, and that procrastination mediated these relationships. Overall, the model is able to predict 9.8% of the variance of academic stress, 23.1% of students wellbeing, and 14% of academic achievement. Moreover, the multi-group routine revealed no moderation effects due to gender, but educational level moderated two relationships, between self-efficacy and academic achievement and between metacognitive strategies and procrastination. Additionally, supplementary models were tested for three specific subjects (Spanish Language, Foreign Language and Mathematics), which showed an improvement in explained variance, being respectively: 29%, 28% and 27%. Results are discussed in light of previous research and in terms of their impact on educational practice.}
}

=== END BIBTEX ENTRIES ===

=== NEW PAPERS BEING ADDED THIS ITERATION ===
@misc{aralikatte2022discursivesocratic,
  title={Discursive Socratic Questioning: (Unsupervised) Interpreting Neural Language Models for Discourse Understanding},
  author={Rahul Aralikatte and Matthew Lamm and Daniel Hardt and Regina Barzilay and Mirella Lapata. 2008 and Modeling and Yonatan Belinkov and Sebastian Gehrmann and Ayal Klein and Eran Hirsch and Ron Eliav and Valentina Pyatkin and J. Mamou and Wei-Jen Ko and Cutter Dalton and Mark Simmons and Greg Fisher and Durrett Junyi and Jessy Li and Dis-737 and Yating Wu and Dananjay Srini-740 and Meichun Webber and Tat-Seng Liu and F. ChuaNancy and 746 and Wenqiang Lei and Yuanxin Xiang and Qian Yuwei Wang and Meichun Zhong and Liu Min-Yen and Kan and Lin-753 and Belinda Z. Li and Maxwell I. Nye and Hwee Ziheng Lin and Tou Ng and Min-Yen Kan and Au-766},
  year={2022},
  url={https://www.semanticscholar.org/paper/76e5069425547d4f53b5aa843a765a305b7fa470}
}

@article{shridhar2022distillingmultistep,
  title={Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions},
  author={Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.00193},
  url={https://www.semanticscholar.org/paper/72123a86eae2cb5c4eae8650f43524039d48875d}
}

@article{shridhar2022distillingreasoning,
  title={Distilling Reasoning Capabilities into Smaller Language Models},
  author={Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2023.findings-acl.441},
  url={https://www.semanticscholar.org/paper/8fd462f6248d5e3f1b6602697c09489086b5655f},
  abstract={Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM}
}

@article{imberti2022divinginto,
  title={Diving into the Deep End: Machine Learning for the Chemist},
  author={S. Imberti},
  year={2022},
  booktitle={ACS Omega},
  doi={10.1021/acsomega.2c04373},
  url={https://www.semanticscholar.org/paper/7c838fcffb45a6c191130627911ef50c5795e9d3},
  abstract={I the past year, ACS Omega has seen a dramatic increase in the number of articles published with an Artificial Intelligence (AI) or Machine Learning, Deep Learning, Neural Networks theme. In 2021, 105 articles were published vs 45 articles published in the previous year, an increase of 133%. This exceptional growth for a fully open access broad scope journal pairs with the growth seen at many other journals in the ACS portfolio. Interestingly, the growth registered in the past 2−3 years is not confined to the journals that specialize in chemical informatics: the Journal of Chemical Information and Modeling, the Journal of Chemical Theory and Computation, and, to some extent, the Journal of Physical Chemistry C. It also encompasses journals in the materials sciences, the physical sciences, measurement science, chemical engineering, and environmental science in the broader ACS portfolio (Figure 1). Looking at the wider publication landscape, the Directory of Open Access Journal lists 65 journal entries for scientific publications that pertain to the topic of AI. Eleven of these were added in the last year alone, and this includes only those journals queried in the computational science category. In addition to these, numerous other open access, broader scope journals also publish work without any perceived evaluation of immediate impact and where existing AI tools have been successfully applied to a variety of chemistry questions. Over the past 10 to 15 years, AI, especially deep learning, has effected dramatic technological progress and proven success in areas such as computer vision, speech recognition, natural language processing, common sense knowledge, strategic reasoning, and robotics. Exceptional results have also been reported in the medical sciences; for example, deep neural networks facilitated accurate diagnosis of skin cancer, and deep learning enabled extraction of new knowledge from old data, enabling accurate prediction of age, gender, smoking status, blood pressure, and heart attack propensity of individuals just by analyzing previously acquired retinal images. But, what about the status of AI and its perception in chemistry? In the ensuing text, as an entreé to the associated Virtual Issue, I will present a brief overview of the perceived usefulness of AI at this time in some fields of the chemical sciences and related areas, based on recently published reviews and perspectives by experts in the area, as well as other resources. A review by Baum et al. charted the growth and distribution of AI-related chemistry publications in the last two decades using the CAS Content Collection, which includes patents as well as research articles. In their paper, they refer to the “Hype Cycle of Emerging Technologies”, and from the data gathered, they determine that AI adoption in life sciences and analytical chemistry has navigated the so-called “peak of inflated expectations” and “trough of disillusionment” and successfully progressed to the “plateau of productivity”. It is common to overestimate the effect of a technology in the short run and underestimate it in the long run (anecdotally known as Amara’s law). For example, despite commercial interests and consequent investments being enormous, to this day, no new drug has yet been synthesized using AI. As another (counter) example, Peiretti and Brunel, in their Perspective published in 2018, ponder whether organic chemistry and in particular retrosynthesis might be the ideal next application of AI techniques. After all, it “f its perfectly” the definition of AI as a problem with “complex input−output relationships [that] are dif f icult or impractical to model procedurally”. However, Baum et al. firmly assess that “there are still areas of Chemistry like organic synthetic chemistry where AI is yet to make an impact”. Still, work is in progress, as standardized formats for reporting a chemical synthesis procedure are being developed and even classified (an essential step for scientific fields to exist) in the new taxonomy of digital chemistry or chemputation. At this stage in the discussion, it may be interesting to explore what the drivers are to greater or lesser success in applying AI methods to scientific problems. This point is examined in the Editorial by Jones et al., which accompanies an excellent JACS Au special collection on “Emerging Chemistry & Machine Learning”. In their overview, three main reasons are indicated. Two of them are quite intrinsic to the method (algorithm development and theoretical derivation of descriptors), while one of them is, notably, not specific but resides in the availability of a collection of standardized, highquality data. A case in point, and unanimously defined as the most spectacular recent success of AI, is the recent prediction of a protein’s three-dimensional structure from its amino acid sequence via AlphaFold. The tool was trained on large publicly available databases, such as the RSCB Protein Data Bank, as well as protein sequences of unknown structure. In return, it is somewhat expected, although not guaranteed, that the new information generated is also made available to the public. The AlphaFold code is now publicly available. It can}
}

@article{mathur2022docinferdocumentlevel,
  title={DocInfer: Document-level Natural Language Inference using Optimal Evidence Selection},
  author={Puneet Mathur and Gautam Kunapuli and Riyaz Ahmad Bhat and Manish Shrivastava and Dinesh Manocha and M. Singh},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2022.emnlp-main.51},
  url={https://www.semanticscholar.org/paper/4430cb7ddb3c4a9860ddabf4f92568a8a03c2b18},
  abstract={We present DocInfer - a novel, end-to-end Document-level Natural Language Inference model that builds a hierarchical document graph enriched through inter-sentence relations (topical, entity-based, concept-based), performs paragraph pruning using the novel SubGraph Pooling layer, followed by optimal evidence selection based on REINFORCE algorithm to identify the most important context sentences for a given hypothesis. Our evidence selection mechanism allows it to transcend the input length limitation of modern BERT-like Transformer models while presenting the entire evidence together for inferential reasoning. We show this is an important property needed to reason on large documents where the evidence may be fragmented and located arbitrarily far from each other. Extensive experiments on popular corpora - DocNLI, ContractNLI, and ConTRoL datasets, and our new proposed dataset called CaseHoldNLI on the task of legal judicial reasoning, demonstrate significant performance gains of 8-12% over SOTA methods. Our ablation studies validate the impact of our model. Performance improvement of 3-6% on annotation-scarce downstream tasks of fact verification, multiple-choice QA, and contract clause retrieval demonstrates the usefulness of DocInfer beyond primary NLI tasks.}
}

@article{lewis2022doesclip,
  title={Does CLIP Bind Concepts? Probing Compositionality in Large Image Models},
  author={Martha Lewis and Nihal V. Nayak and Qinan Yu and Jack Merullo and Ellie Pavlick},
  year={2022},
  booktitle={Findings},
  doi={10.48550/arXiv.2212.10537},
  url={https://www.semanticscholar.org/paper/2de7790ed868510c8001a90c11737fe4e8a01930},
  abstract={Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ‘red cube’ by reasoning over the constituents ‘red’ and ‘cube’. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ‘cube behind sphere’ from ‘sphere behind cube’). To inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We benchmark them on three synthetic datasets – single-object, two-object, and relational – designed to test concept binding. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance drops dramatically. At the same time, CDSMs also perform poorly, with best performance at chance level.}
}

@misc{elsaesser2022downloadfree,
  title={Download Free Film Theory An Introduction Through The Senses Thomas Elsaesser Pdf File Free},
  author={T. Elsaesser},
  year={2022},
  url={https://www.semanticscholar.org/paper/213b3e97d6cbf31ba582b4787c612507a2d545cf}
}

@article{jiang2022draftsketch,
  title={Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
  author={Albert Qiaochu Jiang and S. Welleck and J. Zhou and Wenda Li and Jiacheng Liu and M. Jamnik and Timothée Lacroix and Yuhuai Wu and Guillaume Lample},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.12283},
  url={https://www.semanticscholar.org/paper/7de36d6b14aadc8cdb6ad1340b9ca64b15375bca},
  abstract={The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems.}
}

@article{lu2022dynamicprompt,
  title={Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Kai-Wei Chang and Y. Wu and Song-Chun Zhu and Tanmay Rajpurohit and Peter Clark and A. Kalyan},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2209.14610},
  url={https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4},
  abstract={Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.}
}

@article{melnyk2022economicmathematical,
  title={ECONOMIC AND MATHEMATICAL TOOLS FOR PREDICTING THE CURRENCY EXCHANGE RATE},
  author={Ostap Melnyk and Oleksandr Novoseletskyy},
  year={2022},
  booktitle={Scientific opinion: Economics and Management},
  doi={10.32836/2521-666x/2022-78-24},
  url={https://www.semanticscholar.org/paper/46d121662cd6b020e9945b20c399559bd4c276ab},
  abstract={The article deals with the analysis of existing approaches to exchange rate forecasting. It also includes the review of Ukrainian and foreign scientists on this topic. The authors of this article have considered the main disadvantages and benefits of existing forecasting dimensions, as well as individual methods and models. They indicated ways to facilitate the implementation of currency exchange rate forecasting using neural networks with software libraries for various programming languages and individual software applications, as well. As a result, the authors have systematized knowledge about existing approaches used in the process of currency exchange rate forecasting. There are two dimensions of currency exchange rate forecasting, in particular, intuitive and formalized ones. The intuitive dimension is peculiar to short-term forecasting and is often used in trading. Its main advantages include the ability to consider structural changes in the economy that can significantly affect the exchange rate formation itself and the speed of forecasting. However, the disadvantage of intuitive methods is the inability to prove formally the quality of the obtained forecasts. The advantages of the formalized dimension of forecasting include the ability to prove the quality. Businesses and government agencies use it the most often. Extrapolation methods and machine learning methods are mainly used to predict the exchange rate using formalized methods. Moreover, the reviewed studies indicate that among the well-known extrapolation methods for predicting the exchange rate, autoregressive models (VAR, AR, ARMA, ARIMA, SARIMA, ARCH, GARCH, ARDL) and smoothing methods (floating averages, adaptive methods and models) are used the most frequently. Machine learning methods include neural networks. Trend models have proved to be ineffective for currency exchange rate forecasting. The reason for this appeared to be using large amounts of data for currency exchange rate forecasting, and each fluctuation there directly affects the whole phenomenon.}
}

@article{joshi2022ertestevaluating,
  title={ER-TEST Evaluating Explanation Regularization Methods for NLP Models},
  author={Brihi Joshi and Aaron Chan and Ziyi Liu and Shaoliang Nie and Maziar Sanjabi and Hamed Firooz and Xiang Ren},
  year={2022},
  booktitle={TRUSTNLP},
  doi={10.48550/arXiv.2205.12542},
  url={https://www.semanticscholar.org/paper/506f4614f2be3b02984a1b293553ce07d18b38bb}
}

@article{wang2022erecenhanced,
  title={EREC: Enhanced Language Representations with Event Chains},
  author={Huajie Wang and Yinglin Wang},
  year={2022},
  booktitle={Inf.},
  doi={10.3390/info13120582},
  url={https://www.semanticscholar.org/paper/a41c89afad3e5cfbcaa6dcd4acb02d0cd53a15b1},
  abstract={The natural language model BERT uses a large-scale unsupervised corpus to accumulate rich linguistic knowledge during its pretraining stage, and then, the information is fine-tuned for specific downstream tasks, which greatly improves the understanding capability of various natural language tasks. For some specific tasks, the capability of the model can be enhanced by introducing external knowledge. In fact, these methods, such as ERNIE, have been proposed for integrating knowledge graphs into BERT models, which significantly enhanced its capabilities in related tasks such as entity recognition. However, for two types of tasks, commonsense causal reasoning and predicting the ending of stories, few previous studies have combined model modification and process optimization to integrate external knowledge. Therefore, referring to ERNIE, in this paper, we propose enhanced language representation with event chains (EREC), which focuses on keywords in the text corpus and their implied relations. Event chains are integrated into EREC as external knowledge. Furthermore, various graph networks are used to generate embeddings and to associate keywords in the corpus. Finally, via multi-task training, external knowledge is integrated into the model generated in the pretraining stage so as to enhance the effect of the model in downstream tasks. The experimental process of the EREC model is carried out with a three-stage design, and the experimental results show that EREC has a deeper understanding of the causal relationship and event relationship contained in the text by integrating the event chains, and it achieved significant improvements on two specific tasks.}
}

@article{fredericks2022editorial,
  title={Editorial},
  author={B. Fredericks and M. Nakata and Katelyn Barney},
  year={2022},
  journal={The Australian Journal of Indigenous Education},
  doi={10.55146/ajie.v51i2.624},
  url={https://www.semanticscholar.org/paper/c5186d69aaa32a3f34d8b77217995114ded864db},
  abstract={Welcome to Volume 51.2 of The Australian Journal of Indigenous Education. This is our second volume since our shift to being an open access journal. We are very pleased that AJIE has recently been accepted into the Directory of Open Access Journals and was awarded the DOAJ Seal for best practice in open access. DOAJ is an extensive index of diverse open access journals internationally and their aim is to increase the visibility, accessibility, reputation, usage and impact of quality, peer-reviewed, open access scholarly research journals globally. We are also excited that since the journal became open access in August 2022 there has been over 20,000 views of whole articles and over 24,000 views of abstracts on our new open access website. 
This is a larger volume of AJIE than usual, and we thank the authors and reviewers for their contributions. You play a vital role in ensuring the quality of the journal. We would also like to thank Michelle James for her detailed and astute copyediting for the journal. Special thanks to Senior Publications Officer Sonia Nitchell for her continuing work on importing the large AJIE archive onto the new platform. 
The first suite of articles in this volume focuses on the early childhood context with articles by Locke and Webb providing us with insights into the inclusion of Indigenous knowledges and perspectives in early education and care settings in the first paper and how Aboriginal educators integrated their cultural knowledge and experiences to develop Aboriginal children’s skills in the second. In a South Saami context, Kroik explores preschool teachers’ identity as linguistic role models by means of analysing their own descriptions of language learning. In Canada, Schroeder et al. demonstrate the importance of making curricula relevant to Indigenous children by including content that is culturally relevant and developmentally appropriate. The interrelationships between language, identity and culture from Māori kaumātua (elders both male and female) and whānau (parents and extended family members) from Aotearoa (New Zealand) is explored by Berryman et al. 
The second suite of papers take us into the context of schools. Johnson and Flückiger explore the important role for Aboriginal Education Workers in remote Australian communities, while Goodall et al. draw on student and teacher memories of the early days of Indigenous-controlled adult education provider Tranby Aboriginal Co-operative Ltd. The paper by Guenther et al. analyses My School data for Very Remote Aboriginal schools, showing how the Remote School Attendance Strategy school attendance results compare with similar non-Remote School Attendance Strategy schools. Their findings raise ethical and accountability concerns about the Remote School Attendance Strategy, which they argue lacks evidence of attendance improvement, and which potentially causes harm. Whitau et al. also examine school attendance but in relation to Western Australian Aboriginal young women and the links between racism, teacher–student relationships, and peer connectedness, and how these were related to participant attendance and engagement at school. Moore et al. discuss the Whole of Community Engagement (WCE) initiative, which sought to identify barriers and enablers in Aboriginal students’ pathways to post-compulsory education in six remote communities in Arnhem Land and central Australia. They describe the features that led them to characterise the initiative and the remote community and school context as intercultural and complex. Also in relation to the Whole of Community Engagement initiative, Moore et al. propose an intercultural perspective as a refinement to the both-ways approach to remote education. Osborne et al. focus on aspirations of students, their families and communities at Nyangatjatjara College an independent Aboriginal school distributed across three campuses in the southern region of the Northern Territory. Macdonald and Gringart present a new measurement instrument, the Multi-Dimensional Student Perceptions of School Questionnaire (MSPSQ), validated with a moderate-sized sample of Indigenous and non-Indigenous secondary students in Western Australia. 
The next suite of papers has an international focus with papers from Canada, Aotearoa (New Zealand), Brazil, and Tonga. Stavrou and Murphy explore tensions surrounding Indigenising school mathematics in a Western Canadian prairie province conducted with three Cree elementary school teachers while Denston et al. examine teachers’ perceptions and experiences of a collaborative case study to adapt a literacy approach originally designed for an Aotearoa (New Zealand) English-medium context. Ioris et al. explore the main trends and pending gaps related to indigenous education in Brazil while Fonua et al. shares the stories of 26 successful Tongan science learners who participated in talanoa (open discussion without an agenda) about their engagement, enjoyment, and success in secondary and university science education in Aotearoa (New Zealand). 
The final papers in this volume shift to the university context with Hogarth exploring a small pilot study conducted at a Queensland university examining how academics perceive the inclusion of Indigenous Knowledges within institutional and professional contexts and initial teacher education programs. Forsyth et al. speak to the importance of employing Indigenous methodologies when conducting Indigenous research to improve dental and medical health outcomes for Indigenous peoples. Hook and Jessen reflect on the contentious nature of non-Indigenous academics teaching Indigenous Studies and draw on student survey data to illustrate the conflict between their pedagogic practices, student expectations and the structural impediments to their teaching aims. Smith et al. also provide a personal reflection on the higher education context by discussing the need to have institutional conversations about coloniality, institutional racism and white fragility within tertiary institutions. The final paper in this volume by Gibbs et al. explores the relationships between racism, cultural resilience, and educational engagement and academic outcomes for Aboriginal tertiary students. They highlight that cultural resilience and support is critical to Aboriginal student success within universities. Racism continues to be particularly important to address because, as the 2022 Australian Reconciliation Barometer recently highlighted, experiences of racial prejudice have increased for Aboriginal and Torres Strait Islander people over the last two years and certainly there is much work needed to improve relationships between Indigenous and non-Indigenous people. 
We hope you enjoy reading the articles in this volume and hope the articles lead to further dialogue and discussion about Indigenous educational success both in Australia and internationally. 
Bronwyn Fredericks, Martin Nakata, and Katelyn Barney}
}

@article{brabec2022editorialinvestigation,
  title={Editorial for “Investigation of the Inter‐ and Intra‐Scanner Reproducibility and Repeatability of Radiomics Features in Magnetic Resonance Imaging”},
  author={J. Brabec and F. Lennartsson},
  year={2022},
  journal={Journal of Magnetic Resonance Imaging},
  doi={10.1002/jmri.28190},
  url={https://www.semanticscholar.org/paper/26f5fc56ff98df31c9e54b1e760e976eb46028a2},
  abstract={Radiomics refers to a multistep process of feature extraction from medical images and their analysis by computer algorithms. The key idea is that the information contained in an image may not reveal itself by a naked eye but may become apparent through algorithms. This could lead to new imaging contrasts, perhaps because of the fundamentally different nature of how we, man and computer, arrive at image characterization—in the case of radiologists, the analysis relies on the wiring of a brain, whereas radiomics relies on simple but incremental mathematical operations. Furthermore, radiomics as a process can be divided into consecutive steps: acquisition of the medical images, identification of the volume of interest that relates to the pathology and its segmentation, extraction of relevant features describing the pathology, creating or sharing databases, and, finally, proposition with validation of predictive models. This is the main reason why radiomics holds the promise of discovering hidden correlations and bringing transparency into radiologists’ decisionmaking, respectively. One could summarize that radiomics is rather an approach than a specific method and that images are regarded as data rather than pictures. Radiomics is used to solve tasks by utilizing several families of features: these may be based on morphologic characteristics such as lesion size or volume, image intensity-based histograms or descriptors of the relationships between image voxels, such as gray-level co-occurrence, run-length, size-zone, or distance zone matrices. Features can also be extracted from images that are preprocessed by filters (eg, wavelet or Laplacian filters), as well as may stem from image fractal features. They can also be, however, based on machine learning techniques. One of such important tasks is a texture analysis—which is defined as a spatial distribution of an image that contains information on the local aspects of the tissue. To describe texture, we can use words in natural language such as coarseness, smoothness, or perhaps because it is difficult to grasp this concept in natural language a description by quantifiable features could be orthogonal and thus useful. MRI-based radiomics has already been applied in research, for example, to predict treatment response or outcome based on intensity histogram-based radiomic features. Radiomics could also bridge the gap between radiology and biology by correlating the features with underlying tumor genetics. Other applications include identification of malignant tissue, tumor classification, image guided radiotherapy, or distinguishing true progression from pseudo-progression. The results can be complementary to information obtained from histology, genotyping, laboratory results, clinical reports, or standard radiological reports or can be even corroborated in a decision support system. Although the possibilities of applications seem to be vast, and the theory is appealing, radiomics has not yet lived up to this promise and has not yet transitioned into the clinics because the key limitation of radiomics is its reproducibility. It is an issue even in the case when we use the same computational method and scan the participants on the same MR scanners! This is the question that is addressed in current issue of Journal of Magnetic Resonance in Imaging by the work titled “Investigation of the interand intra-scanner reproducibility and repeatability of radiomics features in magnetic resonance imaging” where the authors studied interand intra-scanner variability of radiomics variables in MRI trough multicenter validation. The authors found that, albeit few do, most of the radiomics features do not pass this minimal test! A way forward is to find a consensus on features that are reproducible, which is where this study contributes. However, one cannot be successful without a deeper understanding of the dependencies: the feature values depend both on parameter choices during calculations, but also on the parameters of the MR pulse-sequence. Based on this understanding, we can define features that are robust with respect to the dependencies. To further facilitate the data-driven approach, although challenging, it is crucial to create large and curated databases that also include a large span of the underlying dependencies. Because, if these are not at hand,}
}

@article{garcaros2022effectsselfregulated,
  title={Effects of self-regulated learning and procrastination on academic stress, subjective well-being, and academic achievement in secondary education},
  author={R. García-Ros and F. Pérez-Gónzalez and J. Tomás and P. Sancho},
  year={2022},
  booktitle={Current Psychology},
  doi={10.1007/s12144-022-03759-8},
  url={https://www.semanticscholar.org/paper/0e9087ca2feb8400039833999290ec0ef09d7bc0},
  abstract={The main objective of this study was to test a structural theoretical model of the effects of self-regulated learning on academic stress, subjective well-being, and academic achievement in Secondary Education, considering academic procrastination as a mediator. An additional aim was to explore whether these relationships were moderated by gender and educational level. Participants were 728 students in compulsory and post-compulsory secondary education in a large city in Eastern Spain. Path analysis results indicated that the proposed model showed satisfactory fit, with the three dimensions of self-regulated learning significantly predicting the educational outcomes considered, and that procrastination mediated these relationships. Overall, the model is able to predict 9.8% of the variance of academic stress, 23.1% of students wellbeing, and 14% of academic achievement. Moreover, the multi-group routine revealed no moderation effects due to gender, but educational level moderated two relationships, between self-efficacy and academic achievement and between metacognitive strategies and procrastination. Additionally, supplementary models were tested for three specific subjects (Spanish Language, Foreign Language and Mathematics), which showed an improvement in explained variance, being respectively: 29%, 28% and 27%. Results are discussed in light of previous research and in terms of their impact on educational practice.}
}

=== END NEW PAPERS ===

PREVIOUS DRAFT OF THE PAPER:
=== ABSTRACT ===
This systematic literature review provides a comprehensive overview of the current landscape of large language models (LLMs) applied to reasoning tasks, with a specific focus on mathematical and logical reasoning. The review synthesizes recent research, highlighting advancements in utilizing LLMs for complex problem-solving, theorem proving, and understanding various forms of logical inference. It identifies key methodologies, datasets, and emergent challenges in this rapidly evolving field. The findings underscore the significant potential of LLMs to augment understanding and application in domains requiring structured reasoning, while also pointing to areas requiring further investigation, such as robustness, interpretability, and ethical considerations. A total of 462 studies were initially identified, and after adding 108 new papers in this iteration, the total number of included studies is 570. The expanded corpus reveals a richer tapestry of approaches, refined performance metrics, and a growing awareness of the nuances involved in LLM-powered reasoning.

=== INTRODUCTION ===
\section{INTRODUCTION}

Mathematical reasoning is a cornerstone of human intelligence, underpinning advancements across science, engineering, finance, and everyday problem-solving \cite{lu2022surveydeep}. The development of artificial intelligence systems capable of emulating and augmenting this capacity has been a long-standing goal in machine learning and natural language processing (NLP) \cite{lu2022surveydeep}. Recent breakthroughs in large language models (LLMs), powered by massive datasets and sophisticated transformer architectures \cite{vaswani2017attention}, have opened up unprecedented opportunities for tackling complex reasoning tasks \cite{lu2022surveydeep, stolfo2022causalframework}. These models have demonstrated remarkable abilities in tasks ranging from solving arithmetic word problems \cite{wei2022chainthought, zhang2022automaticchain} to generating proofs \cite{wu2022autoformalizationwith}, pushing the boundaries of what AI can achieve in domains traditionally considered exclusive to human intellect \cite{lu2022surveydeep, stolfo2022causalframework, lu2022surveydeep}. Understanding how LLMs process and reason about mathematical and logical information is crucial for their effective and ethical deployment \cite{alemany2022methodologycharacterize, zhang2022empiricalinvestigation, yu2022alertadapt}. The continuous development and application of these models in diverse fields like finance \cite{chen2022convfinqaexploring}, healthcare \cite{tewes2022artificialintelligence}, and robotics \cite{liang2022codepolicies, raman2022capecorrective} further underscore the importance of robust reasoning capabilities.

Despite the growing interest, a consolidated view of the current state of research in LLMs for reasoning is often fragmented. Existing surveys frequently focus on broader aspects of deep learning \cite{lu2022surveydeep}, knowledge-enhanced models \cite{hu2022surveyknowledge}, or specific domains like visual reasoning \cite{zhang2022multilayerattention}. This review aims to bridge this gap by systematically analyzing the literature on the application of LLMs to mathematical and logical reasoning. Our motivation stems from the need to provide researchers and practitioners with a clear understanding of the current methodologies, key findings, and outstanding challenges in this domain \cite{zhang2022empiricalinvestigation, kumar2022answerlevelcalibration}. By synthesizing existing work, we aim to identify promising research directions and facilitate further progress \cite{poythress2022semioticanalysis, zimmerman2022assessingphysics}. The inclusion of new papers such as those focusing on educational materials \cite{nuraina2022desainbahan, heru2022designsupplementary}, specialized datasets \cite{liu2022deplotoneshot, alghamdi2022armathdataset}, and debiasing techniques \cite{tian2022debiasingmodels} enriches this overview.

This systematic literature review addresses the following research questions:

1. What are the primary approaches and methodologies employed in applying large language models to mathematical and logical reasoning tasks? \cite{cohen2022thisunicorn, zhang2022multilayerattention, abramson2022applicationpseudologlikelihoods, nam2022achievingunderstanding, wu2022autoformalizationwith, zhang2022automaticchain, shridhar2022automaticgeneration, xiao2022auxiliaryteaching, an2022bevbertmultimodal, chen2022btpkbasedlearning, hua2022bayesvarbrulunified, si2022benchmarkinggpt3, gopinath2022benchmarkinglargescale, gokhale2022benchmarkingspatial, srivastava2022beyondimitation, wei2022chainthought, tefnik2022incontextlearners, behnamghader2022retrieveraugmentedlanguage, schlegel2022transformersreason, carette2022centralsubmonads, lindstrm2022clevrmathdataset, dong2022corrpusdetecting, krell2022crosscontaminationaccelerating, lin2022curriculumlearning, willig2022foundationmodels, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.
2. What are the key advancements and findings reported in the literature regarding LLM performance in these reasoning tasks? \cite{stolfo2022causalframework, lu2022surveydeep, markta2022accuracypupils, yu2022alertadapt, shidqiya2022analysisstudents, jung2022blankcollapse, wang2022chiqalarge, wilson2022classificationopenended, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.
3. What are the identified limitations, challenges, and future research directions in this domain? \cite{stolfo2022causalframework, alemany2022methodologycharacterize, zhang2022multilayerattention, li2022scenariobasedexploration, leemann2022oherencevaluation, wang2022chiqalarge, wilson2022classificationopenended, dong2022corrpuscodebased, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.

To address these questions, we followed the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines \cite{PRISMA} to ensure a rigorous and transparent approach to literature selection, data extraction, and synthesis. The PRISMA flow diagram, detailing the study selection process, would further illustrate this methodology. This paper is structured as follows: Section \ref{sec:methodology} details the methodology employed for this systematic review. Section \ref{sec:results} presents the key findings from the selected studies, organized thematically. Section \ref{sec:discussion} discusses the implications of these findings, identifies research gaps, and outlines future directions. Finally, Section \ref{sec:conclusion} summarizes the review's contributions and offers concluding remarks.

=== METHODOLOGY ===
\section{METHODOLOGY}

This systematic literature review was conducted following the PRISMA 2020 guidelines \cite{PRISMA} to ensure a comprehensive and reproducible search and selection process. The review aimed to identify and synthesize research that explicitly investigates the application of large language models (LLMs) to mathematical and logical reasoning tasks. A total of 570 records were identified and included in the analysis after thorough screening and full-text review.

\subsection{Search Strategy}

A systematic search was performed across several major academic databases, including IEEE Xplore, ACM Digital Library, ScienceDirect, SpringerLink, and arXiv. The search strategy combined keywords related to large language models and mathematical/logical reasoning. The core search string was developed as follows:

(\"large language model*\" OR \"LLM*\" OR \"transformer model*\" OR \"neural language model*\") AND (\"mathematical reasoning\" OR \"math reasoning\" OR \"mathematical problem solving\" OR \"arithmetic reasoning\" OR \"algebraic reasoning\" OR \"calculus reasoning\" OR \"theorem proving\" OR \"logical reasoning\" OR \"commonsense reasoning\" OR \"causal reasoning\")

The search was limited to publications from 2020 to 2022 to capture recent advancements, given the rapid evolution of LLMs \cite{stolfo2022causalframework, lu2022surveydeep, cohen2022thisunicorn, snchez2022clusteringapproach, desogus2022contributionrelationship, wang2022hybridgenetic, zhang2022multilayerattention, ricci2022petrinetbasedapproach, ekong2022ratiocinativestudy, li2022scenariobasedexploration, lu2022surveydeep, hu2022surveyknowledge, zhou2022surveyneural, wankmller2022comparisonapproaches, wang2022comparisonthree, alemany2022methodologycharacterize, kim2022novelmodular, mi2022reviewdevelopment, poythress2022semioticanalysis, song2022thesissubmitted, markta2022accuracypupils, ji2022afrbertattentionbased, gulwani2022aiassistedprogramming, yu2022alertadapt, 2022algorithmmethod, mare2022updatethermal, nam2022achievingunderstanding, hppner2022advantagesdisadvantages, abramson2022applicationpseudologlikelihoods, zhang2022empiricalinvestigation, khan2022executableformal, katra2022experimentationframework, jeon2022informationtheoreticanalysis, bellomarini2022overviewvadalog, amaliyah2022analisiskesulitan, shidqiya2022analysisstudents, yu2022analysiscorrelation, kumar2022answerlevelcalibration, zhou2022applicationthreeflow, alghamdi2022armathdataset, kar2022arggenprompting, tewes2022artificialintelligence, zimmerman2022assessingphysics, kogan2022assessingacademic, gao2022attributedtext, wu2022autoformalizationneural, wu2022autoformalizationwith, zhang2022automaticchain, shridhar2022automaticgeneration, xiao2022auxiliaryteaching, an2022bevbertmultimodal, chen2022btpkbasedlearning, hua2022bayesvarbrulunified, si2022benchmarkinggpt3, gopinath2022benchmarkinglargescale, gokhale2022benchmarkingspatial, srivastava2022beyondimitation, jung2022blankcollapse, wan2022bridgingbetween, raman2022capecorrective, lindstrm2022clevrmathdataset, dong2022corrpusdetecting, krell2022crosscontaminationaccelerating, lin2022curriculumlearning, willig2022foundationmodels, tefnik2022incontextlearners, behnamghader2022retrieveraugmentedlanguage, schlegel2022transformersreason, carette2022centralsubmonads, wei2022chainthought, unknown2022chartingspace, wang2022chiqalarge, wilson2022classificationopenended, dong2022corrpuscodebased, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}. No language restrictions were applied, though the vast majority of relevant publications were in English.

\subsection{Inclusion and Exclusion Criteria}

Studies were included if they met the following criteria:

*   **Topic:** The study must explicitly focus on the application of large language models (or models with comparable scale and architecture, such as advanced transformers) to mathematical or logical reasoning tasks. This includes domains such as arithmetic, algebra, formal logic, causal reasoning, and commonsense reasoning applied to mathematical problems \cite{lu2022surveydeep, stolfo2022causalframework, zhang2022multilayerattention, yu2022alertadapt, amaliyah2022analisiskesulitan, shidqiya2022analysisstudents, zimmerman2022assessingphysics, wu2022autoformalizationwith, zhang2022automaticchain, shridhar2022automaticgeneration, gokhale2022benchmarkingspatial, lindstrm2022clevrmathdataset, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.
*   **Methodology:** The study must present original research, including experimental evaluations, theoretical frameworks, or novel model architectures designed for reasoning with LLMs \cite{yu2022alertadapt, nam2022achievingunderstanding, zhang2022multilayerattention, wu2022autoformalizationwith, raman2022capecorrective, dong2022corrpusdetecting, lin2022curriculumlearning, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.
*   **Publication Type:** Peer-reviewed conference papers, journal articles, and preprints (e.g., from arXiv) were considered \cite{cohen2022thisunicorn, zhang2022multilayerattention, abramson2022applicationpseudologlikelihoods, nam2022achievingunderstanding, zhang2022empiricalinvestigation, khan2022executableformal, katra2022experimentationframework, jeon2022informationtheoreticanalysis, bellomarini2022overviewvadalog, amaliyah2022analisiskesulitan, shidqiya2022analysisstudents, yu2022analysiscorrelation, kumar2022answerlevelcalibration, zhou2022applicationthreeflow, alghamdi2022armathdataset, kar2022arggenprompting, tewes2022artificialintelligence, zimmerman2022assessingphysics, gao2022attributedtext, wu2022autoformalizationneural, wu2022autoformalizationwith, zhang2022automaticchain, shridhar2022automaticgeneration, xiao2022auxiliaryteaching, an2022bevbertmultimodal, chen2022btpkbasedlearning, hua2022bayesvarbrulunified, si2022benchmarkinggpt3, gopinath2022benchmarkinglargescale, gokhale2022benchmarkingspatial, srivastava2022beyondimitation, jung2022blankcollapse, wan2022bridgingbetween, raman2022capecorrective, lindstrm2022clevrmathdataset, dong2022corrpusdetecting, krell2022crosscontaminationaccelerating, lin2022curriculumlearning, willig2022foundationmodels, tefnik2022incontextlearners, behnamghader2022retrieveraugmentedlanguage, schlegel2022transformersreason, carette2022centralsubmonads, wei2022chainthought, unknown2022chartingspace, wang2022chiqalarge, wilson2022classificationopenended, dong2022corrpuscodebased, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.

Studies were excluded if they met any of the following criteria:

*   **Not focused on LLMs:** Studies that did not involve large language models or models of similar scale and architecture (e.g., studies focusing solely on traditional AI methods or smaller neural networks) \cite{kim2022novelmodular, zhang2022multilayerattention}.
*   **Not focused on reasoning:** Studies that applied LLMs to general NLP tasks without a specific emphasis on mathematical or logical reasoning (e.g., text summarization, sentiment analysis, general question answering) \cite{ji2022afrbertattentionbased, mi2022reviewdevelopment, tewes2022artificialintelligence, markta2022accuracypupils}.
*   **Surveys or Reviews:** Papers that primarily summarized existing literature without presenting new empirical results or methodologies, as this review itself serves that purpose \cite{lu2022surveydeep, hu2022surveyknowledge, zhou2022surveyneural}.
*   **Non-English Publications:** While a broad search was performed, only English-language papers were considered for inclusion due to resource constraints.
*   **Insufficient Detail:** Studies lacking sufficient methodological detail or empirical evidence to assess their contribution \cite{song2022thesissubmitted, desogus2022contributionrelationship}.

\subsection{Study Selection and Screening}

The initial search yielded a total of 570 records. After removing duplicates, the remaining 570 records were screened based on their titles and abstracts. All records were initially considered potentially relevant. Following this, full-text articles were retrieved for all 570 records. These full-text articles were then carefully reviewed against the inclusion and exclusion criteria. Any ambiguities were resolved through discussion among the reviewers. The PRISMA flow diagram, though not visually represented here, would detail this iterative process, starting with 'Records identified' (570), 'Records screened' (570), 'Records excluded' (0 based on final full-text review), and 'Studies included' (570).

\subsection{Data Extraction and Synthesis}

For each included study, relevant data were extracted, including:

*   Authors and publication year \cite{cohen2022thisunicorn, stolfo2022causalframework, snchez2022clusteringapproach, desogus2022contributionrelationship, wang2022hybridgenetic, zhang2022multilayerattention, ricci2022petrinetbasedapproach, ekong2022ratiocinativestudy, li2022scenariobasedexploration, lu2022surveydeep, hu2022surveyknowledge, zhou2022surveyneural, wankmller2022comparisonapproaches, wang2022comparisonthree, alemany2022methodologycharacterize, kim2022novelmodular, mi2022reviewdevelopment, poythress2022semioticanalysis, song2022thesissubmitted, markta2022accuracypupils, ji2022afrbertattentionbased, gulwani2022aiassistedprogramming, yu2022alertadapt, 2022algorithmmethod, mare2022updatethermal, nam2022achievingunderstanding, hppner2022advantagesdisadvantages, abramson2022applicationpseudologlikelihoods, zhang2022empiricalinvestigation, khan2022executableformal, katra2022experimentationframework, jeon2022informationtheoreticanalysis, bellomarini2022overviewvadalog, amaliyah2022analisiskesulitan, shidqiya2022analysisstudents, yu2022analysiscorrelation, kumar2022answerlevelcalibration, zhou2022applicationthreeflow, alghamdi2022armathdataset, kar2022arggenprompting, tewes2022artificialintelligence, zimmerman2022assessingphysics, kogan2022assessingacademic, gao2022attributedtext, wu2022autoformalizationneural, wu2022autoformalizationwith, zhang2022automaticchain, shridhar2022automaticgeneration, xiao2022auxiliaryteaching, an2022bevbertmultimodal, chen2022btpkbasedlearning, hua2022bayesvarbrulunified, si2022benchmarkinggpt3, gopinath2022benchmarkinglargescale, gokhale2022benchmarkingspatial, srivastava2022beyondimitation, jung2022blankcollapse, wan2022bridgingbetween, raman2022capecorrective, lindstrm2022clevrmathdataset, dong2022corrpusdetecting, krell2022crosscontaminationaccelerating, lin2022curriculumlearning, willig2022foundationmodels, tefnik2022incontextlearners, behnamghader2022retrieveraugmentedlanguage, schlegel2022transformersreason, carette2022centralsubmonads, wei2022chainthought, unknown2022chartingspace, wang2022chiqalarge, wilson2022classificationopenended, dong2022corrpuscodebased, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.
*   Venue (conference/journal) \cite{cohen2022thisunicorn, stolfo2022causalframework, snchez2022clusteringapproach, wang2022hybridgenetic, zhang2022multilayerattention, ricci2022petrinetbasedapproach, li2022scenariobasedexploration, lu2022surveydeep, hu2022surveyknowledge, zhou2022surveyneural, wankmller2022comparisonapproaches, wang2022comparisonthree, kim2022novelmodular, mi2022reviewdevelopment, poythress2022semioticanalysis, markta2022accuracypupils, ji2022afrbertattentionbased, gulwani2022aiassistedprogramming, yu2022alertadapt, 2022algorithmmethod, mare2022updatethermal, hppner2022advantagesdisadvantages, abramson2022applicationpseudologlikelihoods, zhang2022empiricalinvestigation, khan2022executableformal, katra2022experimentationframework, jeon2022informationtheoreticanalysis, bellomarini2022overviewvadalog, amaliyah2022analisiskesulitan, shidqiya2022analysisstudents, yu2022analysiscorrelation, kumar2022answerlevelcalibration, zhou2022applicationthreeflow, alghamdi2022armathdataset, kar2022arggenprompting, tewes2022artificialintelligence, zimmerman2022assessingphysics, kogan2022assessingacademic, gao2022attributedtext, wu2022autoformalizationneural, wu2022autoformalizationwith, zhang2022automaticchain, shridhar2022automaticgeneration, xiao2022auxiliaryteaching, an2022bevbertmultimodal, chen2022btpkbasedlearning, hua2022bayesvarbrulunified, si2022benchmarkinggpt3, gopinath2022benchmarkinglargescale, gokhale2022benchmarkingspatial, srivastava2022beyondimitation, jung2022blankcollapse, wan2022bridgingbetween, raman2022capecorrective, lindstrm2022clevrmathdataset, dong2022corrpusdetecting, krell2022crosscontaminationaccelerating, lin2022curriculumlearning, willig2022foundationmodels, tefnik2022incontextlearners, behnamghader2022retrieveraugmentedlanguage, schlegel2022transformersreason, carette2022centralsubmonads, wei2022chainthought, unknown2022chartingspace, wang2022chiqalarge, wilson2022classificationopenended, dong2022corrpuscodebased, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.
*   Specific reasoning task addressed (e.g., arithmetic, algebra, logic, theorem proving, commonsense reasoning) \cite{lu2022surveydeep, stolfo2022causalframework, zhang2022empiricalinvestigation, yu2022alertadapt, amaliyah2022analisiskesulitan, shidqiya2022analysisstudents, zimmerman2022assessingphysics, wu2022autoformalizationwith, zhang2022automaticchain, shridhar2022automaticgeneration, gokhale2022benchmarkingspatial, lindstrm2022clevrmathdataset, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.
*   LLM architecture or model used \cite{cohen2022thisunicorn, lu2022surveydeep, ji2022afrbertattentionbased, yu2022alertadapt, abramson2022applicationpseudologlikelihoods, zhang2022empiricalinvestigation, an2022bevbertmultimodal, wang2022chiqalarge, wilson2022classificationopenended, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.
*   Key methodologies and approaches \cite{cohen2022thisunicorn, zhang2022multilayerattention, abramson2022applicationpseudologlikelihoods, nam2022achievingunderstanding, yu2022alertadapt, stolfo2022causalframework, wu2022autoformalizationwith, zhang2022automaticchain, raman2022capecorrective, wei2022chainthought, behnamghader2022retrieveraugmentedlanguage, dong2022corrpuscodebased, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.
*   Main findings and performance metrics \cite{stolfo2022causalframework, lu2022surveydeep, markta2022accuracypupils, yu2022alertadapt, shidqiya2022analysisstudents, jung2022blankcollapse, wang2022chiqalarge, wilson2022classificationopenended, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.
*   Identified limitations and future work \cite{stolfo2022causalframework, alemany2022methodologycharacterize, zhang2022multilayerattention, li2022scenariobasedexploration, leemann2022oherencevaluation, wang2022chiqalarge, wilson2022classificationopenended, dong2022corrpuscodebased, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.

The extracted information was synthesized thematically to identify overarching trends, common methodologies, and significant advancements. The results are presented in Section \ref{sec:results}, organized into thematic subsections. The number of included studies is 570.

\subsection{Quality Assessment}

A formal quality assessment of each study was not conducted as a separate step. Instead, the rigorous application of inclusion and exclusion criteria, and the focus on studies with sufficient methodological detail and empirical evidence, served as an implicit quality filter \cite{wang2022hybridgenetic, wang2022comparisonthree, khan2022executableformal, katra2022experimentationframework, zhang2022empiricalinvestigation, abramson2022applicationpseudologlikelihoods, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}. The synthesis process prioritized studies that demonstrated clear experimental setups, well-defined metrics, and insightful analysis of results.

=== RESULTS ===
\section{RESULTS}

This section presents the key findings from the systematic review of literature on large language models (LLMs) applied to mathematical and logical reasoning tasks. The analysis encompasses methodologies, tasks, and reported outcomes. A total of 570 studies met the inclusion criteria.

\subsection{Publication Trends and Venues}

The majority of the included studies were published between 2020 and 2022, indicating a significant surge in research interest within this timeframe \cite{stolfo2022causalframework, lu2022surveydeep, cohen2022thisunicorn, snchez2022clusteringapproach, desogus2022contributionrelationship, wang2022hybridgenetic, zhang2022multilayerattention, ricci2022petrinetbasedapproach, ekong2022ratiocinativestudy, li2022scenariobasedexploration, lu2022surveydeep, hu2022surveyknowledge, zhou2022surveyneural, wankmller2022comparisonapproaches, wang2022comparisonthree, alemany2022methodologycharacterize, kim2022novelmodular, mi2022reviewdevelopment, poythress2022semioticanalysis, song2022thesissubmitted, markta2022accuracypupils, ji2022afrbertattentionbased, gulwani2022aiassistedprogramming, yu2022alertadapt, 2022algorithmmethod, mare2022updatethermal, nam2022achievingunderstanding, hppner2022advantagesdisadvantages, abramson2022applicationpseudologlikelihoods, zhang2022empiricalinvestigation, khan2022executableformal, katra2022experimentationframework, jeon2022informationtheoreticanalysis, bellomarini2022overviewvadalog, amaliyah2022analisiskesulitan, shidqiya2022analysisstudents, yu2022analysiscorrelation, kumar2022answerlevelcalibration, zhou2022applicationthreeflow, alghamdi2022armathdataset, kar2022arggenprompting, tewes2022artificialintelligence, zimmerman2022assessingphysics, kogan2022assessingacademic, gao2022attributedtext, wu2022autoformalizationneural, wu2022autoformalizationwith, zhang2022automaticchain, shridhar2022automaticgeneration, xiao2022auxiliaryteaching, an2022bevbertmultimodal, chen2022btpkbasedlearning, hua2022bayesvarbrulunified, si2022benchmarkinggpt3, gopinath2022benchmarkinglargescale, gokhale2022benchmarkingspatial, srivastava2022beyondimitation, jung2022blankcollapse, wan2022bridgingbetween, raman2022capecorrective, lindstrm2022clevrmathdataset, dong2022corrpusdetecting, krell2022crosscontaminationaccelerating, lin2022curriculumlearning, willig2022foundationmodels, tefnik2022incontextlearners, behnamghader2022retrieveraugmentedlanguage, schlegel2022transformersreason, carette2022centralsubmonads, wei2022chainthought, unknown2022chartingspace, wang2022chiqalarge, wilson2022classificationopenended, dong2022corrpuscodebased, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}. This rapid growth is largely attributed to the advancements in LLM architectures and their increasing availability \cite{vaswani2017attention}. Prominent venues for this research include major NLP and AI conferences such as the Association for Computational Linguistics (ACL), International Conference on Machine Learning (ICML), Conference on Neural Information Processing Systems (NeurIPS), and the International Conference on Computer Vision (ECCV) \cite{cohen2022thisunicorn, zhang2022multilayerattention, lu2022surveydeep, ji2022afrbertattentionbased, yu2022alertadapt, kumar2022answerlevelcalibration, alghamdi2022armathdataset, kar2022arggenprompting, zimmerman2022assessingphysics, wu2022autoformalizationwith, zhang2022automaticchain, shridhar2022automaticgeneration, gokhale2022benchmarkingspatial, lindstrm2022clevrmathdataset, wang2022chiqalarge, wilson2022classificationopenended, dong2022corrpuscodebased, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}. Journal publications in areas like IEEE Transactions on Knowledge and Data Engineering and Energies also contribute to the corpus \cite{hu2022surveyknowledge, snchez2022clusteringapproach, ricci2022petrinetbasedapproach, zhou2022applicationthreeflow}.

\subsection{Methodological Approaches for Reasoning Tasks}

Several key methodological approaches have emerged in the application of LLMs to reasoning tasks. A prevalent strategy involves fine-tuning pre-trained LLMs on specific datasets tailored for reasoning \cite{yu2022alertadapt, lu2022surveydeep}. For instance, models are fine-tuned on datasets designed for arithmetic, algebraic, or logical reasoning tasks \cite{stolfo2022causalframework, lu2022surveydeep, amaliyah2022analisiskesulitan, shidqiya2022analysisstudents, zimmerman2022assessingphysics, lindstrm2022clevrmathdataset, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}. Another significant direction is the development of specialized LLM architectures or modules that enhance reasoning capabilities. Zhang et al. \cite{zhang2022multilayerattention} proposed a multi-layer attention network for visual commonsense reasoning, which, while not purely mathematical, highlights the importance of fine-grained attention mechanisms for complex reasoning tasks \cite{zhang2022multilayerattention}. Kim et al. \cite{kim2022novelmodular} introduced a novel modular modeling approach for electromechanics, demonstrating how complex systems can be understood through component-based modeling, a principle applicable to building more interpretable reasoning systems \cite{kim2022novelmodular}. Jeon and Van Roy \cite{jeon2022informationtheoreticanalysis} conducted an information-theoretic analysis of compute-optimal neural scaling laws, providing insights into the fundamental trade-offs in model and data size for effective learning, which is crucial for designing reasoning systems \cite{jeon2022informationtheoreticanalysis}. Furthermore, researchers are exploring prompt engineering techniques to guide LLMs towards accurate reasoning solutions. This includes zero-shot, few-shot, and chain-of-thought prompting strategies, which have shown considerable success in eliciting reasoning capabilities from LLMs without explicit task-specific training \cite{lu2022surveydeep, abramson2022applicationpseudologlikelihoods, kumar2022answerlevelcalibration, kar2022arggenprompting, zhang2022automaticchain, shridhar2022automaticgeneration, wei2022chainthought, tefnik2022incontextlearners}. The causal framework proposed by Stolfo et al. \cite{stolfo2022causalframework} is instrumental in understanding the robustness of LLMs to variations in problem descriptions, providing insights into how models arrive at their solutions \cite{stolfo2022causalframework}. This causal analysis is crucial for building trust in LLM-generated reasoning \cite{stolfo2022causalframework}. Abramson and Emami \cite{abramson2022applicationpseudologlikelihoods} applied pseudo-log-likelihoods for natural language scoring, highlighting a zero-shot approach's potential for robustness and efficiency, which is relevant for evaluating reasoning capabilities \cite{abramson2022applicationpseudologlikelihoods}. Wu et al. \cite{wu2022autoformalizationneural, wu2022autoformalizationwith} explored autoformalization for neural theorem proving, a meta-reasoning task that can enhance the capabilities of reasoning models \cite{wu2022autoformalizationneural, wu2022autoformalizationwith}. Raman et al. \cite{raman2022capecorrective} developed CAPE for corrective actions from precondition errors, demonstrating the importance of robust planning and error recovery in embodied agents, which relies on precise reasoning \cite{raman2022capecorrective}. BehnamGhader et al. \cite{behnamghader2022retrieveraugmentedlanguage} and Schlegel et al. \cite{schlegel2022transformersreason} investigate the reasoning capabilities of retriever-augmented language models and transformers in natural language fragments, respectively \cite{behnamghader2022retrieveraugmentedlanguage, schlegel2022transformersreason}. Liang et al. \cite{liang2022codepolicies} proposed \"Code as Policies\" for embodied control, showcasing how LLMs can generate code for complex robotic tasks that require spatial-geometric reasoning and generalization \cite{liang2022codepolicies}. Sahu et al. \cite{sahu2022codequeriesdataset} introduced CodeQueries, a dataset for semantic queries over code, highlighting the challenge of understanding code semantics for answering queries requiring multi-hop reasoning \cite{sahu2022codequeriesdataset}. Zhao et al. \cite{zhao2022collaborativereasoning} presented a method for collaborative reasoning on multi-modal semantic graphs for video-grounded dialogue generation, emphasizing the integration of diverse modalities for complex reasoning \cite{zhao2022collaborativereasoning}. Gouhar et al. \cite{gouhar2022combininglocal} combined local and global approaches for semantic similarity, relevant for understanding nuanced language in reasoning \cite{gouhar2022combininglocal}. Albalak et al. \cite{albalak2022commonsensereasoning} surveyed datasets and benchmarks for commonsense reasoning. Ye et al. \cite{ye2022complementaryexplanations} found that complementary explanations improve in-context learning, highlighting the role of diverse reasoning skills \cite{ye2022complementaryexplanations}. Fu et al. \cite{fu2022complexitybasedprompting} proposed complexity-based prompting for multi-step reasoning, showing that prompts with more steps yield better performance \cite{fu2022complexitybasedprompting}. Li et al. \cite{li2022composingensembles} proposed composing ensembles of pre-trained models via iterative consensus for multimodal tasks \cite{li2022composingensembles}. Kuculo \cite{kuculo2022comprehensiveevent} discussed comprehensive event representations \cite{kuculo2022comprehensiveevent}. Evtikhov and Evtikhov \cite{evtikhov2022computationalexperiment} discussed computational experiments \cite{evtikhov2022computationalexperiment}. The integration of external knowledge, such as knowledge graphs, into LLMs is also a growing area of research, aiming to improve their reasoning accuracy and robustness \cite{hu2022surveyknowledge, zhang2022empiricalinvestigation}. Zhang et al. \cite{zhang2022empiricalinvestigation} empirically investigated commonsense self-supervision with knowledge graphs, showing benefits for language model generalization on downstream reasoning tasks \cite{zhang2022empiricalinvestigation}. Bellomarini et al. \cite{bellomarini2022overviewvadalog} provided an overview of Vadalog, a system for reasoning over large knowledge graphs, demonstrating the importance of structured knowledge representation for advanced reasoning \cite{bellomarini2022overviewvadalog}. Nuraina et al. \cite{nuraina2022desainbahan} and Heru et al. \cite{heru2022designsupplementary} explore the design of teaching materials based on mathematical reasoning activities for students, highlighting the role of structured pedagogical approaches \cite{nuraina2022desainbahan, heru2022designsupplementary}. Liu et al. \cite{liu2022deplotoneshot} presented DePlot for one-shot visual language reasoning by plot-to-table translation, showcasing a novel approach to multimodal reasoning \cite{liu2022deplotoneshot}. Tian et al. \cite{tian2022debiasingmodels} proposed debiasing NLU models via causal intervention and counterfactual reasoning \cite{tian2022debiasingmodels}. Khot et al. \cite{khot2022decomposedprompting} introduced Decomposed Prompting, a modular approach for solving complex tasks by breaking them down into sub-tasks \cite{khot2022decomposedprompting}. Rasal et al. \cite{rasal2022deepstructural} developed Deep Structural Causal Shape Models for causal reasoning about anatomical variations \cite{rasal2022deepstructural}. Hodge et al. \cite{hodge2022designplanning} discussed the design and planning of transdisciplinary investigations, emphasizing the importance of structured approaches in complex research projects \cite{hodge2022designplanning}. Li et al. \cite{li2022designsimulation} proposed a fuzzy controller based on granular computing for system control \cite{li2022designsimulation}. Tsukanov \cite{tsukanov2022designcircular} discussed the design of circular air intakes, involving mathematical modeling for engineering applications \cite{tsukanov2022designcircular}. Zoph et al. \cite{zoph2022designingeffective} focused on designing effective sparse expert models, relevant for scaling LLMs \cite{zoph2022designingeffective}. Albrecht et al. \cite{albrecht2022despitesuperhuman} critiqued LLM suitability for ethical decisions despite \"super-human\" performance \cite{albrecht2022despitesuperhuman}. Khokhlova et al. \cite{khokhlova2022developmentalgorithm} developed an algorithm for labor market analysis using data mining techniques \cite{khokhlova2022developmentalgorithm}. Tekin \cite{tekin2022developmentattitude} conducted a validity and reliability study for a moral literacy skills scale \cite{tekin2022developmentattitude}. Ziborov and Zheldak \cite{ziborov2022developmentselflearning} proposed a self-learning decision support system for steel production \cite{ziborov2022developmentselflearning}. Li and Minervini \cite{li2022differentiablereasoning} assessed systematic generalization in neural models for long stories \cite{li2022differentiablereasoning}.

\subsection{Mathematical and Logical Reasoning Tasks and Performance}

LLMs are being applied to a wide spectrum of mathematical and logical reasoning tasks. Arithmetic reasoning, including solving word problems, has seen significant progress. Models like GPT-3 have demonstrated impressive performance on benchmarks like GSM8K, often by generating step-by-step reasoning processes \cite{stolfo2022causalframework, lu2022surveydeep, zhang2022automaticchain, shridhar2022automaticgeneration, wei2022chainthought}. Algebraic reasoning and equation solving also form a substantial part of the literature \cite{lu2022surveydeep, alghamdi2022armathdataset, lindstrm2022clevrmathdataset, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}. The ability of LLMs to handle symbolic manipulation and abstract concepts in mathematics is a key focus \cite{lu2022surveydeep, khan2022executableformal, amaliyah2022analisiskesulitan}. Xiao et al. \cite{xiao2022auxiliaryteaching} developed an auxiliary teaching system for higher mathematics, indicating the role of AI in aiding mathematical education through structured approaches \cite{xiao2022auxiliaryteaching}. Logical reasoning and theorem proving represent more challenging frontiers. While LLMs are showing promise in generating logical deductions and even assisting in formal theorem proving, this area still requires substantial development \cite{lu2022surveydeep, wu2022autoformalizationneural, wu2022autoformalizationwith}. Poythress \cite{poythress2022semioticanalysis} analyzed multiple systems of logic using semiotic theory, suggesting that human reasoning is richer than any single formal system and highlighting the potential for models to encompass diverse logical frameworks \cite{poythress2022semioticanalysis}. Khan et al. \cite{khan2022executableformal} developed an executable formal model of VHDL, demonstrating the application of formal methods to hardware description languages, which is a form of precise logical reasoning \cite{khan2022executableformal}. Katra et al. \cite{katra2022experimentationframework} also contribute to this by offering a framework for specification and verification of web services, leveraging formalisms akin to logical reasoning \cite{katra2022experimentationframework}. Unknown \cite{unknown2022computerverifiedfoundations} presented computer-verified foundations of metaphysics and an ontology of natural numbers \cite{unknown2022computerverifiedfoundations}. Wagemaker et al. \cite{wagemaker2022concurrentnetkat} introduced Concurrent NetKAT for modeling and analyzing stateful, concurrent networks, relevant for logical reasoning in distributed systems \cite{wagemaker2022concurrentnetkat}. Commonsense reasoning, crucial for many real-world problems, is also being addressed by LLMs \cite{zhang2022multilayerattention, zhang2022empiricalinvestigation, yu2022alertadapt, kumar2022answerlevelcalibration, wan2022bridgingbetween, kim2022cosimcommonsense, albalak2022commonsensereasoning}. Yu et al. \cite{yu2022alertadapt} proposed ALERT to adapt language models to reasoning tasks, finding that finetuning enhances various reasoning skills \cite{yu2022alertadapt}. Cohen et al. \cite{cohen2022thisunicorn} explored personalizing frozen vision-language representations, indicating the potential for LLMs to reason about user-specific visual concepts \cite{cohen2022thisunicorn}. Kumar et al. \cite{kumar2022answerlevelcalibration} focused on answer-level calibration for free-form multiple-choice question answering, highlighting its importance for robust commonsense reasoning evaluations \cite{kumar2022answerlevelcalibration}. Wan et al. \cite{wan2022bridgingbetween} investigated bridging the gap between recognition-level pre-training and commonsensical vision-language tasks, showing the need for specific pre-training strategies for commonsense reasoning \cite{wan2022bridgingbetween}. Kim et al. \cite{kim2022cosimcommonsense} introduced CoSIm, a dataset for counterfactual scene imagination, highlighting challenges in multimodal commonsense reasoning \cite{kim2022cosimcommonsense}. Albalak et al. \cite{albalak2022commonsensereasoning} provided a survey on commonsense reasoning for conversational AI datasets and benchmarks. Ye et al. \cite{ye2022complementaryexplanations} found that complementary explanations improve in-context learning, highlighting the role of diverse reasoning skills \cite{ye2022complementaryexplanations}. Fu et al. \cite{fu2022complexitybasedprompting} proposed complexity-based prompting for multi-step reasoning, showing that prompts with more steps yield better performance \cite{fu2022complexitybasedprompting}. Li et al. \cite{li2022composingensembles} proposed composing ensembles of pre-trained models via iterative consensus for multimodal tasks \cite{li2022composingensembles}. Kuculo \cite{kuculo2022comprehensiveevent} discussed comprehensive event representations \cite{kuculo2022comprehensiveevent}. Evtikhov and Evtikhov \cite{evtikhov2022computationalexperiment} discussed computational experiments \cite{evtikhov2022computationalexperiment}. The performance of LLMs in reasoning is often evaluated using metrics such as accuracy, F1 score, and specific reasoning-focused metrics that assess the coherence and correctness of the generated solution steps \cite{stolfo2022causalframework, yu2022alertadapt, shidqiya2022analysisstudents, gokhale2022benchmarkingspatial, wang2022chiqalarge, wilson2022classificationopenended, dong2022corrpuscodebased}. Markta and Smetáčková \cite{markta2022accuracypupils} investigated the accuracy of pupils' self-assessment in mathematics and language, revealing challenges in self-evaluation that might inform how LLM performance is interpreted and how feedback mechanisms could be designed \cite{markta2022accuracypupils}. Shidqiya and Sukestiyarno \cite{shidqiya2022analysisstudents} analyzed students' mathematical thinking ability in terms of self-efficacy, providing insights into how individual factors can correlate with reasoning proficiency \cite{shidqiya2022analysisstudents}. Specific domains also show LLM applications. Sánchez et al. \cite{snchez2022clusteringapproach} proposed a clustering strategy for the electric vehicle routing problem, showcasing optimization techniques that can be informed by mathematical models and potentially enhanced by LLM-driven reasoning \cite{snchez2022clusteringapproach}. Wang et al. \cite{wang2022hybridgenetic} developed a hybrid genetic algorithm for the flexible job shop scheduling problem, demonstrating complex optimization approaches that could be integrated with or improved by LLM reasoning capabilities \cite{wang2022hybridgenetic}. Gulwani \cite{gulwani2022aiassistedprogramming} discussed AI-assisted programming, including neuro-symbolic techniques, which are relevant for formal and logical reasoning in software development \cite{gulwani2022aiassistedprogramming}. Zimmerman et al. \cite{zimmerman2022assessingphysics} focused on assessing physics quantitative literacy, relevant for understanding how mathematical reasoning is applied in specific scientific contexts \cite{zimmerman2022assessingphysics}. Gokhale et al. \cite{gokhale2022benchmarkingspatial} benchmarked spatial relationships in text-to-image generation, highlighting the challenges in visual-linguistic reasoning \cite{gokhale2022benchmarkingspatial}. Wang et al. \cite{wang2022chiqalarge} introduced ChiQA, a dataset for image-based real-world question answering, emphasizing multi-modal understanding and reasoning \cite{wang2022chiqalarge}. Wilson et al. \cite{wilson2022classificationopenended} utilized NLP for classifying open-ended responses in physics education research, demonstrating the application of these techniques in educational assessment \cite{wilson2022classificationopenended}. Liang et al. \cite{liang2022codepolicies} presented \"Code as Policies\", using LLMs to generate robot policy code for embodied control, requiring spatial-geometric reasoning \cite{liang2022codepolicies}. Sahu et al. \cite{sahu2022codequeriesdataset} introduced CodeQueries, a dataset for semantic queries over code, highlighting the challenge of understanding code semantics for answering queries requiring multi-hop reasoning \cite{sahu2022codequeriesdataset}. Zhao et al. \cite{zhao2022collaborativereasoning} proposed collaborative reasoning for video-grounded dialogue generation \cite{zhao2022collaborativereasoning}. Gouhar et al. \cite{gouhar2022combininglocal} combined local and global approaches for semantic similarity \cite{gouhar2022combininglocal}. Albalak et al. \cite{albalak2022commonsensereasoning} surveyed commonsense reasoning datasets. Ye et al. \cite{ye2022complementaryexplanations} studied complementary explanations for in-context learning \cite{ye2022complementaryexplanations}. Fu et al. \cite{fu2022complexitybasedprompting} proposed complexity-based prompting for multi-step reasoning \cite{fu2022complexitybasedprompting}. Li et al. \cite{li2022composingensembles} proposed composing ensembles of pre-trained models \cite{li2022composingensembles}. Kuculo \cite{kuculo2022comprehensiveevent} discussed comprehensive event representations \cite{kuculo2022comprehensiveevent}. Evtikhov and Evtikhov \cite{evtikhov2022computationalexperiment} discussed computational experiments \cite{evtikhov2022computationalexperiment}. Smith et al. \cite{smith2022constructvldatafree} proposed ConStruct-VL for data-free continual VL concepts learning \cite{smith2022constructvldatafree}. Wagemaker et al. \cite{wagemaker2022concurrentnetkat} introduced Concurrent NetKAT for concurrent network modeling \cite{wagemaker2022concurrentnetkat}. Hurst et al. \cite{hurst2022connectingsymbolic} explored connecting symbolic fractions to proportions using iterative partitioning \cite{hurst2022connectingsymbolic}.

\subsection{Emerging Challenges and Future Directions}

Despite the considerable progress, several challenges persist across various reasoning domains. A primary concern is the robustness of LLMs to adversarial examples or minor perturbations in the input text \cite{stolfo2022causalframework, nam2022achievingunderstanding, srivastava2022beyondimitation, schlegel2022transformersreason}. Models can sometimes rely on superficial patterns rather than genuine mathematical or logical understanding \cite{stolfo2022causalframework, schlegel2022transformersreason}. The interpretability of LLM reasoning is another major area requiring attention; without clear explanations for how solutions are derived, trust and debugging become exceedingly difficult \cite{alemany2022methodologycharacterize, zhang2022multilayerattention, kim2022novelmodular, leemann2022oherencevaluation, chen2022btpkbasedlearning}. This echoes similar concerns in other complex AI applications \cite{cohen2022thisunicorn, zhang2022multilayerattention, chen2022btpkbasedlearning}. Hallucinations, where models generate plausible but incorrect mathematical statements or logical inferences, are another critical issue \cite{lu2022surveydeep, zhang2022multilayerattention}. Scalability and computational cost are also factors, especially for very large models and complex reasoning tasks \cite{abramson2022applicationpseudologlikelihoods, jeon2022informationtheoreticanalysis, srivastava2022beyondimitation, krell2022crosscontaminationaccelerating}. Furthermore, the ability of LLMs to perform novel reasoning or discover new mathematical theorems is still limited compared to human experts \cite{lu2022surveydeep}. While LLMs can process and learn from vast amounts of text and formal specifications \cite{hu2022surveyknowledge, khan2022executableformal}, their capacity for genuine mathematical insight and creativity is an ongoing debate. Ethical implications are increasingly being considered. The potential for bias in LLMs, which can manifest in the mathematical or logical content they generate, is a significant concern \cite{alemany2022methodologycharacterize, li2022scenariobasedexploration}. Li et al. \cite{li2022scenariobasedexploration} explored privacy concerns and adoption likelihood of learning analytics, highlighting the need to consider stakeholder expectations and potential risks in data-driven applications that often involve reasoning over data \cite{li2022scenariobasedexploration}. Tewes \cite{tewes2022artificialintelligence} discussed AI in healthcare, touching upon the ethical and practical considerations of deploying AI in sensitive domains, which extends to reasoning applications \cite{tewes2022artificialintelligence}. New challenges emerge with specialized applications. Mareš et al. \cite{mare2022updatethermal} discussed updating thermal error compensation models, emphasizing the need for adaptive models in real-world engineering applications, which require robust and accurate reasoning about physical phenomena \cite{mare2022updatethermal}. Höppner et al. \cite{hppner2022advantagesdisadvantages} discussed advantages and disadvantages of model transformation languages, implying that the choice of formalisms and languages impacts the feasibility and effectiveness of implementing complex reasoning systems \cite{hppner2022advantagesdisadvantages}. Khuralay et al. \cite{khuralay2022computersimulation} simulated intelligent control systems for cruise missiles, demonstrating complex system modeling \cite{khuralay2022computersimulation}. Smith et al. \cite{smith2022constructvldatafree} proposed ConStruct-VL for data-free continual structured VL concepts learning \cite{smith2022constructvldatafree}. Wagemaker et al. \cite{wagemaker2022concurrentnetkat} introduced Concurrent NetKAT for concurrent network modeling \cite{wagemaker2022concurrentnetkat}. Hurst et al. \cite{hurst2022connectingsymbolic} explored connecting symbolic fractions to proportions using iterative partitioning \cite{hurst2022connectingsymbolic}. Albilali et al. \cite{albilali2022constructingarabic} worked on constructing Arabic reading comprehension datasets \cite{albilali2022constructingarabic}. Rozora and Melnyk \cite{rozora2022constructiongoodnessoffit} focused on constructing goodness-of-fit criteria for impulse response functions \cite{rozora2022constructiongoodnessoffit}. Zamorski et al. \cite{zamorski2022continuallearning} investigated continual learning on 3D point clouds \cite{zamorski2022continuallearning}. Pan et al. \cite{pan2022contrastivelanguageimage} proposed contrastive language-image pre-training with knowledge graphs \cite{pan2022contrastivelanguageimage}. Chen et al. \cite{chen2022convfinqaexploring} explored numerical reasoning in conversational finance QA \cite{chen2022convfinqaexploring}. Ehberger \cite{ehberger2022correctionlanguage} discussed corrections in Dirac's theory of radiation \cite{ehberger2022correctionlanguage}. Chen et al. \cite{chen2022counterfactualdecoding} proposed counterfactual decoding for knowledge-grounded dialogue generation \cite{chen2022counterfactualdecoding}. Li et al. \cite{li2022counterfactualreasoning} questioned the need for world knowledge for causal understanding in counterfactual reasoning \cite{li2022counterfactualreasoning}. Ignacio and Silveira Isoba \cite{ignacio2022courseguides} presented course guides related to curve and surface design \cite{ignacio2022courseguides}. Jonsson et al. \cite{jonsson2022creativemathematical} investigated creative mathematical reasoning and its relation to need for cognition \cite{jonsson2022creativemathematical}. Kumar et al. \cite{kumar2022criticalanalysis} analyzed big data applications using functional linguistics \cite{kumar2022criticalanalysis}. Wolf et al. \cite{wolf2022crosslingualspeaker} worked on cross-lingual speaker identification \cite{wolf2022crosslingualspeaker}. Yu et al. \cite{yu2022crunchqasynthetic} introduced CrunchQA, a dataset for QA over knowledge graphs \cite{yu2022crunchqasynthetic}. Chen and Gao \cite{chen2022curriculumbroadcoverage} introduced Curriculum, a benchmark for linguistic phenomena \cite{chen2022curriculumbroadcoverage}. Cho et al. \cite{cho2022dallevalprobing} evaluated reasoning skills and social biases of text-to-image transformers \cite{cho2022dallevalprobing}. Nuraina et al. \cite{nuraina2022desainbahan} designed teaching materials for mathematical reasoning \cite{nuraina2022desainbahan}. Liu et al. \cite{liu2022deplotoneshot} presented DePlot for visual language reasoning \cite{liu2022deplotoneshot}. Tian et al. \cite{tian2022debiasingmodels} proposed debiasing NLU models via causal intervention \cite{tian2022debiasingmodels}. Khot et al. \cite{khot2022decomposedprompting} introduced decomposed prompting for complex tasks \cite{khot2022decomposedprompting}. Rasal et al. \cite{rasal2022deepstructural} developed Deep Structural Causal Shape Models \cite{rasal2022deepstructural}. Hodge et al. \cite{hodge2022designplanning} discussed transdisciplinary investigation planning \cite{hodge2022designplanning}. Li et al. \cite{li2022designsimulation} proposed fuzzy controllers based on granular computing \cite{li2022designsimulation}. Tsukanov \cite{tsukanov2022designcircular} discussed design of air intakes \cite{tsukanov2022designcircular}. Heru et al. \cite{heru2022designsupplementary} designed supplementary math modules \cite{heru2022designsupplementary}. Zoph et al. \cite{zoph2022designingeffective} designed sparse expert models \cite{zoph2022designingeffective}. Albrecht et al. \cite{albrecht2022despitesuperhuman} critiqued LLMs for ethical decisions \cite{albrecht2022despitesuperhuman}. Khokhlova et al. \cite{khokhlova2022developmentalgorithm} developed an algorithm for labor market analysis \cite{khokhlova2022developmentalgorithm}. Tekin \cite{tekin2022developmentattitude} studied an attitude scale for moral literacy \cite{tekin2022developmentattitude}. Ziborov and Zheldak \cite{ziborov2022developmentselflearning} proposed a self-learning decision support system \cite{ziborov2022developmentselflearning}. Li and Minervini \cite{li2022differentiablereasoning} assessed systematic generalization in neural models \cite{li2022differentiablereasoning}. The implication of these findings for various domains is substantial. In education, LLMs could serve as personalized tutors, providing step-by-step explanations and tailored feedback \cite{ricci2022petrinetbasedapproach, 2022algorithmmethod, shidqiya2022analysisstudents, yu2022analysiscorrelation}. In scientific research, they could assist in hypothesis generation, experimental design, and even the discovery of new mathematical principles \cite{lu2022surveydeep}. The work by Kim et al. \cite{kim2022novelmodular} on modular modeling suggests that understanding complex systems through component-based reasoning is a promising avenue for developing more interpretable AI \cite{kim2022novelmodular}. Poythress \cite{poythress2022semioticanalysis} emphasizes the richness of human reasoning beyond single formal systems, indicating that LLMs need to capture this complexity \cite{poythress2022semioticanalysis}. The work by Xiao et al. \cite{xiao2022auxiliaryteaching} further supports the use of AI in enhancing mathematical education \cite{xiao2022auxiliaryteaching}. However, the risks associated with deploying these models in sensitive areas, such as automated grading or critical problem-solving, necessitate careful consideration of their limitations and potential biases \cite{alemany2022methodologycharacterize, li2022scenariobasedexploration}. The comparison of approaches for imbalanced classification by Wankmller \cite{wankmller2022comparisonapproaches} and the comparative study of covariate effects by Wang \cite{wang2022comparisonthree} highlight the importance of rigorous evaluation and understanding limitations in complex data and modeling scenarios \cite{wankmller2022comparisonapproaches, wang2022comparisonthree}. The development of executable formal models, as shown by Khan et al. \cite{khan2022executableformal}, provides a path for ensuring correctness in formal reasoning \cite{khan2022executableformal}. Katra et al. \cite{katra2022experimentationframework} also contribute to this by offering a framework for specification and verification \cite{katra2022experimentationframework}. The work by Wang et al. \cite{wang2022hybridgenetic} on hybrid genetic algorithms for complex scheduling problems, and Gulwani \cite{gulwani2022aiassistedprogramming} on AI-assisted programming, illustrate the ongoing need for robust and comparative methodological approaches that can be adapted to the nuances of formal and applied reasoning \cite{wang2022hybridgenetic, gulwani2022aiassistedprogramming}. Gao et al. \cite{gao2022attributedtext} and Wu et al. \cite{wu2022autoformalizationneural, wu2022autoformalizationwith} also point to advanced techniques in text generation and formalization that could inform future reasoning systems \cite{gao2022attributedtext, wu2022autoformalizationneural, wu2022autoformalizationwith}. Raman et al. \cite{raman2022capecorrective} demonstrated how LLMs can improve robotic planning by recovering from errors \cite{raman2022capecorrective}. An et al. \cite{an2022bevbertmultimodal} explored multimodal reasoning for navigation \cite{an2022bevbertmultimodal}. Jung et al. \cite{jung2022blankcollapse} provided methods for optimizing sequence decoding, relevant for models that process sequential reasoning steps \cite{jung2022blankcollapse}. Si et al. \cite{si2022benchmarkinggpt3}, Gopinath et al. \cite{gopinath2022benchmarkinglargescale}, Gokhale et al. \cite{gokhale2022benchmarkingspatial}, Wang et al. \cite{wang2022chiqalarge}, and Wilson et al. \cite{wilson2022classificationopenended} highlight the importance of benchmarking diverse reasoning capabilities \cite{si2022benchmarkinggpt3, gopinath2022benchmarkinglargescale, gokhale2022benchmarkingspatial, wang2022chiqalarge, wilson2022classificationopenended}. Dong et al. \cite{dong2022corrpusdetecting, dong2022corrpuscodebased} and Kim et al. \cite{kim2022cosimcommonsense} introduce new datasets and methods for story and scene understanding, respectively \cite{dong2022corrpusdetecting, dong2022corrpuscodebased, kim2022cosimcommonsense}. Lin et al. \cite{lin2022curriculumlearning} proposed curriculum learning for prompt tuning \cite{lin2022curriculumlearning}. Tefnik and Kadlcík \cite{tefnik2022incontextlearners} examined in-context learning \cite{tefnik2022incontextlearners}. BehnamGhader et al. \cite{behnamghader2022retrieveraugmentedlanguage} and Schlegel et al. \cite{schlegel2022transformersreason} investigated reasoning in augmented models and transformer fragments \cite{behnamghader2022retrieveraugmentedlanguage, schlegel2022transformersreason}. Carette et al. \cite{carette2022centralsubmonads} explored theoretical aspects of computation \cite{carette2022centralsubmonads}. Wei et al. \cite{wei2022chainthought} demonstrated chain-of-thought prompting \cite{wei2022chainthought}. Unknown \cite{unknown2022chartingspace} charted quantum field theories \cite{unknown2022chartingspace}. Hua et al. \cite{hua2022bayesvarbrulunified} presented a framework for language evolution analysis \cite{hua2022bayesvarbrulunified}.

\subsection{Mathematical and Logical Reasoning Tasks and Performance}

LLMs are being applied to a wide spectrum of mathematical and logical reasoning tasks. Arithmetic reasoning, including solving word problems, has seen significant progress. Models like GPT-3 have demonstrated impressive performance on benchmarks like GSM8K, often by generating step-by-step reasoning processes \cite{stolfo2022causalframework, lu2022surveydeep, zhang2022automaticchain, shridhar2022automaticgeneration, wei2022chainthought}. Algebraic reasoning and equation solving also form a substantial part of the literature \cite{lu2022surveydeep, alghamdi2022armathdataset, lindstrm2022clevrmathdataset, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}. The ability of LLMs to handle symbolic manipulation and abstract concepts in mathematics is a key focus \cite{lu2022surveydeep, khan2022executableformal, amaliyah2022analisiskesulitan}. Xiao et al. \cite{xiao2022auxiliaryteaching} developed an auxiliary teaching system for higher mathematics, indicating the role of AI in aiding mathematical education through structured approaches \cite{xiao2022auxiliaryteaching}. Logical reasoning and theorem proving represent more challenging frontiers. While LLMs are showing promise in generating logical deductions and even assisting in formal theorem proving, this area still requires substantial development \cite{lu2022surveydeep, wu2022autoformalizationneural, wu2022autoformalizationwith}. Poythress \cite{poythress2022semioticanalysis} analyzed multiple systems of logic using semiotic theory, suggesting that human reasoning is richer than any single formal system and highlighting the potential for models to encompass diverse logical frameworks \cite{poythress2022semioticanalysis}. Khan et al. \cite{khan2022executableformal} developed an executable formal model of VHDL, demonstrating the application of formal methods to hardware description languages, which is a form of precise logical reasoning \cite{khan2022executableformal}. Katra et al. \cite{katra2022experimentationframework} also contribute to this by offering a framework for specification and verification of web services, leveraging formalisms akin to logical reasoning \cite{katra2022experimentationframework}. Unknown \cite{unknown2022computerverifiedfoundations} presented computer-verified foundations of metaphysics and an ontology of natural numbers \cite{unknown2022computerverifiedfoundations}. Wagemaker et al. \cite{wagemaker2022concurrentnetkat} introduced Concurrent NetKAT for modeling and analyzing stateful, concurrent networks, relevant for logical reasoning in distributed systems \cite{wagemaker2022concurrentnetkat}. Commonsense reasoning, crucial for many real-world problems, is also being addressed by LLMs \cite{zhang2022multilayerattention, zhang2022empiricalinvestigation, yu2022alertadapt, kumar2022answerlevelcalibration, wan2022bridgingbetween, kim2022cosimcommonsense, albalak2022commonsensereasoning}. Yu et al. \cite{yu2022alertadapt} proposed ALERT to adapt language models to reasoning tasks, finding that finetuning enhances various reasoning skills \cite{yu2022alertadapt}. Cohen et al. \cite{cohen2022thisunicorn} explored personalizing frozen vision-language representations, indicating the potential for LLMs to reason about user-specific visual concepts \cite{cohen2022thisunicorn}. Kumar et al. \cite{kumar2022answerlevelcalibration} focused on answer-level calibration for free-form multiple-choice question answering, highlighting its importance for robust commonsense reasoning evaluations \cite{kumar2022answerlevelcalibration}. Wan et al. \cite{wan2022bridgingbetween} investigated bridging the gap between recognition-level pre-training and commonsensical vision-language tasks, showing the need for specific pre-training strategies for commonsense reasoning \cite{wan2022bridgingbetween}. Kim et al. \cite{kim2022cosimcommonsense} introduced CoSIm, a dataset for counterfactual scene imagination, highlighting challenges in multimodal commonsense reasoning \cite{kim2022cosimcommonsense}. Albalak et al. \cite{albalak2022commonsensereasoning} provided a survey on commonsense reasoning for conversational AI datasets and benchmarks. Ye et al. \cite{ye2022complementaryexplanations} found that complementary explanations improve in-context learning, highlighting the role of diverse reasoning skills \cite{ye2022complementaryexplanations}. Fu et al. \cite{fu2022complexitybasedprompting} proposed complexity-based prompting for multi-step reasoning, showing that prompts with more steps yield better performance \cite{fu2022complexitybasedprompting}. Li et al. \cite{li2022composingensembles} proposed composing ensembles of pre-trained models via iterative consensus for multimodal tasks \cite{li2022composingensembles}. Kuculo \cite{kuculo2022comprehensiveevent} discussed comprehensive event representations \cite{kuculo2022comprehensiveevent}. Evtikhov and Evtikhov \cite{evtikhov2022computationalexperiment} discussed computational experiments \cite{evtikhov2022computationalexperiment}. The performance of LLMs in reasoning is often evaluated using metrics such as accuracy, F1 score, and specific reasoning-focused metrics that assess the coherence and correctness of the generated solution steps \cite{stolfo2022causalframework, yu2022alertadapt, shidqiya2022analysisstudents, gokhale2022benchmarkingspatial, wang2022chiqalarge, wilson2022classificationopenended, dong2022corrpuscodebased}. Markta and Smetáčková \cite{markta2022accuracypupils} investigated the accuracy of pupils' self-assessment in mathematics and language, revealing challenges in self-evaluation that might inform how LLM performance is interpreted and how feedback mechanisms could be designed \cite{markta2022accuracypupils}. Shidqiya and Sukestiyarno \cite{shidqiya2022analysisstudents} analyzed students' mathematical thinking ability in terms of self-efficacy, providing insights into how individual factors can correlate with reasoning proficiency \cite{shidqiya2022analysisstudents}. Specific domains also show LLM applications. Sánchez et al. \cite{snchez2022clusteringapproach} proposed a clustering strategy for the electric vehicle routing problem, showcasing optimization techniques that can be informed by mathematical models and potentially enhanced by LLM-driven reasoning \cite{snchez2022clusteringapproach}. Wang et al. \cite{wang2022hybridgenetic} developed a hybrid genetic algorithm for the flexible job shop scheduling problem, demonstrating complex optimization approaches that could be integrated with or improved by LLM reasoning capabilities \cite{wang2022hybridgenetic}. Gulwani \cite{gulwani2022aiassistedprogramming} discussed AI-assisted programming, including neuro-symbolic techniques, which are relevant for formal and logical reasoning in software development \cite{gulwani2022aiassistedprogramming}. Zimmerman et al. \cite{zimmerman2022assessingphysics} focused on assessing physics quantitative literacy, relevant for understanding how mathematical reasoning is applied in specific scientific contexts \cite{zimmerman2022assessingphysics}. Gokhale et al. \cite{gokhale2022benchmarkingspatial} benchmarked spatial relationships in text-to-image generation, highlighting the challenges in visual-linguistic reasoning \cite{gokhale2022benchmarkingspatial}. Wang et al. \cite{wang2022chiqalarge} introduced ChiQA, a dataset for image-based real-world question answering, emphasizing multi-modal understanding and reasoning \cite{wang2022chiqalarge}. Wilson et al. \cite{wilson2022classificationopenended} utilized NLP for classifying open-ended responses in physics education research, demonstrating the application of these techniques in educational assessment \cite{wilson2022classificationopenended}. Liang et al. \cite{liang2022codepolicies} presented \"Code as Policies\", using LLMs to generate robot policy code for embodied control, requiring spatial-geometric reasoning \cite{liang2022codepolicies}. Sahu et al. \cite{sahu2022codequeriesdataset} introduced CodeQueries, a dataset for semantic queries over code, highlighting the challenge of understanding code semantics for answering queries requiring multi-hop reasoning \cite{sahu2022codequeriesdataset}. Zhao et al. \cite{zhao2022collaborativereasoning} proposed collaborative reasoning for video-grounded dialogue generation \cite{zhao2022collaborativereasoning}. Gouhar et al. \cite{gouhar2022combininglocal} combined local and global approaches for semantic similarity \cite{gouhar2022combininglocal}. Albalak et al. \cite{albalak2022commonsensereasoning} surveyed commonsense reasoning datasets. Ye et al. \cite{ye2022complementaryexplanations} studied complementary explanations for in-context learning \cite{ye2022complementaryexplanations}. Fu et al. \cite{fu2022complexitybasedprompting} proposed complexity-based prompting for multi-step reasoning \cite{fu2022complexitybasedprompting}. Li et al. \cite{li2022composingensembles} proposed composing ensembles of pre-trained models \cite{li2022composingensembles}. Kuculo \cite{kuculo2022comprehensiveevent} discussed comprehensive event representations \cite{kuculo2022comprehensiveevent}. Evtikhov and Evtikhov \cite{evtikhov2022computationalexperiment} discussed computational experiments \cite{evtikhov2022computationalexperiment}. Smith et al. \cite{smith2022constructvldatafree} proposed ConStruct-VL for data-free continual VL concepts learning \cite{smith2022constructvldatafree}. Wagemaker et al. \cite{wagemaker2022concurrentnetkat} introduced Concurrent NetKAT for concurrent network modeling \cite{wagemaker2022concurrentnetkat}. Hurst et al. \cite{hurst2022connectingsymbolic} explored connecting symbolic fractions to proportions using iterative partitioning \cite{hurst2022connectingsymbolic}.

\subsection{Emerging Challenges and Future Directions}

Despite the considerable progress, several challenges persist across various reasoning domains. A primary concern is the robustness of LLMs to adversarial examples or minor perturbations in the input text \cite{stolfo2022causalframework, nam2022achievingunderstanding, srivastava2022beyondimitation, schlegel2022transformersreason}. Models can sometimes rely on superficial patterns rather than genuine mathematical or logical understanding \cite{stolfo2022causalframework, schlegel2022transformersreason}. The interpretability of LLM reasoning is another major area requiring attention; without clear explanations for how solutions are derived, trust and debugging become exceedingly difficult \cite{alemany2022methodologycharacterize, zhang2022multilayerattention, kim2022novelmodular, leemann2022oherencevaluation, chen2022btpkbasedlearning}. This echoes similar concerns in other complex AI applications \cite{cohen2022thisunicorn, zhang2022multilayerattention, chen2022btpkbasedlearning}. Hallucinations, where models generate plausible but incorrect mathematical statements or logical inferences, are another critical issue \cite{lu2022surveydeep, zhang2022multilayerattention}. Scalability and computational cost are also factors, especially for very large models and complex reasoning tasks \cite{abramson2022applicationpseudologlikelihoods, jeon2022informationtheoreticanalysis, srivastava2022beyondimitation, krell2022crosscontaminationaccelerating}. Furthermore, the ability of LLMs to perform novel reasoning or discover new mathematical theorems is still limited compared to human experts \cite{lu2022surveydeep}. While LLMs can process and learn from vast amounts of text and formal specifications \cite{hu2022surveyknowledge, khan2022executableformal}, their capacity for genuine mathematical insight and creativity is an ongoing debate. Ethical implications are increasingly being considered. The potential for bias in LLMs, which can manifest in the mathematical or logical content they generate, is a significant concern \cite{alemany2022methodologycharacterize, li2022scenariobasedexploration}. Li et al. \cite{li2022scenariobasedexploration} explored privacy concerns and adoption likelihood of learning analytics, highlighting the need to consider stakeholder expectations and potential risks in data-driven applications that often involve reasoning over data \cite{li2022scenariobasedexploration}. Tewes \cite{tewes2022artificialintelligence} discussed AI in healthcare, touching upon the ethical and practical considerations of deploying AI in sensitive domains, which extends to reasoning applications \cite{tewes2022artificialintelligence}. New challenges emerge with specialized applications. Mareš et al. \cite{mare2022updatethermal} discussed updating thermal error compensation models, emphasizing the need for adaptive models in real-world engineering applications, which require robust and accurate reasoning about physical phenomena \cite{mare2022updatethermal}. Höppner et al. \cite{hppner2022advantagesdisadvantages} discussed advantages and disadvantages of model transformation languages, implying that the choice of formalisms and languages impacts the feasibility and effectiveness of implementing complex reasoning systems \cite{hppner2022advantagesdisadvantages}. Khuralay et al. \cite{khuralay2022computersimulation} simulated intelligent control systems for cruise missiles, demonstrating complex system modeling \cite{khuralay2022computersimulation}. Smith et al. \cite{smith2022constructvldatafree} proposed ConStruct-VL for data-free continual VL concepts learning \cite{smith2022constructvldatafree}. Wagemaker et al. \cite{wagemaker2022concurrentnetkat} introduced Concurrent NetKAT for concurrent network modeling \cite{wagemaker2022concurrentnetkat}. Hurst et al. \cite{hurst2022connectingsymbolic} explored connecting symbolic fractions to proportions using iterative partitioning \cite{hurst2022connectingsymbolic}. Albilali et al. \cite{albilali2022constructingarabic} worked on constructing Arabic reading comprehension datasets \cite{albilali2022constructingarabic}. Rozora and Melnyk \cite{rozora2022constructiongoodnessoffit} focused on constructing goodness-of-fit criteria for impulse response functions \cite{rozora2022constructiongoodnessoffit}. Zamorski et al. \cite{zamorski2022continuallearning} investigated continual learning on 3D point clouds \cite{zamorski2022continuallearning}. Pan et al. \cite{pan2022contrastivelanguageimage} proposed contrastive language-image pre-training with knowledge graphs \cite{pan2022contrastivelanguageimage}. Chen et al. \cite{chen2022convfinqaexploring} explored numerical reasoning in conversational finance QA \cite{chen2022convfinqaexploring}. Ehberger \cite{ehberger2022correctionlanguage} discussed corrections in Dirac's theory of radiation \cite{ehberger2022correctionlanguage}. Chen et al. \cite{chen2022counterfactualdecoding} proposed counterfactual decoding for knowledge-grounded dialogue generation \cite{chen2022counterfactualdecoding}. Li et al. \cite{li2022counterfactualreasoning} questioned the need for world knowledge for causal understanding in counterfactual reasoning \cite{li2022counterfactualreasoning}. Ignacio and Silveira Isoba \cite{ignacio2022courseguides} presented course guides related to curve and surface design \cite{ignacio2022courseguides}. Jonsson et al. \cite{jonsson2022creativemathematical} investigated creative mathematical reasoning and its relation to need for cognition \cite{jonsson2022creativemathematical}. Kumar et al. \cite{kumar2022criticalanalysis} analyzed big data applications using functional linguistics \cite{kumar2022criticalanalysis}. Wolf et al. \cite{wolf2022crosslingualspeaker} worked on cross-lingual speaker identification \cite{wolf2022crosslingualspeaker}. Yu et al. \cite{yu2022crunchqasynthetic} introduced CrunchQA, a dataset for QA over knowledge graphs \cite{yu2022crunchqasynthetic}. Chen and Gao \cite{chen2022curriculumbroadcoverage} introduced Curriculum, a benchmark for linguistic phenomena \cite{chen2022curriculumbroadcoverage}. Cho et al. \cite{cho2022dallevalprobing} evaluated reasoning skills and social biases of text-to-image transformers \cite{cho2022dallevalprobing}. Nuraina et al. \cite{nuraina2022desainbahan} and Heru et al. \cite{heru2022designsupplementary} explore the design of teaching materials based on mathematical reasoning activities for students, highlighting the role of structured pedagogical approaches \cite{nuraina2022desainbahan, heru2022designsupplementary}. Liu et al. \cite{liu2022deplotoneshot} presented DePlot for one-shot visual language reasoning by plot-to-table translation, showcasing a novel approach to multimodal reasoning \cite{liu2022deplotoneshot}. Tian et al. \cite{tian2022debiasingmodels} proposed debiasing NLU models via causal intervention and counterfactual reasoning \cite{tian2022debiasingmodels}. Khot et al. \cite{khot2022decomposedprompting} introduced decomposed prompting for complex tasks by breaking them down into sub-tasks \cite{khot2022decomposedprompting}. Rasal et al. \cite{rasal2022deepstructural} developed Deep Structural Causal Shape Models for causal reasoning about anatomical variations \cite{rasal2022deepstructural}. Hodge et al. \cite{hodge2022designplanning} discussed the design and planning of transdisciplinary investigations, emphasizing the importance of structured approaches in complex research projects \cite{hodge2022designplanning}. Li et al. \cite{li2022designsimulation} proposed a fuzzy controller based on granular computing for system control \cite{li2022designsimulation}. Tsukanov \cite{tsukanov2022designcircular} discussed the design of circular air intakes, involving mathematical modeling for engineering applications \cite{tsukanov2022designcircular}. Zoph et al. \cite{zoph2022designingeffective} focused on designing effective sparse expert models, relevant for scaling LLMs \cite{zoph2022designingeffective}. Albrecht et al. \cite{albrecht2022despitesuperhuman} critiqued LLM suitability for ethical decisions despite \"super-human\" performance \cite{albrecht2022despitesuperhuman}. Khokhlova et al. \cite{khokhlova2022developmentalgorithm} developed an algorithm for labor market analysis using data mining techniques \cite{khokhlova2022developmentalgorithm}. Tekin \cite{tekin2022developmentattitude} studied an attitude scale for moral literacy \cite{tekin2022developmentattitude}. Ziborov and Zheldak \cite{ziborov2022developmentselflearning} proposed a self-learning decision support system \cite{ziborov2022developmentselflearning}. Li and Minervini \cite{li2022differentiablereasoning} assessed systematic generalization in neural models for long stories \cite{li2022differentiablereasoning}. The implications of these findings for various domains are substantial. In education, LLMs could serve as personalized tutors, providing step-by-step explanations and tailored feedback \cite{ricci2022petrinetbasedapproach, 2022algorithmmethod, shidqiya2022analysisstudents, yu2022analysiscorrelation}. In scientific research, they could assist in hypothesis generation, experimental design, and even the discovery of new mathematical principles \cite{lu2022surveydeep}. The work by Kim et al. \cite{kim2022novelmodular} on modular modeling suggests that understanding complex systems through component-based reasoning is a promising avenue for developing more interpretable AI \cite{kim2022novelmodular}. Poythress \cite{poythress2022semioticanalysis} emphasizes the richness of human reasoning beyond single formal systems, indicating that LLMs need to capture this complexity \cite{poythress2022semioticanalysis}. The work by Xiao et al. \cite{xiao2022auxiliaryteaching} further supports the use of AI in enhancing mathematical education \cite{xiao2022auxiliaryteaching}. However, the risks associated with deploying these models in sensitive areas, such as automated grading or critical problem-solving, necessitate careful consideration of their limitations and potential biases \cite{alemany2022methodologycharacterize, li2022scenariobasedexploration}. The comparison of approaches for imbalanced classification by Wankmller \cite{wankmller2022comparisonapproaches} and the comparative study of covariate effects by Wang \cite{wang2022comparisonthree} highlight the importance of rigorous evaluation and understanding limitations in complex data and modeling scenarios \cite{wankmller2022comparisonapproaches, wang2022comparisonthree}. The development of executable formal models, as shown by Khan et al. \cite{khan2022executableformal}, provides a path for ensuring correctness in formal reasoning \cite{khan2022executableformal}. Katra et al. \cite{katra2022experimentationframework} also contribute to this by offering a framework for specification and verification \cite{katra2022experimentationframework}.

=== DISCUSSION ===
\section{DISCUSSION}

The systematic review of literature on large language models (LLMs) applied to mathematical and logical reasoning reveals a field characterized by rapid advancements and significant potential, yet also by persistent challenges. The identified research, encompassing 570 studies, underscores the intense interest in utilizing LLMs for tasks that were previously considered intractable for machines \cite{lu2022surveydeep, stolfo2022causalframework, nam2022achievingunderstanding, zhang2022empiricalinvestigation, kumar2022answerlevelcalibration, srivastava2022beyondimitation, wei2022chainthought, lindstrm2022clevrmathdataset, dong2022corrpuscodebased, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}.

Our findings indicate a clear trend towards fine-tuning pre-trained models and employing advanced prompting strategies like chain-of-thought and causal analysis to elicit reasoning capabilities \cite{yu2022alertadapt, stolfo2022causalframework, abramson2022applicationpseudologlikelihoods, zhang2022automaticchain, shridhar2022automaticgeneration, wei2022chainthought, tefnik2022incontextlearners}. The success in areas like arithmetic and algebraic problem-solving is notable, demonstrating that LLMs can internalize and apply mathematical rules to a certain extent \cite{stolfo2022causalframework, lu2022surveydeep, amaliyah2022analisiskesulitan, zhang2022automaticchain, wei2022chainthought}. The development of specialized architectures, such as multi-layer attention networks \cite{zhang2022multilayerattention}, and the integration of external knowledge \cite{hu2022surveyknowledge, zhang2022empiricalinvestigation, bellomarini2022overviewvadalog} are key strategies for enhancing performance. These approaches aim to imbue LLMs with more robust reasoning mechanisms that go beyond surface-level pattern matching \cite{nam2022achievingunderstanding, abramson2022applicationpseudologlikelihoods, wu2022autoformalizationwith, behnamghader2022retrieveraugmentedlanguage}. For instance, Liu et al. \cite{liu2022deplotoneshot} demonstrated a one-shot approach for visual language reasoning by translating plots to tables, which can then be processed by LLMs, highlighting efficient multimodal reasoning strategies \cite{liu2022deplotoneshot}. Tian et al. \cite{tian2022debiasingmodels} proposed debiasing NLU models using causal intervention and counterfactual reasoning, addressing a critical challenge in model reliability \cite{tian2022debiasingmodels}. Khot et al. \cite{khot2022decomposedprompting} introduced decomposed prompting, a modular approach to tackle complex tasks by breaking them into simpler sub-tasks, which can be solved by specialized LLM prompts \cite{khot2022decomposedprompting}. Rasal et al. \cite{rasal2022deepstructural} developed Deep Structural Causal Shape Models, enabling counterfactual mesh generation for causal reasoning in medical imaging \cite{rasal2022deepstructural}. Hodge et al. \cite{hodge2022designplanning} discussed the importance of structured planning in large-scale research collaborations, a principle transferable to designing robust AI reasoning systems \cite{hodge2022designplanning}.

However, significant research gaps remain. The robustness of LLMs to variations in problem formulation or adversarial attacks is a critical concern \cite{stolfo2022causalframework, nam2022achievingunderstanding, srivastava2022beyondimitation, schlegel2022transformersreason}. Understanding the causal factors influencing LLM outputs in mathematical and logical contexts is essential for building reliable systems \cite{stolfo2022causalframework, willig2022foundationmodels}. The interpretability of LLM reasoning is another major area requiring attention; without clear explanations for how solutions are derived, trust and debugging become exceedingly difficult \cite{alemany2022methodologycharacterize, zhang2022multilayerattention, kim2022novelmodular, leemann2022oherencevaluation, chen2022btpkbasedlearning}. This echoes similar concerns in other complex AI applications \cite{cohen2022thisunicorn, zhang2022multilayerattention, chen2022btpkbasedlearning}. Hallucinations, where models generate plausible but incorrect mathematical statements or logical inferences, are another critical issue \cite{lu2022surveydeep, zhang2022multilayerattention}. Scalability and computational cost are also factors, especially for very large models and complex reasoning tasks \cite{abramson2022applicationpseudologlikelihoods, jeon2022informationtheoreticanalysis, srivastava2022beyondimitation, krell2022crosscontaminationaccelerating}. Furthermore, the ability of LLMs to perform novel reasoning or discover new mathematical theorems is still limited compared to human experts \cite{lu2022surveydeep}. While LLMs can process and learn from vast amounts of text and formal specifications \cite{hu2022surveyknowledge, khan2022executableformal}, their capacity for genuine mathematical insight and creativity is an ongoing debate. Ethical implications are increasingly being considered. The potential for bias in LLMs, which can manifest in the mathematical or logical content they generate, is a significant concern \cite{alemany2022methodologycharacterize, li2022scenariobasedexploration}. Li et al. \cite{li2022scenariobasedexploration} explored privacy concerns and adoption likelihood of learning analytics, highlighting the need to consider stakeholder expectations and potential risks in data-driven applications that often involve reasoning over data \cite{li2022scenariobasedexploration}. Tewes \cite{tewes2022artificialintelligence} discussed AI in healthcare, touching upon the ethical and practical considerations of deploying AI in sensitive domains, which extends to reasoning applications \cite{tewes2022artificialintelligence}.

The implication of these findings for various domains is substantial. In education, LLMs could serve as personalized tutors, providing step-by-step explanations and tailored feedback \cite{ricci2022petrinetbasedapproach, 2022algorithmmethod, shidqiya2022analysisstudents, yu2022analysiscorrelation}. Nuraina et al. \cite{nuraina2022desainbahan} and Heru et al. \cite{heru2022designsupplementary} illustrate this by designing educational materials focused on mathematical reasoning, demonstrating how structured learning aids can enhance student capabilities \cite{nuraina2022desainbahan, heru2022designsupplementary}. In scientific research, they could assist in hypothesis generation, experimental design, and even the discovery of new mathematical principles \cite{lu2022surveydeep}. The work by Kim et al. \cite{kim2022novelmodular} on modular modeling suggests that understanding complex systems through component-based reasoning is a promising avenue for developing more interpretable AI \cite{kim2022novelmodular}. Poythress \cite{poythress2022semioticanalysis} emphasizes the richness of human reasoning beyond single formal systems, indicating that LLMs need to capture this complexity \cite{poythress2022semioticanalysis}. The work by Xiao et al. \cite{xiao2022auxiliaryteaching} further supports the use of AI in enhancing mathematical education \cite{xiao2022auxiliaryteaching}. However, the risks associated with deploying these models in sensitive areas, such as automated grading or critical problem-solving, necessitate careful consideration of their limitations and potential biases \cite{alemany2022methodologycharacterize, li2022scenariobasedexploration}. The comparison of approaches for imbalanced classification by Wankmller \cite{wankmller2022comparisonapproaches} and the comparative study of covariate effects by Wang \cite{wang2022comparisonthree} highlight the importance of rigorous evaluation and understanding limitations in complex data and modeling scenarios \cite{wankmller2022comparisonapproaches, wang2022comparisonthree}. The development of executable formal models, as shown by Khan et al. \cite{khan2022executableformal}, provides a path for ensuring correctness in formal reasoning \cite{khan2022executableformal}. Katra et al. \cite{katra2022experimentationframework} also contribute to this by offering a framework for specification and verification \cite{katra22023experimentationframework}. The work by Wang et al. \cite{wang2022hybridgenetic} on hybrid genetic algorithms for complex scheduling problems, and Gulwani \cite{gulwani2022aiassistedprogramming} on AI-assisted programming, illustrate the ongoing need for robust and comparative methodological approaches that can be adapted to the nuances of formal and applied reasoning \cite{wang2022hybridgenetic, gulwani2022aiassistedprogramming}. Gao et al. \cite{gao2022attributedtext} and Wu et al. \cite{wu2022autoformalizationneural, wu2022autoformalizationwith} also point to advanced techniques in text generation and formalization that could inform future reasoning systems \cite{gao2022attributedtext, wu2022autoformalizationneural, wu2022autoformalizationwith}. Raman et al. \cite{raman2022capecorrective} demonstrated how LLMs can improve robotic planning by recovering from errors \cite{raman2022capecorrective}. An et al. \cite{an2022bevbertmultimodal} explored multimodal reasoning for navigation \cite{an2022bevbertmultimodal}. Jung et al. \cite{jung2022blankcollapse} provided methods for optimizing sequence decoding, relevant for models that process sequential reasoning steps \cite{jung2022blankcollapse}. Si et al. \cite{si2022benchmarkinggpt3}, Gopinath et al. \cite{gopinath2022benchmarkinglargescale}, Gokhale et al. \cite{gokhale2022benchmarkingspatial}, Wang et al. \cite{wang2022chiqalarge}, and Wilson et al. \cite{wilson2022classificationopenended} highlight the importance of benchmarking diverse reasoning capabilities \cite{si2022benchmarkinggpt3, gopinath2022benchmarkinglargescale, gokhale2022benchmarkingspatial, wang2022chiqalarge, wilson2022classificationopenended}. Dong et al. \cite{dong2022corrpusdetecting, dong2022corrpuscodebased} and Kim et al. \cite{kim2022cosimcommonsense} introduce new datasets and methods for story and scene understanding, respectively \cite{dong2022corrpusdetecting, dong2022corrpuscodebased, kim2022cosimcommonsense}. Lin et al. \cite{lin2022curriculumlearning} proposed curriculum learning for prompt tuning \cite{lin2022curriculumlearning}. Tefnik and Kadlcík \cite{tefnik2022incontextlearners} examined in-context learning \cite{tefnik2022incontextlearners}. BehnamGhader et al. \cite{behnamghader2022retrieveraugmentedlanguage} and Schlegel et al. \cite{schlegel2022transformersreason} investigated reasoning in augmented models and transformer fragments \cite{behnamghader2022retrieveraugmentedlanguage, schlegel2022transformersreason}. Carette et al. \cite{carette2022centralsubmonads} explored theoretical aspects of computation \cite{carette2022centralsubmonads}. Wei et al. \cite{wei2022chainthought} demonstrated chain-of-thought prompting \cite{wei2022chainthought}. Unknown \cite{unknown2022chartingspace} charted quantum field theories \cite{unknown2022chartingspace}. Hua et al. \cite{hua2022bayesvarbrulunified} presented a framework for language evolution analysis \cite{hua2022bayesvarbrulunified}.

Future research should focus on developing more robust and interpretable LLM architectures for reasoning. Investigating methods for enhancing causal understanding and reducing susceptibility to spurious correlations \cite{stolfo2022causalframework, willig2022foundationmodels} is crucial. Furthermore, exploring novel training paradigms that foster genuine insight, rather than just pattern recognition, is a promising direction \cite{yu2022alertadapt, jeon2022informationtheoreticanalysis, srivastava2022beyondimitation}. The development of standardized benchmarks that rigorously test the limits of LLMs' reasoning abilities, particularly in areas requiring abstract thought and creativity, is also needed \cite{lu2022surveydeep, alghamdi2022armathdataset, srivastava2022beyondimitation, wang2022chiqalarge}. Addressing ethical considerations, including bias mitigation and responsible deployment strategies, will be paramount as LLMs become more integrated into mathematical and logical workflows \cite{alemany2022methodologycharacterize, li2022scenariobasedexploration, tewes2022artificialintelligence}. This review highlights that while LLMs have made impressive strides in emulating mathematical and logical reasoning, the path towards truly intelligent and reliable reasoning agents is still ongoing.

=== CONCLUSION ===
\section{CONCLUSION}

This systematic literature review has provided a comprehensive overview of the burgeoning field of large language models (LLMs) applied to mathematical and logical reasoning. Our analysis, encompassing 570 studies, reveals a dynamic research landscape marked by significant advancements in utilizing LLMs for arithmetic, algebraic, logical, and commonsense reasoning tasks \cite{lu2022surveydeep, stolfo2022causalframework, zhang2022multilayerattention, yu2022alertadapt, amaliyah2022analisiskesulitan, zhang2022empiricalinvestigation, kumar2022answerlevelcalibration, zhang2022automaticchain, shridhar2022automaticgeneration, wei2022chainthought, lindstrm2022clevrmathdataset, kim2022cosimcommonsense, liang2022codepolicies, sahu2022codequeriesdataset, zhao2022collaborativereasoning, gouhar2022combininglocal, albalak2022commonsensereasoning, ye2022complementaryexplanations, fu2022complexitybasedprompting, li2022composingensembles, kuculo2022comprehensiveevent, evtikhov2022computationalexperiment, khuralay2022computersimulation, unknown2022computerverifiedfoundations, smith2022constructvldatafree, wagemaker2022concurrentnetkat, hurst2022connectingsymbolic, albilali2022constructingarabic, rozora2022constructiongoodnessoffit, zamorski2022continuallearning, pan2022contrastivelanguageimage, chen2022convfinqaexploring, ehberger2022correctionlanguage, chen2022counterfactualdecoding, li2022counterfactualreasoning, ignacio2022courseguides, jonsson2022creativemathematical, kumar2022criticalanalysis, wolf2022crosslingualspeaker, yu2022crunchqasynthetic, chen2022curriculumbroadcoverage, cho2022dallevalprobing, nuraina2022desainbahan, liu2022deplotoneshot, tian2022debiasingmodels, khot2022decomposedprompting, rasal2022deepstructural, hodge2022designplanning, li2022designsimulation, tsukanov2022designcircular, heru2022designsupplementary, zoph2022designingeffective, albrecht2022despitesuperhuman, khokhlova2022developmentalgorithm, tekin2022developmentattitude, ziborov2022developmentselflearning, li2022differentiablereasoning}. The dominant methodologies involve fine-tuning pre-trained models and employing advanced prompting techniques, demonstrating LLMs' growing capacity to process and generate reasoning outputs \cite{stolfo2022causalframework, abramson2022applicationpseudologlikelihoods, yu2022alertadapt, zhang2022automaticchain, shridhar2022automaticgeneration, wei2022chainthought}.

Key findings highlight the potential of LLMs to revolutionize areas such as education, by acting as personalized learning aids \cite{ricci2022petrinetbasedapproach, 2022algorithmmethod, shidqiya2022analysisstudents, yu2022analysiscorrelation}, and scientific discovery, by assisting in complex problem-solving and formal verification \cite{lu2022surveydeep, khan2022executableformal, wu2022autoformalizationneural, wu2022autoformalizationwith}. The ability of models to generate step-by-step reasoning pathways, as explored by Stolfo et al. \cite{stolfo2022causalframework}, is a crucial development in making AI-assisted reasoning more transparent. However, this review also underscores critical challenges that warrant further investigation. The robustness of these models against input perturbations \cite{stolfo2022causalframework, nam2022achievingunderstanding, srivastava2022beyondimitation, schlegel2022transformersreason}, their interpretability \cite{zhang2022multilayerattention, kim2022novelmodular, leemann2022oherencevaluation, chen2022btpkbasedlearning}, and the mitigation of biases \cite{alemany2022methodologycharacterize, li2022scenariobasedexploration} remain significant hurdles. \cite{nuraina2022desainbahan, heru2022designsupplementary} show the potential for LLMs to aid in educational material design for mathematical reasoning \cite{nuraina2022desainbahan, heru2022designsupplementary}. \cite{liu2022deplotoneshot} provides a novel approach to multimodal reasoning \cite{liu2022deplotoneshot}, while \cite{tian2022debiasingmodels} and \cite{khot2022decomposedprompting} address model reliability and task decomposition, respectively \cite{tian2022debiasingmodels, khot2022decomposedprompting}.

The contribution of this review lies in its systematic synthesis of current research, identifying convergence in methodologies while also mapping out essential areas for future exploration \cite{poythress2022semioticanalysis, zhang2022empiricalinvestigation, srivastava2022beyondimitation}. The practical implications are far-reaching, promising enhanced tools for mathematicians, logicians, educators, and researchers \cite{gulwani2022aiassistedprogramming, zimmerman2022assessingphysics, xiao2022auxiliaryteaching}. Nevertheless, the responsible development and deployment of LLMs in reasoning-intensive domains necessitate a continuous focus on their limitations and ethical considerations \cite{li2022scenariobasedexploration, tewes2022artificialintelligence}. \cite{albrecht2022despitesuperhuman} and \cite{li2022counterfactualreasoning} highlight the critical need for understanding ethical implications and causal reasoning \cite{albrecht2022despitesuperhuman, li2022counterfactualreasoning}.

In conclusion, while LLMs have demonstrated remarkable progress in emulating mathematical and logical reasoning, the path towards truly intelligent and reliable reasoning agents requires sustained research into robustness, interpretability, and novel reasoning capabilities \cite{yu2022alertadapt, jeon2022informationtheoreticanalysis, srivastava2022beyondimitation}. The insights gained from this review serve as a foundation for future work, guiding efforts towards developing more reliable, trustworthy, and impactful AI systems for reasoning \cite{bellomarini2022overviewvadalog, wu2022autoformalizationwith}.

=== END PREVIOUS DRAFT ===

UPDATED STATISTICS:
- Total papers now included: 135
- Records identified: 462
- Studies included: 462

TASK: COMPLETELY REGENERATE the paper integrating the new papers.

REGENERATION INSTRUCTIONS:
1. Read the previous draft to understand existing structure and themes
2. Review the NEW papers being added (listed in "NEW PAPERS BEING ADDED" section)
3. Integrate new papers throughout ALL sections where relevant
4. In Results section:
   - Add new subsections if new themes emerge from new papers
   - Reorganize existing subsections for better coherence
   - CITE every paper discussed using \cite{citationKey}
5. Update all statistics to reflect new paper count
6. Maintain academic quality and narrative flow
7. Ensure EVERY paper (old and new) is cited using \cite{citationKey}

CITATION REQUIREMENTS:
✓ Use \cite{citationKey} format (e.g., \cite{smith2020deep})
✓ Cite papers from BOTH previous draft AND new additions
✓ Introduction: MINIMUM 5-10 citations
✓ Results: MINIMUM 15-25 citations (more with larger paper count)
✓ Discussion: MINIMUM 10-15 citations
✓ Conclusion: MINIMUM 3-5 citations
✓ Each subsection in Results MUST cite papers relevant to that theme

REGENERATE COMPLETE PAPER:
1. ABSTRACT: Update with new paper count, refined findings (NO citations)
2. INTRODUCTION: Integrate relevant new papers, update scope, CITE extensively
3. METHODOLOGY: Update statistics (cite PRISMA guidelines if needed)
4. RESULTS: **CRITICAL** - Reorganize with new papers, cite ALL papers discussed
5. DISCUSSION: Integrate new findings, synthesize across all papers, CITE extensively
6. CONCLUSION: Update with insights from complete set, CITE key papers

CRITICAL: Return your response as VALID JSON with PROPER ESCAPING:

IMPORTANT JSON ESCAPING RULES:
- Every single backslash in LaTeX commands MUST be escaped as double backslash
- \cite{} becomes \\cite{} in JSON
- \subsection{} becomes \\subsection{} in JSON
- Example: "introduction": "Recent work \\cite{smith2020} shows..."

Return ONLY valid JSON in this EXACT format:
{
  "abstract": "...",
  "introduction": "text with \\cite{} properly escaped",
  "methodology": "...",
  "results": "text with \\subsection{} and \\cite{} properly escaped",
  "discussion": "text with \\cite{} properly escaped",
  "conclusion": "text with \\cite{} properly escaped"
}

VERIFY: Check that ALL backslashes are doubled (\\) in JSON before returning!
