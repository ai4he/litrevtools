@article{stolfo2023mechanisticinterpretation,
  title={A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis},
  author={Alessandro Stolfo and Yonatan Belinkov and Mrinmaya Sachan},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.emnlp-main.435},
  url={https://www.semanticscholar.org/paper/5dc15ac1c92ab7492f121471823fb13a95d273ba},
  abstract={Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.},
  keywords={arxiv:2305.15054}
}

@article{canfora2023novelclassification,
  title={A Novel Classification Technique based on Formal Methods},
  author={G. Canfora and F. Mercaldo and A. Santone},
  year={2023},
  journal={ACM Transactions on Knowledge Discovery from Data},
  doi={10.1145/3592796},
  url={https://www.semanticscholar.org/paper/0e6f4f74c4d83836c3cefe1d754b6bd65e2b16f9},
  abstract={In last years, we are witnessing a growing interest in the application of supervised machine learning techniques in the most disparate fields. One winning factor of machine learning is represented by its ability to easily create models, as it does not require prior knowledge about the application domain. Complementary to machine learning are formal methods, that intrinsically offer safeness check and mechanism for reasoning on failures. Considering the weaknesses of machine learning, a new challenge could be represented by the use of formal methods. However, formal methods require the expertise of the domain, knowledge about modeling language with its semantic and mathematical rigour to specify properties. In this article, we propose a novel learning technique based on the adoption of formal methods for classification thanks to the automatic generation both of the formula and of the model. In this way the proposed method does not require any human intervention and thus it can be applied also to complex/large datasets. This leads to less effort both in using formal methods and in a better explainability and reasoning about the obtained results. Through a set of case studies from different real-world domains (i.e., driver detection, scada attack identification, arrhythmia characterization, mobile malware detection, and radiomics for lung cancer analysis), we demonstrate the usefulness of the proposed method, by showing that we are able to overcome the performances obtained from widespread classification algorithms.}
}

@article{laskar2023systematicstudy,
  title={A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets},
  author={Md Tahmid Rahman Laskar and M Saiful Bari and Mizanur Rahman and Md Amran Hossen Bhuiyan and Shafiq R. Joty and J. Huang},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2305.18486},
  url={https://www.semanticscholar.org/paper/d3060876d9ad4e4e50e1c88a8c04186df00f24e2},
  abstract={The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instructions that we mostly found in ChatGPT and other instruction-tuned models. Our extensive evaluation shows that even though ChatGPT is capable of performing a wide variety of tasks, and may obtain impressive performance in several benchmark datasets, it is still far from achieving the ability to reliably solve many challenging tasks. By providing a thorough assessment of ChatGPT's performance across diverse NLP tasks, this paper sets the stage for a targeted deployment of ChatGPT-like LLMs in real-world applications.},
  keywords={arxiv:2305.18486}
}

@article{yang2023improvedbaseline,
  title={An Improved Baseline for Reasoning Segmentation with Large Language Model},
  author={Senqiao Yang and Tianyuan Qu and Xin Lai and Zhuotao Tian and Bohao Peng and Shu Liu and Jiaya Jia},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.17240},
  url={https://www.semanticscholar.org/paper/46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9},
  abstract={While LISA effectively bridges the gap between segmentation and large language models to enable reasoning segmentation, it poses certain limitations: unable to distinguish different instances of the target region, and constrained by the pre-defined textual response formats. In this work, we introduce LISA++, an update to the existing LISA model, focusing on improving core functionalities while keeping the base architecture intact. The main enhancements in LISA++ include: \textbackslash\{\}textbf\{1) Enhanced Segmentation\}: The instance segmentation ability has been added, providing a more detailed scene analysis along with the existing multi-region semantic segmentation. \textbackslash\{\}textbf\{2) More Natural Conversation\}: Improved capability for multi-turn dialogue, with the ability to incorporate segmentation results directly into text responses, i.e., Segmentation in Dialogue (SiD). These improvements are achieved by curating the existing samples of generic segmentation datasets, aimed specifically at enhancing the segmentation and conversational skills without structural change and additional data sources. Comparative analysis with the original LISA model shows significant advancements in these areas, positioning LISA++ as a notable upgrade in visual understanding and interaction. LISA++'s adaptability and improved features highlight the versatility of the mask-as-embedding paradigm proposed by LISA, and the potential as a foundational model for diverse applications.},
  keywords={arxiv:2312.17240}
}

@article{singh2023assessinggpt4v,
  title={Assessing GPT4-V on Structured Reasoning Tasks},
  author={Mukul Singh and J. Cambronero and Sumit Gulwani and Vu Le and Gust Verbruggen},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.11524},
  url={https://www.semanticscholar.org/paper/559ec2f23e7b65825b614346bdbabdfd8b56a667},
  abstract={Multi-modality promises to unlock further uses for large language models. Recently, the state-of-the-art language model GPT-4 was enhanced with vision capabilities. We carry out a prompting evaluation of GPT-4V and five other baselines on structured reasoning tasks, such as mathematical reasoning, visual data analysis, and code generation. We show that visual Chain-of-Thought, an extension of Chain-of-Thought to multi-modal LLMs, yields significant improvements over the vanilla model. We also present a categorized analysis of scenarios where these models perform well and where they struggle, highlighting challenges associated with coherent multimodal reasoning.},
  keywords={arxiv:2312.11524}
}

@misc{chen2023assessingimpact,
  title={Assessing the Impact of Prompting Methods on ChatGPT's Mathematical Capabilities},
  author={Yuhao Chen and Chloe Wong and Hanwen Yang and Juan Aguenza and Sai Bhujangari and Benthan Vu and Xun Lei and Amisha Prasad and Manny Fluss and Eric Phuong and Minghao Liu and Raja Kumar and Vanshika Vats and James Davis},
  year={2023},
  url={https://www.semanticscholar.org/paper/e02a4282c10c8e7329d4513bd4635635b9d3a50a},
  abstract={This study critically evaluates the efficacy of prompting methods in enhancing the mathematical reasoning capability of large language models (LLMs). The investigation uses three prescriptive prompting methods - simple, persona, and conversational prompting - known for their effectiveness in enhancing the linguistic tasks of LLMs. We conduct this analysis on OpenAI's LLM chatbot, ChatGPT-3.5, on extensive problem sets from the MATH, GSM8K, and MMLU datasets, encompassing a broad spectrum of mathematical challenges. A grading script adapted to each dataset is used to determine the effectiveness of these prompting interventions in enhancing the model's mathematical analysis power. Contrary to expectations, our empirical analysis reveals that none of the investigated methods consistently improves over ChatGPT-3.5's baseline performance, with some causing significant degradation. Our findings suggest that prompting strategies do not necessarily generalize to new domains, in this study failing to enhance mathematical performance.},
  keywords={arxiv:2312.15006}
}

@article{zhao2023automaticmodel,
  title={Automatic Model Selection with Large Language Models for Reasoning},
  author={Xu Zhao and Yuxi Xie and Kenji Kawaguchi and Junxian He and Qizhe Xie},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.14333},
  url={https://www.semanticscholar.org/paper/6e0def929c2ca3109f87bc0d09c4ab5dbf8e7ba3},
  abstract={Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We introduce a model selection method to combine the best of both worlds by employing a large language model (LLM) to dynamically select between them. Our theoretical analysis underscores the feasibility of this method, which is further corroborated by empirical results. Our proposed method demonstrates significant performance improvements across eight reasoning datasets with Codex, ChatGPT, and GPT-4. Additionally, our method is complementary to self-consistency; when integrated, it can further enhance performance while significantly reducing computation costs. Moreover, we achieve new state-of-the-art results on GSM8K and SVAMP, with respective accuracies of 96.8\% and 93.7\%. Our code, data and prompts are available at https://github.com/XuZhao0/Model-Selection-Reasoning},
  keywords={arxiv:2305.14333}
}

@article{holmstrm2023bridgingresource,
  title={Bridging the Resource Gap: Exploring the Efficacy of English and Multilingual LLMs for Swedish},
  author={Oskar Holmström and Jenny Kunz and Marco Kuhlmann},
  year={2023},
  booktitle={RESOURCEFUL},
  url={https://www.semanticscholar.org/paper/322c9e4ef9dc7724935ba818c0ff38d2c3d11483}
}

@article{huang2023largelanguage,
  title={Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations},
  author={Shiyuan Huang and Siddarth Mamidanna and Shreedhar Jangam and Yilun Zhou and Leilani Gilpin},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.11207},
  url={https://www.semanticscholar.org/paper/0f92d5a01baa449edc5592716dd639ec7868c44f},
  abstract={Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce"helpful"responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as"fantastic"and"memorable"in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.},
  keywords={arxiv:2310.11207}
}

@article{sonkar2023codesoliloquies,
  title={Code Soliloquies for Accurate Calculations in Large Language Models},
  author={Shashank Sonkar and Myco Le and Xinghe Chen and Naiming Liu and D. B. Mallick and Richard Baraniuk},
  year={2023},
  booktitle={International Conference on Learning Analytics and Knowledge},
  doi={10.1145/3636555.3636889},
  url={https://www.semanticscholar.org/paper/5558226fa14f7467d25690b97febdf0e8299e432},
  abstract={High-quality conversational datasets are crucial for the successful development of Intelligent Tutoring Systems (ITS) that utilize a Large Language Model (LLM) backend. Synthetic student-teacher dialogues, generated using advanced GPT-4 models, are a common strategy for creating these datasets. However, subjects like physics that entail complex calculations pose a challenge. While GPT-4 presents impressive language processing capabilities, its limitations in fundamental mathematical reasoning curtail its efficacy for such subjects. To tackle this limitation, we introduce in this paper an innovative stateful prompt design. Our design orchestrates a mock conversation where both student and tutorbot roles are simulated by GPT-4. Each student response triggers an internal monologue, or ‘code soliloquy’ in the GPT-tutorbot, which assesses whether its subsequent response would necessitate calculations. If a calculation is deemed necessary, it scripts the relevant Python code and uses the Python output to construct a response to the student. Our approach notably enhances the quality of synthetic conversation datasets, especially for subjects that are calculation-intensive. The preliminary Subject Matter Expert evaluations reveal that our Higgs model, a fine-tuned LLaMA model, effectively uses Python for computations, which significantly enhances the accuracy and computational reliability of Higgs’ responses.},
  keywords={arxiv:2309.12161}
}

@article{wu2023conic10kchallenging,
  title={Conic10K: A Challenging Math Problem Understanding and Reasoning Dataset},
  author={Haoyi Wu and Wenyang Hui and Yezeng Chen and Weiqi Wu and Kewei Tu and Yi Zhou},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2311.05113},
  url={https://www.semanticscholar.org/paper/b6667ba4f586489f12587446c6daaa3f09cfc539},
  abstract={Mathematical understanding and reasoning are crucial tasks for assessing the capabilities of artificial intelligence (AI). However, existing benchmarks either require just a few steps of reasoning, or only contain a small amount of data in one specific topic, making it hard to analyse AI's behaviour with reference to different problems within a specific topic in detail. In this work, we propose Conic10K, a challenging math problem dataset on conic sections in Chinese senior high school education. Our dataset contains various problems with different reasoning depths, while only the knowledge from conic sections is required. Since the dataset only involves a narrow range of knowledge, it is easy to separately analyse the knowledge a model possesses and the reasoning ability it has. For each problem, we provide a high-quality formal representation, the reasoning steps, and the final solution. Experiments show that existing large language models, including GPT-4, exhibit weak performance on complex reasoning. We hope that our findings could inspire more advanced techniques for precise natural language understanding and reasoning. Our dataset and codes are available at https://github.com/whyNLP/Conic10K.},
  keywords={arxiv:2311.05113}
}

@misc{meadows2023controllingequational,
  title={Controlling Equational Reasoning in Large Language Models with Prompt Interventions},
  author={Jordan Meadows and Marco Valentino and Andre Freitas},
  year={2023},
  url={https://www.semanticscholar.org/paper/4200b8253f56de8948d20fd69e7731b92c3ac3a4},
  abstract={This paper investigates how hallucination rates in Large Language Models (LLMs) may be controlled via a symbolic data generation framework, exploring a fundamental relationship between the rate of certain mathematical errors and types of input intervention. Specifically, we systematically generate data for a derivation generation task using a symbolic engine, applying targeted interventions to prompts to perturb features of mathematical derivations such as the surface forms of symbols, equational tree structures, and mathematical context. We then evaluate the effect of prompt interventions across a range of LLMs including fine-tuned T5 models, GPT, and LLaMa-based models. Our experiments suggest that T5-Large can outperform the few-shot performance of GPT-4 on various evaluation sets generated via the framework. However, an extensive evaluation based on human analysis, template-based error detection, and text generation metrics reveals model weaknesses beyond what the reference-based metrics singularly describe. We use these results to tie characteristic distributional footprints of interventions to the human evaluation of LLM derivation quality, potentially leading to significant control over fine-grained mathematical capabilities of language models with respect to specific types of errors.},
  keywords={arxiv:2307.09998}
}

@misc{jiang2023raftketch,
  title={D RAFT , S KETCH , AND P ROVE : G UIDING F ORMAL T HEOREM P ROVERS WITH I NFORMAL P ROOFS},
  author={Albert Qiaochu Jiang and S. Welleck and J. Zhou and Wenda Li and Jiacheng Liu and M. Jamnik and Timothée Lacroix and Guillaume Lample and Yuhuai Wu},
  year={2023},
  url={https://www.semanticscholar.org/paper/46b294941c397699fde0ee7e7fc441f6a755f671}
}

@article{zhou2023davirdata,
  title={DavIR: Data Selection via Implicit Reward for Large Language Models},
  author={Haotian Zhou and Tingkai Liu and Qianli Ma and Yufeng Zhang and Jianbo Yuan and Pengfei Liu and Yang You and Hongxia Yang},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2025.acl-long.452},
  url={https://www.semanticscholar.org/paper/e9f8d5c51e1d889ff3783e9afe7cb475c10e1af1},
  abstract={We introduce DavIR, a model-based data selection method for post-training Large Language Models. DavIR generalizes Reducible Holdout Loss to core-set selection problem of causal language modeling, and quantifies the learnability of a given datum with respect to a pre-trained LLM based on relative reduction in loss during fine-tuning, a metric we show to be closely related to the implicit reward model described in Direct Preference Optimization (DPO). We show that 6\% of Alpaca dataset selected with DavIR can steer both the LLaMA and Gemma model family to produce superior performance compared to the same models trained on the full 52K dataset. We also show that Alpaca dataset compressed with DavIR can be combined with GSM8K dataset to effectively balance open-domain freeform QA and mathematical reasoning capabilities. Finally, we apply the DavIR objective to DPO and develop a normalized DavIR-DPO objective which improves alignment performance of Zephyr-7B-SFT model by 8\% (relative) on AlpacaEval, compared against training on vanilla DPO objective.},
  keywords={arxiv:2310.13008}
}

@article{savage2023diagnosticreasoning,
  title={Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine},
  author={Thomas Savage and Ashwin Nayak and Roberta Gallo and E. Rangan and Jonathan H. Chen},
  year={2023},
  booktitle={npj Digit. Medicine},
  doi={10.1038/s41746-024-01010-1},
  url={https://www.semanticscholar.org/paper/ee692adce2e5cbb42eae2d516641ae907b0483ce},
  abstract={One of the major barriers to using large language models (LLMs) in medicine is the perception they use uninterpretable methods to make clinical decisions that are inherently different from the cognitive processes of clinicians. In this manuscript we develop diagnostic reasoning prompts to study whether LLMs can imitate clinical reasoning while accurately forming a diagnosis. We find that GPT-4 can be prompted to mimic the common clinical reasoning processes of clinicians without sacrificing diagnostic accuracy. This is significant because an LLM that can imitate clinical reasoning to provide an interpretable rationale offers physicians a means to evaluate whether an LLMs response is likely correct and can be trusted for patient care. Prompting methods that use diagnostic reasoning have the potential to mitigate the “black box” limitations of LLMs, bringing them one step closer to safe and effective use in medicine.},
  keywords={arxiv:2308.06834}
}

@article{rafailov2023directpreference,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafael Rafailov and Archit Sharma and E. Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
  year={2023},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/0d1c76d45afa012ded7ab741194baf142117c495},
  abstract={While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
  keywords={arxiv:2305.18290}
}

@article{wang2023enhancingrecommender,
  title={Enhancing Recommender Systems with Large Language Model Reasoning Graphs},
  author={Yan Wang and Zhixuan Chu and Ouyang Xin and Simeng Wang and Hongyan Hao and Yue Shen and Jinjie Gu and Siqiao Xue and James Y. Zhang and Qing Cui and Longfei Li and Jun Zhou and Shenghe Li},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.10835},
  url={https://www.semanticscholar.org/paper/f8a2813614f4e9c8adab3da2cbb667ad7e4d6bcf},
  abstract={Recommendation systems aim to provide users with relevant suggestions, but often lack interpretability and fail to capture higher-level semantic relationships between user behaviors and profiles. In this paper, we propose a novel approach that leverages large language models (LLMs) to construct personalized reasoning graphs. These graphs link a user's profile and behavioral sequences through causal and logical inferences, representing the user's interests in an interpretable way. Our approach, LLM reasoning graphs (LLMRG), has four components: chained graph reasoning, divergent extension, self-verification and scoring, and knowledge base self-improvement. The resulting reasoning graph is encoded using graph neural networks, which serves as additional input to improve conventional recommender systems, without requiring extra user or item information. Our approach demonstrates how LLMs can enable more logical and interpretable recommender systems through personalized reasoning graphs. LLMRG allows recommendations to benefit from both engineered recommendation systems and LLM-derived reasoning graphs. We demonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios in enhancing base recommendation models.},
  keywords={arxiv:2308.10835}
}

@misc{patten2023evaluatingdomain,
  title={Evaluating Domain Specific LLM Performance Within Economics Evaluating Domain Specific LLM Performance Within Economics Using the Novel EconQA Dataset Using the Novel EconQA Dataset},
  author={Tate Van Patten and Van Patten},
  year={2023},
  url={https://www.semanticscholar.org/paper/f73871b36eb728270b5b369d25308f7c63673077}
}

@article{liu2023federatedprompting,
  title={Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering},
  author={Xiangyang Liu and Tianqi Pang and Chenyou Fan},
  year={2023},
  booktitle={Knowledge Science, Engineering and Management},
  doi={10.48550/arXiv.2304.13911},
  url={https://www.semanticscholar.org/paper/a7c0d9bf44045c9d4c41e329e2a87df0ae7e0af6},
  abstract={We investigate how to enhance answer precision in frequently asked questions posed by distributed users using cloud-based Large Language Models (LLMs). Our study focuses on a typical situations where users ask similar queries that involve identical mathematical reasoning steps and problem-solving procedures. Due to the unsatisfactory accuracy of LLMs' zero-shot prompting with standalone questions, we propose to improve the distributed synonymous questions using Self-Consistency (SC) and Chain-of-Thought (CoT) techniques. Specifically, we first retrieve synonymous questions from a crowd-sourced database and create a federated question pool. We call these federated synonymous questions with the same or different parameters SP-questions or DP-questions, respectively. We refer to our methods as Fed-SP-SC and Fed-DP-CoT, which can generate significantly more accurate answers for all user queries without requiring sophisticated model-tuning. Through extensive experiments, we demonstrate that our proposed methods can significantly enhance question accuracy by fully exploring the synonymous nature of the questions and the consistency of the answers.},
  keywords={arxiv:2304.13911}
}

@article{jiang2023forwardbackwardreasoning,
  title={Forward-Backward Reasoning in Large Language Models for Mathematical Verification},
  author={Weisen Jiang and Han Shi and L. Yu and Zheng Liu and Yu Zhang and Zhenguo Li and James T. Kwok},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2024.findings-acl.397},
  url={https://www.semanticscholar.org/paper/801d7ba75fc833aa76ce4863dc1f79e30ee0c23f},
  abstract={Self-Consistency samples diverse reasoning chains with answers and chooses the final answer by majority voting. It is based on forward reasoning and cannot further improve performance by sampling more reasoning chains when saturated. To further boost performance, we introduce backward reasoning to verify candidate answers. Specifically, for mathematical tasks, we mask a number in the question and ask the LLM to answer a backward question created by a simple template, i.e., to predict the masked number when a candidate answer is provided. Instead of using forward or backward reasoning alone, we propose FOBAR to combine FOrward and BAckward Reasoning for verification. Extensive experiments on six standard mathematical data sets and three LLMs show that FOBAR achieves state-of-the-art performance. In particular, FOBAR outperforms Self-Consistency, which uses forward reasoning alone, demonstrating that combining forward and forward reasoning is better. In addition, FOBAR performs better than existing verification methods, showing the effectiveness of the simple template used in backward reasoning and the proposed combination. Extensions to non-mathematical problems are also discussed and validated empirically.},
  keywords={arxiv:2308.07758}
}

@article{gao2023gllavasolving,
  title={G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model},
  author={Jiahui Gao and Renjie Pi and Jipeng Zhang and Jiacheng Ye and Wanjun Zhong and Yufei Wang and Lanqing Hong and Jianhua Han and Hang Xu and Zhenguo Li and Lingpeng Kong},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2312.11370},
  url={https://www.semanticscholar.org/paper/3713112311efbcf785de17fa86e5bf42e4360f77},
  abstract={Large language models (LLMs) have shown remarkable proficiency in human-level reasoning and generation capabilities, which encourages extensive research on their application in mathematical problem solving. However, current work has been largely focused on text-based mathematical problems, with limited investigation in problems involving geometric information. Addressing this gap, we aim to enable LLMs to solve geometric problems by understanding image input. We first analyze the limitations of current Multimodal Large Language Models (MLLMs) in this area: they struggle to accurately comprehending basic geometric elements and their relationships. To overcome these challenges, we take advantage of the unique characteristics of geometric problems (such as unique geometric logical form, and geometric scalability) and the capacity of the textual LLMs to build an enriched multimodal geometry dataset based on existing data. The augmented dataset, Geo170K, contains more than 170K geometric image-caption and question-answer pairs. Utilizing our constructed Geo170K dataset, we develop G-LLaVA, which demonstrates exceptional performance in solving geometric problems, significantly outperforming GPT-4-V on the MathVista benchmark with only 7B parameters.},
  keywords={arxiv:2312.11370}
}

@article{si2023gettingmore,
  title={Getting MoRE out of Mixture of Language Model Reasoning Experts},
  author={Chenglei Si and Weijia Shi and Chen Zhao and Luke Zettlemoyer and Jordan L. Boyd-Graber},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.18653/v1/2023.findings-emnlp.552},
  url={https://www.semanticscholar.org/paper/7283d616e40d7ab7422e3697218f3fc42f292bf2},
  abstract={While recent large language models (LLMs) improve on various question answering (QA) datasets, it remains difficult for a single model to generalize across question types that require distinct reasoning abilities. We provide empirical evidence that state-of-the-art LLMs suffer from poor generalizability on reasoning types beyond those seen in the prompt. To remedy this, we propose a Mixture-of-Reasoning-Experts (MoRE) framework that ensembles diverse specialized language models. We specialize the backbone language model with prompts optimized for different reasoning categories, including factual, multihop, mathematical, and commonsense reasoning. Our key insight is to leverage agreement among the specialized experts to select the best answer for each question, or to abstain from answering. This gives MoRE higher accuracy than any single specialized model on a collection of 12 QA datasets from four reasoning types. Beyond generalizability, the interpretable design of MoRE improves selective question answering results compared to baselines without incorporating inter-expert agreement. This framework is also more interpretable and useful to human consumers of QA outputs. Our human study confirms that presenting expert predictions and the answer selection process helps annotators more accurately calibrate when to trust the system's output. We release all code and data to facilitate future work.},
  keywords={arxiv:2305.14628}
}

@article{patil2023gorillalarge,
  title={Gorilla: Large Language Model Connected with Massive APIs},
  author={Shishir G. Patil and Tianjun Zhang and Xin Wang and Joseph E. Gonzalez},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.52202/079017-4020},
  url={https://www.semanticscholar.org/paper/7d8905a1fd288068f12c8347caeabefd36d0dd6c},
  abstract={Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu},
  keywords={arxiv:2305.15334}
}

@article{zhang2023graphtoolformerempower,
  title={Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT},
  author={Jiawei Zhang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2304.11116},
  url={https://www.semanticscholar.org/paper/0d502a1e300336ae628f5c8b99ee4d3766c8f60b},
  abstract={In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing \{multi-step logic reasoning\}, \{precise mathematical calculation\} and \{perception about the spatial and temporal factors\}. To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools. Specifically, we will investigate to teach Graph-ToolFormer to handle various graph data reasoning tasks in this paper, including both (1) very basic graph data loading and graph property reasoning tasks, ranging from simple graph order and size to the graph diameter and periphery, and (2) more advanced reasoning tasks on real-world graph data, such as bibliographic networks, protein molecules, sequential recommender systems, social networks and knowledge graphs.},
  keywords={arxiv:2304.11116}
}

@article{chai2023graphllmboosting,
  title={GraphLLM: Boosting Graph Reasoning Ability of Large Language Model},
  author={Ziwei Chai and Tianjie Zhang and Liang Wu and Kaiqiao Han and Xiaohai Hu and Xuanwen Huang and Yang Yang},
  year={2023},
  journal={IEEE Transactions on Big Data},
  doi={10.48550/arXiv.2310.05845},
  url={https://www.semanticscholar.org/paper/062fab31d30478b57457c8b7a94d7467f5bd770c},
  abstract={The advancement of Large Language Models (LLMs) has remarkably pushed the boundaries towards artificial general intelligence (AGI), with their exceptional ability on understanding diverse types of information, including but not limited to images and audio. Despite this progress, a critical gap remains in empowering LLMs to proficiently understand and reason on graph data. Recent studies underscore LLMs' underwhelming performance on fundamental graph reasoning tasks. In this paper, we endeavor to unearth the obstacles that impede LLMs in graph reasoning, pinpointing the common practice of converting graphs into natural language descriptions (Graph2Text) as a fundamental bottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering end-to-end approach that synergistically integrates graph learning models with LLMs. This synergy equips LLMs with the ability to proficiently interpret and reason on graph data, harnessing the superior expressive power of graph learning models. Our empirical evaluations across four fundamental graph reasoning tasks validate the effectiveness of GraphLLM. The results exhibit a substantial average accuracy enhancement of 54.44\%, alongside a noteworthy context reduction of 96.45\% across various graph reasoning tasks.},
  keywords={arxiv:2310.05845}
}

@article{son2023haeraebench,
  title={HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models},
  author={Guijin Son and Hanwool Albert Lee and Suwan Kim and Jaecheol Lee and Je Won Yeom and Jihyu Jung and Jung Woo Kim and Songseong Kim},
  year={2023},
  booktitle={International Conference on Language Resources and Evaluation},
  doi={10.48550/arXiv.2309.02706},
  url={https://www.semanticscholar.org/paper/b118deb1e0715a5aeec11e399321501ba09ce2c4},
  abstract={Large language models (LLMs) trained on massive corpora demonstrate impressive capabilities in a wide range of tasks. While there are ongoing efforts to adapt these models to languages beyond English, the attention given to their evaluation methodologies remains limited. Current multilingual benchmarks often rely on back translations or re-implementations of English tests, limiting their capacity to capture unique cultural and linguistic nuances. To bridge this gap for the Korean language, we introduce the HAE-RAE Bench, a dataset curated to challenge models lacking Korean cultural and contextual depth. The dataset encompasses six downstream tasks across four domains: vocabulary, history, general knowledge, and reading comprehension. Unlike traditional evaluation suites focused on token and sequence classification or mathematical and logical reasoning, the HAE-RAE Bench emphasizes a model’s aptitude for recalling Korean-specific knowledge and cultural contexts. Comparative analysis with prior Korean benchmarks indicates that the HAE-RAE Bench presents a greater challenge to non-Korean models by disturbing abilities and knowledge learned from English being transferred.},
  keywords={arxiv:2309.02706}
}

@article{arora2023havellms,
  title={Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models},
  author={Daman Arora and H. Singh and Mausam},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2305.15074},
  url={https://www.semanticscholar.org/paper/2cf1f6c723006f258599fd9f000bb616ae83387a},
  abstract={The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40\%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.},
  keywords={arxiv:2305.15074}
}

@article{dong2023abilitieslarge,
  title={How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition},
  author={Guanting Dong and Hongyi Yuan and Keming Lu and Chengpeng Li and Mingfeng Xue and Dayiheng Liu and Wei Wang and Zheng Yuan and Chang Zhou and Jingren Zhou},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2310.05492},
  url={https://www.semanticscholar.org/paper/5088a04d1a9f42b967f3dcf791145e8aa367fc54},
  abstract={Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, code generation, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specifically focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.},
  keywords={arxiv:2310.05492}
}

@article{du2023improvingfactuality,
  title={Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author={Yilun Du and Shuang Li and A. Torralba and J. Tenenbaum and Igor Mordatch},
  year={2023},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2305.14325},
  url={https://www.semanticscholar.org/paper/4780d0a027c5c5a8e01d7cf697f6296880ffc945},
  abstract={Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such"society of minds"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.},
  keywords={arxiv:2305.14325}
}

@misc{latif2023knowledgedistillation,
  title={Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments},
  author={Ehsan Latif and Luyang Fang and Ping Ma and Xiaoming Zhai},
  year={2023},
  url={https://www.semanticscholar.org/paper/631b5baa2c34f7095ccdd8761086b49148071d78},
  abstract={This study proposes a method for knowledge distillation (KD) of fine-tuned Large Language Models (LLMs) into smaller, more efficient, and accurate neural networks. We specifically target the challenge of deploying these models on resource-constrained devices. Our methodology involves training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model. This is achieved through a specialized loss function tailored to learn from the LLM's output probabilities, ensuring that the student model closely mimics the teacher's performance. To validate the performance of the KD approach, we utilized a large dataset, 7T, containing 6,684 student-written responses to science questions and three mathematical reasoning datasets with student-written responses graded by human experts. We compared accuracy with state-of-the-art (SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models. Results have shown that the KD approach has 3\% and 2\% higher scoring accuracy than ANN and TinyBERT, respectively, and comparable accuracy to the teacher model. Furthermore, the student model size is 0.03M, 4,000 times smaller in parameters and x10 faster in inferencing than the teacher model and TinyBERT, respectively. The significance of this research lies in its potential to make advanced AI technologies accessible in typical educational settings, particularly for automatic scoring.},
  keywords={arxiv:2312.15842}
}

@article{lai2023lisareasoning,
  title={LISA: Reasoning Segmentation via Large Language Model},
  author={Xin Lai and Zhuotao Tian and Yukang Chen and Yanwei Li and Yuhui Yuan and Shu Liu and Jiaya Jia},
  year={2023},
  booktitle={Computer Vision and Pattern Recognition},
  doi={10.1109/CVPR52733.2024.00915},
  url={https://www.semanticscholar.org/paper/ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7},
  abstract={Although perception systems have made remarkable ad-vancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task - reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with a token and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex rea-soning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at github.com/dvlab-research/LISA.},
  keywords={arxiv:2308.00692}
}

@article{yue2023largelanguage,
  title={Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning},
  author={Murong Yue and Jie Zhao and Min Zhang and Liang Du and Ziyu Yao},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.03094},
  url={https://www.semanticscholar.org/paper/00cccb9065f0a59e845d5b4d360ce31cf25036be},
  abstract={Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the"answer consistency"of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and stronger LLMs, respectively, we demonstrate that our proposed LLM cascades can achieve performance comparable to using solely the stronger LLM but require only 40\% of its cost.},
  keywords={arxiv:2310.03094}
}

@article{mo2023largelanguage,
  title={Large Language Models for Travel Behavior Prediction},
  author={Baichuan Mo and Hanyong Xu and Dingyi Zhuang and Ruoyun Ma and Xiaotong Guo and Jinhua Zhao},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.00819},
  url={https://www.semanticscholar.org/paper/113e82ab340af5b9721b8ed50b022e3bb769170e},
  abstract={Travel behavior prediction is a fundamental task in transportation demand management. The conventional methods for travel behavior prediction rely on numerical data to construct mathematical models and calibrate model parameters to represent human preferences. Recent advancement in large language models (LLMs) has shown great reasoning abilities to solve complex problems. In this study, we propose to use LLMs to predict travel behavior with prompt engineering without data-based parameter learning. Specifically, we carefully design our prompts that include 1) task description, 2) travel characteristics, 3) individual attributes, and 4) guides of thinking with domain knowledge, and ask the LLMs to predict an individual's travel behavior and explain the results. We select the travel mode choice task as a case study. Results show that, though no training samples are provided, LLM-based predictions have competitive accuracy and F1-score as canonical supervised learning methods such as multinomial logit, random forest, and neural networks. LLMs can also output reasons that support their prediction. However, though in most of the cases, the output explanations are reasonable, we still observe cases that violate logic or with hallucinations.},
  keywords={arxiv:2312.00819}
}

@article{zhang2023learningproof,
  title={Learning Proof Transformations and Its Applications in Interactive Theorem Proving},
  author={Liao Zhang and Lasse Blaauwbroek and C. Kaliszyk and Josef Urban},
  year={2023},
  booktitle={International Symposium on Frontiers of Combining Systems},
  doi={10.1007/978-3-031-43369-6_13},
  url={https://www.semanticscholar.org/paper/fea0019bc70fd54558a497d12172d524ceeef787},
  abstract={. Interactive theorem provers are today increasingly used to certify mathematical theories. To formally prove a theorem, reasoning procedures called tactics are invoked successively on the proof states starting with the initial theorem statement, transforming them into subsequent intermediate goals, and ultimately discharging all proof obligations. In this work, we develop and experimentally evaluate approaches that predict the most likely tactics that will achieve particular desired transformations of proof states. First, we design several characterizations to efficiently capture the semantics of the proof transformations. Then we use them to create large datasets on which we train state-of-the-art random forests and language models. The trained models are evaluated experimentally, and we show that our best model is able to guess the right tactic for a given proof transformation in 74\% of the cases. Finally, we use the trained methods in two applications: proof shortening and tactic suggesting. To the best of our knowledge, this is the first time that tactic synthesis is trained on proof transformations and assists interactive theorem proving in these ways.}
}

@article{ma2023letsreward,
  title={Let's reward step by step: Step-Level reward model as the Navigators for Reasoning},
  author={Qianli Ma and Haotian Zhou and Tingkai Liu and Jianbo Yuan and Pengfei Liu and Yang You and Hongxia Yang},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.10080},
  url={https://www.semanticscholar.org/paper/44b506d9619b5f957dc2b5588801138f343c0308},
  abstract={Recent years have seen considerable advancements in multi-step reasoning with Large Language Models (LLMs). The previous studies have elucidated the merits of integrating feedback or search mechanisms during model inference to improve the reasoning accuracy. The Process-Supervised Reward Model (PRM), typically furnishes LLMs with step-by-step feedback during the training phase, akin to Proximal Policy Optimization (PPO) or reject sampling. Our objective is to examine the efficacy of PRM in the inference phase to help discern the optimal solution paths for multi-step tasks such as mathematical reasoning and code generation. To this end, we propose a heuristic greedy search algorithm that employs the step-level feedback from PRM to optimize the reasoning pathways explored by LLMs. This tailored PRM demonstrated enhanced results compared to the Chain of Thought (CoT) on mathematical benchmarks like GSM8K and MATH. Additionally, to explore the versatility of our approach, we develop a novel method to automatically generate step-level reward dataset for coding tasks and observed similar improved performance in the code generation tasks. Thus highlighting the robust nature of our reward-model-based approach to inference for reasoning tasks.},
  keywords={arxiv:2310.10080}
}

@article{yue2023mammothbuilding,
  title={MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning},
  author={Xiang Yue and Xingwei Qu and Ge Zhang and Yao Fu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2309.05653},
  url={https://www.semanticscholar.org/paper/a3dd7d33dfaa9e02e43d92e900cba01f52d8c4b9},
  abstract={We introduce MAmmoTH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmoTH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16\% and 32\%. Remarkably, our MAmmoTH-7B model reaches 33\% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23\%, and the MAmmoTH-34B model achieves 44\% accuracy on MATH, even surpassing GPT-4's CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models.},
  keywords={arxiv:2309.05653}
}

@article{chen2023mcckdmulticot,
  title={MCC-KD: Multi-CoT Consistent Knowledge Distillation},
  author={Hongzhan Chen and Siyue Wu and Xiaojun Quan and Rui Wang and Ming Yan and Ji Zhang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2310.14747},
  url={https://www.semanticscholar.org/paper/ef9079f32e806e4d297cee28f36b2321acee9eb3},
  abstract={Large language models (LLMs) have showcased remarkable capabilities in complex reasoning through chain of thought (CoT) prompting. Recently, there has been a growing interest in transferring these reasoning abilities from LLMs to smaller models. However, achieving both the diversity and consistency in rationales presents a challenge. In this paper, we focus on enhancing these two aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple rationales for each question and enforce consistency among the corresponding predictions by minimizing the bidirectional KL-divergence between the answer distributions. We investigate the effectiveness of MCC-KD with different model architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both mathematical reasoning and commonsense reasoning benchmarks. The empirical results not only confirm MCC-KD's superior performance on in-distribution datasets but also highlight its robust generalization ability on out-of-distribution datasets.},
  keywords={arxiv:2310.14747}
}

@article{zeng2023mrgsm8kmetareasoning,
  title={MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation},
  author={Zhongshen Zeng and Pengguang Chen and Shu Liu and Haiyun Jiang and Jiaya Jia},
  year={2023},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/490e465ef0909f01a82916f4adab0c410576c866},
  abstract={In this work, we introduce a novel evaluation paradigm for Large Language Models (LLMs) that compels them to transition from a traditional question-answering role, akin to a student, to a solution-scoring role, akin to a teacher. This paradigm, focusing on"reasoning about reasoning,"hence termed meta-reasoning, shifts the emphasis from result-oriented assessments, which often neglect the reasoning process, to a more comprehensive evaluation that effectively distinguishes between the cognitive capabilities of different models. By applying this paradigm in the GSM8K dataset, we have developed the MR-GSM8K benchmark. Our extensive analysis includes several state-of-the-art models from both open-source and commercial domains, uncovering fundamental deficiencies in their training and evaluation methodologies. Notably, while models like Deepseek-v2 and Claude3-Sonnet closely competed with GPT-4 in GSM8K, their performance disparities expanded dramatically in MR-GSM8K, with differences widening to over 20 absolute points, underscoring the significant challenge posed by our meta-reasoning approach.},
  keywords={arxiv:2312.17080}
}

@article{imani2023mathpromptermathematical,
  title={MathPrompter: Mathematical Reasoning using Large Language Models},
  author={Shima Imani and Liang Du and H. Shrivastava},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2303.05398},
  url={https://www.semanticscholar.org/paper/b626560f19f815808a289ef5c24a17c57320da70},
  abstract={Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose ‘MathPrompter’, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the ‘MultiArith’ dataset (78.7\% - 92.5\%) evaluated using 175B parameter GPT-based LLM.},
  keywords={arxiv:2303.05398}
}

@article{lu2023mathvistaevaluating,
  title={MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  author={Pan Lu and Hritik Bansal and Tony Xia and Jiacheng Liu and Chun-yue Li and Hannaneh Hajishirzi and Hao Cheng and Kai-Wei Chang and Michel Galley and Jianfeng Gao},
  year={2023},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/8946891e94831adc8cddb0d32311cce2445c96d2},
  abstract={Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9\%, substantially outperforming Bard, the second-best performer, by 15.1\%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4\%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.},
  keywords={arxiv:2310.02255}
}

@article{yu2023metamathbootstrap,
  title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models},
  author={L. Yu and Weisen Jiang and Han Shi and Jincheng Yu and Zhengying Liu and Yu Zhang and James T. Kwok and Zheng Li and Adrian Weller and Weiyang Liu},
  year={2023},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/77b1f1c6d1658d120456b9046667cf009ceb39ce},
  abstract={Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4\% on GSM8K and 19.4\% on MATH, exceeding the state-of-the-art models of the same size by 11.5\% and 8.7\%. Particularly, MetaMath-70B achieves an accuracy of 82.3\% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.},
  keywords={arxiv:2309.12284}
}

@article{liang2023mintboosting,
  title={MinT: Boosting Generalization in Mathematical Reasoning via Multi-view Fine-tuning},
  author={Zhenwen Liang and Dian Yu and Xiaoman Pan and Wenlin Yao and Qingkai Zeng and Xiangliang Zhang and Dong Yu},
  year={2023},
  booktitle={International Conference on Language Resources and Evaluation},
  doi={10.48550/arXiv.2307.07951},
  url={https://www.semanticscholar.org/paper/c1c93b4916aa5b8ce1d99a9e59c700e3d13ada36},
  abstract={Reasoning in mathematical domains remains a significant challenge for relatively small language models (LMs). Many current methods focus on specializing LMs in mathematical reasoning and rely heavily on distilling knowledge from powerful yet inefficient large LMs (LLMs). In this work, we explore a new direction that avoids over-reliance on LLM teachers, introducing a multi-view fine-tuning method that efficiently exploits existing mathematical problem datasets with diverse annotation styles. Our approach uniquely considers the various annotation formats as different “views” that may help each other and leverage them in training the model. By postpending distinct instructions to input questions, models can learn to generate solutions in diverse formats in a flexible manner. Experimental results show that our strategy enables relatively small LMs to outperform prior approaches that heavily rely on knowledge distillation, as well as carefully established baselines. Additionally, the proposed method grants the models promising generalization ability across various views and datasets, and the capability to learn from inaccurate or incomplete noisy data. We hope our multi-view training paradigm could inspire future studies in other machine reasoning domains.},
  keywords={arxiv:2307.07951}
}

@article{si2023mixtureprompt,
  title={Mixture of Prompt Experts for Generalizable and Interpretable Question Answering},
  author={Chenglei Si and Weijia Shi and Chen Zhao and Luke S. Zettlemoyer and Jordan L. Boyd-Graber},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2305.14628},
  url={https://www.semanticscholar.org/paper/700f9c8a76d7af0fc550d119aa1d1164a496055e}
}

@article{liao2023modelingcomplex,
  title={Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent},
  author={Haoran Liao and Qinyi Du and Shaohua Hu and Hao He and Yanyan Xu and Jidong Tian and Yaohui Jin},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2312.08926},
  url={https://www.semanticscholar.org/paper/7017c58e19f4db0c38040935cc9fb7b7090a466d},
  abstract={Large language models (LLMs) face challenges in solving complex mathematical problems that require comprehensive capacities to parse the statements, associate domain knowledge, perform compound logical reasoning, and integrate the intermediate rationales. Tackling all these problems once could be arduous for LLMs, thus leading to confusion in generation. In this work, we explore the potential of enhancing LLMs with agents by meticulous decomposition and modeling of mathematical reasoning process. Specifically, we propose a formal description of the mathematical solving and extend LLMs with an agent-based zero-shot framework named \$\textbackslash\{\}bf\{P\}\$lanner-\$\textbackslash\{\}bf\{R\}\$easoner-\$\textbackslash\{\}bf\{E\}\$xecutor-\$\textbackslash\{\}bf\{R\}\$eflector (PRER). We further provide and implement two MathAgents that define the logical forms and inherent relations via a pool of actions in different grains and orientations: MathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with humankind. Experiments on miniF2F and MATH have demonstrated the effectiveness of PRER and proposed MathAgents, achieving an increase of \$12.3\textbackslash\{\}\%\$(\$53.9\textbackslash\{\}\%\textbackslash\{\}xrightarrow\{\}66.2\textbackslash\{\}\%\$) on the MiniF2F, \$9.2\textbackslash\{\}\%\$ (\$49.8\textbackslash\{\}\%\textbackslash\{\}xrightarrow\{\}59.0\textbackslash\{\}\%\$) on MATH, and \$13.2\textbackslash\{\}\%\$(\$23.2\textbackslash\{\}\%\textbackslash\{\}xrightarrow\{\}35.4\textbackslash\{\}\%\$) for level-5 problems of MATH against GPT-4. Further analytical results provide more insightful perspectives on exploiting the behaviors of LLMs as agents.},
  keywords={arxiv:2312.08926}
}

@article{li2023mugglemathassessing,
  title={MuggleMath: Assessing the Impact of Query and Response Augmentation on Math Reasoning},
  author={Chengpeng Li and Zheng Yuan and Guanting Dong and Keming Lu and Jiancan Wu and Chuanqi Tan and Xiang Wang and Chang Zhou},
  year={2023},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2024.acl-long.551},
  url={https://www.semanticscholar.org/paper/84c03d0ec9dd00de3dc7ea7577ceefc9f093c564},
  abstract={In math reasoning with large language models (LLMs), fine-tuning data augmentation by query evolution and diverse reasoning paths is empirically verified effective, profoundly narrowing the gap between open-sourced LLMs and cutting-edge proprietary LLMs. In this paper, we conduct an investigation for such data augmentation in math reasoning and are intended to answer: (1) What strategies of data augmentation are more effective; (2) What is the scaling relationship between the amount of augmented data and model performance; and (3) Can data augmentation incentivize generalization to out-of-domain mathematical reasoning tasks? To this end, we create two new dataset AugGSM8K and AugMATH, by complicating and diversifying the queries and sampling multiple reasoning paths from GSM8K and MATH. We obtained a series of LLMs called MuggleMath by fine-tuning LLaMA models on AugGSM8K and AugMATH. MuggleMath substantially achieves new state-of-the-art on GSM8K and MATH. A log-linear relationship and a segmented log-linear are presented between MuggleMath's performance and the amount of augmented data on GSM8K and MATH, respectively. We also find that it is weak in out-of-domain math reasoning generalization from AugGSM8K to MATH and from AugMATH to GSM8K, which suggests that augmenting queries that cover a broader range of subjects is more beneficial for generalization. We release our codes and augmented data in https://github.com/OFA-Sys/gsm8k-ScRel.},
  keywords={arxiv:2310.05506}
}

@article{xu2023trainstill,
  title={No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function},
  author={Haotian Xu},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2309.03224},
  url={https://www.semanticscholar.org/paper/537335d9aad0ddbaef93e7f88b0db096671ef6ec},
  abstract={Large language models (LLMs) demonstrate impressive language understanding and contextual learning abilities, making them suitable for natural language processing (NLP) tasks and complex mathematical reasoning. However, when applied to mathematical reasoning tasks, LLMs often struggle to generate correct reasoning steps and answers despite having high probabilities for the solutions. To overcome this limitation and enhance the mathematical reasoning capabilities of fine-tuned LLMs without additional fine-tuning steps, we propose a method that incorporates Monte Carlo Tree Search (MCTS) and a lightweight energy function to rank decision steps and enable immediate reaction and precise reasoning. Specifically, we re-formulate the fine-tuned LLMs into a Residual-based Energy Model (Residual-EBM) and employ noise contrastive estimation to estimate the energy function's parameters. We then utilize MCTS with the energy function as a path verifier to search the output space and evaluate the reasoning path. Through extensive experiments on two mathematical reasoning benchmarks, GSM8k and AQUA-RAT, we demonstrate the exceptional capabilities of our method, which significantly improves the pass@1 metric of the fine-tuned model without requiring additional fine-tuning or reinforcement learning with human feedback alignment.},
  keywords={arxiv:2309.03224}
}

@article{yu2023outcomesupervisedvalue,
  title={OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning},
  author={Fei Yu and Anningzhe Gao and Benyou Wang},
  year={2023},
  booktitle={NAACL-HLT},
  doi={10.18653/v1/2024.findings-naacl.55},
  url={https://www.semanticscholar.org/paper/288e64e8adb23d81e291a2cb51e3a56b315023b7},
  abstract={Large language models (LLMs) often struggle with maintaining accuracy throughout multiple multiple reasoning steps, especially in mathematical reasoning where an error in earlier steps can propagate to subsequent ones and it ultimately leading to an incorrect answer. To reduce error propagation, guided decoding is employed to direct the LM decoding on a step-by-step basis. We argue that in guided decoding, assessing the potential of an incomplete reasoning path can be more advantageous than simply ensuring per-step correctness, as the former approach leads towards a correct final answer. This transforms the task into a \$\textbackslash\{\}textit\{value estimation\}\$ problem in planning. Inspired by the findings that \$\textbackslash\{\}textit\{outcome supervision for guided decoding essentially acts as a value model\}\$, we propose Outcome-supervised Value Model (OVM) that employs outcome supervision for training a value model, which prioritizes steps that lead to accurate conclusions. Furthermore, the OVM eliminates the need for labor-intensive annotations of step-level correctness, thereby significantly enhancing its scalability. Our experiments on two multi-step mathematical reasoning datasets, GSM8K and Game of 24, demonstrate the superior performance of the OVM model. Notably, in GSM8K, our \$\textbackslash\{\}textbf\{OVM-7B model achieves state-of-the-art results among LLMs up to 13B parameters\}\$; especially it does not utilize GPT-4 or code execution. These findings offer a novel perspective on the role of outcome supervision in training value models for multi-step reasoning tasks and provide theoretical justification for its advantage in value estimation for guided decoding.},
  keywords={arxiv:2311.09724}
}

@article{paster2023openwebmathopen,
  title={OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text},
  author={Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.06786},
  url={https://www.semanticscholar.org/paper/e92d6b93371c5dc6f3f6fb917f925d6c2fae5492},
  abstract={There is growing evidence that pretraining on high quality, carefully thought-out tokens such as code or mathematics plays an important role in improving the reasoning abilities of large language models. For example, Minerva, a PaLM model finetuned on billions of tokens of mathematical documents from arXiv and the web, reported dramatically improved performance on problems that require quantitative reasoning. However, because all known open source web datasets employ preprocessing that does not faithfully preserve mathematical notation, the benefits of large scale training on quantitive web documents are unavailable to the research community. We introduce OpenWebMath, an open dataset inspired by these works containing 14.7B tokens of mathematical webpages from Common Crawl. We describe in detail our method for extracting text and LaTeX content and removing boilerplate from HTML documents, as well as our methods for quality filtering and deduplication. Additionally, we run small-scale experiments by training 1.4B parameter language models on OpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass the performance of models trained on over 20x the amount of general language data. We hope that our dataset, openly released on the Hugging Face Hub, will help spur advances in the reasoning abilities of large language models.},
  keywords={arxiv:2310.06786}
}

@article{giannos2023performancechatgpt,
  title={Performance of ChatGPT on UK Standardized Admission Tests: Insights From the BMAT, TMUA, LNAT, and TSA Examinations},
  author={P. Giannos and Orestis Delardas},
  year={2023},
  booktitle={JMIR Medical Education},
  doi={10.2196/47737},
  url={https://www.semanticscholar.org/paper/b1a365943652c2245c82e61a37362ba77fb108dc},
  abstract={Background Large language models, such as ChatGPT by OpenAI, have demonstrated potential in various applications, including medical education. Previous studies have assessed ChatGPT’s performance in university or professional settings. However, the model’s potential in the context of standardized admission tests remains unexplored. Objective This study evaluated ChatGPT’s performance on standardized admission tests in the United Kingdom, including the BioMedical Admissions Test (BMAT), Test of Mathematics for University Admission (TMUA), Law National Aptitude Test (LNAT), and Thinking Skills Assessment (TSA), to understand its potential as an innovative tool for education and test preparation. Methods Recent public resources (2019-2022) were used to compile a data set of 509 questions from the BMAT, TMUA, LNAT, and TSA covering diverse topics in aptitude, scientific knowledge and applications, mathematical thinking and reasoning, critical thinking, problem-solving, reading comprehension, and logical reasoning. This evaluation assessed ChatGPT’s performance using the legacy GPT-3.5 model, focusing on multiple-choice questions for consistency. The model’s performance was analyzed based on question difficulty, the proportion of correct responses when aggregating exams from all years, and a comparison of test scores between papers of the same exam using binomial distribution and paired-sample (2-tailed) t tests. Results The proportion of correct responses was significantly lower than incorrect ones in BMAT section 2 (P<.001) and TMUA paper 1 (P<.001) and paper 2 (P<.001). No significant differences were observed in BMAT section 1 (P=.2), TSA section 1 (P=.7), or LNAT papers 1 and 2, section A (P=.3). ChatGPT performed better in BMAT section 1 than section 2 (P=.047), with a maximum candidate ranking of 73\% compared to a minimum of 1\%. In the TMUA, it engaged with questions but had limited accuracy and no performance difference between papers (P=.6), with candidate rankings below 10\%. In the LNAT, it demonstrated moderate success, especially in paper 2’s questions; however, student performance data were unavailable. TSA performance varied across years with generally moderate results and fluctuating candidate rankings. Similar trends were observed for easy to moderate difficulty questions (BMAT section 1, P=.3; BMAT section 2, P=.04; TMUA paper 1, P<.001; TMUA paper 2, P=.003; TSA section 1, P=.8; and LNAT papers 1 and 2, section A, P>.99) and hard to challenging ones (BMAT section 1, P=.7; BMAT section 2, P<.001; TMUA paper 1, P=.007; TMUA paper 2, P<.001; TSA section 1, P=.3; and LNAT papers 1 and 2, section A, P=.2). Conclusions ChatGPT shows promise as a supplementary tool for subject areas and test formats that assess aptitude, problem-solving and critical thinking, and reading comprehension. However, its limitations in areas such as scientific and mathematical knowledge and applications highlight the need for continuous development and integration with conventional learning strategies in order to fully harness its potential.}
}

@article{li2023queryresponse,
  title={Query and Response Augmentation Cannot Help Out-of-domain Math Reasoning Generalization},
  author={Chengpeng Li and Zheng Yuan and Hongyi Yuan and Guanting Dong and Keming Lu and Jiancan Wu and Chuanqi Tan and Xiang Wang and Chang Zhou},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.05506},
  url={https://www.semanticscholar.org/paper/fb8330d41315c471edb42e799de18386a9a0b1a2}
}

@article{luo2023reasoninggraphs,
  title={Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning},
  author={Linhao Luo and Yuan-Fang Li and Gholamreza Haffari and Shirui Pan},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.01061},
  url={https://www.semanticscholar.org/paper/b47e96762351b2dbf7e863ece4640df6194bcc0c},
  abstract={Large language models (LLMs) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM reasoning methods only treat KGs as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs (RoG) that synergizes LLMs with KGs to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where RoG first generates relation paths grounded by KGs as faithful plans. These plans are then used to retrieve valid reasoning paths from the KGs for LLMs to conduct faithful reasoning. Furthermore, RoG not only distills knowledge from KGs to improve the reasoning ability of LLMs through training but also allows seamless integration with any arbitrary LLMs during inference. Extensive experiments on two benchmark KGQA datasets demonstrate that RoG achieves state-of-the-art performance on KG reasoning tasks and generates faithful and interpretable reasoning results.},
  keywords={arxiv:2310.01061}
}

@article{you2023robotenabledconstruction,
  title={Robot-Enabled Construction Assembly with Automated Sequence Planning based on ChatGPT: RoboGPT},
  author={Hengxu You and Yang Ye and Tianyu Zhou and Qi Zhu and Jing Du},
  year={2023},
  booktitle={Buildings},
  doi={10.48550/arXiv.2304.11018},
  url={https://www.semanticscholar.org/paper/798868aa3d1cb0f83a3f97265a8e0d17d649e1de},
  abstract={Robot-based assembly in construction has emerged as a promising solution to address numerous challenges such as increasing costs, labor shortages, and the demand for safe and efficient construction processes. One of the main obstacles in realizing the full potential of these robotic systems is the need for effective and efficient sequence planning for construction tasks. Current approaches, including mathematical and heuristic techniques or machine learning methods, face limitations in their adaptability and scalability to dynamic construction environments. To expand the current robot system’s sequential understanding ability, this paper introduces RoboGPT, a novel system that leverages the advanced reasoning capabilities of ChatGPT, a large language model, for automated sequence planning in robot-based assembly applied to construction tasks. The proposed system adapts ChatGPT for construction sequence planning and demonstrates its feasibility and effectiveness through experimental evaluation including two case studies and 80 trials involving real construction tasks. The results show that RoboGPT-driven robots can handle complex construction operations and adapt to changes on the fly. This paper contributes to the ongoing efforts to enhance the capabilities and performance of robot-based assembly systems in the construction industry, and it paves the way for further integration of large language model technologies in the field of construction robotics.},
  keywords={arxiv:2304.11018}
}

@article{yuan2023scalingrelationship,
  title={Scaling Relationship on Learning Mathematical Reasoning with Large Language Models},
  author={Zheng Yuan and Hongyi Yuan and Cheng Li and Guanting Dong and Chuanqi Tan and Chang Zhou},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2308.01825},
  url={https://www.semanticscholar.org/paper/91206346edbe28abb606d7b3425cd455d4019d4f},
  abstract={Mathematical reasoning is a challenging task for large language models (LLMs), while the scaling relationship of it with respect to LLM capacity is under-explored. In this paper, we investigate how the pre-training loss, supervised data amount, and augmented data amount influence the reasoning performances of a supervised LLM. We find that pre-training loss is a better indicator of the model's performance than the model's parameter count. We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance, and we find better models improve less with enlarged supervised datasets. To augment more data samples for improving model performances without any human effort, we propose to apply Rejection sampling Fine-Tuning (RFT). RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs. We also find RFT brings more improvement for less performant LLMs. Furthermore, we combine rejection samples from multiple models which push LLaMA-7B to an accuracy of 49.3\textbackslash\{\}\% on GSM8K which outperforms the supervised fine-tuning (SFT) accuracy of 35.9\textbackslash\{\}\% significantly.},
  keywords={arxiv:2308.01825}
}

@article{wang2023tsciqteaching,
  title={T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering},
  author={Lei Wang and Yilang Hu and Jiabang He and Xingdong Xu and Ning Liu and Hui-juan Liu and Hengtao Shen},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2305.03453},
  url={https://www.semanticscholar.org/paper/3f758a13d3703b02bdf977f9189230276064da42},
  abstract={Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the external essential information missed. To address these issues, we propose a novel method termed T-SciQ that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a novel data mixing strategy to produce more effective teaching data samples for simple and complex science question answer problems. Extensive experimental results show that our T-SciQ method achieves a new state-of-the-art performance on the ScienceQA benchmark, with an accuracy of 96.18\%. Moreover, our approach outperforms the most powerful fine-tuned baseline by 4.5\%. The code is publicly available at https://github.com/T-SciQ/T-SciQ.},
  keywords={arxiv:2305.03453}
}

@article{sui2023tap4llmtable,
  title={TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning},
  author={Yuan Sui and Jiaru Zou and Mengyu Zhou and Xinyi He and Lun Du and Shi Han and Dongmei Zhang},
  year={2023},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2312.09039},
  url={https://www.semanticscholar.org/paper/00a67af3b7dc785b4813b61d232cc76b4fb2b189},
  abstract={Table reasoning tasks have shown remarkable progress with the development of large language models (LLMs), which involve interpreting and drawing conclusions from tabular data based on natural language (NL) questions. Existing solutions mainly tested on smaller tables face scalability issues and struggle with complex queries due to incomplete or dispersed data across different table sections. To alleviate these challenges, we propose TAP4LLM as a versatile pre-processor suite for leveraging LLMs in table-based tasks effectively. It covers several distinct components: (1) table sampling to decompose large tables into manageable sub-tables based on query semantics, (2) table augmentation to enhance tables with additional knowledge from external sources or models, and (3) table packing\&serialization to convert tables into various formats suitable for LLMs' understanding. In each module, we design and compare several common methods under various usage scenarios, aiming to shed light on the best practices for leveraging LLMs for table-reasoning tasks. Our experiments show that our method improves LLMs' reasoning capabilities in various tabular tasks and enhances the interaction between LLMs and tabular data by employing effective pre-processing.},
  keywords={arxiv:2312.09039}
}

@article{shridhar2023refinementrefine,
  title={The ART of LLM Refinement: Ask, Refine, and Trust},
  author={Kumar Shridhar and Koustuv Sinha and Andrew Cohen and Tianlu Wang and Ping Yu and Ramakanth Pasunuru and Mrinmaya Sachan and Jason Weston and Asli Celikyilmaz},
  year={2023},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2311.07961},
  url={https://www.semanticscholar.org/paper/b7db52a4ad0211bf0b18195d6d247b089da26048},
  abstract={Large Language Models (LLMs) have demonstrated remarkable generative abilities, but can they judge the quality of their own generations and self-improve?A popular concept, referred to as *self-refinement*, postulates that LLMs can detect and correct the errors in their generations when asked to do so. However, recent empirical evidence points in the opposite direction, suggesting that LLMs often struggle to accurately identify errors when reasoning is involved. To address this, we propose a reasoning with a refinement strategy called *ART: Ask, Refine, and Trust*, which *asks* necessary questions to decide when an LLM should *refine* its output, and uses it to affirm or deny *trust* in its refinement by ranking the refinement and the initial prediction. On two multistep reasoning tasks of mathematical word problems (GSM8K) and question answering (StrategyQA), *ART* achieves a performance gain of +5 points over self-refinement baselines, while using a much smaller model as the decision maker. We believe that *ART* with smaller models, making refinement decisions can be a cost-effective alternative to fine-tuning LLMs.},
  keywords={arxiv:2311.07961}
}

@article{jacob2023consensusgame,
  title={The Consensus Game: Language Model Generation via Equilibrium Search},
  author={Athul Paul Jacob and Yikang Shen and Gabriele Farina and Jacob Andreas},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2310.09139},
  url={https://www.semanticscholar.org/paper/5baeb40d658ba6f2f26ecd64b6ce594077a9725d},
  abstract={When applied to question answering and other text generation tasks, language models (LMs) may be queried generatively (by sampling answers from their output distribution) or discriminatively (by using them to score or rank a set of candidate outputs). These procedures sometimes yield very different predictions. How do we reconcile mutually incompatible scoring procedures to obtain coherent LM predictions? We introduce a new, a training-free, game-theoretic procedure for language model decoding. Our approach casts language model decoding as a regularized imperfect-information sequential signaling game - which we term the CONSENSUS GAME - in which a GENERATOR seeks to communicate an abstract correctness parameter using natural language sentences to a DISCRIMINATOR. We develop computational procedures for finding approximate equilibria of this game, resulting in a decoding algorithm we call EQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading comprehension, commonsense reasoning, mathematical problem-solving, and dialog), EQUILIBRIUM-RANKING consistently, and sometimes substantially, improves performance over existing LM decoding procedures - on multiple benchmarks, we observe that applying EQUILIBRIUM-RANKING to LLaMA-7B outperforms the much larger LLaMA-65B and PaLM-540B models. These results highlight the promise of game-theoretic tools for addressing fundamental challenges of truthfulness and consistency in LMs.},
  keywords={arxiv:2310.09139}
}

@article{yeadon2023impactphysics,
  title={The impact of AI in physics education: a comprehensive review from GCSE to university levels},
  author={W. Yeadon and Tom Hardy},
  year={2023},
  booktitle={The Physical Educator},
  doi={10.1088/1361-6552/ad1fa2},
  url={https://www.semanticscholar.org/paper/e2ffb7b4215cbe7b5d06be4a37aacedc8762fd50},
  abstract={With the rapid evolution of artificial intelligence (AI), its potential implications for higher education have become a focal point of interest. This study delves into the capabilities of AI in physics education and offers actionable AI policy recommendations. Using openAI’s flagship gpt-3.5-turbo large language model (LLM), we assessed its ability to answer 1337 physics exam questions spanning general certificate of secondary education (GCSE), A-Level, and introductory university curricula. We employed various AI prompting techniques: Zero Shot, in context learning, and confirmatory checking, which merges chain of thought reasoning with reflection. The proficiency of gpt-3.5-turbo varied across academic levels: it scored an average of 83.4\% on GCSE, 63.8\% on A-Level, and 37.4\% on university-level questions, with an overall average of 59.9\% using the most effective prompting technique. In a separate test, the LLM’s accuracy on 5000 mathematical operations was found to be 45.2\%. When evaluated as a marking tool, the LLM’s concordance with human markers averaged at 50.8\%, with notable inaccuracies in marking straightforward questions, like multiple-choice. Given these results, our recommendations underscore caution: while current LLMs can consistently perform well on physics questions at earlier educational stages, their efficacy diminishes with advanced content and complex calculations. LLM outputs often showcase novel methods not in the syllabus, excessive verbosity, and miscalculations in basic arithmetic. This suggests that at university, there’s no substantial threat from LLMs for non-invigilated physics questions. However, given the LLMs’ considerable proficiency in writing physics essays and coding abilities, non-invigilated examinations of these skills in physics are highly vulnerable to automated completion by LLMs. This vulnerability also extends to pysics questions pitched at lower academic levels. It is thus recommended that educators be transparent about LLM capabilities with their students, while emphasizing caution against overreliance on their output due to its tendency to sound plausible but be incorrect.},
  keywords={arxiv:2309.05163}
}

@article{sun2023thinkongraphdeep,
  title={Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph},
  author={Jiashuo Sun and Chengjin Xu and Lumingyuan Tang and Sai Wang and Chen Lin and Yeyun Gong and Lionel M. Ni and H. Shum and Jian Guo},
  year={2023},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/c7184f9a914dbfbad59faa5aeaf9ba7019dfcf74},
  abstract={Although large language models (LLMs) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs (KG) in LLM reasoning. In this paper, we propose a new LLM-KG integrating paradigm ``\$\textbackslash\{\}hbox\{LLM\}\textbackslash\{\}otimes\textbackslash\{\}hbox\{KG\}\$'' which treats the LLM as an agent to interactively explore related entities and relations on KGs and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph (ToG), in which the LLM agent iteratively executes beam search on KG, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of ToG: 1) compared with LLMs, ToG has better deep reasoning power; 2) ToG has the ability of knowledge traceability and knowledge correctability by leveraging LLMs reasoning and expert feedback; 3) ToG provides a flexible plug-and-play framework for different LLMs, KGs and prompting strategies without any additional training cost; 4) the performance of ToG with small LLM models could exceed large LLM such as GPT-4 in certain scenarios and this reduces the cost of LLM deployment and application. As a training-free method with lower computational cost and better generality, ToG achieves overall SOTA in 6 out of 9 datasets where most previous SOTAs rely on additional training.},
  keywords={arxiv:2307.07697}
}

@article{ott2023thoughtsourcecentral,
  title={ThoughtSource: A central hub for large language model reasoning data},
  author={Simon Ott and Konstantin Hebenstreit and Valentin Li'evin and C. Hother and M. Moradi and Maximilian Mayrhauser and Robert Praas and O. Winther and M. Samwald},
  year={2023},
  booktitle={Scientific Data},
  doi={10.1038/s41597-023-02433-3},
  url={https://www.semanticscholar.org/paper/edc9bf11c4810a77f00ccb96130ff67ee578391e},
  abstract={Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to ‘hallucinate’ facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates seven scientific/medical, three general-domain and five math word question answering datasets.},
  keywords={arxiv:2301.11596}
}

@article{gou2023toratoolintegrated,
  title={ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving},
  author={Zhibin Gou and Zhihong Shao and Yeyun Gong and Yelong Shen and Yujiu Yang and Minlie Huang and Nan Duan and Weizhu Chen},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2309.17452},
  url={https://www.semanticscholar.org/paper/b272513916b45c8517d289d7abee4a53e6832187},
  abstract={Large language models have made significant progress in various language tasks, yet they still struggle with complex mathematics. In this paper, we propose ToRA a series of Tool-integrated Reasoning Agents designed to solve challenging mathematical problems by seamlessly integrating natural language reasoning with the utilization of external tools (e.g., computation libraries and symbolic solvers), thereby amalgamating the analytical prowess of language and the computational efficiency of tools. To train ToRA, we curate interactive tool-use trajectories on mathematical datasets, apply imitation learning on the annotations, and propose output space shaping to further refine models' reasoning behavior. As a result, ToRA models significantly outperform open-source models on 10 mathematical reasoning datasets across all scales with 13\%-19\% absolute improvements on average. Notably, ToRA-7B reaches 44.6\% on the competition-level dataset MATH, surpassing the best open-source model WizardMath-70B by 22\% absolute. ToRA-Code-34B is also the first open-source model that achieves an accuracy exceeding 50\% on MATH, which significantly outperforms GPT-4's CoT result, and is competitive with GPT-4 solving problems with programs. Additionally, we conduct a comprehensive analysis of the benefits and remaining challenges of tool interaction for mathematical reasoning, providing valuable insights for future research.},
  keywords={arxiv:2309.17452}
}

@article{feng2023towardsrevealing,
  title={Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective},
  author={Guhao Feng and Yuntian Gu and Bohang Zhang and Haotian Ye and Di He and Liwei Wang},
  year={2023},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2305.15408},
  url={https://www.semanticscholar.org/paper/c2260403fd5cb2de73491323433e48b6ec36872c},
  abstract={Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the expressivity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that bounded-depth Transformers are unable to directly produce correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language format. Moreover, we show LLMs with CoT are capable of solving a general class of decision-making problems known as Dynamic Programming, thus justifying its power in tackling complex real-world tasks. Finally, extensive experiments on four tasks show that, while Transformers always fail to predict the answers directly, they can consistently learn to generate correct solutions step-by-step given sufficient CoT demonstrations.},
  keywords={arxiv:2305.15408}
}

@article{li2023turningdust,
  title={Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data},
  author={Yiwei Li and Peiwen Yuan and Shaoxiong Feng and Boyuan Pan and Bin Sun and Xinglin Wang and Heda Wang and Kan Li},
  year={2023},
  booktitle={AAAI Conference on Artificial Intelligence},
  doi={10.48550/arXiv.2312.12832},
  url={https://www.semanticscholar.org/paper/9529a6b2032bd08ea4e34954b7a7d2b8b5d5f66b},
  abstract={Large Language Models (LLMs) have performed well on various reasoning tasks, but their inaccessibility and numerous parameters hinder wide application in practice. One promising way is distilling the reasoning ability from LLMs to small models by the generated chain-of-thought reasoning paths. In some cases, however, LLMs may produce incorrect reasoning chains, especially when facing complex mathematical problems. Previous studies only transfer knowledge from positive samples and drop the synthesized data with wrong answers. In this work, we illustrate the merit of negative data and propose a model specialization framework to distill LLMs with negative samples besides positive ones. The framework consists of three progressive steps, covering from training to inference stages, to absorb knowledge from negative data. We conduct extensive experiments across arithmetic reasoning tasks to demonstrate the role of negative data in distillation from LLM.},
  keywords={arxiv:2312.12832}
}

@article{binz2023turninglarge,
  title={Turning large language models into cognitive models},
  author={Marcel Binz and Eric Schulz},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2306.03917},
  url={https://www.semanticscholar.org/paper/56caaf598c1bf36a24385f30ca775b94cf215b6b},
  abstract={Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that -- after finetuning them on data from psychological experiments -- these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become generalist cognitive models, thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole.},
  keywords={arxiv:2306.03917}
}

@article{chen2023universalselfconsistency,
  title={Universal Self-Consistency for Large Language Model Generation},
  author={Xinyun Chen and Renat Aksitov and Uri Alon and Jie Ren and Kefan Xiao and Pengcheng Yin and Sushant Prakash and Charles Sutton and Xuezhi Wang and Denny Zhou},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2311.17311},
  url={https://www.semanticscholar.org/paper/5f66d1a667eec13b5d337c3fc5619bcef95092bd},
  abstract={Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.},
  keywords={arxiv:2311.17311}
}

@article{bello2023unravelingenigma,
  title={Unraveling the Enigma: how can ChatGPT perform so well with language understanding, reasoning, and knowledge processing without having real knowledge or logic?},
  author={Fabio Di Bello},
  year={2023},
  booktitle={ABOUTOPEN},
  doi={10.33393/ao.2023.2618},
  url={https://www.semanticscholar.org/paper/bc782beec8a9e0e3e56e0623136b42a04130369a},
  abstract={Artificial Intelligence (AI) has made significant progress in various domains, but the quest for machines to truly understand natural language has been challenging. Traditional mainstream approaches to AI, while valuable, often struggled to achieve human-level language comprehension. However, the emergence of neural networks and the subsequent adoption of the downstream approach have revolutionized the field, as demonstrated by the powerful and successful language model, ChatGPT.
The deep learning algorithms utilized in large language models (LLMs) differ significantly from those employed in traditional neural networks.
This article endeavors to provide a valuable and insightful exploration of the functionality and performance of generative AI. It aims to accomplish this by offering a comprehensive, yet simplified, analysis of the underlying mathematical models used by systems such as ChatGPT. The primary objective is to explore the diverse performance capabilities of these systems across some important domains such as clinical practice. The article also sheds light on the existing gaps and limitations that impact the quality and reliability of generated answers. Furthermore, it delves into potential strategies aimed at improving the reliability and cognitive aspects of generative AI systems.}
}

@article{luo2023wizardmathempowering,
  title={WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct},
  author={Haipeng Luo and Qingfeng Sun and Can Xu and Pu Zhao and Jian-Guang Lou and Chongyang Tao and Xiubo Geng and Qingwei Lin and Shifeng Chen and Dongmei Zhang},
  year={2023},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2308.09583},
  url={https://www.semanticscholar.org/paper/dd18782960f9ee4c66b79e1518b342ad3f8d19e7},
  abstract={Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM},
  keywords={arxiv:2308.09583}
}

@article{saidi2023mathpvslarge,
  title={math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories},
  author={Hassen Saidi and Susmit Jha and T. Sahai},
  year={2023},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2310.17064},
  url={https://www.semanticscholar.org/paper/4a830a6cba4ec8c87c10348955b6bb633f401c0b},
  abstract={As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few. While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the capabilities of proof assistants, specifically PVS, with LLMs, enabling a bridge between textual descriptions in academic papers and formal specifications in PVS. By harnessing the PVS environment, coupled with data ingestion and conversion mechanisms, we envision an automated process, called \textbackslash\{\}emph\{math-PVS\}, to extract and formalize mathematical theorems from research papers, offering an innovative tool for academic review and discovery.},
  keywords={arxiv:2310.17064}
}

@article{stolfo2022causalframework,
  title={A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models},
  author={Alessandro Stolfo and Zhijing Jin and Kumar Shridhar and B. Scholkopf and Mrinmaya Sachan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.12023},
  url={https://www.semanticscholar.org/paper/9b45af10429681249fafb07c3b6012ea4ce63ffe},
  abstract={We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution.Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution.By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems.Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.},
  keywords={arxiv:2210.12023}
}

@article{lu2022surveydeep,
  title={A Survey of Deep Learning for Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Wenhao Yu and S. Welleck and Kai-Wei Chang},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10535},
  url={https://www.semanticscholar.org/paper/2dbec38fe353ab0e495ad09263389dbc9260824d},
  abstract={Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems in language has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.},
  keywords={arxiv:2212.10535}
}

@article{yu2022alertadapt,
  title={ALERT: Adapt Language Models to Reasoning Tasks},
  author={Ping Yu and Tianlu Wang and O. Yu. Golovneva and Badr AlKhamissi and Gargi Ghosh and Mona T. Diab and Asli Celikyilmaz},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.08286},
  url={https://www.semanticscholar.org/paper/95c11cc5820ba32c60d5f2671f6567b9914a4978},
  abstract={Recent advancements in large language models have enabled them to perform well on complex tasks that require step-by-step reasoning with few-shot learning. However, it is unclear whether these models are applying reasoning skills they have learnt during pre-training , or if they are simply memorizing their training corpus at finer granularity and have learnt to better understand their context.To address this question, we introduce \{pasted macro ‘OUR’\}model, a benchmark and suite of analyses for evaluating reasoning skills of language models. \{pasted macro ‘OUR’\}model enables comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. Our benchmark provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. By using \{pasted macro ‘OUR’\}model we further investigate the role of finetuning. Our extensive empirical analysis shows that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during the finetuning stage compared to pretraining stage. However, we also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.},
  keywords={arxiv:2212.08286}
}

@article{wu2022autoformalizationwith,
  title={Autoformalization with Large Language Models},
  author={Yuhuai Wu and Albert Qiaochu Jiang and Wenda Li and M. Rabe and Charles Staats and M. Jamnik and Christian Szegedy},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2205.12615},
  url={https://www.semanticscholar.org/paper/c28e95a06dfcf13fc65a1cac83722f53e34f12a5},
  abstract={Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence. While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion (\$25.3\textbackslash\{\}\%\$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from \$29.6\textbackslash\{\}\%\$ to \$35.2\textbackslash\{\}\%\$.},
  keywords={arxiv:2205.12615}
}

@article{zhang2022automaticchain,
  title={Automatic Chain of Thought Prompting in Large Language Models},
  author={Zhuosheng Zhang and Aston Zhang and Mu Li and Alexander J. Smola},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/90350aa626bed47b02d0c162462e5b0ca82be6b2},
  abstract={Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like"Let's think step by step"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the"Let's think step by step"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
  keywords={arxiv:2210.03493}
}

@article{raman2022capecorrective,
  title={CAPE: Corrective Actions from Precondition Errors using Large Language Models},
  author={S. S. Raman and Vanya Cohen and Eric Rosen and Ifrah Idrees and D. Paulius and Stefanie Tellex},
  year={2022},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA57147.2024.10611376},
  url={https://www.semanticscholar.org/paper/c82d8d80ea68400adb7faebb2f1cff38dd83093a},
  abstract={Extracting knowledge and reasoning from large language models (LLMs) offers a path to designing intelligent robots. Common approaches that leverage LLMs for planning are unable to recover when actions fail and resort to retrying failed actions without resolving the underlying cause. We propose a novel approach (CAPE) that generates corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans through few-shot reasoning on action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while maintaining semantic correctness and minimizing re-prompting. In VirtualHome, CAPE improves a human-annotated plan correctness metric from 28.89\% to 49.63\% over SayCan, whilst achieving competitive executability. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves correctness by 76.49\% with higher executability compared to SayCan. Our approach enables embodied agents to follow natural language commands and robustly recover from failures.},
  keywords={arxiv:2211.09935}
}

@article{lindstrm2022clevrmathdataset,
  title={CLEVR-Math: A Dataset for Compositional Language, Visual and Mathematical Reasoning},
  author={Adam Dahlgren Lindström and Savitha Sam Abraham},
  year={2022},
  booktitle={International Workshop on Neural-Symbolic Learning and Reasoning},
  doi={10.48550/arXiv.2208.05358},
  url={https://www.semanticscholar.org/paper/74cc3d340039c67bdabaef090d1386fe2c5376ca},
  abstract={We introduce CLEVR-Math, a multi-modal math word problems dataset consisting of simple math word problems involving addition/subtraction, represented partly by a textual description and partly by an image illustrating the scenario. The text describes actions performed on the scene that is depicted in the image. Since the question posed may not be about the scene in the image, but about the state of the scene before or after the actions are applied, the solver envision or imagine the state changes due to these actions. Solving these word problems requires a combination of language, visual and mathematical reasoning. We apply state-of-the-art neural and neuro-symbolic models for visual question answering on CLEVR-Math and empirically evaluate their performances. Our results show how neither method generalise to chains of operations. We discuss the limitations of the two in addressing the task of multi-modal word problem solving.},
  keywords={arxiv:2208.05358}
}

@article{behnamghader2022retrieveraugmentedlanguage,
  title={Can Retriever-Augmented Language Models Reason? The Blame Game Between the Retriever and the Language Model},
  author={Parishad BehnamGhader and Santiago Miret and Siva Reddy},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2212.09146},
  url={https://www.semanticscholar.org/paper/e4758d05c3d4231dd30c656330e156ccc9dbb07b},
  abstract={Augmenting pretrained language models with retrievers to select the supporting documents has shown promise in effectively solving common NLP problems, including language modeling and question answering, in an interpretable way. In this paper, we first study the strengths and weaknesses of different retriever-augmented language models (REALM, \$k\$NN-LM, FiD coupled with DPR, and ATLAS and Flan-T5 coupled with Contriever) in reasoning over the retrieved statements in different tasks. We show how the retrieve-then-read models' limitations in reasoning are rooted both in the retriever module as well as the language model. Our experimental results demonstrate that the similarity metric used by the retrievers is generally insufficient for reasoning tasks. Additionally, we show that the language models in retriever-augmented models do not take the complicated relations between the statements into account, which leads to poor reasoning performance even when using the larger models. Moreover, we analyze the reasoning performance of large language models using multihop retrieval but we only observe minor improvements. Overall, this shows great room for further research in this area.},
  keywords={arxiv:2212.09146}
}

@article{wei2022chainthought,
  title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Ed H. Chi and F. Xia and Quoc Le and Denny Zhou},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5},
  abstract={We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  keywords={arxiv:2201.11903}
}

@article{liang2022codepolicies,
  title={Code as Policies: Language Model Programs for Embodied Control},
  author={Jacky Liang and Wenlong Huang and F. Xia and Peng Xu and Karol Hausman and Brian Ichter and Peter R. Florence and Andy Zeng},
  year={2022},
  booktitle={IEEE International Conference on Robotics and Automation},
  doi={10.1109/ICRA48891.2023.10160591},
  url={https://www.semanticscholar.org/paper/91deaf9d324c8feafc189da0da03e60a60287bca},
  abstract={Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (‘faster’) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  keywords={arxiv:2209.07753}
}

@article{jonsson2022creativemathematical,
  title={Creative Mathematical Reasoning: Does Need for Cognition Matter?},
  author={B. Jonsson and Julia Mossegård and Johan Lithner and Linnea Karlsson Wirebring},
  year={2022},
  booktitle={Frontiers in Psychology},
  doi={10.3389/fpsyg.2021.797807},
  url={https://www.semanticscholar.org/paper/9f6f01cba1158e6bcb17aaa43070ef3b64c59550},
  abstract={A large portion of mathematics education centers heavily around imitative reasoning and rote learning, raising concerns about students’ lack of deeper and conceptual understanding of mathematics. To address these concerns, there has been a growing focus on students learning and teachers teaching methods that aim to enhance conceptual understanding and problem-solving skills. One suggestion is allowing students to construct their own solution methods using creative mathematical reasoning (CMR), a method that in previous studies has been contrasted against algorithmic reasoning (AR) with positive effects on test tasks. Although previous studies have evaluated the effects of CMR, they have ignored if and to what extent intrinsic cognitive motivation play a role. This study investigated the effects of intrinsic cognitive motivation to engage in cognitive strenuous mathematical tasks, operationalized through Need for Cognition (NFC), and working memory capacity (WMC). Two independent groups, consisting of upper secondary students (N = 137, mean age 17.13, SD = 0.62, 63 boys and 74 girls), practiced non-routine mathematical problem solving with CMR and AR tasks and were tested 1 week later. An initial t-test confirmed that the CMR group outperformed the AR group. Structural equation modeling revealed that NFC was a significant predictor of math performance for the CMR group but not for the AR group. The results also showed that WMC was a strong predictor of math performance independent of group. These results are discussed in terms of allowing for time and opportunities for struggle with constructing own solution methods using CMR, thereby enhancing students conceptual understanding.}
}

@article{liu2022deplotoneshot,
  title={DePlot: One-shot visual language reasoning by plot-to-table translation},
  author={Fangyu Liu and Julian Martin Eisenschlos and Francesco Piccinno and Syrine Krichene and Chenxi Pang and Kenton Lee and Mandar Joshi and Wenhu Chen and Nigel Collier and Y. Altun},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10505},
  url={https://www.semanticscholar.org/paper/4d3a49d1439a0b8fbb0e9f588970ad0f1d70dec8},
  abstract={Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than>28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0\% improvement over finetuned SOTA on human-written queries from the task of chart QA.},
  keywords={arxiv:2212.10505}
}

@article{shridhar2022distillingmultistep,
  title={Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions},
  author={Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.00193},
  url={https://www.semanticscholar.org/paper/72123a86eae2cb5c4eae8650f43524039d48875d}
}

@article{shridhar2022distillingreasoning,
  title={Distilling Reasoning Capabilities into Smaller Language Models},
  author={Kumar Shridhar and Alessandro Stolfo and Mrinmaya Sachan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.18653/v1/2023.findings-acl.441},
  url={https://www.semanticscholar.org/paper/8fd462f6248d5e3f1b6602697c09489086b5655f},
  abstract={Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70\% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM},
  keywords={arxiv:2212.00193}
}

@article{jiang2022draftsketch,
  title={Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs},
  author={Albert Qiaochu Jiang and S. Welleck and J. Zhou and Wenda Li and Jiacheng Liu and M. Jamnik and Timothée Lacroix and Yuhuai Wu and Guillaume Lample},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.12283},
  url={https://www.semanticscholar.org/paper/7de36d6b14aadc8cdb6ad1340b9ca64b15375bca},
  abstract={The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce well-structured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9\% to 39.3\% on a collection of mathematical competition problems.},
  keywords={arxiv:2210.12283}
}

@article{lu2022dynamicprompt,
  title={Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning},
  author={Pan Lu and Liang Qiu and Kai-Wei Chang and Y. Wu and Song-Chun Zhu and Tanmay Rajpurohit and Peter Clark and A. Kalyan},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2209.14610},
  url={https://www.semanticscholar.org/paper/3e565c544a8639cc9c7568833e484d7610f5e5d4},
  abstract={Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31\% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in selecting in-context examples.},
  keywords={arxiv:2209.14610}
}

@article{webb2022emergentanalogical,
  title={Emergent analogical reasoning in large language models},
  author={Taylor W. Webb and K. Holyoak and Hongjing Lu},
  year={2022},
  booktitle={Nature Human Behaviour},
  doi={10.1038/s41562-023-01659-w},
  url={https://www.semanticscholar.org/paper/3cbffab9d7981da6662d474aaa056dcbd3c1701e},
  abstract={The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of Generative Pre-trained Transformer (GPT)-3) on a range of analogical tasks, including a non-visual matrix reasoning task based on the rule structure of Raven’s Standard Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT-4 indicated even better performance. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems. Webb et al. show that new artificial intelligence language models, such as Generative Pre-trained Transformer 3, are able to solve analogical reasoning problems at a human-like level of performance.},
  keywords={arxiv:2212.09196}
}

@article{li2022explanationsfrom,
  title={Explanations from Large Language Models Make Small Reasoners Better},
  author={SHIYANG LI and Jianshu Chen and Yelong Shen and Zhiyu Chen and Xinlu Zhang and Zekun Li and Hong Wang and Jingu Qian and Baolin Peng and Yi Mao and Wenhu Chen and Xifeng Yan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.06726},
  url={https://www.semanticscholar.org/paper/7d29a84a589aa5655e5d3fed8d725ea472816599},
  abstract={Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5\% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.},
  keywords={arxiv:2210.06726}
}

@article{anil2022exploringlength,
  title={Exploring Length Generalization in Large Language Models},
  author={Cem Anil and Yuhuai Wu and Anders Andreassen and Aitor Lewkowycz and Vedant Misra and V. Ramasesh and Ambrose Slone and Guy Gur-Ari and Ethan Dyer and Behnam Neyshabur},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2207.04901},
  url={https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a},
  abstract={The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.},
  keywords={arxiv:2207.04901}
}

@article{han2022folionatural,
  title={FOLIO: Natural Language Reasoning with First-Order Logic},
  author={Simeng Han and Hailey Schoelkopf and Yilun Zhao and Zhenting Qi and Martin Riddell and Luke Benson and Lucy Sun and E. Zubova and Yujie Qiao and Matthew Burtell and David Peng and Jonathan Fan and Yixin Liu and Brian Wong and Malcolm Sailor and Ansong Ni and Linyong Nan and Jungo Kasai and Tao Yu and Rui Zhang and Shafiq R. Joty and Alexander R. Fabbri and Wojciech Kryscinski and Xi Victoria Lin and Caiming Xiong and Dragomir R. Radev},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2209.00840},
  url={https://www.semanticscholar.org/paper/5581bf85386737bd3378eec68189759a05280bea},
  abstract={Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO remains a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.},
  keywords={arxiv:2209.00840}
}

@article{creswell2022faithfulreasoning,
  title={Faithful Reasoning Using Large Language Models},
  author={Antonia Creswell and M. Shanahan},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/f0a0e8b6e84207f50db4d24cc4016e40601214ef},
  abstract={Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.},
  keywords={arxiv:2208.14271}
}

@article{taylor2022galacticalarge,
  title={Galactica: A Large Language Model for Science},
  author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and A. Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/7d645a3fd276918374fd9483fd675c28e46506d1},
  abstract={Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.},
  keywords={arxiv:2211.09085}
}

@article{gilson2022welldoes,
  title={How Well Does ChatGPT Do When Taking the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment},
  author={A. Gilson and C. Safranek and Ting Huang and V. Socrates and L. Chi and R. A. Taylor and David Chartash},
  year={2022},
  booktitle={medRxiv},
  doi={10.1101/2022.12.23.22283901},
  url={https://www.semanticscholar.org/paper/7d4867e28b02059eef4cb25bfcd304b2071b30a9},
  abstract={Background: ChatGPT is a 175 billion parameter natural language processing model which can generate conversation style responses to user input. Objective: To evaluate the performance of ChatGPT on questions within the scope of United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as analyze responses for user interpretability. Methods: We used two novel sets of multiple choice questions to evaluate ChatGPT's performance, each with questions pertaining to Step 1 and Step 2. The first was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the userbase. The second, was the National Board of Medical Examiners (NBME) Free 120-question exams. After prompting ChatGPT with each question, ChatGPT's selected answer was recorded, and the text output evaluated across three qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results: On the four datasets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free- Step2, ChatGPT achieved accuracies of 44\%, 42\%, 64.4\%, and 57.8\%. The model demonstrated a significant decrease in performance as question difficulty increased (P=.012) within the AMBOSS- Step1 dataset. We found logical justification for ChatGPT's answer selection was present in 100\% of outputs. Internal information to the question was present in >90\% of all questions. The presence of information external to the question was respectively 54.5\% and 27\% lower for incorrect relative to correct answers on the NBME-Free-Step1 and NBME-Free-Step2 datasets (P<=.001). Conclusion: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at greater than 60\% threshold on the NBME-Free- Step-1 dataset we show that the model is comparable to a third year medical student. Additionally, due to the dialogic nature of the response to questions, we demonstrate ChatGPT's ability to provide reasoning and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as a medical education tool.}
}

@article{hagendorff2022humanlikeintuitive,
  title={Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT},
  author={Thilo Hagendorff and Sarah Fabi and Michal Kosinski},
  year={2022},
  booktitle={Nature Computational Science},
  doi={10.1038/s43588-023-00527-x},
  url={https://www.semanticscholar.org/paper/0bfc05adcddd4fe5d1335d96cc313c41526d4558},
  abstract={We design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics. The reasoning capabilities of OpenAI’s generative pre-trained transformer family were tested using semantic illusions and cognitive reflection tests that are typically used in human studies. While early models were prone to human-like cognitive errors, ChatGPT decisively outperformed humans, avoiding the cognitive traps embedded in the tasks.},
  keywords={arxiv:2306.07622}
}

@article{huang2022innermonologue,
  title={Inner Monologue: Embodied Reasoning through Planning with Language Models},
  author={Wenlong Huang and F. Xia and Ted Xiao and Harris Chan and Jacky Liang and Peter R. Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and P. Sermanet and Noah Brown and Tomas Jackson and Linda Luu and S. Levine and Karol Hausman and Brian Ichter},
  year={2022},
  booktitle={Conference on Robot Learning},
  doi={10.48550/arXiv.2207.05608},
  url={https://www.semanticscholar.org/paper/f3cf71c51b882fe3111d71c4bf104297d38197f8},
  abstract={Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.},
  keywords={arxiv:2207.05608}
}

@article{saparov2022languagemodels,
  title={Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},
  author={Abulhair Saparov and He He},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.01240},
  url={https://www.semanticscholar.org/paper/e7028cd7ea838ab8294ecf26d5a2c0dbb8cfa81a},
  abstract={Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.},
  keywords={arxiv:2210.01240}
}

@article{huang2022largelanguage,
  title={Large Language Models Can Self-Improve},
  author={Jiaxin Huang and S. Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.11610},
  url={https://www.semanticscholar.org/paper/3fa70115248377c3d1517c9f978791a296fbc1dd},
  abstract={Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate"high-confidence"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4\%->82.1\% on GSM8K, 78.2\%->83.0\% on DROP, 90.0\%->94.4\% on OpenBookQA, and 63.4\%->67.9\% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.},
  keywords={arxiv:2210.11610}
}

@article{taesiri2022largelanguage,
  title={Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors},
  author={Mohammad Reza Taesiri and Finlay Macklon and Yihe Wang and Hengshuo Shen and C. Bezemer},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.02506},
  url={https://www.semanticscholar.org/paper/55e3fe05598be7c3dd357d51166869f6571b824f},
  abstract={Video game testing requires game-specific knowledge as well as common sense reasoning about the events in the game. While AI-driven agents can satisfy the first requirement, it is not yet possible to meet the second requirement automatically. Therefore, video game testing often still relies on manual testing, and human testers are required to play the game thoroughly to detect bugs. As a result, it is challenging to fully automate game testing. In this study, we explore the possibility of leveraging the zero-shot capabilities of large language models for video game bug detection. By formulating the bug detection problem as a question-answering task, we show that large language models can identify which event is buggy in a sequence of textual descriptions of events from a game. To this end, we introduce the GameBugDescriptions benchmark dataset, which consists of 167 buggy gameplay videos and a total of 334 question-answer pairs across 8 games. We extensively evaluate the performance of six models across the OPT and InstructGPT large language model families on our benchmark dataset. Our results show promising results for employing language models to detect video game bugs. With the proper prompting technique, we could achieve an accuracy of 70.66\%, and on some video games, up to 78.94\%. Our code, evaluation data and the benchmark can be found on https://asgaardlab.github.io/LLMxBugs},
  keywords={arxiv:2210.02506}
}

@article{singhal2022largelanguage,
  title={Large language models encode clinical knowledge},
  author={K. Singhal and Shekoofeh Azizi and T. Tu and S. Mahdavi and Jason Wei and Hyung Won Chung and Nathan Scales and A. Tanwani and H. Cole-Lewis and S. Pfohl and P. Payne and Martin G. Seneviratne and P. Gamble and C. Kelly and Nathaneal Scharli and A. Chowdhery and P. A. Mansfield and B. A. Y. Arcas and D. Webster and Greg S. Corrado and Yossi Matias and K. Chou and Juraj Gottweis and Nenad Tomašev and Yun Liu and A. Rajkomar and J. Barral and Christopher Semturs and A. Karthikesalingam and Vivek Natarajan},
  year={2022},
  booktitle={Nature},
  doi={10.1038/s41586-023-06291-2},
  url={https://www.semanticscholar.org/paper/6052486bc9144dc1730c12bf35323af3792a1fd0},
  abstract={Med-PaLM, a state-of-the-art large language model for medicine, is introduced and evaluated across several medical question answering tasks, demonstrating the promise of these models in this domain. Large language models (LLMs) have demonstrated impressive capabilities, but the bar for clinical applications is high. Attempts to assess the clinical knowledge of models typically rely on automated evaluations based on limited benchmarks. Here, to address these limitations, we present MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and a new dataset of medical questions searched online, HealthSearchQA. We propose a human evaluation framework for model answers along multiple axes including factuality, comprehension, reasoning, possible harm and bias. In addition, we evaluate Pathways Language Model\^{} 1 (PaLM, a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM\^{} 2 on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA\^{} 3 , MedMCQA\^{} 4 , PubMedQA\^{} 5 and Measuring Massive Multitask Language Understanding (MMLU) clinical topics\^{} 6 ), including 67.6\% accuracy on MedQA (US Medical Licensing Exam-style questions), surpassing the prior state of the art by more than 17\%. However, human evaluation reveals key gaps. To resolve this, we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, knowledge recall and reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal limitations of today’s models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLMs for clinical applications.},
  keywords={arxiv:2212.13138}
}

@article{nam2022learningreason,
  title={Learning to Reason With Relational Abstractions},
  author={A. Nam and Mengye Ren and Chelsea Finn and James L. McClelland},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.02615},
  url={https://www.semanticscholar.org/paper/6d5555348f453bac901c5b57e8a4eeb3074b4071},
  abstract={Large language models have recently shown promising progress in mathematical reasoning when fine-tuned with human-generated sequences walking through a sequence of solution steps. However, the solution sequences are not formally structured and the resulting model-generated sequences may not reflect the kind of systematic reasoning we might expect an expert human to produce. In this paper, we study how to build stronger reasoning capability in language models using the idea of relational abstractions. We introduce new types of sequences that more explicitly provide an abstract characterization of the transitions through intermediate solution steps to the goal state. We find that models that are supplied with such sequences as prompts can solve tasks with a significantly higher accuracy, and models that are trained to produce such sequences solve problems better than those that are trained with previously used human-generated sequences and other baselines. Our work thus takes several steps toward elucidating and improving how language models perform on tasks requiring multi-step mathematical reasoning.},
  keywords={arxiv:2210.02615}
}

@article{zhou2022leasttomostprompting,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Denny Zhou and Nathanael Scharli and Le Hou and Jason Wei and Nathan Scales and Xuezhi Wang and D. Schuurmans and O. Bousquet and Quoc Le and Ed H. Chi},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2205.10625},
  url={https://www.semanticscholar.org/paper/5437e8adab596d7294124c0e798708e050e25321},
  abstract={Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99\% using just 14 exemplars, compared to only 16\% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.},
  keywords={arxiv:2205.10625}
}

@article{yu2022legalprompting,
  title={Legal Prompting: Teaching a Language Model to Think Like a Lawyer},
  author={Fang Yu and Lee Quartey and Frank Schilder},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.01326},
  url={https://www.semanticscholar.org/paper/cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3},
  abstract={Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought (CoT) prompts can improve arithmetic or common sense tasks significantly. We explore how such approaches fare with legal reasoning tasks and take the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning approaches. Our findings show that while CoT prompting and fine-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from specific legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.},
  keywords={arxiv:2212.01326}
}

@misc{mishra2022lilaunified,
  title={Lila: A Unified Benchmark for Mathematical Reasoning},
  author={Swaroop Mishra and Pan Lu and A. Kalyan},
  year={2022},
  url={https://www.semanticscholar.org/paper/a630c70aed27b52f6d04d1e772b153c5a7b6f6fe},
  abstract={Mathematical reasoning skills are essential for general-purpose intelligent systems to perform tasks from grocery shopping to climate modeling. Towards evaluating and improving AI systems in this domain, we propose LILA, a unified mathematical reasoning benchmark consisting of 23 diverse tasks along four dimensions: (i) mathematical abilities e.g., arithmetic, calculus (ii) language format e.g., question-answering, fill-in-the-blanks (iii) language diversity e.g., no language, simple language (iv) external knowledge e.g., commonsense, physics. We construct our benchmark by extending 20 datasets benchmark by collecting task instructions and solutions in the form of Python programs, thereby obtaining explainable solutions in addition to the correct answer. We additionally introduce two evaluation datasets to measure out-of-distribution performance and robustness to language perturbation. Finally, we introduce BHASKARA, a general-purpose mathematical reasoning model trained on LILA. Importantly, we find that multi-tasking leads to significant improvements (average relative improvement of 21.83\% F1 score vs. single-task models), while the best performing model only obtains 60.40\%, indicating the room for improvement in general mathematical reasoning and understanding.},
  keywords={arxiv:2210.17517}
}

@article{razumovskaia2022littleriding,
  title={Little Red Riding Hood Goes around the Globe: Crosslingual Story Planning and Generation with Large Language Models},
  author={E. Razumovskaia and Joshua Maynez and Annie Louis and Mirella Lapata and Shashi Narayan},
  year={2022},
  booktitle={International Conference on Language Resources and Evaluation},
  doi={10.48550/arXiv.2212.10471},
  url={https://www.semanticscholar.org/paper/30cc7ae95583ade1f05226c08c6f6609777aeedd},
  abstract={Previous work has demonstrated the effectiveness of planning for story generation exclusively in a monolingual setting focusing primarily on English. We consider whether planning brings advantages to automatic story generation across languages. We propose a new task of crosslingual story generation with planning and present a new dataset for this task. We conduct a comprehensive study of different plans and generate stories in several languages, by leveraging the creative and reasoning capabilities of large pretrained language models. Our results demonstrate that plans which structure stories into three acts lead to more coherent and interesting narratives, while allowing to explicitly control their content and structure.},
  keywords={arxiv:2212.10471}
}

@article{karpas2022mrklsystems,
  title={MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning},
  author={Ehud Karpas and Omri Abend and Yonatan Belinkov and Barak Lenz and Opher Lieber and Nir Ratner and Y. Shoham and Hofit Bata and Yoav Levine and Kevin Leyton-Brown and Dor Muhlgay and N. Rozen and Erez Schwartz and Gal Shachaf and Shai Shalev-Shwartz and A. Shashua and Moshe Tenenholtz},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2205.00445},
  url={https://www.semanticscholar.org/paper/1bcde55995a957b3e8a595d536b816cb8989cf1d},
  abstract={Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, pronounced"miracle") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs' MRKL system implementation.},
  keywords={arxiv:2205.00445}
}

@article{liu2022mindsgrounded,
  title={Mind's Eye: Grounded Language Model Reasoning through Simulation},
  author={Ruibo Liu and Jason Wei and S. Gu and Te-Yen Wu and Soroush Vosoughi and Claire Cui and Denny Zhou and Andrew M. Dai},
  year={2022},
  booktitle={International Conference on Learning Representations},
  doi={10.48550/arXiv.2210.05359},
  url={https://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8},
  abstract={Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9\% zero-shot, and 46.0\% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.},
  keywords={arxiv:2210.05359}
}

@article{welleck2022naturalprovergrounded,
  title={NaturalProver: Grounded Mathematical Proof Generation with Language Models},
  author={S. Welleck and Jiacheng Liu and Ximing Lu and Hannaneh Hajishirzi and Yejin Choi},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2205.12910},
  url={https://www.semanticscholar.org/paper/d593b9b8d63426f0d6a795dd7f2294619bc03610},
  abstract={Theorem proving in natural mathematical language - the mixture of symbolic and natural language used by humans - plays a central role in mathematical advances and education, and tests aspects of reasoning that are core to intelligence. Yet it has remained underexplored with modern generative models. We study large-scale language models on two new generation tasks: suggesting the next step in a mathematical proof, and full proof generation. We develop NaturalProver, a language model that generates proofs by conditioning on background references (e.g. theorems and definitions that are either retrieved or human-provided), and optionally enforces their presence with constrained decoding. On theorems from the NaturalProofs benchmark, NaturalProver improves the quality of next-step suggestions and generated proofs over fine-tuned GPT-3, according to human evaluations from university-level mathematics students. NaturalProver is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40\% of the time, which is to our knowledge the first demonstration of these capabilities using neural language models.},
  keywords={arxiv:2205.12910}
}

@article{mishra2022numgluesuite,
  title={NumGLUE: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks},
  author={Swaroop Mishra and Arindam Mitra and Neeraj Varshney and Bhavdeep Singh Sachdeva and Peter Clark and Chitta Baral and A. Kalyan},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2204.05660},
  url={https://www.semanticscholar.org/paper/39238a92de090c104936a4f78375b95600e42ce5},
  abstract={Given the ubiquitous nature of numbers in text, reasoning with numbers to perform simple calculations is an important skill of AI systems. While many datasets and models have been developed to this end, state-of-the-art AI systems are brittle; failing to perform the underlying mathematical reasoning when they appear in a slightly different scenario. Drawing inspiration from GLUE that was proposed in the context of natural language understanding, we propose NumGLUE, a multi-task benchmark that evaluates the performance of AI systems on eight different tasks, that at their core require simple arithmetic understanding. We show that this benchmark is far from being solved with neural models including state-of-the-art large-scale language models performing significantly worse than humans (lower by 46.4 \%). Further, NumGLUE promotes sharing knowledge across tasks, especially those with limited training data as evidenced by the superior performance (average gain of 3.4 \% on each task) when a model is jointly trained on all the tasks as opposed to task-specific modeling. Finally, we hope that NumGLUE will encourage systems that perform robust and general arithmetic reasoning within language, a first step towards being able to perform more complex mathematical reasoning.},
  keywords={arxiv:2204.05660}
}

@article{iyer2022optimlscaling,
  title={OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
  author={S. Iyer and Xi Victoria Lin and Ramakanth Pasunuru and Todor Mihaylov and Daniel Simig and Ping Yu and Kurt Shuster and Tianlu Wang and Qing Liu and Punit Singh Koura and Xian Li and Brian O'Horo and Gabriel Pereyra and Jeff Wang and Christopher Dewan and Asli Celikyilmaz and Luke S. Zettlemoyer and Veselin Stoyanov},
  year={2022},
  booktitle={arXiv.org},
  url={https://www.semanticscholar.org/paper/e965e93e76a9e6c4e4863d145b5c007b540d575d},
  abstract={Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.},
  keywords={arxiv:2212.12017}
}

@article{thorburn2022optimizinglanguage,
  title={Optimizing Language Models for Argumentative Reasoning},
  author={Luke Thorburn and Ariel Kruger},
  year={2022},
  booktitle={ArgML@COMMA},
  url={https://www.semanticscholar.org/paper/ae3a6bbe22ea136280e2927807775b3ac8356440}
}

@article{sharma2022overcomingbarriers,
  title={Overcoming Barriers to Skill Injection in Language Modeling: Case Study in Arithmetic},
  author={Mandar Sharma and N. Muralidhar and Naren Ramakrishnan},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.02098},
  url={https://www.semanticscholar.org/paper/1ec4ae64ded99846bcd8a1a3b16fe2875f55c7c1},
  abstract={Through their transfer learning abilities, highly-parameterized large pre-trained language models have dominated the NLP landscape for a multitude of downstream language tasks. Though linguistically proficient, the inability of these models to incorporate the learning of non-linguistic entities (numerals and arithmetic reasoning) limits their usage for tasks that require numeric comprehension or strict mathematical reasoning. However, as we illustrate in this paper, building a general purpose language model that also happens to be proficient in mathematical reasoning is not as straight-forward as training it on a numeric dataset. In this work, we develop a novel framework that enables language models to be mathematically proficient while retaining their linguistic prowess. Specifically, we offer information-theoretic interventions to overcome the catastrophic forgetting of linguistic skills that occurs while injecting non-linguistic skills into language models.},
  keywords={arxiv:2211.02098}
}

@article{zelikman2022parselunified,
  title={Parsel: A Unified Natural Language Framework for Algorithmic Reasoning},
  author={E. Zelikman and Qian Huang and Gabriel Poesia and Noah D. Goodman and Nick Haber},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2212.10561},
  url={https://www.semanticscholar.org/paper/239b5649b12f28fd610de036afba41b9246db6c9}
}

@article{zelikman2022parselalgorithmic,
  title={Parsel🦆: Algorithmic Reasoning with Language Models by Composing Decompositions},
  author={E. Zelikman and Qian Huang and Gabriel Poesia and Noah D. Goodman and Nick Haber},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/e325fe41c8c1d547ccd102ac82be3ec8b23960f2},
  abstract={Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\textbackslash\{\}\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\textbackslash\{\}\% to 85\textbackslash\{\}\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel},
  keywords={arxiv:2212.10561}
}

@article{valmeekam2022planbenchextensible,
  title={PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change},
  author={Karthik Valmeekam and Alberto Olmo and S. Sreedharan and Subbarao Kambhampati},
  year={2022},
  booktitle={Neural Information Processing Systems},
  url={https://www.semanticscholar.org/paper/e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc},
  abstract={Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.},
  keywords={arxiv:2206.10498}
}

@article{yao2022reactsynergizing,
  title={ReAct: Synergizing Reasoning and Acting in Language Models},
  author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/99832586d55f540f603637e458a292406a0ed75d},
  abstract={While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io},
  keywords={arxiv:2210.03629}
}

@misc{jha2022responsiblereasoning,
  title={Responsible Reasoning with Large Language Models and the Impact of Proper Nouns},
  author={Sumit Kumar Jha},
  year={2022},
  url={https://www.semanticscholar.org/paper/a70fcd82d8d26bf4c8b0bdde88e96b6ce2c80ecf}
}

@article{he2022rethinkingwith,
  title={Rethinking with Retrieval: Faithful Large Language Model Inference},
  author={Hangfeng He and Hongming Zhang and D. Roth},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2301.00303},
  url={https://www.semanticscholar.org/paper/490d8006851b1562cfd9ec1f057471f2868289d1},
  abstract={Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.},
  keywords={arxiv:2301.00303}
}

@article{creswell2022selectioninferenceexploiting,
  title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning},
  author={Antonia Creswell and M. Shanahan and I. Higgins},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/d48b29889241551e1ee6622fa78c3fa4159255dd},
  abstract={Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100\% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.},
  keywords={arxiv:2205.09712}
}

@article{wang2022selfconsistencyimproves,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Xuezhi Wang and Jason Wei and D. Schuurmans and Quoc Le and Ed H. Chi and Denny Zhou},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/5f19ae1135a9500940978104ec15a5b8751bc7d2},
  abstract={Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9\%), SVAMP (+11.0\%), AQuA (+12.2\%), StrategyQA (+6.4\%) and ARC-challenge (+3.9\%).},
  keywords={arxiv:2203.11171}
}

@article{xiao2022smoothquantaccurate,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Guangxuan Xiao and Ji Lin and Mickael Seznec and Julien Demouth and Song Han},
  year={2022},
  booktitle={International Conference on Machine Learning},
  doi={10.48550/arXiv.2211.10438},
  url={https://www.semanticscholar.org/paper/2c994fadbb84fb960d8306ee138dbeef41a5b323},
  abstract={Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.},
  keywords={arxiv:2211.10438}
}

@article{zeng2022socraticmodels,
  title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
  author={Andy Zeng and Adrian S. Wong and Stefan Welker and K. Choromanski and F. Tombari and Aveek Purohit and M. Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Peter R. Florence},
  year={2022},
  booktitle={International Conference on Learning Representations},
  url={https://www.semanticscholar.org/paper/ada81a4de88a6ce474df2e2446ad11fea480616e},
  abstract={Large pretrained (e.g.,"foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.},
  keywords={arxiv:2204.00598}
}

@article{zhu2022solvingmath,
  title={Solving Math Word Problem via Cooperative Reasoning induced Language Models},
  author={Xinyu Zhu and Junjie Wang and Lin Zhang and Yuxiang Zhang and Ruyi Gan and Jiaxing Zhang and Yujiu Yang},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2210.16257},
  url={https://www.semanticscholar.org/paper/01f7bb1f9c611b5e849558e445fdccb98a3a3040}
}

@article{lewkowycz2022solvingquantitative,
  title={Solving Quantitative Reasoning Problems with Language Models},
  author={Aitor Lewkowycz and Anders Andreassen and David Dohan and Ethan Dyer and H. Michalewski and V. Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},
  year={2022},
  booktitle={Neural Information Processing Systems},
  doi={10.48550/arXiv.2206.14858},
  url={https://www.semanticscholar.org/paper/ab0e3d3e4d42369de5933a3b4c237780b41c0d77},
  abstract={Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.},
  keywords={arxiv:2206.14858}
}

@article{collins2022structuredflexible,
  title={Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks},
  author={K. M. Collins and Catherine Wong and Jiahai Feng and Megan Wei and J. Tenenbaum},
  year={2022},
  booktitle={Annual Meeting of the Cognitive Science Society},
  doi={10.48550/arXiv.2205.05718},
  url={https://www.semanticscholar.org/paper/7ef9aafc68511afab5b287e62b754576ea37b4ce},
  abstract={Human language offers a powerful window into our thoughts -- we tell stories, give explanations, and express our beliefs and goals through words. Abundant evidence also suggests that language plays a developmental role in structuring our learning. Here, we ask: how much of human-like thinking can be captured by learning statistical patterns in language alone? We first contribute a new challenge benchmark for comparing humans and distributional large language models (LLMs). Our benchmark contains two problem-solving domains (planning and explanation generation) and is designed to require generalization to new, out-of-distribution problems expressed in language. We find that humans are far more robust than LLMs on this benchmark. Next, we propose a hybrid Parse-and-Solve model, which augments distributional LLMs with a structured symbolic reasoning module. We find that this model shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.},
  keywords={arxiv:2205.05718}
}

@article{bertolini2022testinglarge,
  title={Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment},
  author={Lorenzo Bertolini and Julie Weeds and David Weir},
  year={2022},
  booktitle={International Conference on Computational Linguistics},
  url={https://www.semanticscholar.org/paper/1606793daaa20d4a4a78e859c2fd6b4f7535680c}
}

@article{valentino2022textgraphs2022,
  title={TextGraphs 2022 Shared Task on Natural Language Premise Selection},
  author={Marco Valentino and Deborah Ferreira and Mokanarangan Thayaparan and André Freitas and Dmitry Ustalov},
  year={2022},
  booktitle={Workshop on Graph-based Methods for Natural Language Processing},
  url={https://www.semanticscholar.org/paper/43845c54af0246baab17d572953e671487e1dcd1}
}

@article{ozturkler2022thinksumprobabilistic,
  title={ThinkSum: Probabilistic reasoning over sets using large language models},
  author={Batu Mehmet Ozturkler and Nikolay Malkin and Zhen Wang and N. Jojic},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2210.01293},
  url={https://www.semanticscholar.org/paper/370cea8b4220917f45a69358c0303df71f5063c7},
  abstract={Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think – retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum – probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.},
  keywords={arxiv:2210.01293}
}

@article{huang2022towardsreasoning,
  title={Towards Reasoning in Large Language Models: A Survey},
  author={Jie Huang and K. Chang},
  year={2022},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  doi={10.48550/arXiv.2212.10403},
  url={https://www.semanticscholar.org/paper/db4ab91d5675c37795e719e997a2827d3d83cd45},
  abstract={Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
  keywords={arxiv:2212.10403}
}

@article{agrawal2022towardsmathematics,
  title={Towards a Mathematics Formalisation Assistant using Large Language Models},
  author={Ayush Agrawal and Siddhartha Gadgil and Navin Goyal and Ashvni Narayanan and Anand Tadipatri},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2211.07524},
  url={https://www.semanticscholar.org/paper/b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce},
  abstract={Mathematics formalisation is the task of writing mathematics (i.e., definitions, theorem statements, proofs) in natural language, as found in books and papers, into a formal language that can then be checked for correctness by a program. It is a thriving activity today, however formalisation remains cumbersome. In this paper, we explore the abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover. We find that with careful inputdependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75\% accuracy for 120 theorem statements. For proofs quantitative analysis is infeasible and we undertake a detailed case study. We choose a diverse set of 13 theorems at undergrad level with proofs that fit in two-three paragraphs. We show that with a new prompting strategy Codex can formalise these proofs in natural language with at least one out of twelve Codex completion being easy to repair into a complete proof. This is surprising as essentially no aligned data exists for formalised mathematics, particularly for proofs. These results suggest that large language models are a promising avenue towards fully or partially automating formalisation.},
  keywords={arxiv:2211.07524}
}

@article{chen2022unigeounifying,
  title={UniGeo: Unifying Geometry Logical Reasoning via Reformulating Mathematical Expression},
  author={Jiaqi Chen and Tong Li and Jinghui Qin and Pan Lu and Liang Lin and Chongyu Chen and Xiaodan Liang},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2212.02746},
  url={https://www.semanticscholar.org/paper/72fce949725b20428e5f56247fef5c6bd1ce6154},
  abstract={Geometry problem solving is a well-recognized testbed for evaluating the high-level multi-modal reasoning capability of deep models. In most existing works, two main geometry problems: calculation and proving, are usually treated as two specific tasks, hindering a deep model to unify its reasoning capability on multiple math tasks. However, in essence, these two tasks have similar problem representations and overlapped math knowledge which can improve the understanding and reasoning ability of a deep model on both two tasks. Therefore, we construct a large-scale Unified Geometry problem benchmark, UniGeo, which contains 4,998 calculation problems and 9,543 proving problems. Each proving problem is annotated with a multi-step proof with reasons and mathematical expressions. The proof can be easily reformulated as a proving sequence that shares the same formats with the annotated program sequence for calculation problems. Naturally, we also present a unified multi-task Geometric Transformer framework, Geoformer, to tackle calculation and proving problems simultaneously in the form of sequence generation, which finally shows the reasoning ability can be improved on both two tasks by unifying formulation. Furthermore, we propose a Mathematical Expression Pretraining (MEP) method that aims to predict the mathematical expressions in the problem solution, thus improving the Geoformer model. Experiments on the UniGeo demonstrate that our proposed Geoformer obtains state-of-the-art performance by outperforming task-specific model NGS with over 5.6\% and 3.2\% accuracies on calculation and proving problems, respectively.},
  keywords={arxiv:2212.02746}
}

@article{sahu2022unpackinglarge,
  title={Unpacking Large Language Models with Conceptual Consistency},
  author={Pritish Sahu and Michael Cogswell and Yunye Gong and Ajay Divakaran},
  year={2022},
  booktitle={arXiv.org},
  doi={10.48550/arXiv.2209.15093},
  url={https://www.semanticscholar.org/paper/4f4a80148cb8f328aeaee68b34f9797cfb5ea150},
  abstract={If a Large Language Model (LLM) answers"yes"to the question"Are mountains tall?"then does it know what a mountain is? Can you rely on it responding correctly or incorrectly to other questions about mountains? The success of Large Language Models (LLMs) indicates they are increasingly able to answer queries like these accurately, but that ability does not necessarily imply a general understanding of concepts relevant to the anchor query. We propose conceptual consistency to measure a LLM's understanding of relevant concepts. This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are. To compute it we extract background knowledge by traversing paths between concepts in a knowledge base and then try to predict the model's response to the anchor query from the background knowledge. We investigate the performance of current LLMs in a commonsense reasoning setting using the CSQA dataset and the ConceptNet knowledge base. While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency. Our analysis also shows significant variation in conceptual consistency across different kinds of relations, concepts, and prompts. This serves as a step toward building models that humans can apply a theory of mind to, and thus interact with intuitively.},
  keywords={arxiv:2209.15093}
}

@article{madasu2022whatlarge,
  title={What do Large Language Models Learn beyond Language?},
  author={Avinash Madasu and Shashank Srivastava},
  year={2022},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  doi={10.48550/arXiv.2210.12302},
  url={https://www.semanticscholar.org/paper/5efab88c0cdb11c795fa8f44a5d31b40e2a1c261},
  abstract={Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful `inductive biases' for non-linguistic reasoning. On a set of 19 diverse non-linguistic tasks involving quantitative computations, recognizing regular expressions and reasoning over strings. We find that pretrained models significantly outperform comparable non-pretrained neural models. This remains true also in experiments with training non-pretrained models with fewer parameters to account for model regularization effects. We further explore the effect of text domain on LMs by pretraining models from text from different domains and provenances. Our experiments surprisingly reveal that the positive effects of pre-training persist even when pretraining on multi-lingual text or computer code, and even for text generated from synthetic languages. Our findings suggest a hitherto unexplored deep connection between pre-training and inductive learning abilities of language models.},
  keywords={arxiv:2210.12302}
}